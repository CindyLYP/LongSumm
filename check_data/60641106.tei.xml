<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Instance Segmentation and Counting with Recurrent Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-05-30">30 May 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
							<email>mren@cs.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
							<email>zemel@cs.toronto.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Canadian Institute for Advanced Research</orgName>
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Instance Segmentation and Counting with Recurrent Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-05-30">30 May 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1605.09410v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>While convolutional neural networks have gained impressive success recently in solving structured prediction problems such as semantic segmentation, it remains a challenge to differentiate individual object instances in the scene. Instance segmentation is very important in a variety of applications, such as autonomous driving, image captioning, and visual question answering. Techniques that combine large graphical models with low-level vision have been proposed to address this problem; however, we propose an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations. The network is jointly trained to sequentially produce regions of interest as well as a dominant object segmentation within each region. The proposed model achieves state-of-the-art results on the CVPPP leaf segmentation dataset [12] and KITTI vehicle segmentation dataset [5].</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Instance segmentation is a fundamental computer vision problem, which aims to assign pixel-level instance labelling to a given image. While the standard semantic segmentation problem entails assigning class labels to each pixel in an image, it says nothing about the number of instances of each class in the image. Unlike semantic segmentation, instance segmentation is particularly difficult in terms of distinguising nearby and occluded objects. Segmenting at the instance level is useful for many tasks, such as highlighting the outline of objects for improved recognition and allowing robots to delineate and grasp individual objects. Obtaining instance level pixel labels is also significant with respect to general machine understanding of images.</p><p>Counting the objects in an image is also of practical value, and is another problem of interest of this work. Traditionally, counting is performed in a task-specific setting, either by detection followed by regression, or by learning discriminatively with a counting distance metric <ref type="bibr" target="#b7">[8]</ref>. Studies in applications such as image question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> also reveal that counting, especially on everyday objects, is a very challenging task on its own <ref type="bibr" target="#b2">[3]</ref>.</p><p>One of the main challenges of instance segmentation is object occlusion. Classical object detection pipelines <ref type="bibr" target="#b16">[17]</ref> is composed of four stages: proposals, scoring, refinement, and non-maximal suppression (NMS). NMS typically utilizes a hard threshold that is fixed for the entire dataset. In cluttered scenes, NMS may suppress the detection results for a heavily occluded object because it has too much overlap with foreground objects. This challenge remains in the problem of instance segmentation, which is a more difficult version of object detection. One motivation of this work is to introduce a way of performing dynamic NMS to reason about occlusion.</p><p>In addition to object occlusion, another challenge is the dimensionality of the structured output, which is bounded by the number of pixels times the maximum number of objects. Standard fully <ref type="figure">Figure 1</ref>: An illustration of outputs of different components of our end-to-end system, over nine time-steps: Row 1: soft attention at the current glimpse; 2: predicted box; 3: current step segmentation; 4: all segmentations. See https://youtu.be/BMVDhTjEfBU for a video that shows our our instance segmentation algorithm in action.</p><p>convolutional networks (FCN) <ref type="bibr" target="#b10">[11]</ref> will have trouble directly outputting all instance labels in a single shot. Recent work on instance segmentation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref> formulates complex graphical models, which results in a complex and time-consuming pipeline. Furthermore, these models cannot be trained in an end-to-end fashion.</p><p>To tackle both these challenges, we propose a new model based on a recurrent neural network (RNN) that utilizes visual attention, to perform instance segmentation. Our system addresses the dimensionality issue by using a temporal chain that outputs a single instance at a time. It also performs dynamic NMS, using an object that is already segmented to aid in the discovery of an occluded object later in the sequence. Using an RNN to segment one instance at a time is also inspired by human-like iterative and attentive counting processes. For real-world cluttered scenes, iterative counting with attention will likely perform better than a regression model that operates on the global image level.</p><p>In this work, we focus on instance segmentation of a single object-type per image. We evaluate our model on a number of challenging datasets: 1) CVPPP leaf segmentation dataset <ref type="bibr" target="#b11">[12]</ref>; 2) KITTI car segmentation dataset <ref type="bibr" target="#b4">[5]</ref>; and 3) on MS-COCO <ref type="bibr" target="#b9">[10]</ref> images, where we train two different models, for "person" and "zebra" categories, and test the model with images that contain at least one instance of the chosen category. We show state-of-the-art performance on both CVPPP and KITTI dataset, and impressive counting ability on MS-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recurrent attention model</head><p>Our proposed model has four major components: A) a 2D external memory that tracks the state of the segmented objects; B) a box proposal network responsible for localizing objects of interest; C) a segmentation network for segmenting image pixels within the box; and D) a scoring network that determines if an object instance has been found, and also decides when to stop. See <ref type="figure" target="#fig_0">Figure 2</ref> for an illustration of these components.</p><p>Notation. We use the following notation to describe the model architecture: x ∈ R H×W is the input image; t indexes the iterations of the model, and τ indexes the glimpses of the inner RNN; y t , y * t ∈ (0, 1) H×W is the segmentation output/ground-truth sequence; s t , s * t ∈ (0, 1) is the confidence score output/ground-truth sequence; W(z) = w T z + b is a learned affine transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Part A: Model input and external 2D memory</head><p>We explore three variants of our model which differ in the first component. In one formulation the input is the raw image, and in the others the image is fed into a pretrained fully convolutional network (FCN). This Pretrained FCN has two channels of output. The first is a pixel-level foreground segmentation, produced by a variant of the DeconvNet <ref type="bibr" target="#b12">[13]</ref> with skip connections. In addition to predicting this foreground mask, as a second channel we followed the work of Uhrig et al. <ref type="bibr" target="#b22">[23]</ref> by producing a 2-d object angle map. For each foreground pixel, we calculate its relative angle towards  the centroid of the object, and quantize the angle into 8 different classes, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. The angle map forces the model to learn object boundary information, which is missing in the foreground segmentation. The architecture and training of these components are detailed in the Appendix. We denote D 0 (x) as the pretrained FCN applied to the original image.</p><p>To facilitate learning to sequentially enumerate the objects, we incorporate an external 2D memory in the RNN structure. We treat this 2D memory as the third channel of the model input. We explore two alternative formulations of this memory: 1) a cumulative canvas that stores the full history of segmentation outputs, and 2) a convolutional-RNN with extra parameters to dynamically adapt memory storage. Both operate at the full input resolution to precisely deal with occlusion.</p><p>1. Cumulative canvas. We hypothesize that providing information of the completed segmentation helps the network reason about occluded objects and determine the next region of interest. The first channel of the canvas keeps adding new pixels from the output of the previous time step.</p><formula xml:id="formula_0">d Canvas t = [c t , D 0 (x)] , c Canvas 0 = 0, c Canvas t = max(c t−1 , y t−1 ) ∀t &gt; 0<label>(1)</label></formula><p>2. Convolutional LSTM. One issue of the cumulative canvas is that the recurrent connection from the output of the previous time step into the canvas sometimes leads to training instability. In practice, we observe that reducing the gradient flowing back from the input of the canvas aids training. An alternative is to learn the "addition" operation with another RNN. Convolutional LSTM <ref type="bibr" target="#b19">[20]</ref> is a form of RNN that uses convolution as its recurrent operator and thus is able to efficiently process a 2D image input and store a 2D hidden state. We initialize the hidden state of the ConvLSTM with the FCN output, and feed the output segmentation back into the ConvLSTM (See <ref type="figure" target="#fig_0">Figure 2</ref>, right). This allows the gradient to flow through the ConvLSTM without introducing instability.</p><formula xml:id="formula_1">d ConvLSTM 0 = D 0 (x), d ConvLSTM t = ConvLSTM(d t−1 , y t−1 ) ∀t &gt; 0 (2) 2.2 Part B: Box network</formula><p>The box network localizes objects of interest. The CNN in the box network outputs a H × W × L feature map u box,t . We employ a "soft-attention" mechanism here to extract useful information along spatial dimensions and feed a dimension L vector into the glimpse LSTM. Since one single glimpse may not give the upper network enough information to decide where exactly to draw the box, we allow the glimpse LSTM to look at different locations. α is initialized to be uniform over all locations, and τ indexes the glimpses.</p><formula xml:id="formula_2">u box,t = CNN(d t ), z t,τ = LSTM( h,w α h,w t,τ u h,w,l box,t , z t,τ −1 ), α t,τ +1 = MLP(z t,τ )<label>(3)</label></formula><p>We pass the LSTM's hidden state through a linear layer to obtain predicted box coordinates. We parameterize the box by its normalized center g X,Y , and log size log δ X,Y . A scaling factor γ is also predicted by the linear layer, and used when re-projecting the patch to the original image size.</p><formula xml:id="formula_3">(g X,Y , logδ X,Y , log σ X,Y , γ) = W(z t,end ) (4) g X , g Y = (g X + 1) W 2 , (g Y + 1) H 2 , δ X , δ Y =δ X W,δ Y H (5)</formula><p>Extracting a sub-region. We follow DRAW <ref type="bibr" target="#b6">[7]</ref> and use a Gaussian interpolation kernel to extract an N × N patch from thex, a concatenation of the original image with d t . We further allow the model to output rectangular patches to account for different shapes of the object.</p><formula xml:id="formula_4">µ i X , µ j Y = g X + (δ X + 1) • (i − N/2 + 0.5)/N, g Y + (δ Y + 1) • (j − N/2 + 0.5)/N (6) F X [a, i], F Y [b, j] = 1 √ 2πσ X exp − (a − µ i X ) 2 2σ 2 X , 1 √ 2πσ Y exp − (b − µ j Y ) 2 2σ 2 Y (7) p t = Extract(x t , F Y , F X ) ≡ F T Yxt F X<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Part C: Segmentation network</head><p>The remaining task is to segment out the pixels that belong to the dominant object within the window. In the segmentation network, we adopt a variant of the DeconvNet <ref type="bibr" target="#b12">[13]</ref> with skip connections, which appends deconvolution (or convolution transpose) layers after convolution layers to upsample the low-resolution feature map to a full-size segmentation. After the fully convolutional layers, we get a patch-level segmentation prediction heat mapỹ t . We then re-project this patch prediction to the original image using the transpose of the previous computed Gaussian filters. The learned γ to magnifies the signal within the bounding box, and a constant β to suppresses the pixels outside the box. Lastly, the sigmoid function produces final segmentation values between 0 and 1.</p><formula xml:id="formula_5">y t = sigmoid γ • Extract(ỹ t , F T Y , F T X ) − β<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Part D: Scoring network</head><p>To estimate the number of objects in the image, and to terminate our sequential process, we incorporate a scoring network, similar to the one presented in <ref type="bibr" target="#b17">[18]</ref>. Our scoring network takes information from the box and segmentation network to produce a score between 0 and 1.</p><formula xml:id="formula_6">s t = sigmoid(W(z t,end ) + W(u segm ))<label>(10)</label></formula><p>Termination condition. We train the entire model with a sequence length determined by the maximum number of objects plus one. During inference, we cut off iterations once the output score goes below 0.5. The loss function (described below) encourages scores to decrease monotonically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Loss functions</head><p>Joint loss. The total loss function is a sum of three losses: the segmentation matching IoU loss L y ; the box IoU loss L b ; and the score cross-entropy loss L s :</p><formula xml:id="formula_7">L(y, b, s) = L y (y, y * ) + L b (b, b * ) + L s (s, s * )<label>(11)</label></formula><p>(a) Matching IoU loss (mIOU). A primary challenge of instance segmentation involves matching model and ground-truth instances. we compute a maximum-weighted bipartite graph matching between the output instances and ground-truth instances <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b17">[18]</ref>. Matching makes the loss insensitive to the ordering of the ground-truth instances. Unlike coverage scores proposed in <ref type="bibr" target="#b20">[21]</ref> it directly penalizes both false positive and false negative segmentation. The matching weight M i,j is the IoU score between a pair of segmentation. We use the Hungarian algorithm to compute the matching.</p><formula xml:id="formula_8">M i,j = softIOU(y i , y * j ) ≡ y i • y * j y i + y * j − y i • y * j (12) L y (y, y * ) = −mIOU(y, y * ) ≡ − 1 N i,j M i,j 1[match(y i ) = y * j ]<label>(13)</label></formula><p>(b) Soft box IoU loss. Although the exact IoU can be derived from the 4-d box coordinates, its gradient vanishes when two boxes do not overlap, which can be problematic for gradient-based learning. Instead, we propose a soft version of the box IoU. We use the same Gaussian filter to re-project a constant patch on the original image, pad the ground-truth boxes and compute the mIOU between the predicted box and the matched padded ground-truth bounding box.</p><formula xml:id="formula_9">b t = sigmoid(γ • Extract(1, F T Y , F T X ) − β) (14) L b (b, b * ) = −mIOU(b, Pad(b * ))<label>(15)</label></formula><p>(c) Monotonic score loss. To facilitate automatic termination, the network should output more confident objects first. We proposed a loss function that encourages monotonically decreasing values in the score output. Iterations with target score 1 are compared to the lower bound of preceding scores, and 0 targets to the upper bound of subsequent scores.</p><formula xml:id="formula_10">L s (s, s * ) = t −s * t log max t =t...T −1 {s t } − (1 − s * t ) log 1 − min t =0...t {s t }<label>(16)</label></formula><p>2.6 Training procedure and post-processing Bootstrap training. The box and segmentation networks rely on the output of each other to make decisions for the next time-step. Because of the coupled nature of the two networks, we propose a bootstrap training procedure: these networks are pre-trained with ground-truth segmentation and boxes, respectively, and in later stages we replace the ground-truth with the model predicted values.</p><p>Scheduled sampling. To smooth out the transition between stages, we explore the idea of "scheduled sampling" <ref type="bibr" target="#b1">[2]</ref> where we gradually remove the reliance on ground-truth segmentation at the input of the network. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, during training there is a dynamic switch in the input of the external memory, to utilize either the maximally overlapping ground-truth instance segmentation, or the output of the network from the previous time step.</p><p>Post-processing. We truncate segmentation outside the predicted foreground mask, fill holes with the labels from the nearest neighboring predicted instance, and remove object segmentation of size smaller than 425 square pixels. We study the effect of post-processing in ablation studies in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Instance segmentation has recently received a burst of research attention, as it provides higher level of precision of image understanding compared to object detection and semantic segmentation.</p><p>Instance segmentation using graphical models. An early exploration of instance segmentation proposes a multi-stage pipeline composed of patch-wise features based on deep learning, combined into a segmentation tree <ref type="bibr" target="#b20">[21]</ref>. They formulated a new loss function, the coverage score, that calculated the amount of ground truth regions not covered by the model's instance segmentation. More recently, Zhang et al. <ref type="bibr" target="#b25">[26]</ref> formulated a dense CRF for instance segmentation; . They apply a CNN on dense image patches to make local predictions, and constructed a dense CRF to produce globally consistent labellings. Their key contribution is a shifting-label potential that encourages consistency across different patches. They achieved strong results on the challenging KITTI object dataset; however, the graphical model formulation entails long running times, and their energy functions are dependent on instances being connected and having a clear depth ordering.</p><p>Instance segmentation using CNN. Liang et al. <ref type="bibr" target="#b8">[9]</ref> used a CNN to generate pixel-level object size information, and used clustering as a post-processing step. They added a regressor at the top of the CNN to estimate the count, which is the total number of clusters. An erroneous count can usually leads to poor segmentation. Dai et al. <ref type="bibr" target="#b3">[4]</ref> proposed a pipeline-based approach and won the MS-COCO instance segmentation challenge. Their method first predicts bounding box proposals and extracts regions of interest (ROI), then uses shared features to perform segmentation within each ROI. Their architecture can also be fine-tuned end-to-end. However, since their method is based on detector proposals, it does not explicitly handle object occlusions, which may lead it to fail during non-maximal suppression (NMS). Uhrig et al. <ref type="bibr" target="#b22">[23]</ref> presented another approach with FCN, and achieved very impressive results. Their FCN outputs three channels: semantic segmentation, object orientation and depth. Post-processing based on template matching and instance fusion produce the instance identities. Their approach is based on bottom-up clustering since their FCN can only provide pixel-level information, whereas our model is processing the image in a top-down fashion. Importantly, they also used ground-truth depth labels in training their model.</p><p>Instance segmentation using RNN. Another recent line of research, e.g. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref> employs endto-end recurrent neural networks (RNN) to perform object detection and segmentation. A permutation agnostic loss function based on maximum weighted bipartite matching was proposed by <ref type="bibr" target="#b21">[22]</ref>. To process an entire image, they treat each element of a 15 × 20 feature map individually. Similarly, our box proposal network also uses an RNN to generate box proposals: instead of running the image 300 times through the RNN, we only run it once by using a soft attention mechanism <ref type="bibr" target="#b23">[24]</ref>. Romera-Paredes and Torr <ref type="bibr" target="#b17">[18]</ref> use convolutional LSTM (ConvLSTM) <ref type="bibr" target="#b19">[20]</ref> to produce instance segmentation directly. However, since their ConvLSTM is required to handle object detection, inhibition, and segmentation all at the same time on a global scale, the final output loses precision. They add a dense CRF to restore the resolution. Compared to their approach, our segmentation network operates on a local level. Instead of resorting to graphical models, we added skip connections to restore the resolution.</p><p>Instance counting. Previous work on object counting in images has mainly focused on crowds of pedestrians and biological cells <ref type="bibr" target="#b7">[8]</ref>. Chattopadhyay et al. <ref type="bibr" target="#b2">[3]</ref> focused on counting questions in VQA and proposed detector approaches as well as a regression based method ("associative subitizing") that works on a 3 × 3 field of CNN features level. Note that unlike our approach, this method does not provide instance segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>CVPPP leaf segmentation. One instance segmentation benchmark is the CVPPP plant leaf dataset <ref type="bibr" target="#b11">[12]</ref>, which was developed due to the importance of instance segmentation in plant phenotyping. We ran the A1 subset of CVPPP plant leaf segmentation dataset. We trained our model on 128 labelled images, and report results on the 33 test images. We compare our performance to <ref type="bibr" target="#b17">[18]</ref>, and other top approaches that were published with the CVPPP conference; see the collation study <ref type="bibr" target="#b18">[19]</ref> for details of these other approaches.</p><p>KITTI car segmentation. Instance segmentation also provides rich information in the context of autonomous driving. Following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b22">23]</ref>, we also evaluated the model performance on KITTI car segmentation dataset. We trained the model with 3712 training images, and report performance on 144 test images. We also examine the relative importance of model components via ablation studies.</p><p>MS-COCO counting. Additionally, we train class specific models on MS-COCO and test counting performance on the results. We chose "person" and "zebra" because these are the two of the most common classes in VQA questions. We report counting performance on images with at least one instance of the class: 677 zebra images, and 21,634 "person" images.  <ref type="bibr" target="#b17">[18]</ref> 66.6 (8.7) 1.1 (0.9) MSU <ref type="bibr" target="#b18">[19]</ref> 66.7 (7.6) 2.3 (1.6) Nottingham <ref type="bibr" target="#b18">[19]</ref> 68.3 (6.3) 3.8 (2.0) Wageningen <ref type="bibr" target="#b24">[25]</ref> 71.1 (6.2) 2.2 (1.6) IPK <ref type="bibr" target="#b13">[14]</ref> 74. <ref type="bibr" target="#b3">4</ref>      Evaluation metrics. We report the metrics used by the other studies in the respective benchmarks: symmetric best dice (SBD) for leaf segmentation (see <ref type="bibr">Equations 17,</ref><ref type="bibr" target="#b17">18)</ref> and mean (weighted) coverage (MWCov, MUCov) for car segmentation (see <ref type="bibr">Equations 19,</ref><ref type="bibr" target="#b19">20)</ref>. The coverage scores measure the instance-wise IoU for each ground-truth instance averaged over the image; MWCov further weights the score by the size of the ground-truth instance segmentation (larger objects get larger weights).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DICE(A, B)</head><formula xml:id="formula_11">= 2|A ∪ B| |A| + |B| BD({A i }, B) = max i DICE(A i , B)<label>(17)</label></formula><p>SBD</p><formula xml:id="formula_12">({ŷ i }, {y j }) = min   1 N j BD({ŷ i }, y j ), 1 N i BD(ŷ i , {y j })  <label>(18)</label></formula><p>MWCov</p><formula xml:id="formula_13">({y i }, {y * j }) = 1 N i |y i | i |y i | max j IoU(y i , y * j )<label>(19)</label></formula><p>MUCov Counting is measured in absolute difference in count (|DiC|) (see Equation 21), average false positive (AvgFP), and average false negative (AvgFN). False positive is the number of predicted instances that do not overlap with the ground-truth, and false negative is the number of ground-truth instances that do not overlap with the prediction.</p><formula xml:id="formula_14">({y i }, {y * j }) = 1 N i max j IoU(y i , y * j )<label>(20)</label></formula><formula xml:id="formula_15">|DiC| = 1 N i |count i − count * i |<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results &amp; discussion</head><p>In the leaf segmentation task, our best model outperforms the previous state-of-the-art by a large margin in both segmentation and counting. <ref type="table" target="#tab_0">Table 1</ref> shows that the models with FCN overfit and scores lower than the simpler version. This is sensible as the dataset size is small, and including the FCN significantly increases the input dimension and number of parameters.</p><p>In the car segmentation task, our model achieves the state-of-the-art MWCov shown in <ref type="table" target="#tab_3">Table 3</ref>, but our MUCov is lower than results reported by Uhrig et al. <ref type="bibr" target="#b22">[23]</ref>. One possible explanation is their inclusion of depth information during training, which may help the model disambiguate distant object boundaries. Moreover, their bottom-up "instance fusion" method plays a crucial role (omitting this leads to a steep performance drop); this likely helps segment smaller objects, whereas our box network does not reliably detect distant cars.</p><p>In the zebra counting task, we found that our model outperforms the detector and NMS method, and associative-subitizing methods <ref type="bibr" target="#b2">[3]</ref>, but we are not doing as well in the person category. However, relative to these regression-based methods, our model permits insight into the recognition of each instance by inspecting the output segmentation. <ref type="figure" target="#fig_3">Figure 6</ref> shows the relation between counting performance and number of instances. Mean absolute difference in count is around 1 for up to 18 leaves, 7 cars, 4 zebras and 3 people.</p><p>From the figures above we see our model is handling a significant amount of object occlusion and truncation. We verified that the external memory helps with the counting process as the network first segments the more salient objects and then accounts for the occluded instances. In addition, our segmentation network can handle a range of object sizes because of the design of the box network.</p><p>We found that using scheduled sampling results in much better performance. It helps by making training resemble testing, gradually forcing the model to carry out a full sequence during training instead of relying on ground-truth input. Finally, the convolutional and attentional architecture significantly reduces the number of parameters and the performance is quite strong despite being trained with only 100 leaf images and 1000 zebra images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we borrow intuition from human counting and formulate instance segmentation and counting as a recurrent attentive process. Our end-to-end recurrent architecture demonstrates significant improvement compared to earlier exploration of using RNN on the same tasks, and shows state-of-the-art results on challenging leaf and car segmentation datasets. We address the classic object occlusion problem with a recurrent external memory, and the attention structure brings segmentation at a fine resolution. Our model also shows promising counting performance on a portion of the MS-COCO dataset.</p><p>Image GT Ours Image GT Ours <ref type="figure">Figure 8</ref>: Examples of our instance segmentation output on MS-COCO zebra images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Left: Detailed network design. Right: Sketch of training, and scheduled sampling; during training, the weighting of ground-truth instance segmentations relative to model predictions (θt) decays to zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the output of the pretrained FCN. Left: input image. Middle: predicted foreground. Right: predicted angle map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Examples of our instance segmentation output on CVPPP leaf dataset. In this paper, instance colors are determined by the order of the model output sequence.ImageGT Ours Examples of our instance segmentation output on KITTI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Absolute difference in count between the model and ground-truth as a function of the number of objects, in the various datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Examples of our instance segmentation output on MS-COCO person images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Leaf segmentation and counting performance</figDesc><table><row><cell>SBD ↑</cell><cell>|DiC| ↓</cell></row><row><cell>RIS+CRF</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Counting performance on MS-COCO</figDesc><table><row><cell></cell><cell cols="3">Category MWCov ↑ |DiC| ↓</cell></row><row><cell>detect [3]</cell><cell>Person</cell><cell>-</cell><cell>3.22</cell></row><row><cell cols="2">aso-sub [3] Person</cell><cell>-</cell><cell>1.36</cell></row><row><cell>Ours</cell><cell>Person</cell><cell>48.3</cell><cell>2.09</cell></row><row><cell>detect [3]</cell><cell>Zebra</cell><cell>-</cell><cell>2.56</cell></row><row><cell cols="2">aso-sub [3] Zebra</cell><cell>-</cell><cell>1.03</cell></row><row><cell>Ours</cell><cell>Zebra</cell><cell>65.3</cell><cell>0.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>KITTI vehicle segmentation resultsFG IoU↑ MWCov ↑ MUCov ↑ AvgFP ↓ AvgFN ↓</figDesc><table><row><cell>No Sched. Samp.</cell><cell>87.6</cell><cell>78.1</cell><cell>62.9</cell><cell>0.139</cell><cell>0.361</cell></row><row><cell>No Post Proc.</cell><cell>83.0</cell><cell>78.7</cell><cell>65.8</cell><cell>0.222</cell><cell>0.431</cell></row><row><cell>DenseCRFv1 [27]</cell><cell>77.3</cell><cell>70.9</cell><cell>52.2</cell><cell>0.597</cell><cell>0.736</cell></row><row><cell>DenseCRFv2 [26]</cell><cell>78.5</cell><cell>74.1</cell><cell>55.2</cell><cell>0.417</cell><cell>0.833</cell></row><row><cell>FCN+Depth [23]</cell><cell>84.1</cell><cell>79.7</cell><cell>75.8</cell><cell>0.201</cell><cell>0.159</cell></row><row><cell>Ours (Canvas)</cell><cell>86.9</cell><cell>72.2</cell><cell>59.4</cell><cell>0.090</cell><cell>0.438</cell></row><row><cell>Ours (FCN + Canvas)</cell><cell>86.8</cell><cell>81.0</cell><cell>66.6</cell><cell>0.167</cell><cell>0.396</cell></row><row><cell cols="2">Ours (FCN + ConvLSTM) 86.8</cell><cell>82.2</cell><cell>67.7</cell><cell>0.188</cell><cell>0.382</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training procedure specification</head><p>We used the Adam optimizer with learning rate 0.001 and batch size of 8. The learning rate is multiplied by 0.85 for every 5000 steps of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Scheduled sampling</head><p>We denote θ t as the probability of feeding in ground-truth segmentation that has the greatest overlap with the previous prediction, as opposed to model output. θ t follows exponential decay as training goes on, and for larger t, the decay occurs later:</p><p>Γ t = 1 + log(1 + Kt)</p><p>where epoch is the training index, S, S 2 , and K are constants. In the experiments reported here, their values are 10000, 2885, and 3.</p><p>B Model architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Foreground + Orientation FCN</head><p>We resize the image to uniform size. For CVPPP and MS-COCO dataset, we adopt a uniform size of × 224, and for KITTI, we adopt 128 × 448. <ref type="table">Table 4</ref> lists the specification of all layers.  The box network takes in 9 channel input. Either directly from the output of the FCN, or from the hidden state of the ConvLSTM. It goes through a CNN structure again and uses the attention vector predicted by the LSTM to perform dynamic pooling in the last layer. The CNN hyperparameters are listed in <ref type="table">Table 6</ref> and the LSTM and glimpse MLP hyperparameters are listed in <ref type="table">Table 7</ref>. The glimpse MLP takes input from the hidden state of the LSTM and ouputs a vector of normalized weighting over all the box CNN feature map spatial grids.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Segmentation network</head><p>The segmentation networks takes in a patch size 48 × 48 with multiple channels. The first three channels are the original image R, G, B channels. Then there are 8 channels of orientation angles, and then 1 channel of foreground heat map, all predicted by FCN. Full detail is listed in <ref type="table">Table 8</ref>.</p><p>Constant β is chosen to be 5. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Counting everyday objects in everyday scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1604.03505</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.04412</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to count leaves in rosette plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision Problems in Plant Phenotyping (CVPPP)</title>
		<meeting>the Computer Vision Problems in Plant Phenotyping (CVPPP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Special Issue on Fine-grained Categorization in Ecological Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Finely-grained annotated datasets for imagebased plant phenotyping</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3-d histogram-based segmentation and leaf detection for rosette plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Klukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning to decompose for object detection and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno>abs/1511.06449</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Recurrent instance segmentation. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno>abs/1511.08250</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Leaf segmentation in plant phenotyping: A collation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Scharr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Klukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Polder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vukadinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="585" to="606" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Instance segmentation of indoor scenes using a coverage loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<idno>abs/1506.04878</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1604.05096</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-leaf tracking from fluorescence plant videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Instance-level segmentation with deep densely connected MRFs. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>abs/1512.06735</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
