<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-03-30">30 March 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Adams</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Llano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Brunel</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Hawkins</surname></persName>
							<email>jhawkins@numenta.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subutai</forename><surname>Ahmad</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">State University of New York at Stony Brook</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Numenta, Inc</orgName>
								<address>
									<settlement>Redwood City</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-03-30">30 March 2016</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.3389/fncir.2016.00023</idno>
					<note type="submission">Received: 30 October 2015 Accepted: 14 March</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neocortex</term>
					<term>prediction</term>
					<term>neocortical theory</term>
					<term>active dendrites</term>
					<term>sequence memory</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Excitatory neurons in the neocortex have thousands of excitatory synapses. The proximal synapses, those closest to the cell body, have a relatively large effect on the likelihood of a cell generating an action potential. However, a majority of the synapses are distal, or far from the cell body. The activation of a single distal synapse has little effect at the soma, and for many years it was hard to imagine how the thousands of distal synapses could play an important role in determining a cell's responses <ref type="bibr" target="#b35">(Major et al., 2013)</ref>. It has been observed that dendrite branches are active processing elements. The activation of several distal synapses within close spatial and temporal proximity can lead to a local dendritic NMDA spike and consequently a significant and sustained depolarization of the soma <ref type="bibr" target="#b1">(Antic et al., 2010;</ref><ref type="bibr" target="#b35">Major et al., 2013)</ref>. This has led some researchers to suggest that dendritic branches act as independent pattern recognizers <ref type="bibr" target="#b45">(Poirazi et al., 2003;</ref><ref type="bibr" target="#b46">Polsky et al., 2004)</ref>. However, the functional and theoretical benefits of networks of neurons with active dendrites as compared to a multi-layer network of neurons without active dendrites are unclear <ref type="bibr" target="#b45">(Poirazi et al., 2003)</ref>.</p><p>Lacking a theory of why neurons need active dendrites, almost all artificial neural networks, such as those used in deep learning <ref type="bibr" target="#b29">(LeCun et al., 2015)</ref> and spiking neural networks <ref type="bibr" target="#b34">(Maass, 1997)</ref>, use artificial neurons with simplified dendritic models, introducing the possibility they may be missing key functional aspects of biological neural tissue. To understand how the neocortex works and to build systems that work on the same principles as the neocortex we need an understanding of how biological neurons integrate input from thousands of synapses and whether active dendrites play an essential role. Of course, neurons cannot be understood in isolation. Therefore, we also need a complementary theory of how networks of neurons, each with active dendrites, work together toward a common purpose.</p><p>In this paper we introduce such a theory. First, we show how a typical pyramidal neuron with active dendrites and thousands of synapses can recognize hundreds of independent patterns of cellular activity. We show that a neuron can recognize these independent patterns even in the presence of large amounts of noise and variability as long as overall neural activity is sparse. Next we introduce a neuron model where the inputs to different parts of the dendritic tree serve different purposes. In this model the patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron. Patterns recognized by a neuron's distal synapses act as predictions by depolarizing the cell without directly causing an action potential. By this mechanism neurons can learn to predict their activation in hundreds of unique contexts. Then we show how a network of such neurons can learn and recall sequences of patterns. The network relies on a competitive process where previously depolarized neurons emit a spike sooner than nondepolarized neurons. When combined with fast local inhibition the network's activation state is biased toward its predictions. A cycle of activation leading to prediction leading to activation etc. forms the basis of sequence memory.</p><p>We describe a set of learning and activation rules required for neurons with active dendrites. A network of standard linear or non-linear neurons with a simplified dendrite structure cannot easily implement these activation and learning rules. To do so would require the introduction of several complex and biologically unlikely features. Therefore, we have chosen to use a neuron model that includes active dendrites as well as proximal, basal, and apical dendrite integration zones, which we believe more closely matches known neuron anatomy and physiology.</p><p>Through simulation we illustrate that the sequence memory network exhibits numerous desirable properties such as on-line learning, multiple simultaneous predictions, and robustness. The overall theory is consistent with a large body of experimental evidence. We outline a number of detailed biological predictions that can be used to further test the theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neurons Reliably Recognize Multiple Sparse Patterns</head><p>It is common to think of a neuron as computing a single weighted sum of all of its synapses. This notion, sometimes called a "point neuron, " forms the basis of almost all artificial neural networks ( <ref type="figure">Figure 1A)</ref>.</p><p>Active dendrites suggest a different view of the neuron, where neurons recognize many independent unique patterns <ref type="bibr" target="#b45">(Poirazi et al., 2003;</ref><ref type="bibr" target="#b46">Polsky et al., 2004;</ref><ref type="bibr" target="#b25">Larkum and Nevian, 2008)</ref>. Experimental results show that the coincident activation of 8-20 FIGURE 1 | Comparison of neuron models. (A) The neuron model used in most artificial neural networks has few synapses and no dendrites. (B) A neocortical pyramidal neuron has thousands of excitatory synapses located on dendrites (inset). The co-activation of a set of synapses on a dendritic segment will cause an NMDA spike and depolarization at the soma. There are three sources of input to the cell. The feedforward inputs (shown in green) which form synapses proximal to the soma, directly lead to action potentials. NMDA spikes generated in the more distal basal and apical dendrites depolarize the soma but typically not sufficiently to generate a somatic action potential. (C) An HTM model neuron models dendrites and NMDA spikes with an array of coincident detectors each with a set of synapses (only a few of each are shown).</p><p>synapses in close spatial proximity on a dendrite will combine in a non-linear fashion and cause an NMDA dendritic spike <ref type="bibr" target="#b27">(Larkum et al., 1999;</ref><ref type="bibr" target="#b52">Schiller et al., 2000;</ref><ref type="bibr" target="#b53">Schiller and Schiller, 2001;</ref><ref type="bibr" target="#b35">Major et al., 2013)</ref>. Thus, a small set of neighboring synapses acts as a pattern detector. It follows that the thousands of synapses on a cell's dendrites act as a set of independent pattern detectors. The detection of any of these patterns causes an NMDA spike and subsequent depolarization at the soma.</p><p>It might seem that 8-20 synapses could not reliably recognize a pattern of activity in a large population of cells. However, robust recognition is possible if the patterns to be recognized are sparse; i.e., few neurons are active relative to the population <ref type="bibr" target="#b43">(Olshausen and Field, 2004)</ref>. For example, consider a population of 200K cells where 1% (2000) of the cells are active at any point in time. We want a neuron to detect when a particular pattern occurs in the 200K cells. If a section of the neuron's dendrite forms new synapses to just 10 of the 2000 active cells, and the threshold for generating an NMDA spike is 10, then the dendrite will detect the target pattern when all 10 synapses receive activation at the same time. Note that the dendrite could falsely detect many other patterns that share the same 10 active cells. However, if the patterns are sparse, the chance that the 10 synapses would become active for a different random pattern is small. In this example it is only 9.8 × 10 −21 .</p><p>The probability of a false match can be calculated precisely as follows. Let n represent the size of the cell population and a the number of active cells in that population at a given point in time, for sparse patterns a ≪ n. Let s be the number of synapses on a dendritic segment and θ be the NMDA spike threshold. We say the segment recognizes a pattern if at least θ synapses become active, i.e., at least θ of the s synapses match the currently active cells.</p><p>Assuming a random distribution of patterns, the exact probability of a false match is given by:</p><formula xml:id="formula_0">s b=θ s b × n − s a − b n a<label>(1)</label></formula><p>The denominator is simply the total number of possible patterns containing a active cells in a population of n total cells. The numerator counts the number of patterns that would connect to θ or more of the s synapses on one dendritic segment. A more detailed description of this equation can be found in <ref type="bibr" target="#b0">Ahmad and Hawkins (2016)</ref>. The equation shows that a non-linear dendritic segment can robustly classify a pattern by sub-sampling (forming synapses to only a small number of the cells in the pattern to be classified). <ref type="table">Table A</ref> in S1 Text lists representative error probabilities calculated from Equation (1).</p><p>By forming more synapses than necessary to generate an NMDA spike, recognition becomes robust to noise and variation. For example, if a dendrite has an NMDA spike threshold of 10, but forms 20 synapses to the pattern it wants to recognize, twice as many as needed, it allows the dendrite to recognize the target pattern even if 50% of the cells are changed or inactive.</p><p>The extra synapses also increase the likelihood of a false positive error. Although the chance of error has increased, Equation <ref type="formula" target="#formula_0">1</ref>shows that it is still tiny when the patterns are sparse. In the above example, doubling the number of synapses and hence introducing a 50% noise tolerance, increases the chance of error to only 1.6 × 10 −18 . Table B in S1 Text lists representative error rates when the number of synapses exceeds the threshold.</p><p>The synapses recognizing a given pattern have to be colocated on a dendritic segment. If they lie within 40 µm of each other then as few as eight synapses are sufficient to create an NMDA spike <ref type="bibr" target="#b36">(Major et al., 2008)</ref>. If the synapses are spread out along the dendritic segment, then up to 20 synapses are needed <ref type="bibr" target="#b35">(Major et al., 2013)</ref>. A dendritic segment can contain several hundred synapses; therefore each segment can detect multiple patterns. If synapses that recognize different patterns are mixed together on the dendritic segment, it introduces an additional possibility of error by co-activating synapses from different patterns. The probability of this type of error depends on how many sets of synapses share the dendritic segment and the sparsity of the patterns to be recognized. For a wide range of values the chance for this type of error is still low (Table C in S1 Text). Thus, the placement of synapses to recognize a particular pattern is somewhat precise (they must be on the same dendritic segment and ideally within 40 µm of each other), but also somewhat imprecise (mixing with other synapses is unlikely to cause errors).</p><p>If we assume an average of 20 synapses are allocated to recognize each pattern, and that a neuron has 6000 synapses, then a cell would have the ability to recognize approximately different patterns. This is a rough approximation, but makes evident that a neuron with active dendrites can learn to reliably recognize hundreds of patterns within a large population of cells. The recognition of any one of these patterns will depolarize the cell. Since all excitatory neurons in the neocortex have thousands of synapses, and, as far as we know, they all have active dendrites, then each and every excitatory neocortical neuron recognizes hundreds of patterns of neural activity.</p><p>In the next section we propose that most of the patterns recognized by a neuron do not directly lead to an action potential, but instead play a role in how networks of neurons make predictions and learn sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Three Sources of Synaptic Input to Cortical Neurons</head><p>Neurons receive excitatory input from different sources that are segregated on different parts of the dendritic tree. <ref type="figure">Figure 1B</ref> shows a typical pyramidal cell, the most common excitatory neuron in the neocortex. We show the input to the cell divided into three zones. The proximal zone receives feedforward input. The basal zone receives contextual input, mostly from nearby cells in the same cortical region <ref type="bibr" target="#b63">(Yoshimura et al., 2000;</ref><ref type="bibr" target="#b44">Petreanu et al., 2009;</ref><ref type="bibr" target="#b48">Rah et al., 2013)</ref>. The apical zone receives feedback input <ref type="bibr" target="#b55">(Spruston, 2008)</ref>. (The second most common excitatory neuron in the neocortex is the spiny stellate cell; we suggest they be considered similar to pyramidal cells minus the apical dendrites.) We propose the three zones of synaptic integration on a neuron (proximal, basal, and apical) serve the following purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proximal synapses define the classic receptive field of a cell</head><p>The synapses on the proximal dendrites (typically several hundred) have a relatively large effect at the soma and therefore are best situated to define the basic receptive field response of the neuron <ref type="bibr" target="#b55">(Spruston, 2008)</ref>. If the coincident activation of a subset of the proximal synapses is sufficient to generate a somatic action potential and if the inputs to the proximal synapses are sparsely active, then the proximal synapses will recognize multiple unique feedforward patterns in the same manner as discussed earlier. Therefore, the feedforward receptive field of a cell can be thought of as a union of feedforward patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basal synapses learn transitions in sequences</head><p>We propose that basal dendrites of a neuron recognize patterns of cell activity that precede the neuron firing, in this way the basal dendrites learn and store transitions between activity patterns. When a pattern is recognized on a basal dendrite it generates an NMDA spike. The depolarization due to an NMDA spike attenuates in amplitude by the time it reaches the soma, therefore when a basal dendrite recognizes a pattern it will depolarize the soma but not enough to generate a somatic action potential <ref type="bibr" target="#b1">(Antic et al., 2010;</ref><ref type="bibr" target="#b35">Major et al., 2013)</ref>. We propose this sub-threshold depolarization is an important state of the cell. It represents a prediction that the cell will become active shortly and plays an important role in network behavior. A slightly depolarized cell fires earlier than it would otherwise if it subsequently receives sufficient feedforward input. By firing earlier it inhibits neighboring cells, creating highly sparse patterns of activity for correctly predicted inputs. We will explain this mechanism more fully in a later section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apical synapses invoke a top-down expectation</head><p>The apical dendrites of a neuron also generate NMDA spikes when they recognize a pattern <ref type="bibr" target="#b4">(Cichon and Gan, 2015)</ref>. An apical NMDA spike does not directly affect the soma. Instead it can lead to a Ca 2+ spike in the apical dendrite <ref type="bibr" target="#b10">(Golding et al., 1999;</ref><ref type="bibr" target="#b26">Larkum et al., 2009)</ref>. A single apical Ca 2+ spike will depolarize the soma, but typically not enough to generate a somatic action potential <ref type="bibr" target="#b1">(Antic et al., 2010)</ref>. The interaction between apical Ca 2+ spikes, basal NMDA spikes, and somatic action potentials is an area of ongoing research <ref type="bibr" target="#b24">(Larkum, 2013</ref>), but we can say that under many conditions a recognized pattern on an apical dendrite will depolarize the cell and therefore have a similar effect as a recognized pattern on a basal dendrite. We propose that the depolarization caused by the apical dendrites is used to establish a top-down expectation, which can be thought of as another form of prediction. <ref type="figure">Figure 1C</ref> shows an abstract model of a pyramidal neuron we use in our software simulations. We model a cell's dendrites as a set of threshold coincidence detectors; each with its own synapses. If the number of active synapses on a dendrite/coincidence detector exceeds a threshold the cell detects a pattern. The coincidence detectors are in three groups corresponding to the proximal, basal, and apical dendrites of a pyramidal cell. We refer to this model neuron as an "HTM neuron" to distinguish it from biological neurons and point neurons. HTM is an acronym for Hierarchical Temporal Memory, a term used to describe our models of neocortex <ref type="bibr" target="#b14">(Hawkins et al., 2011)</ref>. HTM neurons used in the simulations for this paper have 128 dendrite/coincidence detectors with up to 40 synapses per dendrite. For clarity, <ref type="figure">Figure 1C</ref> shows only a few dendrites and synapses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The HTM Model Neuron</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Networks of Neurons Learn Sequences</head><p>Because all tissue in the neocortex consists of neurons with active dendrites and thousands of synapses, it suggests there are common network principles underlying everything the neocortex does. This leads to the question, what network property is so fundamental that it is a necessary component of sensory inference, prediction, language, and motor planning?</p><p>We propose that the most fundamental operation of all neocortical tissue is learning and recalling sequences of patterns <ref type="bibr" target="#b15">(Hawkins and Blakeslee, 2004)</ref>, what Karl Lashley famously called "the most important and also the most neglected problem of cerebral physiology" <ref type="bibr" target="#b28">(Lashley, 1951)</ref>. More specifically, we propose that each cellular layer in the neocortex implements a variation of a common sequence memory algorithm. We propose cellular layers use sequence memory for different purposes, which is why cellular layers vary in details such as size and connectivity. In this paper we illustrate what we believe is the basic sequence memory algorithm without elaborating on its variations.</p><p>We started our exploration of sequence memory by listing several properties required of our network in order to model the neocortex.</p><p>(1) On-line learning Learning must be continuous. If the statistics of the world change, the network should gradually and continually adapt with each new input.</p><p>(2) High-order predictions Making correct predictions with complex sequences requires the ability to incorporate contextual information from the past. The network needs to dynamically determine how much temporal context is needed to make the best predictions. The term "highorder" refers to "high-order Markov chains" which have this property.</p><p>(3) Multiple simultaneous predictions Natural data streams often have overlapping and branching sequences. The sequence memory therefore needs to make multiple predictions at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(4) Local learning rules</head><p>The sequence memory must only use learning rules that are local to each neuron. The rules must be local in both space and time, without the need for a global objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(5) Robustness</head><p>The memory should exhibit robustness to high levels of noise, loss of neurons, and natural variation in the input. Degradation in performance under these conditions should be gradual.</p><p>All these properties must occur simultaneously in the context of continuously streaming data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mini-Columns and Neurons: Two Representations</head><p>High-order sequence memory requires two simultaneous representations. One represents the feedforward input to the network and the other represents the feedforward input in a particular temporal context. To illustrate this requirement, consider two abstract sequences "ABCD" and "XBCY, " where each letter represents a sparse pattern of activation in a population of neurons. Once these sequences are learned the network should predict "D" when presented with sequence "ABC" and it should predict "Y" when presented with sequence "XBC." Therefore, the internal representation during the subsequence "BC" must be different in the two cases; otherwise the correct prediction cannot be made after "C" is presented. <ref type="figure" target="#fig_0">Figure 2</ref> illustrates how we propose these two representations are manifest in a cellular layer of cortical neurons. The panels in <ref type="figure" target="#fig_0">Figure 2</ref> represent a slice through a single cellular layer in the neocortex <ref type="figure" target="#fig_0">(Figure 2A)</ref>. The panels are greatly simplified for clarity. <ref type="figure" target="#fig_0">Figure 2B</ref> shows how the network represents two input sequences before the sequences are learned. <ref type="figure" target="#fig_0">Figure 2C</ref> shows how the network represents the same input after the sequences are learned. Each feedforward input to the network is converted into a sparse set of active mini-columns. (Mini-columns in the neocortex span multiple cellular layers. Here we are only referring to the cells in a mini-column in one cellular layer.) All the neurons in a mini-column share the same feedforward receptive fields. If an unanticipated input arrives, then all the cells in the selected mini-columns will recognize the input pattern and become active. However, in the context of a previously learned sequence, one or more of the cells in the mini-columns will be depolarized. The depolarized cells will be the first to generate an action potential, inhibiting the other cells nearby. Thus, a predicted input will lead to a very sparse pattern of cell activation that is unique to a particular element, at a particular location, in a particular sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Basal Synapses Are the Basis of Sequence Memory</head><p>In this theory, cells use their basal synapses to learn the transitions between input patterns. With each new feedforward input some cells become active via their proximal synapses. Other cells, using their basal synapses, learn to recognize this active pattern and upon seeing the pattern again, become depolarized, thereby predicting their own feedforward activation in the next input. Feedforward input activates cells, while basal input generates predictions. As long as the next input matches the current prediction, the sequence continues, <ref type="figure">Figure 3</ref>. <ref type="figure">Figure 3A</ref> shows both active cells and predicted cells while the network follows a previously learned sequence.</p><p>Often the network will make multiple simultaneous predictions. For example, suppose that after learning the sequences "ABCD" and "XBCY" we expose the system to just the ambiguous sub-sequence "BC." In this case we want the system to simultaneously predict both "D" and "Y." <ref type="figure">Figure 3B</ref> illustrates how the network makes multiple predictions when the input is ambiguous. The number of simultaneous predictions that can be made with low chance of error can again be calculated via Equation (1). Because the predictions tend to be highly sparse, it is possible for a network to predict dozens of patterns simultaneously without confusion. If an input matches any of the predictions it will result in the correct highly-sparse representation. If an input does not match any of the predictions all the cells in a column will become active, indicating an unanticipated input.</p><p>Although every cell in a mini-column shares the same feedforward response, their basal synapses recognize different patterns. Therefore, cells within a mini-column will respond uniquely in different learned temporal contexts, and overall levels of activity will be sparser when inputs are anticipated. Both of these attributes have been observed <ref type="bibr" target="#b60">(Vinje and Gallant, 2002;</ref><ref type="bibr" target="#b62">Yen et al., 2007;</ref><ref type="bibr" target="#b37">Martin and Schröder, 2013)</ref>.</p><p>For one of the cells in the last panel of <ref type="figure">Figure 3A</ref>, we show three connections the cell used to make a prediction. In real neurons, and in our simulations, a cell would form 15 to 40 connections to a subset of a larger population of active cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apical Synapses Create a Top-Down Expectation</head><p>Feedback axons between neocortical regions often form synapses (in layer 1) with apical dendrites of pyramidal neurons whose cell bodies are in layers 2, 3, and 5. It has long been speculated that these feedback connections implement some form of expectation or bias <ref type="bibr" target="#b23">(Lamme et al., 1998</ref>). Our neuron model suggests a mechanism for top-down expectation in the neocortex. <ref type="figure">Figure 4</ref> shows how a stable feedback pattern to apical dendrites can predict multiple elements in a sequence all at the same time. When a new feedforward input arrives it will be interpreted as part of the predicted sequence. The feedback biases the input toward a particular interpretation. Again, because the patterns are sparse, many patterns can be simultaneously predicted.</p><p>Thus, there are two types of prediction occurring at the same time. Lateral connections to basal dendrites predict the next input, and top-down connections to apical dendrites predict multiple sequence elements simultaneously. The physiological interaction between apical and basal dendrites is an area of active research <ref type="bibr" target="#b24">(Larkum, 2013)</ref> and will likely lead to a more nuanced interpretation of their roles in inference and prediction. However, we propose that the mechanisms shown in <ref type="figure" target="#fig_0">Figures 2-4</ref> are likely to continue to play a role in that final interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Synaptic Learning Rule</head><p>Our neuron model requires two changes to the learning rules by which most neural models learn. First, learning occurs by growing and removing synapses from a pool of "potential" synapses <ref type="bibr" target="#b3">(Chklovskii et al., 2004)</ref>. Second, Hebbian learning and synaptic change occur at the level of the dendritic segment, not the entire neuron <ref type="bibr" target="#b56">(Stuart and Häusser, 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Potential synapses</head><p>For a neuron to recognize a pattern of activity it requires a set of co-located synapses (typically 15-20) that connect to a subset of the cells that are active in the pattern to be recognized. Learning to recognize a new pattern is accomplished by the formation of a set of new synapses collocated on a dendritic segment. are not yet learned. Each sequence element invokes a sparse set of mini-columns, only three in this illustration. All the cells in a mini-column become active if the input is unexpected, which is the case prior to learning the sequences. (C) After learning the two sequences, the inputs invoke the same mini-columns but only one cell is active in each column, labeled B ′ , B ′′ , C ′ , C ′′ , D ′ , and Y ′′ . Because C ′ and C ′′ are unique, they can invoke the correct high-order prediction of either Y or D.</p><p>FIGURE 3 | Basal connections to nearby neurons predict the next input. (A) Using one of the sequences from <ref type="figure" target="#fig_0">Figure 2</ref>, both active cells (black) and depolarized/predicted cells (red) are shown. The first panel shows the unexpected input A, which leads to a prediction of the next input B ′ (second panel). If the subsequent input matches the prediction then only the depolarized cells will become active (third panel), which leads to a new prediction (fourth panel). The lateral synaptic connections used by one of the predicted cells are shown in the rightmost panel. In a realistic network every predicted cell would have 15 or more connections to a subset of a large population of active cells. (B) Ambiguous sub-sequence "BC" (which is part of both ABCD and XBCY) is presented to the network. The first panel shows the unexpected input B, which leads to a prediction of both C ′ and C ′′ . The third panel shows the system after input C. Both sets of predicted cells become active, which leads to predicting both D and Y (fourth panel). In complex data streams there are typically many simultaneous predictions. <ref type="figure" target="#fig_1">Figure 5</ref> shows how we model the formation of new synapses in a simulated HTM neuron. For each dendritic segment we maintain a set of "potential" synapses between the dendritic segment and other cells in the network that could potentially form a synapse with the segment <ref type="bibr" target="#b3">(Chklovskii et al., 2004)</ref>. The number of potential synapses is larger than the number of actual synapses. We assign each potential synapse a scalar value called "permanence" which represents stages of growth of the synapse. A permanence value close to zero represents an axon and dendrite with the potential to form a synapse but that have not commenced growing one. A 1.0 permanence value represents an axon and dendrite with a large fully formed synapse.</p><p>The permanence value is incremented and decremented using a Hebbian-like rule. If the permanence value exceeds a threshold, such as 0.3, then the weight of the synapse is 1, if the permanence value is at or below the threshold then the weight of the synapse is 0. The threshold represents the establishment of a synapse, albeit one that could easily disappear. A synapse with a permanence value of 1.0 has the same effect as a synapse with a permanence value at threshold but is not as easily forgotten. Using a scalar permanence value enables on-line learning in the presence of noise. A previously unseen input pattern could be noise or it could be the start of a new trend that will repeat in the future. By growing new synapses, the network can start to learn a new FIGURE 4 | Feedback to apical dendrites predicts entire sequences. This figure uses the same network and representations as <ref type="figure" target="#fig_0">Figure 2</ref>. Area labeled "apical dendrites" is equivalent to layer 1 in neocortex; the apical dendrites (not shown) from all the cells terminate here. In the figure, the following assumptions have been made. The network has previously learned the sequence ABCD as was illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. A constant feedback pattern was presented to the apical dendrites during the learned sequence, and the cells that participate in the sequence B ′ C ′ D ′ have formed synapses on their apical dendrites to recognize the constant feedback pattern. After the feedback connections have been learned, presentation of the feedback pattern to the apical dendrites is simultaneously recognized by all the cells that would be active sequentially in the sequence. These cells, shown in red, become depolarized (left pane). When a new feedforward input arrives it will lead to the sparse representation relevant to the predicted sequence (middle panel). If a feedforward pattern cannot be interpreted as part of the expected sequence (right panel) then all cells in the selected columns become active indicative of an anomaly. In this manner apical feedback biases the network to interpret any input as part of an expected sequence and detects if an input does not match any one of the elements in the expected sequence.</p><p>pattern when it is first encountered, but only act differently after several presentations of the new pattern. Increasing permanence beyond the threshold means that patterns experienced more than others will take longer to forget.</p><p>HTM neurons and HTM networks rely on distributed patterns of cell activity, thus the activation strength of any one neuron or synapse is not very important. Therefore, in HTM simulations we model neuron activations and synapse weights with binary states. Additionally, it is well known that biological synapses are stochastic <ref type="bibr" target="#b8">(Faisal et al., 2008)</ref>, so a neocortical theory cannot require precision of synaptic efficacy. Although scalar states and weights might improve performance, they are not required from a theoretical point of view and all of our simulations have performed well without them. The formal learning rules used in our HTM network simulations are presented in the Materials and Methods section. <ref type="figure">Figure 6</ref> illustrates the performance of a network of HTM neurons implementing a high-order sequence memory. The network used in <ref type="figure">Figure 6</ref> consists of 2048 mini-columns with 32 neurons per mini-column. Each neuron has 128 basal dendritic segments, and each dendritic segment has up to 40 actual synapses. Because this simulation is designed to only illustrate properties of sequence memory it does not include apical synapses. The network exhibits all five of the desired properties for sequence memory listed earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIMULATION RESULTS</head><p>Although we have applied HTM networks to many types of real-world data, in <ref type="figure">Figure 6</ref> we use an artificial data set to more clearly illustrate the network's properties. The input is a stream of elements, where every element is converted to a 2% sparse activation of mini-columns (40 active columns out of 2048 total). The network learns a predictive model of the data based on observed transitions in the input stream. In <ref type="figure">Figure 6</ref> the data stream fed to the network contains a mixture of random elements and repeated sequences. The embedded sequences are six elements long and require high-order temporal context for full disambiguation and best prediction accuracy, e.g., "XABCDE" and "YABCFG." For this simulation we designed the input data stream such that the maximum possible average prediction accuracy is 50% and this is only achievable by using high-order representations. <ref type="figure">Figure 6A</ref> illustrates on-line learning and high-order predictions. The prediction accuracy of the HTM network over time is shown in red. The prediction accuracy starts at zero and increases as the network discovers the repeated temporal patterns mixed within the random transitions. For comparison, the accuracy of a first-order network (created by using only one cell per column) is shown in blue. After sufficient learning, the high-order HTM network achieves the maximum possible prediction accuracy of 50% whereas the first-order network only achieves about 33% accuracy. After the networks reached their maximum performance the embedded sequences were modified. The accuracy drops at that point, but since the network is continually learning it recovers by learning the new high-order patterns. <ref type="figure">Figure 6B</ref> illustrates the robustness of the network. After the network reached stable performance we inactivated a random selection of neurons. At up to about 40% cell death there was FIGURE 6 | Simulation results of the sequence memory network. The input stream used for this figure contained high-order sequences mixed with random elements. The maximum possible average prediction accuracy of this data stream is 50%. (A) High-order on-line learning. The red line shows the network learning and achieving maximum possible performance after about 2500 sequence elements. At element 3000 the sequences in the data stream were changed. Prediction accuracy drops and then recovers as the model learns the new temporal structure. For comparison, the lower performance of a first-order network is shown in blue. (B) Robustness of the network to damage. After the network reached stable performance we inactivated a random selection of neurons. At up to 40% cell death there is almost no impact on performance. At greater than 40% cell death the performance of the network declines but then recovers as the network relearns using remaining neurons.</p><p>minimal impact on performance. This robustness is due to the noise tolerance described earlier that occurs when a dendritic segment forms more synapses than necessary to generate an NMDA spike. At higher levels of cell death the network performance initially declines but then recovers as the network relearns the patterns using the remaining neurons.</p><p>As implied in part by <ref type="figure">Figure 6B</ref> the model is highly robust and fairly insensitive to various parameter settings. The most critical parameters are the dendritic spike threshold and the number of synapses stored per pattern. When setting these parameters it is important to keep in mind the tables in S1 Text, where we list parameters associated with low error rates. In particular it is important to ensure the dendritic threshold and synapses per pattern are high enough (at least 12 and 20, respectively). As discussed earlier, these numbers correspond closely to experimental literature. A more detailed discussion of these equations can be found in <ref type="bibr" target="#b0">Ahmad and Hawkins (2016)</ref>. Section Materials and Methods includes the specific parameters used in these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head><p>We presented a model pyramidal neuron that is substantially different than model neurons used in most artificial neural networks. The key features of the model neuron are its use of active dendrites and multiple synaptic integration zones <ref type="bibr">(proximal, basal, and apical)</ref>. Active dendrites permit the neuron to reliably recognize hundreds of independent patterns in large populations of cells. The synaptic integration zones play functionally unique roles enabling a neuron to predict transitions and sequences in cell activity. In this model only the proximal synapses lead directly to action potentials, patterns detected on the basal and apical dendrites depolarize the cell, representing predictions.</p><p>We showed that a network of these neurons, when combined with fast local inhibition, learns sequences in streams of data. Basal synapses detect contextual patterns that predict the next feedforward input. Apical synapses detect feedback patterns that predict entire sequences. The operation of the neuron and the network rely on neural activity being sparse. The sequence memory model learns continuously, uses variable amounts of temporal context to make predictions, can make multiple simultaneous predictions, uses only local learning rules, and is robust to failure of network elements, noise, and pattern variation.</p><p>It has been suggested that a neuron with active dendrites can be equivalently modeled with a multi-layer perceptron <ref type="bibr" target="#b45">(Poirazi et al., 2003)</ref>. Thus, the functional and theoretical benefits of active dendrites were unclear. The sequence memory model described in this paper suggests such a benefit by assigning unique roles to the different synaptic integration zones. For example, our model pyramidal neuron is directly activated only by patterns detected on proximal synapses whereas it maintains longer lasting subthreshold depolarization for patterns detected on basal and apical dendrites. Also, inhibitory effects of the network do not apply equally to the different synaptic integration zones. And finally, the unsupervised learning rules are different and operate at different time scales depending on the integration zone. Although one could conceivably create a circuit of standard perceptron-like neurons that encompasses all of these operations, we suggest that using a neuron model containing active dendrites and unique integration zones is a more elegant and parsimonious approach. It also more closely reflects the underlying biology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship to Previous Models</head><p>It is instructive to compare our proposed biological sequence memory mechanism to other sequence memory techniques used in the field of machine learning. The most common technique is Hidden Markov Models (HMMs) <ref type="bibr" target="#b47">(Rabiner and Juang, 1986)</ref>.</p><p>HMMs are widely applied, particularly in speech recognition. The basic HMM is a first-order model and its accuracy would be similar to the first-order model shown in <ref type="figure">Figure 6A</ref>.</p><p>Variations of HMMs can model restricted high-order sequences by encoding high-order states by hand. Time delay neural networks (TDNNs) <ref type="bibr" target="#b61">(Waibel, 1989</ref>) allow a feedforward neural network to handle a limited subset of high-order sequences by explicitly incorporating delayed inputs. More recently, recurrent neural networks, specifically long short-term memory (LSTM) <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber, 1997)</ref>, have become popular, often outperforming HMMs and TDNNs. Unlike HTM networks, neither HMMs, TDNNs, nor LSTMs attempt to model biology in any detail; as such they provide little insight into neuronal or neocortical functions. The primary functional advantages of the HTM model over both these techniques are its ability to learn continuously, its superior robustness, and its ability to make multiple simultaneous predictions. A more detailed comparison can be found in S1 <ref type="table">Table and</ref> in <ref type="bibr" target="#b6">Cui et al. (2015)</ref>.</p><p>There are a number of biologically motivated sequence memory models that are related. Coincidence detection in pyramidal cells has been discussed in the context of temporal thalamocortical binding and 40 Hz oscillations <ref type="bibr" target="#b32">(Llinás et al., 1994)</ref>. A number of papers have studied spiking neuron models <ref type="bibr" target="#b34">(Maass, 1997;</ref><ref type="bibr" target="#b7">Deneve, 2008;</ref><ref type="bibr" target="#b9">Ghosh-Dastidar and Adeli, 2009;</ref><ref type="bibr" target="#b19">Jahnke et al., 2015)</ref> in the context of sequence learning. These models are more biophysically detailed than the neuron models used in the machine learning literature. They show how spike-timing-dependent plasticity (STDP) can lead to a cell becoming responsive to a particular sequence of presynaptic spikes and to a specific time delay between each spike <ref type="bibr" target="#b51">(Ruf and Schmitt, 1997;</ref><ref type="bibr" target="#b49">Rao and Sejnowski, 2000;</ref><ref type="bibr" target="#b11">Gütig and Sompolinsky, 2006)</ref>. <ref type="bibr" target="#b39">Memmesheimer et al. (2014)</ref> show that a number of precisely timed sequences can be learned and replayed, with applications in modeling the rich vocal outputs of songbirds. These models are generally restricted to Markovian (nonhigh order) sequences and have not been applied to complex real-world tasks.</p><p>In general spiking neuron models are at a lower level of detail than the HTM model proposed in this paper. They explicitly model integration times of postsynaptic potentials and the corresponding time delays are typically sub-millisecond to a few milliseconds. They also typically deal with a very small subset of the synapses on a neuron and do not explicitly model nonlinear active dendrites or multiple synaptic integration zones (but see <ref type="bibr" target="#b30">Legenstein and Maass, 2011)</ref>. The focus of our work is at a larger scale. The work presented in this paper models neurons with full sets of synapses, active dendrites, and multiple synaptic integration zones. The networks encompass tens of thousands of neurons arranged in columns and layers. The resulting model is a computationally sophisticated sequence memory that can be applied to real world problems <ref type="bibr" target="#b6">(Cui et al., 2015)</ref>. One limitation of the HTM model presented in this paper is that it does not deal with the specific timing of sequences. An interesting direction for future research therefore is to connect these two levels of modeling, i.e., to create biophysically detailed models that operate at the level of a complete layer of cells. Some progress is reported in <ref type="bibr" target="#b2">Billaudelle and Ahmad (2015)</ref>, but there remains much to do on this front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Capacity and Generalization</head><p>There exists significant past work on understanding the capacity of systems with linear and non-linear neurons and computing elements <ref type="bibr" target="#b5">(Cover, 1965;</ref><ref type="bibr" target="#b59">Vapnik et al., 1994)</ref>, and their corresponding error rates <ref type="bibr" target="#b12">(Haussler, 1988</ref><ref type="bibr" target="#b13">(Haussler, , 1990</ref>. Sparse neural systems have been studied in <ref type="bibr" target="#b20">Kanerva (1988)</ref>, <ref type="bibr" target="#b42">Olshausen and Field (1997)</ref> as well as more recent work such as <ref type="bibr" target="#b19">Jahnke et al. (2015)</ref>. The literature to date has not included a complete characterization of error rates of sparse representations with parameters that correspond to cortical neurons. In this paper we extend previous work to show that it is possible to reliably recognize high dimensional sparse patterns with the small number of synapses required to initiate NMDA spikes on active dendrites.</p><p>The capacity of various forms of sequence memory has been studied before <ref type="bibr" target="#b54">(Sompolinsky and Kanter, 1986;</ref><ref type="bibr" target="#b50">Riedel et al., 1988;</ref><ref type="bibr" target="#b31">Leibold and Kempter, 2006)</ref>. In our model it is straightforward to obtain an estimate of sequence capacity. Although we refer to the network model as a "sequence memory, " it is actually a memory of transitions. There is no representation or concept of the length of sequences or of the number of stored sequences. The network only learns transitions between inputs. Therefore, the capacity of a network is measured by how many transitions a given network can store. This can be calculated as the product of the expected duty cycle of an individual neuron (cells per column/column sparsity) times the number of patterns each neuron can recognize on its basal dendrites. For example, a network where 2% of the columns are active, each column has 32 cells, and each cell recognizes 200 patterns on its basal dendrites, can store approximately 320,000 transitions ((32/0.02) * 200). The capacity scales linearly with the number of cells per column and the number of patterns recognized by the basal synapses of each neuron.</p><p>Our model enables the representation of complex high order (non-Markovian) sequences. The model can automatically learn very long-range temporal dependencies. As such, an important capacity metric is how many times a particular input can appear in different temporal contexts without confusion. This is analogous to how many times a particular musical interval can appear in melodies without confusion, or how many times a particular word can be memorized in different sentences. If minicolumns have 32 cells it does not mean a particular pattern can have only 32 different representations. For example, if we assume 40 active columns per input, 32 cells per column, and one active cell per column, then there are 32 40 possible representations of each input pattern, a practically unlimited number. Therefore, the practical limit is not representational but memory-based. The capacity is determined by how many transitions can be learned with a particular sparse set of columns.</p><p>So far we have only discussed cellular layers where all cells in the network can potentially connect to all other cells with equal likelihood. This works well for small networks but not for large networks. In the neocortex, it is well known that most regions have a topological organization. For example cells in region V1 receive feedforward input from only a small part of the retina and receive lateral input only from a local area of V1. HTM networks can be configured this way by arranging the columns in a 2D array and selecting the potential synapses for each dendrite using a 2D probability distribution centered on the neuron. Topologically organized networks can be arbitrarily large.</p><p>A key consideration in learning algorithms is the issue of generalization, or the ability to robustly deal with novel patterns. The sequence memory mechanism we have outlined learns by forming synapses to small samples of active neurons in streams of sparse patterns. The properties of sparse representations naturally allow such a system to generalize. Two randomly selected sparse patterns will have very little overlap. Even a small overlap (such as 20%) is highly significant and implies that the representations share significant semantic meaning. Dendritic thresholds are lower than the actual number of synapses on each segment, thus segments will recognize novel but semantically related patterns as similar. The system will see similarity between different sequences and make novel predictions based on analogy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testable Predictions</head><p>There are several testable predictions that follow from this theory.</p><p>(1) The theory provides an algorithmic explanation for the experimentally observed phenomenon that overall cell activity becomes sparser during a continuous predictable sensory stream <ref type="bibr" target="#b60">(Vinje and Gallant, 2002;</ref><ref type="bibr" target="#b62">Yen et al., 2007;</ref><ref type="bibr" target="#b37">Martin and Schröder, 2013)</ref>. In addition, it predicts that unanticipated inputs will result in higher cell activity, which should be correlated vertically within mini-columns. Anticipated inputs on the other hand will result in activity that is uncorrelated within mini-columns. It is worth noting that mini-columns are not a strict requirement of this theory. The model only requires the presence of small groups of cells that share feedforward responses and that are mutually inhibitory. We refer to these groups as mini-columns, but the columnar aspect is not a requirement, and the groupings could be independent of actual mini-columns. (2) A second core prediction of the theory is that the current pattern of cell activity contains information about past stimuli. Early experimental results supporting this prediction have been reported in <ref type="bibr" target="#b41">Nikolic et al. (2009)</ref>. Further studies are required to validate the exact nature of dynamic cell activity and the role of temporal context in high order sequences.</p><p>(3) Synaptic plasticity should be localized to dendritic segments that have been depolarized via synaptic input followed a short time later by a back action potential. This effect has been reported <ref type="bibr" target="#b33">(Losonczy et al., 2008)</ref>, though the phenomenon has yet to be widely established. (4) There should be few, ideally only one, excitatory synapses formed between a given axon and a given dendritic segment.</p><p>If an excitatory axon made many synapses in close proximity onto a single dendrite then the presynaptic cell would dominate in causing an NMDA spike. Two, three, or even four synapses from a single axon onto a single dendritic segment could be tolerated, but if axons routinely made more synapses to a single dendritic segment it would lead to errors. Pure Hebbian learning would seem to encourage forming multiple synapses. To prevent this from happening we predict the existence of a mechanism that actively discourages the formation of a multiple synapses after one has been established. An axon can form synapses onto different dendritic segments of the same neuron without causing problems, therefore we predict this mechanism will be spatially localized within dendritic segments or to a local area of an axonal arbor. (5) When a cell depolarized by an NMDA spike subsequently generates an action potential via proximal input, it needs to inhibit all other nearby excitatory cells. This requires a fast, probably single spike, inhibition. Fast-spiking basket inhibitory cells are the most likely source for this rapid inhibition <ref type="bibr" target="#b18">(Hu et al., 2014</ref>). (6) All cells in a mini-column need to learn common feedforward responses. This requires a mechanism to encourage all the cells in a mini-column to become active simultaneously while learning feedforward patterns. This requirement for mutual excitation seems at odds with the prior requirement for mutual inhibition when one or more cells are slightly depolarized. We do not have a specific proposal for how these two requirements are met but we predict a mechanism where sometimes cells in a column are mutually excited and at other times they are mutually inhibited.</p><p>Different cortical regions and different layers are known to have variations. Cells in different layers may have different dendritic lengths and numbers of synapses <ref type="bibr" target="#b57">(Thomson and Bannister, 2003)</ref>. We predict that despite these differences all excitatory neurons are learning transitions or sequences. Neurons with fewer dendritic segments and fewer synapses would have learned fewer transitions. We suggest variations in the number of dendrites and synapses are largely a result of the number of discoverable patterns in the data and less to do with functional differences in the neurons themselves. Pyramidal neurons are common in the hippocampus. Hence, parts of our neuron and network models might apply to the hippocampus. However, the hippocampus is known for fast learning, which is incompatible with growing new synapses, as synapse formation can take hours in an adult <ref type="bibr" target="#b22">(Knott et al., 2002;</ref><ref type="bibr" target="#b58">Trachtenberg et al., 2002;</ref><ref type="bibr" target="#b40">Niell et al., 2004;</ref><ref type="bibr" target="#b17">Holtmaat and Svoboda, 2009)</ref>. Rapid learning could be achieved in our model if instead of growing new synapses, a cell had a multitude of inactive, or "silent" synapses <ref type="bibr" target="#b21">(Kerchner and Nicoll, 2008)</ref>. Rapid learning would then occur by turning silent synapses into active synapses. The downside of this approach is a cell would need many more synapses, which is metabolically expensive. Pyramidal cells in hippocampal region CA2 have several times the number of synapses as pyramidal cells in neocortex <ref type="bibr" target="#b38">(Megías et al., 2001)</ref>. If most of these synapses were silent it would be evidence to suggest that region CA2 is also implementing a variant of our proposed sequence memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MATERIALS AND METHODS</head><p>Here we formally describe the activation and learning rules for an HTM sequence memory network. There are three basic aspects to the rules: initialization, computing cell states, and updating synapses on dendritic segments. These steps are described below, along with notation and some implementation details.</p><p>Notation: Let N represent the number of mini-columns in the layer, M the number of cells per column, and NM the total number of cells in the layer. Each cell can be in an active state, in a predictive (depolarized) state, or in a non-active state. Each cell maintains a set of segments each with a number of synapses. (In this figure we use the term "synapse" to refer to "potential synapses" as described in the body of the paper. Thus, at any point in time some of the synapses will have a weight of 0 and some will have a weight of 1.) At any time step t, the set of active cells is represented by the M×N binary matrix A t , where a t ij is the activity of the i'th cell in the j'th column. Similarly, the M×N binary matrix t denotes cells in a predictive state at time t, where π t ij is the predictive state of the i'th cell in the j'th column. Each cell is associated with a set of distal segments, D ij , such that D d ij represents the d'th segment of the i'th cell in the j'th column. Each distal segment contains a number of synapses, representing lateral connections from a subset of the other NM − cells. Each synapse has an associated permanence value (see <ref type="figure" target="#fig_1">Figure 5)</ref>. Therefore, D d ij itself is also an M × N sparse matrix. If there are s potential synapses associated with the segment, the matrix contains s non-zero elements representing permanence values. A synapse is considered connected if its permanence value is above a connection threshold. We use D d ij to denote a binary matrix containing only the connected synapses.</p><p>(1) Initialization: the network is initialized such that each segment contains a set of potential synapses (i.e., with nonzero permanence value) to a randomly chosen subset of cells in the layer. The permanence values of these potential synapses are chosen randomly: initially some are connected (above threshold) and some are unconnected. (2) Computing cell states: All the cells in a mini-column share the same feed forward receptive fields. We assume that an inhibitory process has already selected a set of k columns that best match the current feed forward input pattern. We denote this set as W t . The active state for each cell is calculated as follows:</p><formula xml:id="formula_1">a t ij =     </formula><p>1 if j ∈ W t and π t−1 ij = 1 1 if j ∈ W t and i π t−1 ij = 0 0 otherwise</p><p>(2)</p><p>The first line will activate a cell in a winning column if it was previously in a predictive state. If none of the cells in a winning column were in a predictive state, the second line will activate all cells in that column. The predictive state for the current time step is then calculated as follows:</p><formula xml:id="formula_2">π t ij = 1 if ∃ d D d ij • A t 1 &gt; θ 0 otherwise<label>(3)</label></formula><p>Threshold θ represents the NMDA spiking threshold and • represents element-wise multiplication. At a given point in time, if there are more than θ connected synapses with active presynaptic cells, then that segment will be active (generate an NMDA spike). A cell will be depolarized if at least one segment is active.</p><p>(3) Updating segments and synapses: the HTM synaptic plasticity rule is a Hebbian-like rule. If a cell was correctly predicted (i.e., it was previously in a depolarized state and subsequently became active via feedforward input), we reinforce the dendritic segment that was active and caused the depolarization. Specifically, we choose those segments D d ij such that:</p><formula xml:id="formula_3">∀ j∈W t π t−1 ij &gt; 0 and D d ij • A t−1 1 &gt; θ<label>(4)</label></formula><p>The first term selects winning columns that contained correct predictions. The second term selects those segments specifically responsible for the prediction. If a winning column was unpredicted, we need to select one cell that will represent the context in the future if the current sequence transition repeats. To do this we select the cell with the segment that was closest to being active, i.e., the segment that had the most input even though it was below threshold. LetḊ d ij denote a binary matrix containing only the positive entries in D d ij . We reinforce a segment where the following is true:</p><formula xml:id="formula_4">∀ j∈W t i π t−1 ij = 0 and Ḋ d ij • A t−1 1 = max i ( Ḋ d ij • A t−1 1 )<label>(5)</label></formula><p>Reinforcing the above segments is straightforward: we wish to reward synapses with active presynaptic cells and punish synapses with inactive cells. To do that we decrease all the permanence values by a small value p − and increase the permanence values corresponding to active presynaptic cells by a larger value p + :</p><formula xml:id="formula_5">△D d ij = p + Ḋ d ij • A t−1 − p −Ḋd ij<label>(6)</label></formula><p>The above rules deal with cells that are currently active. We also apply a very small decay to active segments of cells that did not become active. This can happen if segments were mistakenly reinforced by chance:</p><formula xml:id="formula_6">△D d ij = p −−Ḋd</formula><p>ij where a t ij = 0 and D d ij</p><formula xml:id="formula_7">• A t−1 1 &gt; θ<label>(7)</label></formula><p>The matrices △D d ij are added to the current matrices of permanence values at every time step.</p><p>Implementation details: in our software implementation, we make some simplifying assumptions that greatly speed up simulation time for larger networks. Instead of explicitly initializing a complete set of synapses across every segment and every cell, we greedily create segments on a random cell and initialize potential synapses on that segment by sampling from currently active cells. This happens only when there is no match to any existing segment. In our simulations N = 2048, M = 32, k = 40. We typically randomly connect between 20 and 40 synapses on a segment, and θ is around 15. Permanence values vary from 0 to 1 with a connection threshold of 0.5. p + and p − are small values that are tuned based on the individual dataset but typically less than 0.1. The full source code for the implementation is available on Github at https://github. com/numenta/nupic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHOR CONTRIBUTIONS</head><p>JH conceived of the overall theory and the detailed mapping to neuroscience, helped design the simulations, and wrote most of the paper. SA provided feedback on the theory, created the mathematical framework for computing error in sparse representations, created the mathematical formulation of the algorithm, designed and implemented the simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUNDING</head><p>Numenta is a privately held company. Its funding sources are independent investors and venture capitalists.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 2 |</head><label>2</label><figDesc>Representing sequences in cortical cellular layers. (A) The neocortex is divided into cellular layers. The panels in this figure show part of one generic cellular layer. For clarity, the panels only show 21 mini-columns with 6 cells per column. (B) Input sequences ABCD and XBCY</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 5 |</head><label>5</label><figDesc>Learning by growing new synapses. Learning in an HTM neuron is modeled by the growth of new synapses from a set of potential synapses. A "permanence" value is assigned to each potential synapse and represents the growth of the synapse. Learning occurs by incrementing or decrementing permanence values. The synapse weight is a binary value set to 1 if the permanence is above a threshold.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Frontiers in Neural Circuits | www.frontiersin.orgMarch 2016 | Volume 10 | Article 23</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Frontiers in Neural Circuits | www.frontiersin.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">March 2016 | Volume 10 | Article 23</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Matthew Larkum, Murray Sherman, Mike Merzenich, and Ahmed El Hady for their inspiring research and help with this manuscript. We also thank numerous collaborators at Numenta over the years for many discussions, especially Yuwei Cui for his extensive help in editing and improving this manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head><p>The Supplementary Material for this article can be found online at: http://journal.frontiersin.org/article/10.3389/fncir.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2016.00023</head><p>Conflict of Interest Statement: The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p><p>Numenta has some patents relevant to the work. Numenta has stated that use of its intellectual property, including all the ideas contained in this work, is free for non-commercial research purposes. In addition Numenta has released all pertinent source code as open source under a GPL V3 license (which includes a patent peace provision).</p><p>Copyright © 2016 Hawkins and Ahmad. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hawkins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00720</idno>
		<title level="m">How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>q-bio.NC</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The decade of the dendritic NMDA spike</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Short</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Ikonomu</surname></persName>
		</author>
		<idno type="DOI">10.1002/jnr.22444</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Res</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="2991" to="3001" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Billaudelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02142</idno>
		<title level="m">Porting HTM models to the Heidelberg neuromorphic computing platform</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>q-bio.NC</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cortical rewiring and information storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Mel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Svoboda</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature03012</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">431</biblScope>
			<biblScope unit="page" from="782" to="788" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Branch-specific dendritic Ca 2+ spikes cause persistent synaptic plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cichon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-B</forename><surname>Gan</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14251</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">520</biblScope>
			<biblScope unit="page" from="180" to="185" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<idno type="DOI">10.1109/PGEC.1965.264137</idno>
	</analytic>
	<monogr>
		<title level="j">Electron. Comput. IEEE Trans. EC</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="326" to="334" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Continuous online sequence learning with an unsupervised neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Surpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hawkins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05463</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.NE</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian spiking neurons II: learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deneve</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2008.20.1.118</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="118" to="145" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Noise in the nervous system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P J</forename><surname>Selen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn2258</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="292" to="303" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spiking neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh-Dastidar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adeli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1142/S0129065709002002</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="295" to="308" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dendritic calcium spike initiation and repolarization are controlled by distinct potassium channel subtypes in CA1 pyramidal neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mickus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Spruston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="8789" to="8798" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The tempotron: a neuron that learns spike timing-based decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gütig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn1643</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="420" to="428" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantifying inductive bias: AI learning algorithms and Valiant&apos;s learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<idno type="DOI">10.1016/0004-3702</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">88</biblScope>
			<biblScope unit="page" from="90002" to="90003" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Probably approximately correct learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="1101" to="1108" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">AAAI-90</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cortical learning algorithm and hierarchical temporal memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dubinsky</surname></persName>
		</author>
		<ptr target="http://numenta.org/resources/HTM_CorticalLearningAlgorithms.pdf" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>in Numenta Whitepaper, 1-68. Available online at</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blakeslee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Times Books Henry Holt and Company</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Experience-dependent structural synaptic plasticity in the mammalian brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holtmaat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Svoboda</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn2699</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="647" to="658" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast-spiking, parvalbumin+ GABAergic interneurons: from cellular design to microcircuit function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="DOI">10.1126/science.1255263</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="page" from="1255263" to="1255263" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A unified dynamic model for learning, replay, and sharp-wave/ripples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jahnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Timme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-M</forename><surname>Memmesheimer</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.3977-14.2015</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="16236" to="16258" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kanerva</surname></persName>
		</author>
		<title level="m">Sparse Distributed Memory</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Silent synapses and the emergence of a postsynaptic mechanism for LTP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Kerchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Nicoll</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn2501</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="813" to="825" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Formation of dendritic spines with GABAergic synapses induced by whisker stimulation in adult mice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quairiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Genoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Welker</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0896-6273(02</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="663" to="666" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feedforward, horizontal, and feedback processing in the visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Lamme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Supèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Spekreijse</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0959-4388(98)80042-1</idno>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="529" to="535" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larkum</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tins.2012.11.006</idno>
	</analytic>
	<monogr>
		<title level="j">Trends Neurosci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Synaptic clustering by dendritic signalling mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Larkum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nevian</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conb.2008.08.013</idno>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Synaptic integration in tuft dendrites of layer 5 pyramidal neurons: a new unifying principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Larkum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nevian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schiller</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1171958</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">325</biblScope>
			<biblScope unit="page" from="756" to="760" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new cellular mechanism for coupling inputs arriving at different cortical layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Larkum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sakmann</surname></persName>
		</author>
		<idno type="DOI">10.1038/18686</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">398</biblScope>
			<biblScope unit="page" from="338" to="341" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The problem of serial order in behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lashley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cerebral Mechanisms in Behavior</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1951" />
			<biblScope unit="page" from="112" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Branch-specific plasticity enables selforganization of nonlinear computation in single neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.5684-10.2011</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="10787" to="10802" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Memory capacity for sequences in a recurrent network with biological constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leibold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kempter</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.2006.18.4.904</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="904" to="941" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Content and context in temporal thalamocortical binding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Llinás</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ribary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Temporal Coding in the Brain</title>
		<editor>G. Buzsáki, R. Llinás, W. Singer, A. Berthoz, and Y. Christen</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="251" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compartmentalized dendritic plasticity and input feature storage in neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Losonczy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Makara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Magee</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature06725</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="page" from="436" to="441" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Networks of spiking neurons: the third generation of neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0893-6080</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">97</biblScope>
			<biblScope unit="page" from="11" to="18" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Active properties of neocortical pyramidal neuron dendrites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Larkum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schiller</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-neuro-062111-150343</idno>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spatiotemporally graded NMDA spike/plateau potentials in basal dendrites of neocortical pyramidal neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Tank</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.00011.2008</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="2584" to="2601" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Functional heterogeneity in neighboring neurons of cat primary visual cortex in response to both artificial and natural stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schröder</surname></persName>
		</author>
		<idno type="DOI">10.1523/JNEUROSCI.4071-12.2013</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7325" to="7344" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Total number and distribution of inhibitory and excitatory synapses on hippocampal CA1 pyramidal cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Megías</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Emri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Gulyás</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0306-4522(00</idno>
	</analytic>
	<monogr>
		<title level="j">Neuroscience</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="496" to="502" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning precisely timed spikes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Memmesheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ölveczky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2014.03.026</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="925" to="938" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">In vivo imaging of synapse formation on a growing dendritic arbor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Niell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn1191</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="254" to="260" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distributed fading memory for stimulus properties in the primary visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nikolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Häusler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.1000260</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1000260</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sparse coding with an overcomplete basis set: a strategy employed by V1? Vision Res</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0042-6989(97</idno>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sparse coding of sensory inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conb.2004.07.007</idno>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="481" to="487" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The subcellular organization of neocortical excitatory connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petreanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Sternson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Svoboda</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature07709</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">457</biblScope>
			<biblScope unit="page" from="1142" to="1145" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pyramidal neuron as two-layer neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename></persName>
		</author>
		<idno type="DOI">10.1016/S0896-6273(03)00149-1</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="989" to="999" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Computational subunits in thin dendrites of pyramidal cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Mel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schiller</surname></persName>
		</author>
		<idno type="DOI">10.1038/nn1253</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="621" to="627" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An introduction to hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<idno type="DOI">10.1109/MASSP.1986.1165342</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Mag</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4" to="16" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Thalamocortical input onto layer 5 pyramidal neurons measured using quantitative large-scale array tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Rah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Colonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mishchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Fetter</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncir.2013.00177</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neural Circuits</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">177</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Predictive sequence learning in recurrent neocortical circuits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="164" to="170" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Temporal sequences and chaos in neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kühn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Hemmen</surname></persName>
		</author>
		<idno type="DOI">10.1103/physreva.38.1105</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1105" to="1108" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning temporally encoded patterns in networks of spiking neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ruf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1009697008681</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="9" to="18" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">NMDA spikes in basal dendrites of cortical pyramidal neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Koester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schiller</surname></persName>
		</author>
		<idno type="DOI">10.1038/35005094</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="page" from="285" to="289" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">NMDA receptor-mediated dendritic spikes and coincident signal amplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schiller</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0959-4388(00)00217-8</idno>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="343" to="348" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Temporal association in asymmetric neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kanter</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.57.2861</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="2861" to="2864" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pyramidal neurons: dendritic structure and synaptic integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Spruston</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrn2286</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="206" to="221" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Dendritic coincidence detection of EPSPs and action potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Häusser</surname></persName>
		</author>
		<idno type="DOI">10.1038/82910</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="63" to="71" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Interlaminar connections in the neocortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Bannister</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/13.1.5</idno>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="5" to="14" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Long-term in vivo imaging of experience-dependent synaptic plasticity in adult cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Trachtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Knott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Sanes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Welker</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature01273</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">420</biblScope>
			<biblScope unit="page" from="788" to="794" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Measuring the VC-dimension of a learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1994.6.5.851</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="851" to="876" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Natural stimulation of the nonclassical receptive field increases information transmission efficiency in V1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Vinje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2904" to="2915" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Modular construction of time-delay neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1989.1.1.39</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Heterogeneity in the responses of adjacent neurons to natural stimuli in cat striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1152/jn.00747.2006</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="1326" to="1341" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Properties of horizontal and vertical inputs to pyramidal cells in the superficial layers of the cat visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1931" to="1940" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
