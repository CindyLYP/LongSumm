<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepMath -Deep Sequence Models for Premise Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-01-26">26 Jan 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
							<email>alemi@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
							<email>fchollet@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Een</surname></persName>
							<email>een@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
							<email>geoffreyi@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
							<email>szegedy@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
							<email>josef.urban@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Czech Technical University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepMath -Deep Sequence Models for Premise Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-01-26">26 Jan 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1606.04442v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the handengineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving on a large scale. * Authors listed alphabetically. All contributions are considered equal. † Supported by ERC Consolidator grant nr. 649043 AI4REASON.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Mathematics underpins all scientific disciplines. Machine learning itself rests on measure and probability theory, calculus, linear algebra, functional analysis, and information theory. Complex mathematics underlies computer chips, transit systems, communication systems, and financial infrastructure -thus the correctness of many of these systems can be reduced to mathematical proofs.</p><p>Unfortunately, these correctness proofs are often impractical to produce without automation, and present-day computers have only limited ability to assist humans in developing mathematical proofs and formally verifying human proofs. There are two main bottlenecks: (1) lack of automated methods for semantic or formal parsing of informal mathematical texts (autoformalization), and (2) lack of strong automated reasoning methods to fill in the gaps in already formalized human-written proofs.</p><p>The two bottlenecks are related. Strong automated reasoning can act as a semantic filter for autoformalization, and successful autoformalization would provide a large corpus of computer-understandable facts, proofs, and theory developments. Such a corpus would serve as both background knowledge to fill in gaps in human-level proofs and as a training set to guide automated reasoning. Such guidance is crucial: exhaustive deductive reasoning tools such as today's resolution/superposition automated theorem provers (ATPs) quickly hit combinatorial explosion, and are unusable when reasoning with a very large number of facts without careful selection <ref type="bibr" target="#b3">[4]</ref>.</p><p>In this work, we focus on the latter bottleneck. We develop deep neural networks that learn from a large repository of manually formalized computer-understandable proofs. We learn the task that is essential for making today's ATPs usable over large formal corpora: the selection of a limited number of most relevant facts for proving a new conjecture. This is known as premise selection.</p><p>The main contributions of this work are:</p><p>• A demonstration for the first time that neural network models are useful for aiding in large scale automated logical reasoning without the need for hand-engineered features. <ref type="table">• The comparison of various network architectures (including convolutional, recurrent and</ref> hybrid models) and their effect on premise selection performance. • A method of semantic-aware "definition"-embeddings for function symbols that improves the generalization of formulas with symbols occurring infrequently. This model outperforms previous approaches. • Analysis showing that neural network based premise selection methods are complementary to those with hand-engineered features: ensembling with previous results produce superior results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Formalization and Theorem Proving</head><p>In the last two decades, large corpora of complex mathematical knowledge have been formalized: encoded in complete detail so that computers can fully understand the semantics of complicated mathematical objects. The process of writing such formal and verifiable theorems, definitions, proofs, and theories is called Interactive Theorem Proving (ITP).</p><p>The ITP field dates back to 1960s <ref type="bibr" target="#b16">[16]</ref> and the Automath system by N.G. de Bruijn <ref type="bibr" target="#b8">[9]</ref>. ITP systems include HOL (Light) <ref type="bibr" target="#b15">[15]</ref>, Isabelle <ref type="bibr" target="#b37">[37]</ref>, Mizar <ref type="bibr" target="#b12">[13]</ref>, Coq <ref type="bibr" target="#b6">[7]</ref>, and ACL2 <ref type="bibr" target="#b23">[23]</ref>. The development of ITP has been intertwined with the development of its cousin field of Automated Theorem Proving (ATP) <ref type="bibr" target="#b31">[31]</ref>, where proofs of conjectures are attempted fully automatically. Unlike ATP systems, ITP systems allow human-assisted formalization and proving of theorems that are often beyond the capabilities of the fully automated systems.</p><p>Large ITP libraries include the Mizar Mathematical Library (MML) with over 50,000 lemmas, and the core Isabelle, HOL, Coq, and ACL2 libraries with thousands of lemmas. These core libraries are a basis for large projects in formalized mathematics and software and hardware verification. Examples in mathematics include the HOL Light proof of the Kepler conjecture (Flyspeck project) <ref type="bibr" target="#b13">[14]</ref>, the Coq proofs of the Feit-Thompson theorem <ref type="bibr" target="#b11">[12]</ref> and Four Color theorem <ref type="bibr" target="#b10">[11]</ref>, and the verification of most of the Compendium of Continuous Lattices in Mizar <ref type="bibr" target="#b1">[2]</ref>. ITP verification of the seL4 kernel <ref type="bibr" target="#b25">[25]</ref> and CompCert compiler <ref type="bibr" target="#b27">[27]</ref> show comparable progress in large scale software verification. While these large projects mark a coming of age of formalization, ITP remains labor-intensive. For example, Flyspeck took about 20 person-years, with twice as much for Feit-Thompson. Behind this cost are our two bottlenecks: lack of tools for autoformalization and strong proof automation.</p><p>Recently the field of Automated Reasoning in Large Theories (ARLT) <ref type="bibr" target="#b35">[35]</ref> has developed, including AI/ATP/ITP (AITP) systems called hammers that assist ITP formalization <ref type="bibr" target="#b3">[4]</ref>. Hammers analyze the full set of theorems and proofs in the ITP libraries, estimate the relevance of each theorem, and apply optimized translations from the ITP logic to simpler ATP formalism. Then they attack new conjectures using the most promising combinations of existing theorems and ATP search strategies. Recent evaluations have proved 40% of all Mizar and Flyspeck theorems fully automatically <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. However, there is significant room for improvement: with perfect premise selection (a perfect choice of library facts) ATPs can prove at least 56% of Mizar and Flyspeck instead of today's 40% <ref type="bibr" target="#b3">[4]</ref>. In the next section we explain the premise selection task and the experimental setting for measuring such improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Premise Selection, Experimental Setting and Previous Results</head><p>Given a formal corpus of facts and proofs expressed in an ATP-compatible format, our task is Definition (Premise selection problem). Given a large set of premises P, an ATP system A with given resource limits, and a new conjecture C, predict those premises from P that will most likely lead to an automatically constructed proof of C by A.</p><p>We use the Mizar Mathematical Library (MML) version 4.181.1147 as the formal corpus and E prover <ref type="bibr" target="#b32">[32]</ref>     This version of MML was used for the latest AITP evaluation reported in <ref type="bibr" target="#b21">[21]</ref>. There are 57,917 proved Mizar theorems and unnamed top-level lemmas in this MML organized into 1,147 articles. This set is chronologically ordered by the order of articles in MML and by the order of theorems in the articles. Proofs of later theorems can only refer to earlier theorems. This ordering also applies to 88,783 other Mizar formulas (encoding the type system and other automation known to Mizar) used in the problems. The formulas have been translated into first-order logic formulas by the MPTP system <ref type="bibr" target="#b34">[34]</ref> (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><formula xml:id="formula_0">(v1_topreal2(A) &amp; m1_subset_1(A, k1_zfmisc_1(u1_struct_0(k15_euclid(2))))) =&gt; v1_jordan1(A)) ) ).</formula><p>Our goal is to automatically prove as many theorems as possible, using at each step all previous theorems and proofs. We can learn from both human proofs and ATP proofs, but previous experiments <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b20">20]</ref> show that learning only from the ATP proofs is preferable to including human proofs if the set of ATP proofs is sufficiently large. Since for 32,524 (56.2%) of the 57,917 theorems an ATP proof was previously found by a combination of manual and learning-based premise selection <ref type="bibr" target="#b21">[21]</ref>, we use only these ATP proofs for training.</p><p>The 40% success rate from <ref type="bibr" target="#b21">[21]</ref> used a portfolio of 14 AITP methods using different learners, ATPs, and numbers of premises. The best single method proved 27.3% of the theorems. Only fast and simple learners such as k-nearest-neighbors, naive Bayes, and their ensembles were used, based on hand-crafted features such as the set of (normalized) sub-terms and symbols in each formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Motivation for the use of Deep Learning</head><p>Strong premise selection requires models capable of reasoning over mathematical statements, here encoded as variable-length strings of first-order logic. In natural language processing, deep neural networks have proven useful in language modeling <ref type="bibr" target="#b28">[28]</ref>, text classification <ref type="bibr" target="#b7">[8]</ref>, sentence pair scoring <ref type="bibr" target="#b2">[3]</ref>, conversation modeling <ref type="bibr" target="#b36">[36]</ref>, and question answering <ref type="bibr" target="#b33">[33]</ref>. These results have demonstrated the ability of deep networks to extract useful representations from sequential inputs without hand-tuned feature engineering. Neural networks can also mimic some higher-level reasoning on simple algorithmic tasks <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b18">18]</ref>.  <ref type="figure">Figure 3</ref>: (left) Our network structure. The input sequences are either character-level (section 5.1) or word-level (section 5.2). We use separate models to embed conjecture and axiom, and a logistic layer to predict whether the axiom is useful for proving the conjecture. (right) A convolutional model.</p><p>The Mizar data set is also an interesting case study in neural network sequence tasks, as it differs from natural language problems in several ways. It is highly structured with a simple context free grammar -the interesting task occurs only after parsing. The distribution of lengths is wide, ranging from 5 to 84,299 characters with mean 304.5, and from 2 to 21,251 tokens with mean 107.4 (see <ref type="figure" target="#fig_1">Figure 2</ref>). Fully recurrent models would have to back-propagate through 100s to 1000s of characters or 100s of tokens to embed a whole statement. Finally, there are many rare words -60.3% of the words occur fewer than 10 times -motivating the definition-aware embeddings in section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Overview of our approach</head><p>The full premise selection task takes a conjecture and a set of axioms and chooses a subset of axioms to pass to the ATP. We simplify from subset selection to pairwise relevance by predicting the probability that a given axiom is useful for proving a given conjecture. This approach depends on a relatively sparse dependency graph. Our general architecture is shown in <ref type="figure">Figure 3</ref>(left): the conjecture and axiom sequences are separately embedded into fixed length real vectors, then concatenated and passed to a third network with two fully connected layers and logistic loss. During training time, the two embedding networks and the joined predictor path are trained jointly.</p><p>As discussed in section 3, we train our models on premise selection data generated by a combination of various methods, including k-nearest-neighbor search on hand-engineered similarity metrics. We start with a first stage of character-level models, and then build second and later stages of word-level models on top of the results of earlier stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Stage 1: Character-level models</head><p>We begin by avoiding special purpose engineering by treating formulas on the character-level using an 80 dimensional one-hot encoding of the character sequence. These sequences are passed to a weight shared network for variable length input. For the embedding computation, we have explored the following architectures:</p><p>1. Pure recurrent LSTM <ref type="bibr" target="#b17">[17]</ref> and GRU <ref type="bibr" target="#b5">[6]</ref> networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A pure multi-layer convolutional network with various numbers of convolutional layers (with strides) followed by a global temporal max-pooling reduction (see <ref type="figure">Figure 3(right)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>A recurrent-convolutional network, that uses convolutional layers to produce a shorter sequence which is processed by a LSTM.</p><p>The exact architectures used are specified in the experimental section.</p><p>It is computationally prohibitive to compute a large number of (conjecture, axiom) pairs due to the costly embedding phase. Fortunately, our architecture allows caching the embeddings for conjectures and axioms and evaluating the shared portion of the network for a given pair. This makes it practical to consider all pairs during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Stage 2: Word-level models</head><p>The character-level models are limited to word and structure similarity within the axiom or conjecture being embedded. However, many of the symbols occurring in a formula are defined by formulas earlier in the corpus, and we can use the axiom-embeddings of those symbols to improve model performance.</p><p>Since Mizar is based on first-order set theory, definitions of symbols can be either explicit or implicit. An explicit definition of x sets x = e for some expression e, while an implicit definition states a property of the defined object, such as defining a function f (x) by ∀x.f (f (x)) = g(x). To avoid manually encoding the structure of implicit definitions, we embed the entire statement defining a symbol f , and then use the stage 1 axiom-embedding corresponding to the whole statement as a word-level embeddings.</p><p>Ideally, we would train a single network that embeds statements by recursively expanding and embedding the definitions of the defined symbols. Unfortunately, this recursion would dramatically increase the cost of training since the definition chains can be quite deep. For example, Mizar defines real numbers in terms of non-negative reals, which are defined as Dedekind cuts of non-negative rationals, which are defined as ratios of naturals, etc. As an inexpensive alternative, we reuse the axiom embeddings computed by a previously trained character-level model, mapping each defined symbol to the axiom embedding of its defining statement. Other tokens such as brackets and operators are mapped to fixed pseudo-random vectors of the same dimension.</p><p>Since we embed one token at a time ignoring the grammatical structure, our approach does not require a parser: a trivial lexer is implemented in a few lines of Python. With word-level embeddings, we use the same architectures with shorter input sequence to produce axiom and conjecture embeddings for ranking the (conjecture, axiom) pairs. Iterating this approach by using the resulting, stronger axiom embeddings as word embeddings multiple times for additional stages did not yield measurable gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>For training and evaluation we use a subset of 32,524 out of 57,917 theorems that are known to be provable by an ATP given the right set of premises. We split off a random 10% of these (3,124 statements) for testing and validation. Also, we held out 400 statements from the 3,124 for monitoring training progress, as well as for model and checkpoint selection. Final evaluation was done on the remaining 2,724 conjectures. Note that we only held out conjectures, but we trained on all statements as axioms. This is comparable to our k-NN baseline which is also trained on all statements as axioms.</p><p>The randomized selection of the training and testing sets may also lead to learning from future proofs: a proof P j of theorem T j written after theorem T i may guide the premise selection for T i . However, previous k-NN experiments show similar performance between a full 10-fold cross-validation and incremental evaluation as long as chronologically preceding formulas participate in proofs of only later theorems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Metrics</head><p>For each conjecture, our models output a ranking of possible premises. Our primary metric is the number of conjectures proved from the top-k premises, where k = 16, 32, . . . , 1024. This metric can accommodate alternative proofs but is computationally expensive. Therefore we additionally measure the ranking quality using the average maximum relative rank of the testing premise set. Formally, average max relative rank is</p><formula xml:id="formula_1">aMRR = mean C max P ∈Ptest(C) rank(P, P avail (C)) |P avail (C)|</formula><p>where C ranges over conjectures, P avail (C) is the set of premises available to prove C, P test (C) is the set of premises for conjecture C from the test set, and rank(P, P avail (C)) is the rank of premise P among the set P avail (C) according to the model. The motivation for aMRR is that conjectures are easier to prove if all their dependencies occur early in the ranking.</p><p>Since it is too expensive to rank all axioms for a conjecture during continuous evaluation, we approximate our objective. For our holdout set of 400 conjectures, we select all true dependencies P test (C) and 128 fixed random false dependencies from P avail (C) − P test (C) and compute the average max relative rank in this ordering. Note that aMRR is nonzero even if all true dependencies are ordered before false dependencies; the best possible value is 0.051. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Network Training</head><p>The neural networks were trained using asynchronous distributed stochastic gradient descent using the Adam optimizer <ref type="bibr" target="#b24">[24]</ref> with up to 20 parallel NVIDIA K-80 GPU workers per model. We used the TensorFlow framework <ref type="bibr" target="#b0">[1]</ref> and the Keras library <ref type="bibr" target="#b4">[5]</ref>. The weights were initialized using <ref type="bibr" target="#b9">[10]</ref>. Polyak averaging with 0.9999 decay was used for producing the evaluation weights <ref type="bibr" target="#b30">[30]</ref>. The character level models were trained with maximum sequence length 2048 characters, where the word-level (and definition embedding) based models had a maximum sequence length of 500 words. For good performance, especially for low cutoff thresholds, it was critical to employ negative mining during training. A side process was continuously evaluating many (conjecture, axiom) pairs. For each conjecture, we pick the lowest scoring statements that have higher score than the lowest scoring true positive. A queue of previously mined negatives is maintained for producing a mixture of examples in which the ratio of mined instances is about 25% and the rest are randomly selected premises. Negative mining was crucial for good quality: at the top-16 cutoff, the number of proved theorems on the test set has doubled. For the union of proof attempts over all cutoff thresholds, the ratio of successful proofs has increased from 61.3% to 66.4% for the best neural model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Experimental Results</head><p>Our best selection pipeline uses a stage-1 character-level convolutional neural network model to produce word-level embeddings for the second stage. The baseline uses distance-weighted k-NN <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b21">21]</ref> with handcrafted semantic features <ref type="bibr" target="#b22">[22]</ref>. For all conjectures in our holdout set, we consider all the chronologically preceding statements (lemmas, definitions and axioms) as premise candidates. In the DeepMath case, premises were ordered by their logistic scores. E prover was applied to the top-k of the premise-candidates for each of the cutoffs k ∈ (16, 32, . . . , 1024) until a proof is found or k = 1024 fails. <ref type="table" target="#tab_5">Table 1</ref> reports the number of theorems proved with a cutoff value at most the k in the leftmost column. For E prover, we used auto strategy with a soft time limit of 90 seconds, a hard time limit of 120 seconds, a memory limit of 4 GB, and a processed clauses limit of 500,000.</p><p>Our most successful models employ simple convolutional networks followed by max pooling (as opposed to recurrent networks like LSTM/GRU), and the two stage definition-based def-CNN outperforms the naïve word-CNN word embedding significantly. In the latter the word embeddings were learned in a single pass; in the former they are fixed from the stage-1 character-level model. For each architecture (cf. <ref type="figure" target="#fig_2">Figure 4</ref>) two convolutional layers perform best. Although our models differ significantly from each other, they differ even more from the k-NN baseline based on hand-crafted features. The right column of <ref type="table" target="#tab_5">Table 1</ref> shows the result if we average the prediction score of the stage-1 model with that of the definition based stage-2 model. We also experimented with character-based RNN models using shorter sequences: these lagged behind our long-sequence CNN models but performed significantly better than those RNNs trained on longer sequences. This suggest that RNNs could be improved by more sophisticated optimization techniques such as curriculum learning.  Each entry is the number (%) of theorems proved by E prover using that particular model to rank the premises. The union of def-CNN and char-CNN proves 69.8% of the test set, while the union of the def-CNN and k-NN proves 74.25%. This means that the neural network predictions are more complementary to the k-NN predictions than to other neural models. The union of all methods proves 2218 theorems (80.9%) and just the neural models prove 2151 (78.4%).</p><formula xml:id="formula_2">Cutoff k-NN Baseline (%) char-CNN (%) word-CNN (%) def-CNN-LSTM (%) def-CNN (%) def+char-CNN (%)<label>(24.</label></formula><p>Also, when we applied two of the premise selection models on those Mizar statements that were not proven automatically before, we managed to prove 823 additional of them.</p><p>(c) Jaccard similarities between proved sets of conjectures across models. Each of the neural network model prediction are more like each other than those of the k-NN baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work we provide evidence that even simple neural models can compete with hand-engineered features for premise selection, helping to find many new proofs. This translates to real gains in automatic theorem proving. Despite these encouraging results, our models are relatively shallow networks with inherent limitations to representational power and are incapable of capturing high level properties of mathematical statements. We believe theorem proving is a challenging and important domain for deep learning methods, and that more sophisticated optimization techniques and training methodologies will prove more useful than in less structured domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(top) The final statement of the Mizar formalization of the Jordan curve theorem. (bottom) The translation to first-order logic, using name mangling to ensure uniqueness across the entire corpus. (a) Length in chars. (b) Length in words. (c) Word occurrences. (d) Dependencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Histograms of statement lengths, occurrences of each word, and statement dependencies in the Mizar corpus translated to first order logic. The wide length distribution poses difficulties for RNN models and batching, and many rarely occurring words make it important to take definitions of words into account.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Specification of the different embedder networks.6.3 Network ArchitecturesAll our neural network models use the general architecture fromFig 3: a classifier on top of the concatenated embeddings of an axiom and a conjecture. The same classifier architecture was used for all models: a fully-connected neural network with one hidden layer of size 1024. For each model, the axiom and conjecture embedding networks have the same architecture without sharing weights. The details of the embedding networks are shown in Fig 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Jordan curve theorem in Mizar for C being Simple_closed_curve holds C is Jordan;:: Translation to first order logic fof(t99_jordan, axiom, (! [A] : (</figDesc><table><row><cell>:: t99_jordan:</cell></row><row><cell>version 1.9 as the underlying ATP system. The following list exemplifies a small non-</cell></row><row><cell>ftp://mizar.uwb.edu.pl/pub/system/i386-linux/mizar-7.13.01_4.181.</cell></row><row><cell>1147-i386-linux.tar</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Results of ATP premise selection experiments with hard negative mining on a test set of 2,742 theorems.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Best sustained test results obtained by the above models. Lower values are better. This was monitored continuously during training on a holdout set with 400 theorems, using all true positive premises and 128 randomly selected negatives. In this setup, the lowest attainable max average relative rank with perfect predictions is 0.051.</figDesc><table><row><cell cols="2">Model Test min average relative rank</cell></row><row><cell>char-CNN</cell><cell>0.0585</cell></row><row><cell>word-CNN</cell><cell>0.06</cell></row><row><cell>def-CNN-LSTM</cell><cell>0.0605</cell></row><row><cell>def-CNN</cell><cell>0.0575</cell></row><row><cell>(d)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Cezary Kaliszyk for providing us with an improved baseline model. Also many thanks go to the Google Brain team for their generous help with the training infrastructure. We would like to thank Quoc Le for useful discussions on the topic and to Sergio Guadarrama for his help with TensorFlow-slim.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Compendium of Continuous Lattices in MIZAR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bancerek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rudnicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Autom. Reasoning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="189" to="224" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baudiš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pichl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vyskočil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Šedivý</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06127</idno>
		<title level="m">Sentence pair scoring: Towards unified framework for text comprehension</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hammering towards QED</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Formalized Reasoning</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="148" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02367</idno>
		<title level="m">Gated feedback recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<ptr target="http://coq.inria.fr" />
	</analytic>
	<monogr>
		<title level="j">The Coq Proof Assistant</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The mathematical language AUTOMATH, its usage, and some of its extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Bruijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Automatic Demonstration</title>
		<editor>M. Laudet</editor>
		<meeting>the Symposium on Automatic Demonstration<address><addrLine>Versailles, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag LNM 125</publisher>
			<date type="published" when="1968-12" />
			<biblScope unit="page" from="29" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The four colour theorem: Engineering of a formal proof</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gonthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Mathematics, 8th Asian Symposium</title>
		<editor>D. Kapur</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007-12-15" />
			<biblScope unit="volume">5081</biblScope>
			<biblScope unit="page">333</biblScope>
		</imprint>
	</monogr>
	<note>Revised and Invited Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A machine-checked proof of the Odd Order Theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gonthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asperti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Avigad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bertot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Garillot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahboubi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Biha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rideau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solovyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Théry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>S. Blazy, C. Paulin-Mohring, and D. Pichardie</editor>
		<imprint>
			<biblScope unit="volume">7998</biblScope>
			<biblScope unit="page" from="163" to="179" />
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mizar in a nutshell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korniłowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Naumowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Formalized Reasoning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="245" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Hales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Magron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mclaughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nipkow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Obua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pleso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rute</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solovyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zumkeller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>A formal proof of the Kepler conjecture. CoRR, abs/1501.02155</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HOL Light: A tutorial introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>M. K. Srivas and A. J. Camilleri</editor>
		<imprint>
			<biblScope unit="volume">1166</biblScope>
			<biblScope unit="page" from="265" to="269" />
			<date type="published" when="1996" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">History of interactive theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wiedijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Logic</title>
		<editor>J. H. Siekmann</editor>
		<meeting><address><addrLine>North-Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="135" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08228</idno>
		<title level="m">Neural gpus learn algorithms</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stronger automation for Flyspeck by feature weighting and strategy evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPiC Series</title>
		<editor>J. C. Blanchette and J. Urban</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2013" />
			<publisher>EasyChair</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning-assisted automated reasoning with Flyspeck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Autom. Reasoning</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="213" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MizAR 40 for Mizar 40</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Autom. Reasoning</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="256" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient semantic features for automated reasoning over large theories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaliszyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vyskocil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<editor>Q. Yang and M. Wooldridge</editor>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3084" to="3090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An ACL2 tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Moore</surname></persName>
		</author>
		<editor>Mohamed et al.</editor>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="17" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">seL4: formal verification of an operatingsystem kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andronick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Elphinstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Derrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elkaduwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Engelhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kolanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from multiple proofs: First experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuehlwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
		<idno>PAAR-2012</idno>
	</analytic>
	<monogr>
		<title level="j">EPiC Series</title>
		<editor>P. Fontaine, R. A. Schmidt, and S. Schulz</editor>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="82" to="94" />
			<date type="published" when="2013" />
			<publisher>EasyChair</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Formal verification of a realistic compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Leroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theorem Proving in Higher Order Logics, 21st International Conference</title>
		<meeting><address><addrLine>TPHOLs; Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008-08-18" />
			<biblScope unit="volume">5170</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Handbook of Automated Reasoning (in 2 volumes)</title>
		<editor>J. A. Robinson and A. Voronkov</editor>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Elsevier and MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">E -A Brainiac Theorem Prover</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Commun</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MPTP 0.2: Design, implementation, and initial experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Autom. Reasoning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="21" to="43" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Theorem proving in large formal mathematics as an emerging AI field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vyskočil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Reasoning and Mathematics: Essays in Memory of William McCune</title>
		<editor>M. P. Bonacina and M. E. Stickel</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">7788</biblScope>
			<biblScope unit="page" from="240" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<title level="m">A neural conversational model</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The Isabelle framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Paulson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nipkow</surname></persName>
		</author>
		<editor>Mohamed et al.</editor>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
