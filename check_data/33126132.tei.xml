<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning the Structure of Generative Models without Labeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
						</author>
						<title level="a" type="main">Learning the Structure of Generative Models without Labeled Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model&apos;s dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the 1regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100× faster than a maximum likelihood approach and selects 1/4 as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Supervised machine learning traditionally depends on access to labeled training data, a major bottleneck in developing new methods and applications. In particular, deep learning methods require tens of thousands or more labeled data points for each specific task. Collecting these labels is often prohibitively expensive, especially when specialized domain expertise is required, and major technology companies are investing heavily in hand-curating labeled training data <ref type="bibr" target="#b17">(Metz, 2016;</ref><ref type="bibr" target="#b9">Eadicicco, 2017)</ref>. Aiming to overcome this bottleneck, there is growing interest in using generative models to synthesize training data from weak super-Stanford University, Stanford, California. Correspondence to: Stephen Bach &lt;bach@cs.stanford.edu&gt;.</p><p>Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s). vision sources such as heuristics, knowledge bases, and weak classifiers trained directly on noisy sources. Rather than treating training labels as gold-standard inputs, such methods model training set creation as a process in order to generate training labels at scale. The true class label for a data point is modeled as a latent variable that generates the observed, noisy labels. After fitting the parameters of this generative model on unlabeled data, a distribution over the latent, true labels can be inferred.</p><p>The structure of such generative models directly affects the inferred labels, and prior work assumes that the structure is user-specified <ref type="bibr" target="#b0">(Alfonseca et al., 2012;</ref><ref type="bibr" target="#b28">Takamatsu et al., 2012;</ref><ref type="bibr" target="#b26">Roth &amp; Klakow, 2013b;</ref><ref type="bibr" target="#b22">Ratner et al., 2016)</ref>. One option is to assume that the supervision sources are conditionally independent given the latent class label. However, statistical dependencies are common in practice, and not taking them into account leads to misjudging the accuracy of the supervision. We cannot rely in general on users to specify the structure of the generative model, because supervising heuristics and classifiers might be independent for some data sets but not others. We therefore seek an efficient method for automatically learning the structure of the generative model from weak supervision sources alone.</p><p>While structure learning in the supervised setting is wellstudied (e.g., <ref type="bibr" target="#b16">Meinshausen &amp; Bühlmann, 2006;</ref><ref type="bibr" target="#b34">Zhao &amp; Yu, 2006;</ref><ref type="bibr" target="#b23">Ravikumar et al., 2010</ref>, see also Section 6), learning the structure of generative models for weak supervision is challenging because the true class labels are latent. Although we can learn the parameters of generative models for a given structure using stochastic gradient descent and Gibbs sampling, modeling all possible dependencies does not scale as an alternative to model selection. For example, estimating all possible correlations for a modestly sized problem of 100 weak supervision sources takes over 40 minutes. (For comparison, our proposed approach solves the same problem in 15 seconds.) As users develop their supervision heuristics, rerunning parameter learning to identify dependencies becomes a prohibitive bottleneck.</p><p>We propose an estimator to learn the dependency structure of a generative model without using any labeled training data. Our method maximizes the 1 -regularized marginal pseudolikelihood of each supervision source's output independently, selecting those dependencies that have nonzero arXiv:1703.00854v2 <ref type="bibr">[cs.</ref>LG] 9 Sep 2017</p><p>weights. This estimator is analogous to maximum likelihood for logistic regression, except that we marginalize out our uncertainty about the latent class label. Since the pseudolikelihood is a function of one free variable and marginalizes over one other variable, we compute the gradient of the marginal pseudolikelihood exactly, avoiding the need for approximating the gradient with Gibbs sampling, as is done for maximum likelihood estimation.</p><p>Our analysis shows that the amount of data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Intuitively, this follows from the fact that learning the generative model's parameters is possible when there are a sufficient number of better-than-random supervision sources available. With enough signal to estimate the latent class labels better than random guessing, those estimates can be refined until the model is identified.</p><p>We run experiments to confirm these predictions. We also compare against the alternative approach of considering all possible dependencies during parameter learning. We find that our method is 100× faster. In addition, our method returns 1/4 as many extraneous correlations on synthetic data when tuned for comparable recall. Finally, we demonstrate that on real-world applications of weak supervision, using generative models with automatically learned dependencies improves performance. We find that our method provides on average 1.5 F1 points of improvement over existing, user-developed information extraction applications on PubMed abstracts and hardware specification sheets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>When developing machine learning systems, the primary bottleneck is often curating a sufficient amount of labeled training data. Hand labeling training data is expensive, time consuming, and often requires specialized knowledge. Recently researchers have proposed methods for synthesizing labels from noisy label sources using generative models. (See Section 6 for a summary.) We ground our work in one framework, data programming <ref type="bibr" target="#b22">(Ratner et al., 2016)</ref>, that generalizes many approaches in the literature.</p><p>In data programming, weak supervision sources are encoded as labeling functions, heuristics that label data points (or abstain). A generative probabilistic model is fit to estimate the accuracy of the labeling functions and the strength of any user-specified statistical dependencies among their outputs. In this model, the true class label for a data point is a latent variable that generates the labeling function outputs. After fitting the parameters of the generative model, a distribution over the latent, true labels can be estimated and be used to train a discriminative model by minimizing the expected loss with respect to that distribution.</p><p>We formally describe this setup by first specifying for each data point x i a latent random variable y i ∈ {−1, 1} that is its true label. For example, in an information extraction task, x i might be a span of text. Then, y i can represent whether it is a mention of a company or not (entity tagging). Alternatively, x i might be a more complex structure, such as a tuple of canonical identifiers along with associated mentions in a document, and then y i can represent whether a relation of interest over that tuple is expressed in the document (relation extraction).</p><p>We do not have access to y i (even at training time), but we do have n user-provided labeling functions λ 1 , . . . , λ n that can be applied to x i to produce outputs Λ i1 , . . . , Λ in . For example, for the company-tagging task mentioned above, a labeling function might apply the regular expression .+\sInc\. to a span of text and return whether it matched. The domain of each Λ ij is {−1, 0, 1}, corresponding to false, abstaining, and true. Generalizing to the multiclass case is straightforward.</p><p>Our goal is to estimate a probabilistic model that generates the labeling-function outputs Λ ∈ {−1, 0, 1} m×n . A common assumption is that the outputs are conditionally independent given the true label, and that the relationship between Λ and y is governed by n accuracy dependencies</p><formula xml:id="formula_0">φ Acc j (Λ i , y i ) := y i Λ ij</formula><p>with a parameter θ Acc j modeling how accurate each labeling function λ j is. We refer to this structure as the conditionally independent model, and specify it as</p><formula xml:id="formula_1">p θ (Λ, Y ) ∝ exp   m i=1 n j=1 θ Acc j φ Acc j (Λ i , y i )   ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">Y := y 1 , . . . , y m .</formula><p>We estimate the parameters θ by minimizing the negative log marginal likelihood p θ (Λ) for an observed matrix of labeling function outputsΛ:</p><formula xml:id="formula_3">arg min θ − log Y p θ (Λ, Y ) .<label>(2)</label></formula><p>Optimizing the likelihood is straightforward using stochastic gradient descent. The gradient of objective (2) with respect to parameter θ Acc</p><formula xml:id="formula_4">j is m i=1 E Λ,Y ∼θ φ Acc j (Λ i , y i ) − E Y ∼θ|Λ φ Acc j (Λ i , y i ) ,</formula><p>the difference between the corresponding sufficient statistic of the joint distribution p θ and the same distribution conditioned onΛ. In practice, we can interleave samples to estimate the gradient and gradient steps very tightly, taking a small step after each sample of each variable Λ ij or y i , similarly to contrastive divergence <ref type="bibr" target="#b11">(Hinton, 2002)</ref>.</p><p>The conditionally independent model is a common assumption, and using a more sophisticated generative model currently requires users to specify its structure. In the rest of the paper, we address the question of automatically identifying the dependency structure from the observationsΛ without observing Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Structure Learning without Labels</head><p>Statistical dependencies arise naturally among weak supervision sources. In data programming, users often write labeling functions with directly correlated outputs or even labeling functions deliberately designed to reinforce others with narrow, more precise heuristics. To address this issue, we generalize the conditionally independent model as a factor graph with additional dependencies, including higher-order factors that connect multiple labeling function outputs for each data point x i and label y i . We specify the general model as</p><formula xml:id="formula_5">p θ (Λ, Y ) ∝ exp m i=1 t∈T s∈St θ t s φ t s (Λ i , y i ) .<label>(3)</label></formula><p>Here T is the set of dependency types of interest, and S t is a set of index tuples, indicating the labeling functions that participate in each dependency of type t ∈ T . We start by defining standard correlation dependencies of the form</p><formula xml:id="formula_6">φ Cor jk (Λ i , y i ) := 1{Λ ij = Λ ik } .</formula><p>We refer to such dependencies as pairwise among labeling functions because they depend only on two labeling function outputs. We can also consider higher-order dependencies that involve more variables, such as conjunction dependencies of the form</p><formula xml:id="formula_7">φ And jk (Λ i , y i ) := 1{Λ ij = y i ∧ Λ ik = y i } .</formula><p>Estimating the structure of the distribution p θ (Λ, Y ) is challenging because Y is latent; we never observe its value, even during training. We must therefore work with the marginal likelihood p θ (Λ). Learning the parameters of the generative model jointly requires Gibbs sampling to estimate gradients. As the number of possible dependencies increases at least quadratically in the number of labeling functions, this heavyweight approach to learning does not scale (see Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Objective</head><p>We can scale up learning over many potentially irrelevant dependencies by optimizing a different objective: the log marginal pseudolikelihood of the outputs of a single labeling function λ j , i.e., conditioned on the outputs of the others λ \j , using 1 regularization to induce sparsity. The objective is</p><formula xml:id="formula_8">arg min θ − log p θ (Λ j |Λ \j ) + θ 1 (4) = arg min θ − m i=1 log yi p θ (Λ ij , y i |Λ i\j ) + θ 1 ,</formula><p>where &gt; 0 is a hyperparameter.</p><p>By conditioning on all other labeling functions in each term</p><formula xml:id="formula_9">log yi p θ (Λ ij , y i |Λ i\j )</formula><p>, we ensure that the gradient can be computed in polynomial time with respect to the number of labeling functions, data points, and possible dependencies; without requiring any sampling or variational approximations. The gradient of the log marginal pseudolikelihood is the difference between two expectations: the sufficient statistics conditioned on all labeling functions but λ j , and conditioned on all labeling functions:</p><formula xml:id="formula_10">− ∂ log p(Λ j |Λ \j ) ∂θ t s = α − β,<label>(5)</label></formula><p>where</p><formula xml:id="formula_11">α := m i=1 Λij ,yi p θ (Λ ij , y i |Λ i\j )φ t s ((Λ ij ,Λ i\j ), y i ) β := m i=1 yi p(y i |Λ i )φ t s (Λ i , y i ).</formula><p>Note that in the definition of α, φ t s operates on the value of Λ ij set in the summation and the observed values ofΛ i\j .</p><p>We optimize for each labeling function λ j in turn, selecting those dependencies with parameters that have a sufficiently large magnitude and adding them to the estimated structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation</head><p>We implement our method as Algorithm 1, a stochastic gradient descent (SGD) routine. At each step of the descent, the gradient (5) is estimated for a single data point, which can be computed in closed form. Using SGD has two advantages. First, it requires only first-order gradient information. Other methods for 1 -regularized regression like interior-point methods <ref type="bibr" target="#b14">(Koh et al., 2007)</ref> usually require computing second-order information. Second, the obser-vationsΛ can be processed incrementally. Since data programming operates on unlabeled data, which is often abundant, scalability is crucial. To implement regularization as part of SGD, we use an online truncated gradient method <ref type="bibr" target="#b15">(Langford et al., 2009)</ref>.</p><p>In practice, we find that the only parameter that requires tuning is , which controls the threshold and regularization s + Kη }</p><formula xml:id="formula_12">for θ t s in θ where j ∈ s do if |θ t s | &gt; then D ← D ∪ {(s, t)} return D</formula><p>strength. Higher values induce more sparsity in the selected structure. For the other parameters, we use the same values in all of our experiments: step size η = m −1 , epoch count T = 10, and truncation frequency K = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis</head><p>We provide guarantees on the probability that Algorithm successfully recovers the exact dependency structure. We first provide a general recovery guarantee for all types of possible dependencies, including both pairwise and higherorder dependencies. However, in many cases, higher-order dependencies are not necessary to model the behavior of the labeling functions. In fact, as we demonstrate in Section 5.3, in many useful models there are only accuracy dependencies and pairwise correlations. In this case, we show as a corollary to our general result that the number of samples required is sublinear in the number of possible dependencies, specifically O(n log n).</p><p>Previous analyses for the supervised case do not carry over to the unsupervised setting because the problem is no longer convex. For example, analysis of an analogous method for supervised Ising models <ref type="bibr" target="#b23">(Ravikumar et al., 2010</ref>) relies on Lagrangian duality and a tight duality gap, which does not hold for our estimation problem. Instead, we reason about a region of the parameter space in which we can estimate Y well enough that we can eventually ap-proach the true model. We now state the conditions necessary for our guarantees. First are two standard conditions that are needed to guarantee that the dependency structure can be recovered with any number of samples. One, we must have some set Θ ⊂ R M of feasible parameters. Two, the true model is in Θ, i.e., there exists some choice of θ * ∈ Θ such that</p><formula xml:id="formula_13">π * (Λ, Y ) = p θ * (Λ, Y ), ∀Λ ∈ {−1, 0, 1} m×n , Y ∈ {−1, 1} m (6)</formula><p>where π * is the true distribution.</p><p>Next, let Φ j denote the set of dependencies that involve either labeling function λ j or the true label y. For any feasible parameter θ ∈ Θ and j ∈ {1, . . . , n}, there must exist c &gt; 0 such that</p><formula xml:id="formula_14">cI + m i=1 Cov (Λ,Y )∼p θ (Φ j (Λ, Y ) | Λ i =Λ i ) m i=1 Cov (Λ,Y )∼p θ (Φ j (Λ, Y ) | Λ i\j =Λ i\j ).<label>(7)</label></formula><p>This means that for each labeling function, we have a better estimate of the dependencies with the labeling function than without. It is analogous to assumptions made to analyze parameter learning in data programming.</p><p>Finally, we require that all non-zero parameters be bounded away from zero. That is, for all θ i = 0, and some κ &gt; 0, we have that</p><formula xml:id="formula_15">|θ i | ≥ κ.<label>(8)</label></formula><p>Under these conditions, we are able to provide guarantees on the probability of finding the correct dependency structure. First, we present guarantees for all types of possible dependencies in Theorem 1, proved in Appendix A.2. For this theorem, we define d j to be the number of possible dependencies involving either Λ j or y, and we define d as the</p><formula xml:id="formula_16">largest of d 1 , . . . , d n .</formula><p>Theorem 1. Suppose we run Algorithm 1 on a problem where conditions (6), (7), and (8) are satisfied. Then, for any δ &gt; 0, an unlabeled input dataset of size</p><formula xml:id="formula_17">m ≥ 32d c 2 κ 2 log 2nd δ</formula><p>is sufficient to recover the exact dependency structure with a probability of at least 1 − δ.</p><p>For general dependencies, d can be as large as the number of possible dependencies due to the fact that higher-order dependencies can connect the true label and many labeling functions. The rate of Theorem 1 rate is therefore not directly comparable to that of <ref type="bibr" target="#b23">Ravikumar et al. (2010)</ref>, which applies to Ising models with pairwise dependencies.</p><p>As we demonstrate in Section 5.3, however, real-world applications can be improved by modeling just pairwise correlations among labeling functions. If only considering these dependencies, then d will only be 2n − 1, rather than the number of potential dependencies. In Corollary 2, we show that a number of samples needed in this case is O(n log n). Notice that this is sublinear in the number of possible dependencies, which is O(n 2 ).</p><p>Corollary 2. Suppose we run Algorithm 1 on a problem where conditions (6), <ref type="formula" target="#formula_14">7</ref>, and (8) are satisfied. Additionally, assume that the only potential dependencies are accuracy and correlation dependencies. Then, for any δ &gt; 0, an unlabeled input dataset of size</p><formula xml:id="formula_18">m ≥ 64n c 2 κ 2 log 4n δ</formula><p>is sufficient to recover the exact dependency structure with a probability of at least 1 − δ.</p><p>In this case, we see the difference in analyses between the unsupervised and supervised settings. Whereas the rate of Corollary 2 depends on the maximum number of dependencies that could affect a variable in the model class, the rate of <ref type="bibr" target="#b23">Ravikumar et al. (2010)</ref> depends cubically on the maximum number of dependencies that actually affect any variable in the true model and only logarithmically in the maximum possible degree. In the supervised setting, the guaranteed rate is therefore tighter for very sparse models. However, as we show in Section 5.1, the guaranteed rates in both settings are pessimistic, and in practice they appear to scale at the same rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We implement our method as part of the open source framework Snorkel 1 and evaluate it in three ways. First, we measure how the probability of returning the exact correlation structure is affected by the problem parameters using synthetic data, confirming our analysis that its sample complexity is sublinear in the number of possible dependencies. In fact, we find that in practice the sample complexity is lower than the theoretically guaranteed rate, matching the rate seen in practice for fully supervised structure learning. Second, we compare our method to estimating structures via parameter learning over all possible dependencies. We demonstrate using synthetic data that our method is 100× faster and more accurate, selecting 1/4 as many extraneous correlations on average. Third, we apply our method to real-world applications built using data programming, such as information extraction from PubMed journal abstracts and hardware specification sheets. In these applications, users did not specify any dependencies between the label- <ref type="figure">Figure 1</ref>. Algorithm 1 returns the true structure consistently when the control parameter γ reaches 1.0 for the number of samples defined by (9). The number of samples required to identify a model in practice scales logarithmically in n, the number of labeling functions.</p><p>ing functions they authored; however, as we detail in Section 5.3, these dependencies naturally arise, for example due to explicit composing, relaxing, or tightening of labeling function heuristics; related distant supervision sources; or multiple concurrent developers writing labeling functions. We show that learning this structure improves performance over the conditionally independent model, giving an average 1.5 F1 point boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Sample Complexity</head><p>We test how the probability that Algorithm 1 returns the correct correlation structure depends on the true distribution. Our analysis in Section 4 guarantees that the sample complexity grows at worst on the order O(n log n) for n labeling functions. In practice, we find that structure learning performs better than this guaranteed rate, depending linearly on the number of true correlations and logarithmically on the number of possible correlations. This matches the observed behavior for fully supervised structure learning for Ising models <ref type="bibr" target="#b23">(Ravikumar et al., 2010)</ref>, which is also tighter than the best known theoretical guarantees.</p><p>To demonstrate this behavior, we attempt to recover the true dependency structure using a number of samples defined as</p><formula xml:id="formula_19">m := 750 • γ • d * • log n<label>(9)</label></formula><p>where d * is the maximum number of dependencies that affect any one labeling function. For example, in the conditionally independent model d * = 1 and in a model with one correlation d * = 2. We vary the control parameter γ from 0.1 to 2.0 to determine the point at which m is sufficiently large for Algorithm 1 to recover the true dependency structure. (The constant 750 was selected so that it succeeds with high probability around γ = 1.0.)</p><p>We first test the effect of varying n, the number of labeling functions. For n ∈ {25, 50, 75, 100}, we set two pairs of labeling functions to be correlated with θ Cor jk = 0.25. We set θ Acc j = 1.0 for all j. We then generate m samples for each setting of γ over 100 trials. We next test the effect of varying d * , the maximum number of dependencies that affect a labeling function in the true distribution. For 25 labeling functions, we add correlations to the true model to form cliques of increasing degree. All parameters are the same as in the previous experiment. <ref type="figure" target="#fig_0">Figure 2</ref> shows that for increasing values of d * , (9) again predicts the number of samples for Algorithm 1 to succeed. That the curves are aligned for different values of d * shows that the sample complexity in practice scales linearly in d * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Maximum Likelihood</head><p>We next compare Algorithm 1 with an alternative approach. Without an efficient structure learning method, one could maximize the marginal likelihood of the observationsΛ while considering all possible dependencies. To measure the benefits of maximizing the marginal pseudolikelihood, we compare its performance against an analogous maximum likelihood estimation routine that also uses stochastic gradient descent, but instead uses Gibbs sampling to estimate the intractable gradient of the objective.</p><p>We create different distributions over n labeling functions by selecting with probability 0.05 pairs of labeling functions to make correlated. Again, the strength of correlation is set at θ Cor jk = 0.25 and accuracy is set at θ Acc j = 1.0. We generate 100 distributions for n ∈ {25, 30, 35, . . . , 100}. For each distribution we generate 10,000 samples and attempt to recover the true correlation structure. We first compare running time between the two methods. Our implementation of maximum likelihood estimation is designed for speed: for every sample taken to estimate the gradient, a small update to the parameters is performed. This approach is state-of-the-art for high-speed learning for factor graphs <ref type="bibr" target="#b32">(Zhang &amp; Ré, 2014)</ref>. However, the need for sampling the variables Λ and Y is still computationally expensive. <ref type="figure" target="#fig_2">Figure 3</ref> shows that by avoiding variable sampling, using pseudolikelihood is 100× faster.</p><p>We next compare the accuracy of the two methods, which depends on the regularization . The ideal is to maximize the probability of perfect recall with few extraneous correlations, because subsequent parameter estimation can reduce the influence of an extraneous correlation but cannot discover a missing correlation. We tune independently for each method. <ref type="figure" target="#fig_3">Figure 4 (top)</ref> shows that maximum pseudolikelihood is able to maintain higher levels of recall than maximum likelihood as the problem size increases. <ref type="figure" target="#fig_3">Figure 4 (bottom)</ref> shows that even tuned for better recall, maximum pseudolikelihood is more precise, returning 1/4 as many extraneous correlations. We interpret this improved accuracy as a benefit of computing the gradient for a data point exactly, as opposed to using Gibbs sampling to estimate it as in maximum likelihood estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Real-World Applications</head><p>We evaluate our method on several real-world information extraction applications, comparing the performance of data programming using dependencies selected by our method with the conditionally independent model <ref type="table" target="#tab_0">(Table 1</ref>). In the data programming method, users express a variety of weak supervision rules and sources such as regular expression patterns, distant supervision from dictionaries and existing knowledge bases, and other heuristics as labeling functions. Due to the noisy and overlapping nature of these labeling functions, correlations arise. Learning this correlation structure gives an average improvement of 1.5 F1 points. Comparison of structure learning with using maximum likelihood parameter estimation to select a model structure. Even when tuned for better recall (top), structure learning is also more precise, returning 1/4 as many extraneous correlations (bottom).</p><p>Extracting structured information from unstructured text by identifying mentioned entities and relations is a challenging task that is well studied in the context of weak supervision <ref type="bibr" target="#b1">(Bunescu &amp; Mooney, 2007;</ref><ref type="bibr" target="#b0">Alfonseca et al., 2012;</ref><ref type="bibr" target="#b22">Ratner et al., 2016)</ref>. We consider three tasks: extracting mentions of specific diseases from the scientific literature (Disease Tagging); extracting mentions of chemicals inducing diseases from the scientific literature (Chemical-Disease); and extracting mentions of electronic device polarity from PDF parts sheet tables (Device Polarity). In the first two applications, we consider a training set of 500 unlabeled abstracts from PubMed, and in the third case 100 PDF parts sheets consisting of mixed text and tabular data. We use hand-labeled test sets to evaluate on the candidatemention-level performance, which is the accuracy of the classifier in identifying correct mentions of specific entities or relations, given a set of candidate mentions. For example, in Chemical-Disease, we consider as candidates all pairs of co-occurring chemical-disease mention pairs as identified by standard preprocessing tools .</p><p>We see that modeling the correlations between labeling functions gives gains in performance which appear to be correlated with the total number of sources. For example, in the disease tagging application, we have 233 labeling ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/PubTator/index.cgi functions, the majority of which check for membership in specific subtrees of a reference disease ontology using different matching heuristics. There is overlap in the labeling functions which check identical subtrees of the ontology, and we see that our method increases end performance by a significant 2.6 F1 points by modeling this structure.</p><p>Examining the Chemical-Disease task, we see that our method identifies correlations that are both obviously true and ones that are more subtle. For example, our method learns dependencies between labeling functions that are compositions of one another, such as one labeling function checking for the pattern [CHEM] induc. * <ref type="bibr">[DIS]</ref>, and a second labeling function checking for this pattern plus membership in an external knowledge base of known chemical-disease relations. Our method also learns more subtle correlations: for example, it selected a correlation between a labeling function that checks for the presence of a chemical mention in between the chemical and disease mentions comprising the candidate, and one that checks for the pattern . * -induced appearing in between.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Accelerating Application Development</head><p>Our method is in large part motivated by the new programming model introduced by weak supervision, and the novel hurdles that developers face. For example in the Disease Tagging application above, we observed developers significantly slowed down in trying to to leverage the rich disease ontologies and matching heuristics they had available without introducing too many dependencies between their labeling functions. In addition to being slowed down, we also observed developers running into significant pitfalls due to unnoticed correlations between their weak supervision sources. In one collaborator's application, for every labeling function that referenced the words in a sentence, a corresponding labeling function referenced the lemmas, which were often identical, and this significantly degraded performance. By automatically learning dependencies, we were able to significantly mitigate the effects of such correlations. We therefore envision an accelerated development process enabled by our method.</p><p>To further explore the way in which our method can protect against such types of failure modes, we consider adding correlated, random labeling functions to those used in the Chemical-Disease task. <ref type="figure">Figure 5</ref> shows the average estimated accuracy of copies of a random labeling function. An independent model grows more confident that the random noise is accurate. However, with structure learning, we identify that the noisy sources are not independent and they therefore do not outvote the real labeling functions. In this way, structure learning can protect against failures as users experiment with sources of weak supervision.  <ref type="figure">Figure 5</ref>. Structure learning identifies and corrects correlated, random labeling functions added to the Chemical-Disease task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Structure learning is a well-studied problem, but most work has assumed access to hand-labeled training data. Some of the earliest work has focused on generalized linear models. The lasso <ref type="bibr" target="#b29">(Tibshirani, 1996)</ref>, linear regression with 1 regularization, is a classic technique. <ref type="bibr" target="#b34">Zhao &amp; Yu (2006)</ref> showed that the lasso is a consistent structure estimator. The Dantzig selector <ref type="bibr" target="#b2">(Candes &amp; Tao, 2007)</ref> is another structure estimator for linear models that uses 1 , which can learn in the high-dimensional setting where there are more possible dependencies than samples. <ref type="bibr" target="#b19">Ng (2004)</ref> showed that 1 -regularized logistic regression has sample complexity logarithmic in the number of features. 1 regularization has also been used as a prior for compressed sensing (e.g., <ref type="bibr" target="#b7">Donoho &amp; Elad, 2003;</ref><ref type="bibr" target="#b30">Tropp, 2006;</ref><ref type="bibr" target="#b31">Wainwright, 2009)</ref>.</p><p>Regularized estimators have also been used to select structures for graphical models. <ref type="bibr" target="#b16">Meinshausen &amp; Bühlmann (2006)</ref> showed that parameter learning with 1 regularization for Gaussian graphical models under similar assumptions also consistently selects the correct structure. Most similar to our proposed estimator, <ref type="bibr" target="#b23">Ravikumar et al. (2010)</ref> propose a fully supervised pseudolikelihood estimator for Ising models. Also related is the work of <ref type="bibr" target="#b3">Chandrasekaran et al. (2012)</ref>, which considers learning the structure of Gaussian graphical models with latent variables. Other techniques for learning the structure of graphical models include grafting <ref type="bibr" target="#b21">(Perkins et al., 2003;</ref><ref type="bibr" target="#b35">Zhu et al., 2010)</ref> and the information bottleneck approach for learning Bayesian networks with latent variables <ref type="bibr" target="#b10">(Elidan &amp; Friedman, 2005)</ref>.</p><p>Using heuristic sources of labels is increasingly common.</p><p>Treating labels from a single heuristic source as gold labels is called distant supervision <ref type="bibr" target="#b4">(Craven &amp; Kumlien, 1999;</ref><ref type="bibr" target="#b18">Mintz et al., 2009)</ref>. Some methods use multi-instance learning to reduce the noise in a distant supervision source <ref type="bibr" target="#b24">(Riedel et al., 2010;</ref><ref type="bibr" target="#b12">Hoffmann et al., 2011)</ref>. Others use hierarchical topic models to generate additional training data for weak supervision, but they do not support user-provided heuristics <ref type="bibr" target="#b0">(Alfonseca et al., 2012;</ref><ref type="bibr" target="#b28">Takamatsu et al., 2012;</ref><ref type="bibr" target="#b25">Roth &amp; Klakow, 2013a;</ref>. Previous methods that support heuristics for weak supervision (e.g., <ref type="bibr" target="#b1">Bunescu &amp; Mooney, 2007;</ref><ref type="bibr" target="#b27">Shin et al., 2015)</ref> do not model the noise inherent in these sources. Also, <ref type="bibr" target="#b8">Downey &amp; Etzioni (2008)</ref> showed that PAC learning is possible without hand-labeled data if the features monotonically order data by class probability.</p><p>Estimating the accuracy of multiple label sources without a gold standard is a classic problem <ref type="bibr" target="#b6">(Dawid &amp; Skene, 1979)</ref>, and many proposed approaches are generalized in the data programming framework. <ref type="bibr" target="#b20">Parisi et al. (2014)</ref> proposed a spectral approach to estimating the accuracy of members of classifier ensembles. Many methods for crowdsourcing estimate the accuracy of workers without hand-labeled data (e.g., <ref type="bibr" target="#b5">Dalvi et al., 2013;</ref><ref type="bibr" target="#b13">Joglekar et al., 2015;</ref><ref type="bibr" target="#b33">Zhang et al., 2016)</ref>. In data programming, the scaling of data to label sources is different from crowdsourcing; a relatively small number of sources label all the data. We can therefore learn rich dependency structures among the sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Directions</head><p>We showed that learning the structure of a generative model enables higher quality data programming results. Our method for structure learning is also 100× faster than a maximum likelihood approach. If data programming and other forms of weak supervision are to make machine learning tools easier to develop, selecting accurate structures for generative models with minimal user intervention is a necessary capability. Interesting questions remain. Can the guarantee of Theorem 1 be tightened for higher-order dependencies to match the pairwise case of Corollary 2? Preliminary experiments show that they converge at similar rates in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs</head><p>In this appendix, we provide proofs for Theorem and Corollary 2 from the main text. In Section A.1, we provide an outline of the proof and state several lemmas. In Section A.2, we prove Theorem 1. In Section A.3, we prove Corollary 2, which follows directly from Theorem 1. In Section A.4, we prove the lemmas stated in Section A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Outline and Lemma Statements</head><p>A.1.1. OUTLINE OF THEOREM 1 PROOF</p><p>We first show that the negative marginal log-pseudolikelihood is strongly convex under condition (7). In particular, in Lemma 1, we derive the gradient and Hessian of each term of the negative marginal log-pseudolikelihood, and in Lemma 2, we show that the negative marginal log-pseudolikelihood is strongly convex under condition (7).</p><p>Next, in Lemma 3, we show that, under condition (6), the gradient of the negative marginal log-pseudolikelihood at the true parameter θ * is small with high probability.</p><p>Finally, we show that if we run SGD until convergence and then truncate, we will recover the exact sparsity structure with high probability. In Lemma 4, we show that if the true parameter θ * has a small gradient, then the empirical minimumθ will be close to it, and in Lemma 5, we show that the correct sparsity structure is recovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2. LEMMA STATEMENTS</head><p>We now formally state the lemmas used in our proof. Lemma 1. Given a family of maximum-entropy distributions</p><formula xml:id="formula_20">p θ (x) = 1 Z θ exp(θ T φ(x)),</formula><p>for some function of sufficient statistics h : Ω → R M , if we let J be the negative log-pseudolikelihood objective for some event A ⊆ Ω,</p><formula xml:id="formula_21">J(θ) = − log p x∼p θ (x ∈ A | Λ \j ),</formula><p>then its gradient is</p><formula xml:id="formula_22">∇J(θ) = −E x∼p θ φ(x) | x ∈ A, Λ \j + E x∼p θ φ(x) | Λ \j</formula><p>and its Hessian is</p><formula xml:id="formula_23">∇ 2 J(θ) = − Cov x∼p θ φ(x) | x ∈ A, Λ \j + Cov x∼p θ φ(x) | Λ \j</formula><p>Lemma 2. Let J be the empirical negative log-pseudolikelihood objective for the event</p><formula xml:id="formula_24">Λ j =Λ j J(θ) = − m i=1 log p x∼p θ (Λ ij =Λ ij | Λ i\j =Λ i\j ) .</formula><p>Let Θ j denote the set of parameters corresponding to dependencies incident on either labeling function λ j or the true label y, and let Θ \j denote all the set of all remaining parameters.</p><p>Then, J(θ) is independent of the variables in Θ \j , and under condition (7), J(θ) is strongly convex on the variables in Θ j with a parameter of strong convexity of c.</p><p>Lemma 3. Let d j be the number of dependencies that involve either λ j or y, and let θ * be the true parameter specified by condition (6). Define W as the gradient of the negative log-pseudolikelihood of λ j at this point</p><formula xml:id="formula_25">W = −∇J(θ * ; X).</formula><p>Then, for any δ.</p><formula xml:id="formula_26">Pr( W ∞ ≥ δ) ≤ 2d j exp −mδ 2</formula><p>Lemma 4. Let J be a c-strongly convex function in d dimensions, and letθ be the minimizer of J.</p><formula xml:id="formula_27">Suppose J(θ * ) ∞ ≤ δ. Then, θ − θ * ∞ ≤ δ c √ d</formula><p>Lemma 5. Suppose that conditions (6), (7), and (8) are satisfied. Suppose we run Algorithm 1 with m samples, a sufficiently small step size η, a sufficiently large number of epochs T , and truncate once at the end with Kη = κ/2. Then, the probability that we fail to recover the exact sparsity structure is at most 2nd exp −mc 2 κ 2 32d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Proof of Theorem 1</head><p>Theorem 1. Suppose we run Algorithm 1 on a problem where conditions (6), (7), and (8) are satisfied. Then, for any δ &gt; 0, an unlabeled input dataset of size</p><formula xml:id="formula_28">m ≥ 32d c 2 κ 2 log 2nd δ</formula><p>is sufficient to recover the exact dependency structure with a probability of at least − δ.</p><p>Proof. If follows from Lemma 5 that the probability that we fail to recover the sparsity structure is at most</p><formula xml:id="formula_29">2nd exp −mc 2 κ 2 32d .</formula><p>By using the provided m, the probability of failure is at most</p><formula xml:id="formula_30">2nd exp − 32d c 2 κ 2 log 2nd δ c 2 κ 2 32d = 2nd exp − log 2nd δ = δ.</formula><p>Thus, we will succeed with probability at least − δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Proof of Corollary 2</head><p>Corollary 3. Suppose we run Algorithm 1 on a problem where conditions (6), (7), and (8) are satisfied. Additionally, assume that the only potential dependencies are accuracy and correlation dependencies. Then, for any δ &gt; 0, an unlabeled input dataset of size</p><formula xml:id="formula_31">m ≥ 64n c 2 κ 2 log 4n δ</formula><p>is sufficient to recover the exact dependency structure with a probability of at least − δ.</p><p>Proof. In this case, each labeling function λ j is involved in n − 1 with other labeling functions, and the true label y is involved in n dependencies. Thus, d = (n − 1) + n &lt; 2n.</p><p>We can then apply Theorem 1 to show that the probability of success is at least − τ for the specified m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Proofs of Lemmas</head><p>Lemma 1. Given a family of maximum-entropy distributions</p><formula xml:id="formula_32">p θ (x) = 1 Z θ exp(θ T φ(x)),</formula><p>for some function of sufficient statistics h : Ω → R M , if we let J be the negative log-pseudolikelihood objective for some event A ⊆ Ω,</p><formula xml:id="formula_33">J(θ) = − log p x∼p θ (x ∈ A | Λ \j ),</formula><p>then its gradient is</p><formula xml:id="formula_34">∇J(θ) = −E x∼p θ φ(x) | x ∈ A, Λ \j + E x∼p θ φ(x) | Λ \j</formula><p>and its Hessian is</p><formula xml:id="formula_35">∇ 2 J(θ) = − Cov x∼p θ φ(x) | x ∈ A, Λ \j + Cov x∼p θ φ(x) | Λ \j</formula><p>Proof. We first rewrite the netative log-pseudolikelihood as</p><formula xml:id="formula_36">J(θ) = − log Pr x∼π θ (x ∈ A | Λ \j ) = − log Pr x∼π θ (x ∈ A, Λ \j ) Pr x∼π θ (Λ \j ) = − log x∈A,Λ \j p θ (x) x∈Λ \j p θ (x) = − log x∈A,Λ \j exp(θ T φ(x)) x∈Λ \j exp(θ T φ(x)) = − log x∈A,Λ \j exp(θ T φ(x)) + log x∈Λ \j exp(θ T φ(x)).</formula><p>We now derive the gradient</p><formula xml:id="formula_37">∇J(θ) = ∇   − log x∈A,Λ \j exp(θ T φ(x)) + log x∈Λ \j exp(θ T φ(x))   = −∇ log x∈A,Λ \j exp(θ T φ(x)) + ∇ log x∈Λ \j exp(θ T φ(x)) = − x∈A,Λ \j φ(x) exp(θ T φ(x))</formula><p>x∈A,Λ \j exp(θ T φ(x))</p><formula xml:id="formula_38">+ x∈Λ \j φ(x) exp(θ T φ(x)) x∈Λ \j exp(θ T φ(x)) = −E x∼p θ φ(x) | x ∈ A, Λ \j + E x∼p θ φ(x) | Λ \j</formula><p>We now derive the Hessian</p><formula xml:id="formula_39">∇ 2 J(θ) = ∇ − x∈A,Λ \j φ(x) exp(θ T φ(x))</formula><p>x∈A,Λ \j exp(θ T φ(x))</p><formula xml:id="formula_40">+ x∈Λ \j φ(x) exp(θ T φ(x)) x∈Λ \j exp(θ T φ(x)) = −∇ x∈A,Λ \j φ(x) exp(θ T φ(x))</formula><p>x∈A,Λ \j exp(θ T φ(x))</p><formula xml:id="formula_41">+ ∇ x∈Λ \j φ(x) exp(θ T φ(x)) x∈Λ \j exp(θ T φ(x)) = −    x∈A,Λ \j φ(x)φ(x) T exp(θ T φ(x))</formula><p>x∈A,Λ \j exp(θ T φ(x))</p><formula xml:id="formula_42">− x∈A,Λ \j φ(x) exp(θ T φ(x)) x∈A,Λ \j φ(x) exp(θ T φ(x)) T x∈A,Λ \j exp(θ T φ(x)) 2    +   </formula><p>x∈Λ \j φ(x)φ(x) T exp(θ T φ(x))</p><p>x∈Λ \j exp(θ T φ(x))</p><formula xml:id="formula_43">− x∈Λ \j φ(x) exp(θ T φ(x)) x∈Λ \j φ(x) exp(θ T φ(x)) T x∈Λ \j exp(θ T φ(x)) 2    = − E x∼p θ φ(x)φ(x) T | x ∈ A, Λ \j − E x∼p θ φ(x) | x ∈ A, Λ \j E x∼p θ φ(x) | x ∈ A, Λ \j T + E x∼p θ φ(x)φ(x) T | x ∈ Λ \j − E x∼p θ φ(x) | x ∈ Λ \j E x∼p θ φ(x) | x ∈ Λ \j T = − Cov x∼p θ φ(x) | x ∈ A, Λ \j + Cov x∼p θ φ(x) | Λ \j .</formula><p>Lemma 2. Let J be the empirical negative log-pseudolikelihood objective for the event</p><formula xml:id="formula_44">Λ j =Λ j J(θ) = − m i=1 log p x∼p θ (Λ ij =Λ ij | Λ i\j =Λ i\j ) .</formula><p>Let Θ j denote the set of parameters corresponding to dependencies incident on either labeling function λ j or the true label y, and let Θ \j denote all the set of all remaining parameters.</p><p>Then, J(θ) is independent of the variables in Θ \j , and under condition (7), J(θ) is strongly convex on the variables in Θ j with a parameter of strong convexity of c.</p><p>Proof. First, we show that J(θ) is independent of the variables in Θ \j . We simplify J(θ) as</p><formula xml:id="formula_45">J(θ) = m i=1   − log x∈Λ exp(θ T φ(x)) + log x∈Λ \j exp(θ T φ(x))   = m i=1   − log x∈Λ exp θ T j φ j (x) + θ T \j φ \j (x) + log x∈Λ \j exp θ T j φ j (x) + θ T \j φ \j (x)   = m i=1   − log x∈Λ exp θ T j φ j (x) exp θ T \j φ \j (x) + log x∈Λ \j exp θ T j φ j (x) exp θ T \j φ \j (x)   = m i=1   − log   exp θ T \j φ \j (x) x∈Λ exp θ T j φ j (x)   + log   exp θ T \j φ \j (x) x∈Λ \j exp θ T j φ j (x)     = m i=1   − log exp θ T \j φ \j (x) − log x∈Λ exp θ T j φ j (x) + log exp θ T \j φ \j (x) + log x∈Λ \j exp θ T j φ j (x)   = m i=1   − log x∈Λ exp θ T j φ j (x) + log x∈Λ \j exp θ T j φ j (x)   ,</formula><p>which does not depend on any variables in Θ \j .</p><p>Next, we prove that J(θ) is c-strongly convex in the variabes in Θ j . By combining the previous result and Lemma 1, we can derive the Hessian</p><formula xml:id="formula_46">∇ 2 J(Θ j ) = m i=1 − Cov (Λ,Y )∼p θ (Φ j (Λ, Y ) | Λ i =Λ i ) + Cov (Λ,Y )∼p θ (Φ j (Λ, Y ) | Λ i\j =Λ i\j ) = − m i=1 Cov (Λ,Y )∼p θ (Φ j (Λ, Y ) | Λ i =Λ i ) + m i=1</formula><p>Cov (Λ,Y )∼p θ (Φ j (Λ, Y ) | Λ i\j =Λ i\j ).</p><p>It then follows from condition (7) that</p><formula xml:id="formula_47">cI ∇ 2 J(Θ j ),</formula><p>which implies that J is c-strongly convex on variables in Θ j .</p><p>Lemma 3. Let d j be the number of dependencies that involve either λ j or y, and let θ * be the true parameter specified by condition (6). Define W as the gradient of the negative log-pseudolikelihood of λ j at this point W = −∇J(θ * ; X).</p><p>Then, for any δ.</p><formula xml:id="formula_48">Pr( W ∞ ≥ δ) ≤ 2d j exp −mδ 2 8</formula><p>Proof. From Lemma 1, we know that each element of W can be written as the average of m i.i.d. terms. From condition <ref type="formula" target="#formula_14">7</ref>, we know that the terms have zero mean, and we also know that the terms are bounded in absolute value by 2, due to the fact that the dependencies have values falling in the interval [−1, 1].</p><p>We can alternatively think of the average of the terms as the sum of m i.i.d. zero-mean random variables that are bounded in absolute value by 2 m . The two-sided Azuma's inequality bounds the probability that any term in W is large.</p><formula xml:id="formula_49">Pr(|W j | ≥ δ) ≤ 2 exp −δ 2 2 m i=1 2 m 2 ≤ 2 exp −mδ 2 8</formula><p>The union bound then bounds the probability that any component of W is large.</p><formula xml:id="formula_50">Pr( W ∞ ≥ δ) ≤ 2d j exp −mδ 2 8</formula><p>Lemma 4. Let J be a c-strongly convex function in d dimensions, and letθ be the minimizer of J. Suppose J(θ * ) ∞ ≤ δ. Then,</p><formula xml:id="formula_51">θ − θ * ∞ ≤ δ c √ d</formula><p>Proof. Because J is c-strongly convex,</p><formula xml:id="formula_52">∇J(θ * ) − ∇J(θ) T (θ * −θ) ≥ c θ * −θ 2 2 .</formula><p>∇J(θ) = 0, so ∇J(θ * ) T (θ * −θ) ≥ c θ * −θ .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Algorithm 1 returns the true structure consistently when the control parameter γ reaches 1.0 for the number of samples defined by (9). The number of samples required to identify a model in practice scales linearly in d * , the maximum number of dependencies affecting any labeling function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure showsthe fraction of times Algorithm 1 returns the correct correlation structure as a function of the control parameter γ. That the curves are aligned for different values of n shows that the sample complexity in practice scales logarithmically in n.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparison of structure learning with using maximum likelihood parameter estimation to select a model structure. Structure learning is two orders of magnitude faster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of structure learning with using maximum likelihood parameter estimation to select a model structure. Even when tuned for better recall (top), structure learning is also more precise, returning 1/4 as many extraneous correlations (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Candidate-mention scores of information extraction applications trained with data programming using generative models with no dependency structure (Independent) and learned dependency structure (Structure).</figDesc><table><row><cell>APPLICATION</cell><cell>INDEPENDENT P R F1</cell><cell>P</cell><cell>STRUCTURE R</cell><cell>F1</cell><cell cols="3">F1 DIFF. # LFS # COR. % CORR.</cell></row><row><cell>DISEASE TAGGING</cell><cell>60.4 73.3 66.3</cell><cell cols="3">68.0 69.8 68.9</cell><cell>2.6</cell><cell>233</cell><cell>315</cell><cell>1.17%</cell></row><row><cell>CHEMICAL-DISEASE</cell><cell>45.1 69.2 54.6</cell><cell cols="3">46.8 69.0 55.9</cell><cell>1.3</cell><cell>33</cell><cell>21</cell><cell>3.98%</cell></row><row><cell>DEVICE POLARITY</cell><cell>78.9 99.6 88.1</cell><cell cols="3">80.5 98.6 88.7</cell><cell>0.6</cell><cell>12</cell><cell>32</cell><cell>48.49%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">snorkel.stanford.edu</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to Christopher De Sa for helpful discussions, and Henry Ehrenberg and Sen Wu for assistance with experiments. We gratefully acknowledge the support of the Defense Advanced Research Projects Agency <ref type="formula">(</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof. First, we bound the probability that we fail to correctly recover the dependencies involving λ j . By Lemma 3, we can bound the probability that the gradient is large at θ * by</p><p>If this is the case, then upon truncation, the correct dependencies will be recovered for λ j . We now use the union bound to show that we will fail to recover the exact sparsity structure with probability at most 2nd exp −mc 2 κ 2 32d .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pattern learning for relation extraction with a hierarchical topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Delort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garrido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to extract relations from the Web using minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Dantzig selector: Statistical estimation when p is much larger than n. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2313" to="2351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent variable graphical model selection via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1935" to="1967" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Systems for Molecular Biology (ISMB)</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aggregating crowdsourced binary ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference (WWW)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society C</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimally sparse representation in general (nonorthogonal) dictionaries via 1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the USA</title>
		<meeting>the National Academy of Sciences of the USA</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="2197" to="2202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Look ma, no hands: Analyzing the monotonic feature abstraction for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Baidu&apos;s Andrew Ng on the future of artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Eadicicco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Time</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning hidden variable networks: The information bottleneck approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="81" to="127" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comprehensive and reliable crowd assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An interior-point method for large-scale 1 -regularized logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1519" to="1555" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse online learning via truncated gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="777" to="801" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High-dimensional graphs and variable selection with the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1436" to="1462" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Google&apos;s hand-fed AI now gives answers, not just search results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Metz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature selection, l 1 vs. l 2 regularization, and rotational invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ranking and combining multiple predictors without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the USA</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1253" to="1258" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Grafting: Fast, incremental feature selection by gradient descent in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Theiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1333" to="1356" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets, quickly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Highdimensional Ising model selection using 1 -regularized logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1287" to="1319" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature-based models for improving the quality of noisy training data for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Information and Knowledge Management (CIKM)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining generative and discriminative model scores for distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Incremental knowledge base construction using DeepDive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1310" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reducing wrong labels in distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Just relax: Convex programming methods for identifying sparse signals in noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1030" to="1051" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sharp thresholds for high-dimensional and noisy sparsity recovery using 1 -constrained quadratic programming (lasso)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2183" to="2202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DimmWitted: A study of mainmemory statistical analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1283" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spectral methods meet EM: A provably optimal algorithm for crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On model selection consistency of lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2541" to="2563" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grafting-Light: Fast, incremental feature selection and structure learning of Markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
