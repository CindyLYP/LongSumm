<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Split-Level I/O Scheduling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suli</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison IBM Research-Almaden*</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Harter</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison IBM Research-Almaden*</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison IBM Research-Almaden*</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selvaraj</forename><surname>Salini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison IBM Research-Almaden*</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Kowsalya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison IBM Research-Almaden*</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Krishnamurthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison IBM Research-Almaden*</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rini</forename><forename type="middle">T</forename><surname>Al-Kiswany</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison IBM Research-Almaden*</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaushik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison IBM Research-Almaden*</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison IBM Research-Almaden*</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison IBM Research-Almaden*</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Split-Level I/O Scheduling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2815400.2815421</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We introduce split-level I/O scheduling, a new framework that splits I/O scheduling logic across handlers at three layers of the storage stack: block, system call, and page cache. We demonstrate that traditional block-level I/O schedulers are unable to meet throughput, latency, and isolation goals. By utilizing the split-level framework, we build a variety of novel schedulers to readily achieve these goals: our Actually Fair Queuing scheduler reduces priority-misallocation by 28×; our Split-Deadline scheduler reduces tail latencies by 4×; our Split-Token scheduler reduces sensitivity to interference by 6×. We show that the framework is general and operates correctly with disparate file systems (ext4 and XFS). Finally, we demonstrate that split-level scheduling serves as a useful foundation for databases (SQLite and Post-greSQL), hypervisors (QEMU), and distributed file systems (HDFS), delivering improved isolation and performance in these important application scenarios.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deciding which I/O request to schedule, and when, has long been a core aspect of the operating system storage stack <ref type="bibr" target="#b7">[11,</ref><ref type="bibr" target="#b9">13,</ref><ref type="bibr" target="#b18">22,</ref><ref type="bibr" target="#b23">27,</ref><ref type="bibr" target="#b24">28,</ref><ref type="bibr" target="#b25">29,</ref><ref type="bibr" target="#b27">31,</ref><ref type="bibr" target="#b34">38,</ref><ref type="bibr" target="#b39">43,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" target="#b41">45,</ref><ref type="bibr" target="#b50">54]</ref>. Each of these approaches has improved different aspects of I/O scheduling; for example, research in single-disk schedulers incorporated rotational awareness <ref type="bibr" target="#b24">[28,</ref><ref type="bibr" target="#b25">29,</ref><ref type="bibr" target="#b40">44]</ref>; other research tackled the problem of scheduling within a multi-disk array <ref type="bibr" target="#b49">[53,</ref><ref type="bibr" target="#b53">57]</ref>; more recent work has targeted flash-based devices <ref type="bibr" target="#b26">[30,</ref><ref type="bibr" target="#b32">36]</ref>, tailoring the behavior of the scheduler to this new and important class of storage device. All of these optimizations and techniques are important; in sum total, these systems can improve overall system performance dramatically <ref type="bibr" target="#b18">[22,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" target="#b53">57]</ref> as well as provide other desirable properties (including fairness across processes <ref type="bibr" target="#b13">[17]</ref> and the meeting of deadlines <ref type="bibr" target="#b52">[56]</ref>).</p><p>Most I/O schedulers (hereafter just "schedulers") are built at the block level within an operating system, beneath the file system and just above the device itself. Such block-level schedulers are given a stream of requests and are thus faced with the question: which requests should be dispatched, and when, in order to achieve the goals of the system?</p><p>Unfortunately, making decisions at the block level is problematic, for two reasons. First, and most importantly, the block-level scheduler fundamentally cannot reorder certain write requests; file systems carefully control write ordering to preserve consistency in the event of system crash or power loss <ref type="bibr" target="#b17">[21,</ref><ref type="bibr" target="#b21">25]</ref>. Second, the blocklevel scheduler cannot perform accurate accounting; the scheduler lacks the requisite information to determine which application was responsible for a particular I/O request. Due to these limitations, block-level schedulers cannot implement a full range of policies.</p><p>An alternate approach, which does not possess these same limitations, is to implement scheduling much higher in the stack, namely with system calls <ref type="bibr" target="#b15">[19]</ref>. System-call scheduling intrinsically has access to necessary contextual information (i.e., which process has issued an I/O). Unfortunately, system-call scheduling is no panacea, as the low-level knowledge required to build effective schedulers is not present. For example, at the time of a read or write, the scheduler cannot predict whether the request will generate I/O or be satisfied by the page cache, information which can be useful in reordering requests <ref type="bibr" target="#b8">[12,</ref><ref type="bibr" target="#b45">49]</ref>. Similarly, the file system will likely transform a single write request into a series of reads and writes, depending on the crash-consistency mechanism employed (e.g., journaling <ref type="bibr" target="#b21">[25]</ref> or copy-on-write <ref type="bibr" target="#b38">[42]</ref>); scheduling without exact knowledge of how much I/O load will be generated is difficult and error prone.</p><p>In this paper, we introduce split-level I/O scheduling, a novel scheduling framework in which a scheduler is constructed across several layers. By implementing a judiciously selected set of handlers at key junctures within the storage stack (namely, at the system-call, page-cache, and block layers), a developer can implement a scheduling discipline with full control over behavior and with no loss in high-or low-level information. Split schedulers can determine which processes issued I/O (via graph tags that track causality across levels) and accurately estimate I/O costs. Furthermore, memory notifications make schedulers aware of write work as soon as possible (not tens of seconds later when writeback occurs). Finally, split schedulers can prevent file systems from imposing orderings that are contrary to scheduling goals.</p><p>We demonstrate the generality of split scheduling by implementing three new schedulers: AFQ (Actually-Fair Queuing) provides fairness between processes, Split-Deadline observes latency goals, and Split-Token isolates performance. Compared to similar schedulers in other frameworks, AFQ reduces priority-misallocation errors by 28×, Split-Deadline reduces tail latencies by 4×, and Split-Token improves isolation by 6× for some workloads. Furthermore, the split framework is not specific to a single file system; integration with two file systems (ext4 <ref type="bibr" target="#b30">[34]</ref> and XFS <ref type="bibr" target="#b43">[47]</ref>) is relatively simple.</p><p>Finally, we demonstrate that the split schedulers provide a useful base for more complex storage stacks. Split-Token provides isolation for both virtual machines (QEMU) and data-intensive applications (HDFS), and Split-Deadline provides a solution to the database community's "fsync freeze" problem <ref type="bibr" target="#b0">[2,</ref><ref type="bibr">9,</ref><ref type="bibr">10]</ref>. In summary, we find split scheduling to be simple and elegant, yet compatible with a variety of scheduling goals, file systems, and real applications.</p><p>The rest of this paper is organized as follows. We evaluate existing frameworks and describe the challenges they face ( §2). We discuss the principles of split scheduling ( §3) and our implementation in Linux ( §4). We implement three split schedulers as case studies ( §5), discuss integration with other file systems ( §6), and evaluate our schedulers with real applications ( §7). Finally, we discuss related work ( §8) and conclude ( §9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>Block-level schedulers are severely limited by their inability to gather information from and exert control over other levels of the storage stack. As an example, we consider the Linux CFQ scheduler, which supports an ionice utility that can put a process in idle mode. According to the man page: "a program running with idle I/O priority will only get disk time when no other program has asked for disk I/O" <ref type="bibr" target="#b5">[7]</ref>. Unfortunately, CFQ has little control over write bursts from idle-priority processes, as writes are buffered above the block level.</p><p>We demonstrate this problem by running a normal process A alongside an idle-priority process B. A reads sequentially from a large file. B issues random writes over a one-second burst. <ref type="figure" target="#fig_0">Figure 1</ref> shows the result: B quickly finishes while A (whose performance is shown via the CFQ line) takes over five minutes to recover. Block-level schedulers such as CFQ are helpless to prevent processes from polluting write buffers with expensive I/O. As we will see, other file-system features such as journaling and delayed allocation are similarly problematic. The idle policy is one of many possible scheduling goals, but the difficulties it faces at the block level are not unique. In this section, we consider three different scheduling goals, identifying several shared needs ( §2.1). Next, we describe prior scheduling frameworks ( §2.2). Finally, we show these frameworks are fundamentally unable to meet scheduler needs when running in conjunction with a modern file system ( §2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Framework Support for Schedulers</head><p>We now consider three I/O schedulers: priority, deadline, and resource-limit, identifying what framework support is needed to implement these schedulers correctly.</p><p>Priority: These schedulers aim to allocate I/O resources fairly between processes based on their priorities [1]. To do so, a scheduler must be able to track which process is responsible for which I/O requests, estimate how much each request costs, and reorder higher-priority requests before lower-priority requests.</p><p>Deadline: These schedulers observe deadlines for I/O operations, offering predictable latency to applications that need it <ref type="bibr" target="#b2">[3]</ref>. A deadline scheduler needs to map an application's deadline setting to each request and issue lower-latency requests before other requests.</p><p>Resource-Limit: These schedulers cap the resources an application may use, regardless of overall system load. Limits are useful when resources are purchased and the seller does not wish to give away free I/O. Resource-Limit schedulers need to know the cost and causes of I/O operations in order to throttle correctly.</p><p>Although these schedulers have distinct goals, they have three common needs. First, schedulers need to be able to map causes to identify which process is responsible for an I/O request. Second, schedulers need to estimate costs in order to optimize performance and perform accounting properly. Third, schedulers need to be able to reorder I/O requests so that the operations most important to achieving scheduling goals are served first. Unfortunately, as we will see, current block-level and systemcall schedulers cannot meet all of these requirements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Framework Architectures</head><p>Scheduling frameworks offer hooks to which schedulers can attach. Via these hooks, a framework passes information and exposes control to schedulers. We categorize frameworks by the level at which the hooks are available. <ref type="figure" target="#fig_1">Figure 2</ref>(a) illustrates block-level scheduling, the traditional approach implemented in Linux <ref type="bibr" target="#b6">[8]</ref>, FreeBSD <ref type="bibr" target="#b37">[41]</ref>, and other systems <ref type="bibr" target="#b4">[6]</ref>. Clients initiate I/O requests via system calls, which are translated to block-level requests by the file system. Within the block-scheduling framework, these requests are then passed to the scheduler along with information describing them: their location on storage media, size, and the submitting process. Based on such information, a scheduler may reorder the requests according to some policy. For example, a scheduler may accumulate many requests in its internal queues and later dispatch them in an order that improves sequentiality. <ref type="figure" target="#fig_1">Figure 2</ref>(b) show the system-call scheduling architecture (SCS) proposed by Craciunas et al. <ref type="bibr" target="#b15">[19]</ref>. Instead of operating beneath the file system and deciding when block requests are sent to the storage device, a systemcall scheduler operates on top of the file system and decides when to issue I/O related system calls (read, write, etc.). When a process invokes a system call, the scheduler is notified. The scheduler may put the process to sleep for a time before the body of the system call runs. Thus, the scheduler can reorder the calls, controlling when they become active within the file system. <ref type="figure" target="#fig_1">Figure 2</ref>(c) shows the hooks of the split framework, which we describe in a later section ( §4.2). In addition to introducing novel page-cache hooks, the split framework supports select system-call and block-level hooks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">File-System Challenges</head><p>Schedulers allocate disk I/O to processes, but processes do not typically use hard disks or SSDs directly. Instead, processes request service from a file system, which in turn translates requests to disk I/O. Unfortunately, file systems make it challenging to satisfy the needs of the scheduler. We now examine the implications of writeback, delayed allocation, journaling, and caching for schedulers, showing how these behaviors fundamentally require a restructuring of the I/O scheduling framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Delayed Writeback and Allocation</head><p>Delayed writeback is a common technique for postponing I/O by buffering dirty data to write at a later time. Procrastination is useful because the work may go away by itself (e.g., the data could be deleted) and, as more work accumulates, more efficiencies can be gained (e.g., sequential write patterns may become realizable).</p><p>Some file systems also delay allocation to optimize data layout <ref type="bibr" target="#b30">[34,</ref><ref type="bibr" target="#b43">47]</ref>. When allocating a new block, the file system does not immediately decide its on-disk location; another task will decide later. More information (e.g., file sizes) becomes known over time, so delaying allocation enables more informed decisions.</p><p>Both delayed writeback and allocation involve filesystem level delegation, with one process doing I/O work on behalf of other processes. A writeback process submits buffers that other processes dirtied and may also dirty metadata structures on behalf of other processes. Such delegation obfuscates the mapping from requests to processes. To block-level schedulers, the writeback task sometimes appears responsible for all write traffic.</p><p>We evaluate Linux's priority-based block scheduler, CFQ (Completely Fair Queuing) [1], using an asynchronous write workload. CFQ aims to allocate disk time fairly among processes (in proportion to priority). We launch eight threads with different priorities, ranging from 0 (highest) to 7 (lowest): each writes to its own file sequentially. A thread's write throughput should be proportional to its priority, as shown by the expectation line of <ref type="figure" target="#fig_2">Figure 3</ref> (left). Unfortunately, CFQ ignores priorities, treating all threads equally. <ref type="figure" target="#fig_2">Figure 3</ref> (right) shows why: to CFQ all the requests appear to have a priority of 4, because the writeback thread (a priority-4 process) submits all the writes on behalf of the eight benchmark threads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Journaling</head><p>Many modern file systems use journals for consistent updates <ref type="bibr" target="#b11">[15,</ref><ref type="bibr" target="#b30">34,</ref><ref type="bibr" target="#b43">47]</ref>. While details vary across file systems, most follow similar journaling protocols to commit data to disk; here, we discuss ext4's ordered-mode to illustrate how journaling severely complicates scheduling.</p><p>When changes are made to a file, ext4 first writes the affected data blocks to disk, then creates a journal transaction which contains all related metadata updates and commits that transaction to disk, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. The data blocks (D 1 , D 2 , D 3 ) must be written before the journal transaction, as updates become durable as soon as the transaction commits, and ext4 needs to prevent the metadata in the journal from referring to data blocks containing garbage. After metadata is journaled, ext4 eventually checkpoints the metadata in place.</p><p>Transaction writing and metadata checkpointing are both performed by kernel processes instead of the processes that initially caused the updates. This form of write delegation also complicates cause mapping.</p><p>More importantly, journaling prevents block-level schedulers from reordering. Transaction batching is a well-known performance optimization <ref type="bibr" target="#b21">[25]</ref>, but block schedulers have no control over which writes are batched, so the journal may batch together writes that are important to scheduling goals with less-important writes. For example, in <ref type="figure" target="#fig_3">Figure 4</ref>, suppose A is higher priority than B. A's fsync depends on transaction commit, which depends on writing B's data. Priority is thus inverted.</p><p>When metadata (e.g., directories or bitmaps) is shared among files, journal batching may be necessary for correctness (not just performance). In <ref type="figure" target="#fig_3">Figure 4</ref>, the journal could have conceivably batched M 1 and M 2 separately; however, M 1 depends on D 2 , data written by a process C to a different file, and thus A's fsync depends on the persistence of C's data. Unfortunately (for schedulers), metadata sharing is common in file systems.</p><p>The inability to reorder is especially problematic for a deadline scheduler: a block-request deadline completely loses its relevance if one request's completion depends on the completion of unrelated I/Os. To demonstrate, we run two threads A (small) and B (big) with Linux's Block-Deadline scheduler <ref type="bibr" target="#b2">[3]</ref>, setting the block-request deadline to 20 ms for each. Thread A does 4 KB appends, calling fsync after each. Thread B does N bytes of random writes (N ranges from 16 KB to 4 MB) followed by an fsync. <ref type="figure" target="#fig_4">Figure 5</ref> shows that even though A only writes one block each time, A's fsync latency depends on how much data B flushes each time.</p><p>Most file systems enforce ordering for correctness, so these problems occur with other crash-consistency mechanisms as well. For example, in log-structured files systems <ref type="bibr" target="#b38">[42]</ref>, writes appended earlier are durable earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Caching and Write Amplification</head><p>Sequentially reading or writing N bytes from or to a file often does not result in N bytes of sequential disk I/O for several reasons. First, file systems use different disk layouts, and layouts change as file systems age; hence, sequential file-system I/O may become random disk I/O. Second, file-system reads and writes may be absorbed by caches or write buffers without causing I/O. Third, some file-system features amplify I/O. For example, reading a file block may involve additional metadata reads, and writing a file block may involve additional journal writes. These behaviors prevent system-call schedulers from accurately estimating costs.</p><p>To show how this inability hurts system-call schedulers, we evaluate SCS-Token <ref type="bibr" target="#b14">[18]</ref>. In SCS-Token, a process's resource usage is limited by the number of tokens it possesses. Per-process tokens are generated at a fixed rate, based on user settings. When the process issues a system call, SCS blocks the call until the process has enough tokens to pay for it.</p><p>We attempt to isolate a process A's I/O performance from a process B by throttling B's resource usage. If SCS-Token works correctly, A's performance will vary little with respect to B's I/O patterns. To test this behavior, we configure A to sequentially read from a large file while B runs workloads with different I/O patterns. Each of the B workloads involve repeatedly accessing R bytes sequentially from a 10 GB file and then randomly seeking to a new offset. We explore 7 values for R (from 4 KB to MB) for both reads and writes (14 workloads total). In each case, B is throttled to 10 MB/s.   <ref type="table" target="#tab_0">Table 1</ref> summarizes how different needs are met (or not) by each framework. The block-level framework fails to support correct cause mapping (due to write delegation such as journaling and delayed allocation) or control over reordering (due to file-system ordering requirements). The system-call framework solves these two problems, but fails to provide enough information to schedulers for accurate cost estimation because it lacks low-level knowledge. These problems are general to many file systems; even if journals are not used, similar issues arise from the ordering constraints imposed by other mechanisms such as copy-on-write techniques <ref type="bibr" target="#b12">[16]</ref> or soft updates <ref type="bibr" target="#b17">[21]</ref>. Our split framework meets all the needs in <ref type="table" target="#tab_0">Table 1</ref> by incorporating ideas from the other two frameworks and exposing additional memory-related hooks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Split Framework Design</head><p>Existing frameworks offer insufficient reordering control and accounting knowledge. Requests are queued, batched, and processed at many layers of the stack, thus the limitations of single-layer frameworks are unsurprising. We propose a holistic alternative: all important decisions about when to perform I/O work should be exposed as scheduling hooks, regardless of the level at which those decisions are made in the stack. We now discuss how these hooks support correct cause mapping ( §3.1), accurate cost estimation ( §3.2), and reordering ( §3.3).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block Syscall Split Cause Mapping</head><formula xml:id="formula_0">✖ ✔ ✔ Cost Estimation ✔ ✖ ✔ Reordering ✖ ✔ ✔</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cause Mapping</head><p>A scheduler must be able to map I/O back to the processes that caused it to accurately perform accounting even when some other process is submitting the I/O. Metadata is usually shared, and I/Os are usually batched, so there may be multiple causes for a single dirty page or a single request. Thus, the split framework tags I/O operations with sets of causes, instead of simple scalar tags (e.g., those implemented by Mesnier et al. <ref type="bibr" target="#b31">[35]</ref>).</p><p>Write delegation ( §2.3.1) further complicates cause mapping when one process is dirtying data (not just submitting I/O) on behalf of other processes. We call such processes proxies; examples include the writeback and journaling tasks. Our framework tags proxy process to identify the set of processes being served instead of the proxy itself. These tags are created when a process starts dirtying data for others and cleared when it is done. <ref type="figure" target="#fig_7">Figure 7</ref> illustrates how our framework tracks multiple causes and proxies. Processes P1 and P2 both dirty the same data page, so the page's tag includes both processes in its set. Later, a writeback process, P3, writes the dirty buffer to disk. In doing so, P3 may need to dirty the journal and metadata, and will be marked as a proxy for {P1, P2} (the tag is inherited from the page it is writing back). Thus, P1 and P2 are considered responsible when P3 dirties other pages, and the tag of these pages will be marked as such. The tag of P3 is cleared when it finishes submitting the data page to the block level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cost Estimation</head><p>Many policies require schedulers to know how much I/O costs, in terms of device time or other metrics. An I/O pattern's cost is influenced by file-system features, such as caches and write buffers, and by device properties (e.g., random I/O is cheaper on flash than hard disk).</p><p>Costs can be most accurately estimated at the lowest levels of the stack, immediately above hardware (or better still in hardware, if possible). At the block level, re- quest locations are known, so sequentiality-based models can estimate costs. Furthermore, this level is below all file-system features, so accounting is less likely to overestimate costs (e.g., by counting cache reads) or underestimate costs (e.g., by missing journal writes).</p><p>Unfortunately, writes may be buffered for a long time (e.g., 30 seconds) before being flushed to the block level. Thus, while block-level accounting may accurately estimate the cost of a write, it is not aware of most writes until some time after they enter the system via a write system call. Thus, if prompt accounting is more important than accurate accounting (e.g., for interactive systems), accounting should be done at the memory level. Without memory-level information, a scheduler could allow a low-priority process to fill the write buffers with gigabytes of random writes, as we saw earlier <ref type="figure" target="#fig_0">(Figure 1)</ref>.</p><p>Figure shows the trade-off between accounting at the memory level (write buffer) and block level (request queue). At the memory level, schedulers do not know whether dirty data will be deleted before a flush, whether other writers will overwrite dirty data, or whether I/O will be sequential or random. A scheduler can guess how sequential buffered writes will be based on file offsets, but delayed allocation prevents certainty about the layout. After a long delay, on-disk locations and other details are known for certain at the block level.</p><p>The cost of buffered writes depends on future workload behavior, which is usually unknown. Thus, we believe all scheduling frameworks are fundamentally limited and cannot provide cost estimation that is both prompt and accurate. Our framework exposes hooks at both the memory and block levels, enabling each scheduler to handle the trade-off in the manner most suitable to its goals. Schedulers may even utilize hooks at both levels. For example, Split-Token ( §5.3) promptly guesses write costs as soon as buffers are dirtied, but later revises that estimate when more information becomes available (e.g., when the dirty data is flushed to disk).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reordering</head><p>Most schedulers will want to reorder I/O to achieve good performance as well as to meet more specific goals (e.g., low latency or fairness). Reordering for performance requires knowledge of the device (e.g., whether it is useful to reorder for sequentiality), and is best done at a lower level in the stack. We enable reordering at the block level by exposing hooks for both block reads and writes.</p><p>Unfortunately, the ability to reorder writes at the block level is greatly limited by file systems ( §2.3.2). Thus, reordering hooks for writes (but not reads, which are not entangled by journals) are also exposed above the file system, at the system-call level. By controlling when write system calls run, a scheduler can control when writes become visible to the file system and prevent ordering requirements that conflict with scheduling goals.</p><p>Many storage systems have calls that modify metadata, such as mkdir and creat in Linux; the split framework also exposes these. This approach presents an advantage over the SCS framework, which cannot correctly schedule these calls. In particular, the cost of a metadata update greatly depends on file-system internals, of which SCS schedulers are unaware. Split schedulers, however, can observe metadata writes at the block level and accordingly charge the responsible applications.</p><p>File-system synchronization points (e.g., fsync or similar) require all dependent data to be flushed to disk and typically invoke the file system's ordering mechanism. Unfortunately, logically independent operations often must wait for the synchronized updates to complete ( §2.3.2), so the ability to schedule fsync is essential. Furthermore, writes followed by fsync are more costly than writes by themselves, so schedulers should be able to treat the two patterns differently. Thus, the split framework also exposes fsync scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Split Scheduling in Linux</head><p>Split-style scheduling could be implemented in a variety of storage stacks. In this work, we implement it in Linux, integrating with the ext4 and XFS file systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cross-Layer Tagging</head><p>In Linux, I/O work is described by different function calls and data structures at different layers. For example, a write request may be represented by (a) the arguments to vfs write at the system-call level, (b) a buffer head structure in memory, and (c) a bio structure at the block level. Schedulers in our framework see the same requests in different forms, so it is useful to have a uniform way to describe I/O across layers. We add a causes tagging structure that follows writes through the stack and identifies the original processes that caused an I/O operation. Split schedulers can thereby correctly map requests back to the application from any layer.</p><p>Writeback and journal tasks are marked as I/O proxies, as described earlier ( §3.1). In ext4, writeback calls the ext4 da writepages function ("da" stands for "delayed allocation"), which writes back a range of pages of a given file. We modify this function so that as it does allocation for the pages, it sets the writeback thread's proxy state as appropriate. For the journal proxy, we modify jbd2 (ext4's journal) to keep track of all tasks responsible for adding changes to the current transaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scheduling Hooks</head><p>We now describe the hooks we expose, which are split across the system-call, memory, and block levels. Table 2 lists a representative sample of the split hooks.</p><p>System Call: These hooks allow schedulers to intercept entry and return points for various I/O system calls. A scheduler can delay the execution of a system call by simply sleeping in the entry hook. Like SCS, we intercept writes, so schedulers can separate writes before the file system entangles them. Unlike SCS, we do not intercept reads (no file-system mechanism entangles reads, so scheduling reads below the cache is preferable). Two metadata-write calls, creat and mkdir, and the Linux synchronization call, fsync, are also exposed to the scheduler. It would be useful (and straightforward) to support other metadata calls in the future (e.g., unlink).</p><p>Note that in our implementation, the caller is blocked until the system call is scheduled. Other implementations are possible, such as buffering the system calls and returning immediately, or simply returning EAGAIN to tell the caller to issue the system call later. We choose this particular implementation because of its simplicity and POSIX compliance. Linux itself blocks writes (when there are too many dirty pages) and fsyncs, and most applications already deal with this behavior using separate threads; what we do is no different.</p><p>Memory: These hooks expose page-cache internals to schedulers. In Linux, a writeback thread (pdflush) decides when to pass I/O to the block-level scheduler, which then decides when to pass that I/O to disk. Both components are performing scheduling tasks, and separating them is inefficient (e.g., writeback could flush more aggressively if it knew when the disk was idle). We add two hooks to inform the scheduler when buffers are dirtied or deleted. The buffer-dirty hook notifies the scheduler when a process dirties a buffer or when a dirty buffer is modified. In the latter case, the framework tells the scheduler which processes previously dirtied the buffer; depending on policy, the scheduler could revise accounting statistics, shifting some (or all) of the responsibility for the I/O to the last writer. The buffer-free hooks tell the scheduler if a buffer is deleted before writeback. Schedulers can either rely on Linux to perform writeback and throttle write system calls to control how much dirty data accumulates before writeback, or they can take complete control of the writeback. We evaluate the trade-off of these two approaches later ( §7.1.2). Block: These hooks are identical to those in Linux's original scheduling framework; schedulers are notified when requests are added to the block level or completed by the disk. Although we did not modify the function interfaces at this level, schedulers implementing these hooks in our framework are more informed, given tags within the request structures that identify the responsible processes. The Linux scheduling framework has over a dozen other block-level hooks for initialization, request merging, and convenience. We support all these as well for compatibility, but do not discuss them here.</p><p>Implementing the split-level framework in Linux involves ∼300 lines of code, plus some file-system integration effort, which we discuss later ( §6). While representing a small change in the Linux code base, it enables powerful scheduling capabilities, as we will show.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overhead</head><p>In this section, we evaluate the time and space overhead of the split framework. In order to isolate framework overhead from individual scheduler overhead, we compare no-op schedulers implemented in both our framework and the block framework (a no-op scheduler issues all I/O immediately, without any reordering). <ref type="figure" target="#fig_9">Figure 9</ref> shows our framework imposes no noticeable time overhead, even with 100 threads.</p><p>The split framework introduces some memory overhead for tagging writes with causes structures ( §4.1). Memory overheads roughly correspond to the number of dirty write buffers. To measure this overhead, we instru- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory overhead is shown for an HDFS worker with 8 GB of RAM under a write-heavy workload. Maximum and average overhead is measured as a function of the Linux dirty ratio setting. dirty background ratio is set to half of dirty ratio.</head><p>ment kmalloc and kfree to track the number of bytes allocated for tags over time. For our evaluation, we run HDFS with a write-heavy workload, measuring allocations on a single worker machine. <ref type="figure" target="#fig_0">Figure 10</ref> shows the results: with the default Linux settings, average overhead is 14.5 MB (0.2% of total RAM); the maximum is 23.3 MB. Most tagging is on the write buffers; thus, a system tuned for more buffering should have higher tagging overheads. With a 50% dirty ratio [5], maximum usage is still only 52.2 MB (0.6% of total RAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Scheduler Case Studies</head><p>In this section, we evaluate the split framework's ability to support a variety of scheduling goals. We implement AFQ ( §5.1), Split-Deadline ( §5.2), and Split-Token ( §5.3), and compare these schedulers to similar schedulers in other frameworks. Unless otherwise noted, all experiments run on top of ext4 with the Linux 3.2.51 kernel (most XFS results are similar but usually not shown). Our test machine has an eight-core, 1.4 GHz CPU and 16 GB of RAM. We use 500 GB Western Digital hard drives (AAKX) and an 80 GB Intel SSD (X25-M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">AFQ: Actually Fair Queuing</head><p>As shown earlier ( §2.1), CFQ's inability to correctly map requests to processes causes unfairness, due to the lack of information Linux's elevator framework provides. Moreover, file-system ordering requirements limit CFQ's reordering options, causing priority inversions. In order to overcome these two drawbacks, we introduce AFQ (Actually Fair Queuing scheduler) to allocate I/O fairly among processes according to their priorities.</p><p>Design: AFQ employs a two-level scheduling strategy. Reads are handled at the block level and writes (and calls that cause writes, such as fsync) are handled at the system-call level. This design allows reads to hit the cache while protecting writes from journal entanglement. Beneath the journal, low-priority blocks may be prerequisites for high-priority fsync calls, so writes at the block level are dispatched immediately. AFQ chooses I/O requests to dequeue at the block and system-call levels using the stride algorithm <ref type="bibr" target="#b47">[51]</ref>. Whenever a block request is dispatched to disk, AFQ charges the responsible processes for the disk I/O. The I/O cost is based on a simple seek model. Evaluation: We compare AFQ to CFQ with four workloads, shown in <ref type="figure" target="#fig_0">Figure 11</ref>. <ref type="figure" target="#fig_0">Figure 11(a)</ref> shows read performance on AFQ and CFQ for eight threads, with priorities ranging from 0 (high) to 7 (low), each reading from its own file sequentially. We see that AFQ's performance is similar to CFQ, and both respect priorities. <ref type="figure" target="#fig_0">Figure 11</ref>(b) shows asynchronous sequential-write performance, again with eight threads. This time, CFQ fails to respect priorities because of write delegation, but AFQ correctly maps I/O requests via split tags, and thus respects priorities. On average, CFQ deviates from the ideal by 82%, AFQ only by 16% (a 5× improvement). <ref type="figure" target="#fig_0">Figure 11</ref>(c) shows synchronous random-write performance: we set up 5 threads per priority level, and each keeps randomly writing and flushing (with fsync) 4 KB blocks. The average throughput of threads at each priority level is shown. CFQ again fails to respect priority; using fsync to force data to disk invokes ext4's journaling mechanism and keeps CFQ from reordering to favor high-priority I/O. AFQ, however, blocks low-priority fsyncs when needed, improving throughput for highpriority threads. As shown, AFQ is able to respect priority, deviating from the ideal value only by 3% on average while CFQ deviates by 86% (a 28× improvement).</p><p>Finally, <ref type="figure" target="#fig_0">Figure 11(</ref>  write buffer. One thread at each priority level keeps overwriting 4 MB of data in its own file. Both CFQ and AFQ get very high performance as expected, though AFQ is slightly slower (AFQ needs to do significant bookkeeping for each write system call). The plot has no fairness goal line as there is no contention for disk resources.</p><p>In general, AFQ and CFQ have similar performance; however, AFQ always respects priorities, while CFQ only respects priorities for the read workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Deadline</head><p>As shown earlier ( <ref type="figure" target="#fig_1">Figure 5 in  §2.3.</ref>2), Block-Deadline does poorly when trying to limit tail latencies, due to its inability to reorder block I/Os in the presence of filesystem ordering requirements. Split-level scheduling, with system-call scheduling capabilities and memorystate knowledge, is better suited to this task.</p><p>Design: We implement the Split-Deadline scheduler by modifying the Linux deadline scheduler (Block-Deadline). Block-Deadline maintains two deadline queues and two location queues (for both read and write requests) <ref type="bibr" target="#b2">[3]</ref>. In Split-Deadline, an fsync-deadline queue is used instead of a block-write deadline queue. During operation, if no read request or fsync is going to expire, block-level read and write requests are issued from the location queues to maximize performance. If some read requests or fsync calls are expiring, they are issued before their deadlines.</p><p>Split-Deadline monitors how much data is dirtied for one file using the buffer-dirty hook and thereby estimates the cost of an fsync. If there is an fsync pending that may affect other processes by causing too much I/O, it will not be issued directly. Instead, the scheduler asks the kernel to launch asynchronous writeback of the file's dirty data and waits until the amount of dirty data drops to a point such that other deadlines would not be affected by issuing the fsync. Asynchronous writeback does not generate a file-system synchronization point and has no deadline, so other operations are not forced to wait.</p><p>Evaluation: We compare Split-Deadline to Block-Deadline for a database-like workload on both hard disk drive (HDD) and solid state drive (SSD). We set up two threads A (small) and B (big); thread A appends to a small file one block (4 KB) at a time and calls fsync (this mimics database log appends) while thread B writes 1024 blocks randomly to a large file and then calls fsync (this mimics database checkpointing). The deadline settings are shown in <ref type="table" target="#tab_3">Table 3</ref>. We choose shorter block-write deadlines than fsync deadlines because each fsync causes multiple block writes; however, our results do not appear sensitive to the exact values chosen. Linux's Block-Deadline scheduler does not support setting different deadlines for different processes, so we add this feature to enable a fair comparison. <ref type="figure" target="#fig_0">Figure 12</ref> shows the experiment results on both HDD and SSD. We can see that when no I/O from B is interfering, both schedulers give A low-latency fsyncs. After B starts issuing big fsyncs, however, Block-Deadline starts to fail: A's fsync latencies increase by an order of magnitude; this happens because B generates too much bursty I/O when calling fsync, and the scheduler has no knowledge of or control over when they are coming. Worse, A's operations become dependent on these I/Os.</p><p>With Split-Deadline, however, A's fsync latencies mostly fluctuate around the deadline, even when B is calling fsync after large writes. Sometimes A exceeds its goal slightly because our estimate of the fsync cost is not perfect, but latencies are always relatively near the target. Such performance isolation is possible because Split-Deadline can reorder to spread the cost of bursty I/Os caused by fsync without forcing others to wait.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Token Bucket</head><p>Earlier, we saw that SCS-Token <ref type="bibr" target="#b14">[18]</ref> fails to isolate performance ( <ref type="figure" target="#fig_1">Figure 6 in  §2.3.3)</ref>. In particular, the throughput of a process A was sensitive to the activities of another process B. SCS underestimates the I/O cost of some B workloads, and thus does not sufficiently throttle B. In this section, we evaluate Split-Token, a reimplementation of token bucket in our framework.</p><p>Design: As with SCS-Token, throttled processes are given tokens at a set rate. I/O costs tokens, I/O is blocked if there are no tokens, and the number of tokens that may be held is capped. Split-Token throttles a process's system-call writes and block-level reads if and only if the number of tokens is negative. System-call reads are never throttled (to utilize the cache). Block writes are never throttled (to avoid entanglement).</p><p>Our implementation uses memory-level and blocklevel hooks for accounting. The scheduler promptly charges tokens as soon as buffers are dirtied, and then revises when the writes are later flushed to the block level ( §3.2), charging more tokens (or refunding them) based on amplification and sequentiality. Tokens represent bytes, so accounting normalizes the cost of an I/O pattern to the equivalent amount of sequential I/O (e.g., 1 MB of random I/O may be counted as 10 MB).</p><p>Split-Token estimates I/O cost based on two models, both of which assume an underlying hard disk (simpler models could be used on SSD). When buffers are first dirtied at the memory level, a preliminary model estimates cost based on the randomness of request offsets within the file. Later, when the file system allocates space on disk for the requests and flushes them to the block level, a disk model revises the cost estimate. The second model is more accurate because it can consider more factors than the first model, such as whether the file system introduced any fragmentation, and whether the file is located near other files being written.</p><p>Evaluation: We repeat our earlier SCS experiments ( <ref type="figure" target="#fig_5">Figure 6</ref>) with Split-Token, as shown in <ref type="figure" target="#fig_0">Figure 13</ref>. We observe that whether B does reads or writes has little effect on A (the A lines are near each other). Whether B's pattern is sequential or random also has little impact (the lines are flat). Across all workloads, the standard deviation of A's performance is 7 MB, about a 6× improvement over SCS (SCS-Token's deviation was 41 MB). We now directly compare SCS-Token with Split-Token using a broader range of read and write workloads for process B. I/O can be random (expensive), sequential, or served from memory (cheap). As before, A is an unthrottled reader, and B is throttled to 1 MB/s of normalized I/O. <ref type="figure" target="#fig_0">Figure 14 (left)</ref> shows that Split-Token is near the isolation target all six times, whereas SCS-Token significantly deviates three times (twice by more than 50%), again showing Split-Token provides better isolation.</p><p>After isolation, a secondary goal is the best performance for throttled processes, which we measure in <ref type="figure" target="#fig_0">Figure 14 (right)</ref>. Sometimes B is faster with SCS-Token, but only because SCS-Token is incorrectly sacrificing isolation for A (e.g., B does faster random reads with SCS-Token, but A's performance drops over 80%). We consider the cases where SCS-Token did provide isolation. First, Split-Token is 2.3× faster for "read-mem". SCS-Token logic must run on every read system call, whereas Split-Token does not. SCS-Token still achieves nearly 2 GB/s, though, indicating cache hits are not throttled. Although the goal of SCS-Token was to do systemcall scheduling, Craciunas et al. needed to modify the file system to tell which reads are cache hits <ref type="bibr" target="#b15">[19]</ref>. Second, Split-Token is 837× faster for "write-mem". SCS-Token does write accounting at the system-call level, so it does not differentiate buffer overwrites from new writes. Thus, SCS-Token unnecessarily throttles B. With Split-Token, B's throughput does not reach 1 MB/s for "readseq" because the intermingled I/Os from A and B are no longer sequential; we charge it to both A and B.</p><p>We finally evaluate Split-Token for a large number of threads; we repeat the six workloads of <ref type="figure" target="#fig_0">Figure 14</ref>, this time varying the number of B threads performing the I/O task (all threads of B share the same I/O limit). <ref type="figure" target="#fig_0">Figure 15</ref> shows the results. For sequential read, the number of B threads has no impact on A's performance, as desired. We do not show random read, sequential write, or random write, as these lines would appear the same as the read-sequential line (varying at most by 1.7%). However, when B is reading or writing to memory, A's performance is only steady if B has 128 threads or less. Since the B threads do not incur any disk I/O, our I/O scheduler does not throttle them, leaving the B threads free to dominate the CPU, indirectly slowing A. To confirm this, we do an experiment (also shown in <ref type="figure" target="#fig_0">Figure 15)</ref> where B threads simply execute a spin loop, issuing no I/O; A's performance still suffers in this case. This reminds us of the usefulness of CPU schedulers in addition to I/O schedulers: if a process does not receive enough CPU time, it may not be able to issue requests fast enough to fully utilize the storage system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Implementation Effort</head><p>Implementing different schedulers within the split framework is not only possible, but relatively easy: Split-AFQ takes ∼950 lines of code to implement, Split-Deadline takes ∼750 lines of code, and Split-Token takes ∼950 lines of code. As a comparison, Block-CFQ takes more than 4000 lines of code, Block-Deadline takes ∼500 lines of code, and SCS-Token takes ∼2000 lines of code (SCS-Token is large because there is not a clean separation between the scheduler and framework).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">File System Integration</head><p>Thus far we have presented results with ext4; now, we consider the effort necessary to integrate ext4 and other file systems, in particular XFS, into the split framework. Integrating a file system involves (a) tagging relevant data structures the file system uses to represent I/O in memory and (b) identifying the proxy mechanisms in the file system and properly tagging the proxies.</p><p>In Linux, part (a) is mostly file-system independent as many file systems use generic page buffer data structures to represent I/O. Both ext4 and XFS rely heavily on the buffer head structure, which we already tag properly. Thus we are able to integrate XFS buffers with split tags by adding just two lines of code, and ext4 with less than 10 lines. In contrast, btrfs <ref type="bibr" target="#b29">[33]</ref> uses its own buffer structures, so integration would require more effort. Part (b), on the other hand, is highly file-system specific, as different file systems use different proxy mechanisms. For ext4, the journal task acts as a proxy when writing the physical journal, and the writeback task acts as a proxy when doing delayed allocation. XFS uses logical journaling, and has its own journal implementation. For a copy-on-write file system, garbage collection would be another important proxy mechanism. Properly tagging these proxies is a bit more involved. In ext4, it takes 80 lines of code across 5 different files. Fortunately, such proxy mechanisms typically only involve metadata, so for data-dominated workloads, partial integration with only (a) should work relatively well.</p><p>In order to verify the above hypotheses, we have fully integrated ext4 with the split framework, and only partially integrated XFS with part (a). We evaluate the effectiveness of our partial XFS integration on both dataintensive and metadata-intensive workloads. <ref type="figure" target="#fig_0">Figure 16</ref> repeats our earlier isolation experiment <ref type="figure" target="#fig_0">(Figure 13</ref>), but with XFS; these experiments are dataintensive. Split-Token again provides significant isolation, with A only having a deviation of 12.8 MB. In fact, all the experiments we show earlier are data intensive, and XFS has similar results (not shown) as ext4.  <ref type="figure" target="#fig_0">Figure 17</ref> shows the performance of a metadataintense workload for both XFS and ext4. In this experiment, A reads sequentially while B repeatedly creates empty files and flushes them to disk with fsync. B is throttled, A is not. B sleeps between each create for a time varied on the x-axis. As shown in the left plot, B's sleep time influences A's performance significantly with XFS, but with ext4 A is isolated. The right plot explains why: with ext4, B's creates are correctly throttled, regardless of how long B sleeps. With XFS, however, B is unthrottled because XFS does not give the scheduler enough information to map the metadata writes (which are performed by journal tasks) back to B.</p><p>We conclude that some file systems can be partially integrated with minimal effort, and data-intense workloads will be well supported. Support for metadata workloads, however, requires more effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Real Applications</head><p>In this section, we explore whether the split framework is a useful foundation for databases ( §7.1), virtual machines ( §7.2), and distributed file systems ( §7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Databases</head><p>To show how real databases could benefit from Split-Deadline's low-latency fsyncs, we measure transactionresponse time for SQLite3 <ref type="bibr" target="#b22">[26]</ref> and PostgreSQL <ref type="bibr">[10]</ref> running with both Split-Deadline and Block-Deadline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">SQLite3</head><p>We run SQLite3 on a hard disk drive. For Split-Deadline, we set short deadlines (100 ms) for fsyncs on the writeahead log file and reads from the database file and set long deadlines (10 seconds) for fsyncs on the database file. For Block-Deadline, the default settings (50 ms for block reads and 500 ms for block writes) are used. We make minor changes to SQLite to allow concurrent log appends and checkpointing and to set appropriate deadlines. For our benchmark, we randomly update rows in a large table, measure transaction latencies, and run checkpointing in a separate thread whenever the number of dirty buffers reaches a threshold. <ref type="figure" target="#fig_0">Figure 18</ref>(a) shows the transaction tail latencies (99th and 99.9th percentiles) when we change the checkpoint- ing threshold. When checkpoint thresholds are larger, checkpointing is less frequent, fewer transactions are affected, and thus the 99th line falls. Unfortunately, this approach does not eliminate tail latencies; instead, it concentrates the cost on fewer transactions, so the 99.9th line continues to rise. In contrast, <ref type="figure" target="#fig_0">Figure 18</ref>(b) shows that Split-Deadline provides much smaller tail latencies (a 4× improvement for 1K buffers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">PostgreSQL</head><p>We run PostgreSQL [10] on top of an SSD and benchmark it using pgbench <ref type="bibr" target="#b3">[4]</ref>, a TPC-B like workload. We change PostgreSQL to set I/O deadlines for each worker thread. We want consistently low transaction latencies (within 15 ms), so we set the foreground fsync deadline to 5 ms, and the background checkpointing fsync deadline to 200 ms for Split-Deadline. For Block-Deadline, we set the block write deadline to 5 ms. For block reads, a deadline of 5 ms is used for both Split-Deadline and Block-Deadline. Checkpoints occur every 30 seconds. <ref type="figure" target="#fig_0">Figure 19</ref> shows the cumulative distribution of the transaction latencies. We can see that when running on top of Block-Deadline, 4% of transactions fail to meet their latency target, and over 1% take longer than 500 ms. After further inspection, we found that the latency spikes happen at the end of each checkpoint period, when the system begins to flush a large amount of dirty data to disk using fsync. Such data flushing interferes with foreground I/Os, causes long transaction latency and low system throughput. The database community has long experienced this "fsync freeze" problem, and has no great solution for it <ref type="bibr" target="#b0">[2,</ref><ref type="bibr">9,</ref><ref type="bibr">10]</ref>. We show next that Split-Deadline provides a simple solution to this problem.</p><p>When running Split-Deadline, we have the ability to schedule fsyncs and minimize their performance impact to foreground transactions. However, pdflush (Linux's writeback task) may still submit many writeback I/Os without scheduler involvement and interfere with foreground I/Os. Split-Deadline maintains deadlines in this case by limiting the amount of data pdflush may flush at any given time by throttling write system calls. In <ref type="figure">Fig</ref> When pdflush is disabled, though, Split-Deadline has complete control of writeback, and can allow more dirty data in the system without worrying about untimely writeback I/Os. It then initiates writeback in a way that both observes deadlines and optimizes performance, thus eliminating tail latencies while maintaining low median latencies, as shown in <ref type="figure" target="#fig_0">Figure 19</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Virtual Machines (QEMU)</head><p>Isolation is especially important in cloud environments, where customers expect to be isolated from other (potentially malicious) customers. To evaluate our framework's usefulness in this environment, we repeat our token-bucket experiment in <ref type="figure" target="#fig_0">Figure 14</ref>, this time running the unthrottled process A and throttled process B in separate QEMU instances. The guests run a vanilla kernel; the host runs our modified kernel. Thus, throttling is on the whole VM, not just the benchmark we run inside. We use an 8 GB machine with a four-core 2.5 GHz CPU. <ref type="figure" target="#fig_1">Figure 20</ref> shows the results for QEMU running over both SCS and Split-Token on the host. The isolation results for A (left) are similar to the results when we ran A and B directly on the host <ref type="figure" target="#fig_0">(Figure 14)</ref>: with Split-Token, A is always well isolated, but with SCS, A experiences major slowdowns when B does random I/O.</p><p>The throughput results for B (right) are more interesting: whereas before SCS greatly slowed memory-bound workloads, now SCS and Split-Token provide equal performance for these workloads. This is because when a throttled process is memory bound, it is crucial for performance that a caching/buffering layer exist above the scheduling layer. The split and QEMU-over-SCS stacks have this property (and memory workloads are fast), but the raw-SCS stack does not. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Distributed File Systems (HDFS)</head><p>To show that local split scheduling is a useful foundation to provide isolation in a distributed environment, we integrate HDFS with Split-Token to provide isolation to HDFS clients. We modify the client-to-worker protocol so workers know which account should be billed for disk I/O generated by the handling of a particular RPC call. Account information is propagated down to Split-Token and across to other workers (for pipelined writes). We evaluate our modified HDFS on a 256-core Cloud-Lab cluster (one NameNode and seven workers, each with 32 cores). Each worker has 8 GB of RAM and a 1 TB disk. We run an unthrottled group of four threads and a throttled group of four threads. Each thread sequentially writes to its own HDFS file. <ref type="figure" target="#fig_0">Figure 21(a)</ref> shows the result for varying rate limits on the x-axis. The summed throughput (i.e., that of both throttled and unthrottled writers) is similar to throughput when HDFS runs over CFQ without any priorities set. With Split-Token, though, smaller rate caps on the throttled threads provide the unthrottled threads with better performance (e.g., the gray bars get more throughput when the black bars are locally throttled to 16 MB/s).</p><p>Given there are seven datanodes, and data must be triply written for replication, the expected upper bound on total I/O is (ratecap/3) * 7. The dashed lines show these upper bounds in <ref type="figure" target="#fig_0">Figure 21(a)</ref>; the black bars fall short. We found that many tokens go unused on some workers due to load imbalance. The hashed black bars represent the potential HDFS write I/O that was thus lost.</p><p>In <ref type="figure" target="#fig_0">Figure 21</ref>(b), we try to improve load balance by decreasing the HDFS block size from 64 MB (the default) to 16 MB. With smaller blocks, fewer tokens go unused, and the throttled writers achieve I/O rates nearer the upper bound. We conclude that local scheduling can be used to meet distributed isolation goals; however, throttled applications may get worse-than-expected performance if the system is not well balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Multi-Layer Scheduling: A number of works argue that efficient I/O scheduling requires coordination at multiple layers in the storage stack <ref type="bibr" target="#b41">[45,</ref><ref type="bibr" target="#b46">50,</ref><ref type="bibr" target="#b48">52,</ref><ref type="bibr" target="#b52">56]</ref>. Riska et al. <ref type="bibr" target="#b36">[40]</ref> evaluated the effectiveness of optimizations at various layers of the I/O path, and found that superior performance is yielded by combining optimizations at various layers. Redline <ref type="bibr" target="#b52">[56]</ref> tries to avoid system unresponsiveness during fsync by scheduling at both the buffer cache level and the block level. Argon <ref type="bibr" target="#b46">[50]</ref> combines mechanisms at different layers to achieve performance insulation. However, compared to these ad-hoc approaches, our framework provides a systematic way for schedulers to plug in logic at different layers of the storage stack while still maintaining modularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cause Mapping and Tagging:</head><p>The need for correctly accounting resource consumption to the responsible entities arises in different contexts. Banga et al. <ref type="bibr" target="#b10">[14]</ref> found that kernel consumes resources on behalf of applications, causing difficulty in scheduling. The hypervisor may also do work on behalf of a virtual machine, making it difficult to isolate performance <ref type="bibr" target="#b20">[24]</ref>. We identify the same problem in I/O scheduling, and propose tagging as a general solution. Both Differentiated Storage Services (DSS) <ref type="bibr" target="#b31">[35]</ref> and IOFlow <ref type="bibr" target="#b44">[48]</ref> also tag data across layers. DSS tags the type of data, IOFlow tags the type and cause, and split scheduling tags with a set of causes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software-Defined Storage Stack:</head><p>In the spirit of moving toward a more software-defined storage (SDS) stack, the split-level framework exposes knowledge and control at different layers to a centralized entity, the scheduler. The IOFlow <ref type="bibr" target="#b44">[48]</ref> stack is similar to split scheduling in this regard; both tag I/O across layers and have a central controller.</p><p>IOFlow, however, operates at the distributed level; the lowest IOFlow level is an SMB server that resides above a local file system. IOFlow does not address the core file-system issues, such as write delegation or ordering requirements, and thus likely has the same disadvantages as system-call scheduling. We believe that the problems introduced by the local file systems, which we identify and solve in this paper, are inherent to any storage stack. We argue any complete SDS solutions would need to solve them and thus our approach is complementary. Combining IOFlow with split scheduling, for example, could be very useful: flows could be tracked through hypervisor, network, and local-storage layers.</p><p>Shue et al. <ref type="bibr" target="#b42">[46]</ref> provision I/O resources in a keyvalue store (Libra) by co-designing the application and I/O scheduler; however, they noted that "OS-level effects due to filesystem operations [ . . . ]are beyond Libra's reach"; building such applications with the split framework should provide more control.</p><p>Exposing File-System Mechanisms: Split-level scheduling requires file systems to expose certain mechanisms (journaling, delayed allocation, etc.) to the framework by properly tagging them as proxies. Others have also found that exposing file-system information is helpful <ref type="bibr" target="#b16">[20,</ref><ref type="bibr" target="#b33">37,</ref><ref type="bibr" target="#b51">55]</ref>. For example, in Featherstitch <ref type="bibr" target="#b16">[20]</ref>, filesystem ordering requirements are exposed to the outside as dependency rules so that the kernel can make informed decisions about writeback.</p><p>Other I/O Scheduling Techniques: Different approaches have been proposed to improve different aspects of I/O scheduling: to better incorporate rotationalawareness <ref type="bibr" target="#b24">[28,</ref><ref type="bibr" target="#b25">29,</ref><ref type="bibr" target="#b40">44]</ref>, to better support different storage devices <ref type="bibr" target="#b26">[30,</ref><ref type="bibr" target="#b32">36]</ref>, or to provide better QoS guarantees <ref type="bibr" target="#b19">[23,</ref><ref type="bibr" target="#b28">32,</ref><ref type="bibr" target="#b35">39]</ref>. All these techniques are complementary to our work and can be incorporated into our framework as new schedulers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we have shown that single-layer schedulers operating at either the block level or systemcall level fail to support common goals due to a lack of coordination with other layers. While our experiments indicate that simple layering must be abandoned, we need not sacrifice modularity. In our split framework, the scheduler operates across all layers, but is still abstracted behind a collection of handlers. This approach is relatively clean, and enables pluggable scheduling. Supporting a new scheduling goal simply involves writing a new scheduler plug-in, not reengineering the entire storage system. Our hope is that split-level scheduling will inspire future vertical integration in storage stacks. Our source code is available at http://research.cs.wisc.edu/adsl/Software/split.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Write Burst. B's one-second random-write burst severely degrades A's performance for over five minutes. Putting B in CFQ's idle class provides no help.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Scheduling Architectures. The boxes show where scheduler hooks exist for reordering I/O requests or doing accounting. Sometimes reads and writes are handled differently at different levels, as indicated by "R" and "W".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>CFQ Throughput. The left plot shows sequential write throughput for different priorities. The right plot the portion of requests for each priority seen by CFQ. Unfortunately, the "Completely Fair Scheduler" is not even slightly fair for sequential writes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Journal Batching. Arrows point to events that must occur before the event from which they point. The event for the blocks is a disk write. The event for an fsync is a return.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>I/O Latency Dependencies. Thread A keeps issuing fsync to flush one block of data to disk, while thread B flushes multiple blocks using fsync. This plot shows how A's latency depends on B's I/O size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>SCS Token Bucket: Isolation. The performance of two processes is shown: a sequential reader, A, and a throttled process, B. B may read (black) or write (gray), and performs runs of different sizes (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6</head><label>6</label><figDesc>shows how A's performance varies with B's I/O patterns. Note the large gap between the performance of A with B reading vs. writing. When B is performing sequential writes, A's throughput is as high as 125 MB/s; when B is performing random reads, A's throughput drops to 25 MB/s in the worst case. Writes appear cheaper than reads because write buffers absorb I/O and make it more sequential. Across the 14 workloads, A's throughput has a standard deviation of 41 MB, indicating A is highly sensitive to B's patterns. SCS-Token fails to isolate A's performance by throttling B, as SCS-Token cannot correctly estimate the cost of B's I/O pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Set Tags and I/O Proxies. Our tags map metadata and journal I/O to the real causes, P1 and P2, not P3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Accounting: Memory vs. Block Level. Disk locations for buffered writes may not be known (indicated by the question marks on the blocks) if allocations are delayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Time Overhead. The split framework scales well with the number of concurrent threads doing I/O to an SSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Space Overhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>AFQ Priority. The plots show the percentage of throughput that threads of each priority receive. The lines show the goal distributions; the labels indicate total throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Fsync Latency Isolation. Dashed and solid lines present the goal latencies of A and B respectively. Dots represent the actual latency of B's calls, and pluses represent the actual latency of A's calls. The shaded area represents the time when B's fsyncs are being issued.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Isolation: Split-Token with ext4. The same as Figure 6, but for our Split-Token implementation. A is the unthrottled sequential reader, and B is the throttled process performing I/O of different run sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Split-Token vs. SCS-Token. Left: A's throughput slowdown is shown. Right: B's performance is shown. Process A achieves about 138 MB/s when running alone, and B is throttled to 1 MB/s of normalized I/O, so there should be a 0.7% slowdown for A (shown by a target line). The x-axis indicates B's workload; A always reads sequentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Split-Token Scalability. A's throughput isshown as a function of the number of B threads performing a given activity. Goal performance is 101.7 MB (these numbers were taken on a 32-core CloudLab node with a 1 TB drive).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :Figure 17 :</head><label>1617</label><figDesc>Isolation: Split-Token with XFS. This is the same as Figure 6 and Figure 14, but for XFS running with our Split implementation of token bucket. Metadata: Split-Token with XFS and ext4. Process A sequentially reads while B creates and flushes new, empty files. A's throughput is shown as function of how long B sleeps between operations (left). B's create frequency is also shown for the same experiments (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 :</head><label>18</label><figDesc>SQLite Transaction Latencies. 99th and 99.9th percentiles of the transaction latencies are shown. The x-axis is the number of dirty buffers we allow before checkpoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 :</head><label>19</label><figDesc>PostgreSQL Transaction Latencies. A CDF of transaction latencies is shown for three systems. Split-Pdflush is Split-Deadline, but with pdflush controlling writeback separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 21 :</head><label>21</label><figDesc>HDFS Isolation. Solid-black and gray bars represent the total throughput of throttled and unthrottled HDFS writers, respectively. Dashed lines represent an upper bound on throughput; solid lines represent Block-CFQ throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Framework Properties. A ✔ indicates a given scheduling functionality can be supported with the framework, and an ✖ indicates a functionality cannot be supported.</figDesc><table><row><cell></cell><cell>!</cell><cell></cell></row><row><cell>!</cell><cell>!</cell><cell>!</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Split Hooks. The "Origin" column shows which hooks are new and which are borrowed from other frameworks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Deadline Settings.</figDesc><table><row><cell>For Block-Deadline, we</cell></row><row><cell>set deadlines for block-level writes; for Split-Deadline, we set</cell></row><row><cell>deadlines for fsyncs.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the anonymous reviewers and Angela Demke Brown (our shepherd) for their insightful feedback. We thank the members of the ADSL research group for their suggestions and comments on this work at various stages.</p><p>This material was supported by funding from NSF grants CNS-1421033, CNS-1319405, and CNS-1218405 as well as generous donations from Cisco, EMC, Facebook, Google, Huawei, IBM, Los Alamos National Laboratory, Microsoft, NetApp, Samsung, Symantec, Seagate, and VMware as part of the WISDOM research institute sponsorship. Tyler Harter is supported by the NSF Fellowship. Samer Al-Kiswany is supported by the NSERC Postdoctoral Fellowship. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and may not reflect the views of NSF or other institutions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.postgresql.org/message-id/" />
		<title level="m">Database/kernel community topic at collaboration summit 2014</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gc10663@suse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deadline</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/block/deadline-iosched.txt" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Documentation for pgbench</title>
		<ptr target="http://http://www" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://technet.microsoft.com/en-us/magazine/2007.02.vistakernel.aspx" />
		<title level="m">Inside the Windows Vista Kernel: Part 1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<ptr target="http://linux.die.net/man/1/ionice" />
		<title level="m">Linux man page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Notes on the Generic Block Layer Rewrite in Linux 2</title>
		<ptr target="https://www.kernel.org/doc/Documentation/block/biodoc.txt" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Avoiding File System Micromanagement with Range Writes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;08</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information and Control in Gray-Box Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="43" to="56" />
			<date type="published" when="2001" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Systems</surname></persName>
		</author>
		<title level="m">Three Easy Pieces. Arpaci-Dusseau Books</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Resource containers: A new facility for resource management in server systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="45" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Best</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Overview</surname></persName>
		</author>
		<ptr target="http://jfs.sourceforge.net/project/pub/jfs.pdf" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bonwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zfs</surname></persName>
		</author>
		<ptr target="http://opensolaris.org/os/community/zfs/docs/zfs_last.pdf" />
		<title level="m">The Last Word in File Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time disk scheduling in a mixed-media file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Time Technology and Applications Symposium</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The TAP Project: Traffic Shaping System Calls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Craciunas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Röck</surname></persName>
		</author>
		<ptr target="http://tap.cs.uni-salzburg.at/downloads.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">I/O Resource Management Through System Call Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Craciunas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Röck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="44" to="54" />
			<date type="published" when="2008-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized File System Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mammarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Los Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hovsepian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;07</title>
		<meeting><address><addrLine>Stevenson, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="page" from="307" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Soft Updates: A Solution to the Metadata Update Problem in File Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Mckusick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="127" to="153" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>TOCS)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Characterization of Linux Kernel Behavior Under Errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalbarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSN &apos;03</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An Arrival Curve Based Approach for QoS Guarantees in Shared Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Varman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pclock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 2007 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
	<note>SIGMETRICS &apos;07, ACM</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Enforcing performance isolation across virtual machines in xen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="342" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reimplementing the Cedar File System Using Logging and Group Commit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;87</title>
		<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sqlite</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disk scheduling: FCFS vs.SSTF revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="645" to="653" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Implementation of a Rotation-Latency-Sensitive Disk Scheduler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chiueh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000-03" />
			<pubPlace>SUNY, Stony Brook</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. ECSL-TR81</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Disk Scheduling Algorithms Based on Rotational Position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
		<idno>HPL-CSP-91-7</idno>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Hewlett Packard Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Disk Schedulers for Solid State Drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
		<idno>EMSOFT &apos;09</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh ACM International Conference on Embedded Software</title>
		<meeting>the Seventh ACM International Conference on Embedded Software<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards Higher Disk Head Utilization: Extracting &quot;Free&quot; Bandwidth From Busy Disk Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lumb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;00</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facade: Virtual storage devices with performance guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Lumb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST &apos;03</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Btrfs Filesystem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mason</surname></persName>
		</author>
		<ptr target="oss.oracle.com/projects/btrfs/dist/documentation/btrfs-ukuug.pdf" />
		<imprint>
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The New Ext4 Filesystem: Current Status and Future Plans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dilge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ottawa Linux Symposium (OLS &apos;07</title>
		<meeting><address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Differentiated Storage Services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mesnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Akers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM Symposium on Operating Systems Principles (SOSP &apos;11) (Cascais</title>
		<meeting>the 23rd ACM Symposium on Operating Systems Principles (SOSP &apos;11) (Cascais<address><addrLine>Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient Flash I/O Scheduler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fios: A Fair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">All File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chidambaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alagappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Kiswany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>Broomfield, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Portable I/O Scheduling with the Disk Mimic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="297" to="310" />
		</imprint>
	</monogr>
	<note>General Track</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient Guaranteed Disk Request Scheduling with Fahrrad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Povzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaldewey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maltzahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys &apos;08</title>
		<meeting><address><addrLine>Glasgow, Scotland UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating Block-level Optimization Through the IO Path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Riska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Larkby-Lahet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX &apos;07</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Checconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sched</surname></persName>
		</author>
		<ptr target="http://info.iet.unipi.it/~luigi/papers/20090508-geom_sched-slides.pdf" />
		<title level="m">Framework for Disk Scheduling within GEOM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Design and Implementation of a Log-Structured File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="52" />
			<date type="published" when="1992-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An Introduction to Disk Drive Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ruemmler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="17" to="28" />
			<date type="published" when="1994-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Disk Scheduling Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Winter &apos;90</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-01" />
			<biblScope unit="page" from="313" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cello: A Disk Scheduling Framework for Next-generation Operating Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS &apos;98</title>
		<meeting><address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06" />
			<biblScope unit="page" from="44" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">From application requests to Virtual IOPs: Provisioned key-value storage with Libra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth European Conference on Computer Systems</title>
		<meeting>the Ninth European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Scalability in the XFS File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doucette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX 1996</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">IOFlow: A Software-Defined Storage Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>O'shea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Talpey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="182" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Latency Management in Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Meter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;00</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10" />
			<biblScope unit="page" from="103" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Argon: Performance insulation for shared storage servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abd-El-Malek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST &apos;07</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Stride Scheduling: Deterministic Proportional Share Resource Management. Massachusetts Institute of Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Weihl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Laboratory for Computer Science</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Balancing fairness and efficiency in tiered storage systems with bottleneck-aware allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Varman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST &apos;13</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The HP AutoRAID Hierarchical Storage System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Staelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="136" />
			<date type="published" when="1996-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scheduling Algorithms for Modern Disk Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMET-RICS &apos;94</title>
		<meeting><address><addrLine>Nashville, TN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-05" />
			<biblScope unit="page" from="241" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">EXPLODE: A Lightweight, General System for Finding Serious Storage System Errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Engler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;06</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Redline: First Class Support for Interactivity in Commodity Operating Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E B</forename><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;08</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Trading Capacity for Performance in a Disk Array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krish-Namurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;00</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
