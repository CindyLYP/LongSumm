{"1008": "a summary generated by editnet may include sentences that were either extracted , abstracted or of both types. moreover, per considered sentence, editnet may decide not to take either of these decisions and completely reject sentence. using the cnn/dailymail dataset we demonstrate that editnet s summarization quality is highly competitive to that obtained by both the state-of-the-art abstractive-only and extractive-only baselines. in order to make an educated decision, the editor considers both sentence representations esi and asi as its input, together with two additional auxiliary representations. given the four representations as an input, the editor s decision for sentence si s is implemented using two fully -connected layers. such a network architecture allows to capture various complex interactions between the different inputs. for example, the network may learn that given the global context, one of the sentence versions may allow to produce a summary with a better coverage. as another example, based on the interaction between both sentence versions with either of the local or global contexts ( and possibly among the last two), the network may learn that both sentence versions may only add superfluous or redundant information to the summary, and therefore, decide to reject both. this paper introduces a novel soft labeling approach for extracting and abstracting the contextual information of a text. given a text input with l extracted sentences, we utilize a given summary quality metric r (s) which can be used to evaluate the quality of any given summary s. we next explain how each soft-label y ( i) is estimated. given text s ( with l extracted sentences) , let ( 1 , l) denote its editing decisions 1such a representation is basically a combination of a temporal convolutional model followed by a bilstm encoder. 2the first and last chunks would only have two consecutive sentences. we define the following soft crossentropy loss: l ( s) l sisie, a, r y ( i) log p ( i) , where , for a given sentence si s, y ( i) denotes its soft-label for decision. we next explain how we train the editor using a novel soft labeling approach. we conclude this section with the description of how we train the editor using a novel soft labeling approach. this demonstrates that, editnet has a high capability of utilizing abstraction, while being also able to maintain or reject the original extracted text whenever it is estimated to provide the best benefit for the summarys quality.", "1000": "###", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1007": "###", "1009": "###", "1010": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1018": "###", "1019": "###", "1020": "###", "1021": "###"}