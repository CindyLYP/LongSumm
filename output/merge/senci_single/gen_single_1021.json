{"1021": "feature selection is an important problem in statistics and machine learning for interpretable predictive modeling and scientific discoveries. code is available at url. this paper considers the problem of finding a feature selection gate that is maximal with respect to the product of marginals pxpy and the joint distribution of ( x, y, pw xpy) and ( x, y, pw xpy) . this problem can be written as finding a sparse selector or gate w rdx such thatd ( pw x, y, pw xpy) is maximal, i.e. supw, w0sd ( pw x, y, pw xpy) this problem can be written in the following penalized form: ( p) : sup ff epxyf ( x , y) epxpyf ( x, y) w0 we can relabel f ( x , y) f ( x , y) and write ( p) as: supff epxyf ( x , y) epxpyf ( x, y) w0 we can relabel f ( x , y) f ( x , y) epxpyf ( x, y) w0 we can relabel f ( x , y) f ( x , y) epxpyf ( x, y) w0 we can re in this paper we propose a new method for feature selection based on nonlinear sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in multiple kernel learning and group sparsity in group sparsity in multiple kernel learning and group sparsity in multiple kernel learning in this paper we propose to learn a feature map as a deep neural network. the architecture of the network can be problem dependent, but we focus here on a particular architecture: deep relu networks with biases removed. as we show below, using our sparsity inducing gradient penalties with such networks, results in input sparsity at the level of the witness function f of sic. this is desirable since it allows for an interpretable model, similar to the effect of lasso with linear models, our sparsity inducing gradient penalties result in a nonlinear self-explainable witness function f 23, with explicit sparse dependency on the inputs. deep relu networks with no biases, homogeneity and input sparsity via gradient penalties. in this paper two methods that provably control the false discovery rate ( fdr) in feature selection is an important problem for reproducible discoveries. inspired by importance scores in random forest, we define boosted sic as the arithmetic mean or the geometric mean of controlling the false discovery rate ( fdr) in feature selection is an important problem for reproducible discoveries. we are interested in measuring the conditional dependency between a feature xj and the response variable y conditionally on the other features noted xj hence we propose to use generative models for sampling from xj xj. the principle in hrt that we specify here for sic in algorithm is the following: instead of refitting sic under h0, we evaluate the mean of the witness function of sic on a holdout set sampled under h0 ( using conditional generators for r rounds) and obtain that has now twice the dimension , i.e for each real feature j, there is the real importance score jdx and the importance score jdx. the deviation of the mean of the witness function under h0 from the real distribution gives us p-values. we use the benjamini-hochberg procedure on those p-values to achieve a target fdr. sic is an interpretable linear model that is able to capture nonlinear relationships and are successful in practice as they can capture nonlinear relationships similar to sic. random forests have a heuristic for determining feature importance and are successful in practice as they can capture nonlinear relationships similar to sic. we believe sic can potentially leverage the deep learning toolkit for going beyond tabular data where random forests excel, to more structured data such as time series or graph data. finally, sic relates to saliency based posthoc interpretation of deep models such as while those method use the gradient information for a posthoc analysis, sic incorporates this information to guide the learning towards the important features. we first validate our methods and compare them to baseline models in simulation studies on synthetic datasets where the ground truth is available by construction. for this we generate the data according to a model y f ( x) where the model f ( x) and the noise define the specific synthetic dataset. in particular, the value of y only depends on a subset of features xi, i 1, , p through f (), and performance is quantified in terms of tpr and fdr in discovering them among the irrelevant features. in this paper we introduce a new dependency measure called sic, which can be seen as quantifying how much dependency as measured by a coordinate j. it is to the best of our knowledge the first dependency criterion that decomposes in the sum of contributions of each coordinate, and hence it is an interpretable dependency measure. moreover, j are normalized importance scores of each feature j, and their ranking can be used to assess feature importance. the main advantage compared to lasso is that we have a highly nonlinear decision function, that has better capacity of capturing dependencies between x and y nonconvex sic with stochastic block coordinate descent ( bcd) . we are interested in measuring the conditional dependency between a feature xj and the other features noted xj hence we have the following null hypothesis: h0 : xj y xj pxj xjpyxxj in order to simulate the null hypothesis, we propose to use generative models for sampling from xj xj.", "1000": "###", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1007": "###", "1008": "###", "1009": "###", "1010": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1018": "###", "1019": "###", "1020": "###"}