<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NEWSQA: A MACHINE COMPREHENSION DATASET</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-02-07">7 Feb 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
							<email>adam.trischler@maluuba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
							<email>tong.wang@maluuba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
							<email>justin.harris@maluuba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
							<email>alessandro.sordoni@maluuba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
							<email>phil.bachman@maluuba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
							<email>k.suleman@maluuba.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Maluuba Research Montréal</orgName>
								<address>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NEWSQA: A MACHINE COMPREHENSION DATASET</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-02-07">7 Feb 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1611.09830v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https://datasets.maluuba.com/NewsQA.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Almost all human knowledge is recorded in the medium of text. As such, comprehension of written language by machines, at a near-human level, would enable a broad class of artificial intelligence applications. In human students we evaluate reading comprehension by posing questions based on a text passage and then assessing a student's answers. Such comprehension tests are appealing because they are objectively gradable and may measure a range of important abilities, from basic understanding to causal reasoning to inference <ref type="bibr" target="#b15">(Richardson et al., 2013)</ref>. To teach literacy to machines, the research community has taken a similar approach with machine comprehension (MC).</p><p>Recent years have seen the release of a host of MC datasets. Generally, these consist of (document, question, answer) triples to be used in a supervised learning framework. Existing datasets vary in size, difficulty, and collection methodology; however, as pointed out by <ref type="bibr" target="#b14">Rajpurkar et al. (2016)</ref>, most suffer from one of two shortcomings: those that are designed explicitly to test comprehension <ref type="bibr" target="#b15">(Richardson et al., 2013)</ref> are too small for training data-intensive deep learning models, while those that are sufficiently large for deep learning <ref type="bibr" target="#b6">(Hermann et al., 2015;</ref><ref type="bibr" target="#b7">Hill et al., 2016;</ref> are generated synthetically, yielding questions that are not posed in natural language and that may not test comprehension directly <ref type="bibr" target="#b3">(Chen et al., 2016)</ref>. More recently, <ref type="bibr" target="#b14">Rajpurkar et al. (2016)</ref> sought to overcome these deficiencies with their crowdsourced dataset, SQuAD.</p><p>Here we present a challenging new largescale dataset for machine comprehension: NewsQA. NewsQA contains 119,633 natural language questions posed by crowdworkers on 12,744 news articles from CNN. Answers to these questions consist of spans of text within the corresponding article highlighted also by crowdworkers. To build NewsQA we utilized a four-stage collection process designed to encourage exploratory, curiosity-based questions that reflect human information seeking. CNN articles were chosen as the source material because they have been used in the past <ref type="bibr" target="#b6">(Hermann et al., 2015)</ref> and, in our view, machine comprehension systems are particularly suited to high-volume, rapidly changing information sources like news.</p><p>As <ref type="bibr" target="#b19">Trischler et al. (2016a)</ref>, <ref type="bibr" target="#b3">Chen et al. (2016)</ref>, and others have argued, it is important for datasets to be sufficiently challenging to teach models the abilities we wish them to learn. Thus, in line with <ref type="bibr" target="#b15">Richardson et al. (2013)</ref>, our goal with NewsQA was to construct a corpus of questions that necessitates reasoning-like behaviors -for example, synthesis of information across different parts of an article. We designed our collection methodology explicitly to capture such questions.</p><p>The challenging characteristics of NewsQA that distinguish it from most previous comprehension tasks are as follows:</p><p>1. Answers are spans of arbitrary length within an article, rather than single words or entities. 2. Some questions have no answer in the corresponding article (the null span). 3. There are no candidate answers from which to choose. 4. Our collection process encourages lexical and syntactic divergence between questions and answers. 5. A significant proportion of questions requires reasoning beyond simple word-and contextmatching (as shown in our analysis).</p><p>Some of these characteristics are present also in SQuAD, the MC dataset most similar to NewsQA. However, we demonstrate through several metrics that NewsQA offers a greater challenge to existing models.</p><p>In this paper we describe the collection methodology for NewsQA, provide a variety of statistics to characterize it and contrast it with previous datasets, and assess its difficulty. In particular, we measure human performance and compare it to that of two strong neural-network baselines. Humans significantly outperform powerful question-answering models. This suggests there is room for improvement through further advances in machine comprehension research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED DATASETS</head><p>NewsQA follows in the tradition of several recent comprehension datasets. These vary in size, difficulty, and collection methodology, and each has its own distinguishing characteristics. We agree with  who have said "models could certainly benefit from as diverse a collection of datasets as possible." We discuss this collection below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MCTEST</head><p>MCTest <ref type="bibr" target="#b15">(Richardson et al., 2013</ref>) is a crowdsourced collection of 660 elementary-level children's stories with associated questions and answers. The stories are fictional, to ensure that the answer must be found in the text itself, and carefully limited to what a young child can understand. Each question comes with a set of 4 candidate answers that range from single words to full explanatory sentences. The questions are designed to require rudimentary reasoning and synthesis of information across sentences, making the dataset quite challenging. This is compounded by the dataset's size, which limits the training of expressive statistical models. Nevertheless, recent comprehension models have performed well on MCTest <ref type="bibr" target="#b16">(Sachan et al., 2015;</ref><ref type="bibr" target="#b22">Wang et al., 2015)</ref>, including a highly structured neural model <ref type="bibr" target="#b19">(Trischler et al., 2016a)</ref>. These models all rely on access to the small set of candidate answers, a crutch that NewsQA does not provide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CNN/DAILY MAIL</head><p>The CNN/Daily Mail corpus <ref type="bibr" target="#b6">(Hermann et al., 2015)</ref> consists of news articles scraped from those outlets with corresponding cloze-style questions. Cloze questions are constructed synthetically by deleting a single entity from abstractive summary points that accompany each article (written presumably by human authors). As such, determining the correct answer relies mostly on recognizing textual entailment between the article and the question. The named entities within an article are identified and anonymized in a preprocessing step and constitute the set of candidate answers; contrast this with NewsQA in which answers often include longer phrases and no candidates are given.</p><p>Because the cloze process is automatic, it is straightforward to collect a significant amount of data to support deep-learning approaches: CNN/Daily Mail contains about 1.4 million question-answer pairs. However, <ref type="bibr" target="#b3">Chen et al. (2016)</ref> demonstrated that the task requires only limited reasoning and, in fact, performance of the strongest models <ref type="bibr" target="#b20">Trischler et al., 2016b;</ref><ref type="bibr" target="#b18">Sordoni et al., 2016)</ref> nearly matches that of humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CHILDREN'S BOOK TEST</head><p>The Children's Book Test (CBT) <ref type="bibr" target="#b7">(Hill et al., 2016</ref>) was collected using a process similar to that of CNN/Daily Mail. Text passages are 20-sentence excerpts from children's books available through Project Gutenberg; questions are generated by deleting a single word in the next (i.e., 21st) sentence. Consequently, CBT evaluates word prediction based on context. It is a comprehension task insofar as comprehension is likely necessary for this prediction, but comprehension may be insufficient and other mechanisms may be more important.</p><p>2.4 BOOKTEST  convincingly argue that, because existing datasets are not large enough, we have yet to reach the full capacity of existing comprehension models. As a remedy they present BookTest. This is an extension to the named-entity and common-noun strata of CBT that increases their size by over 60 times.  demonstrate that training on the augmented dataset yields a model ) that matches human performance on CBT. This is impressive and suggests that much is to be gained from more data, but we repeat our concerns about the relevance of story prediction as a comprehension task. We also wish to encourage more efficient learning from less data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">SQUAD</head><p>The comprehension dataset most closely related to NewsQA is SQuAD <ref type="bibr" target="#b14">(Rajpurkar et al., 2016)</ref>. It consists of natural language questions posed by crowdworkers on paragraphs from high-PageRank Wikipedia articles. As in NewsQA, each answer consists of a span of text from the related paragraph and no candidates are provided. Despite the effort of manual labelling, SQuAD's size is significant and amenable to deep learning approaches: 107,785 question-answer pairs based on 536 articles.</p><p>Although SQuAD is a more realistic and more challenging comprehension task than the other largescale MC datasets, machine performance has rapidly improved towards that of humans in recent months. The SQuAD authors measured human accuracy at 0.905 in F1 (we measured human F1 at 0.807 using a different methodology); at the time of writing, the strongest published model to date achieves 0.778 F1 <ref type="bibr" target="#b25">(Wang et al., 2016)</ref>. This suggests that new, more difficult alternatives like NewsQA could further push the development of more intelligent MC systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COLLECTION METHODOLOGY</head><p>We collected NewsQA through a four-stage process: article curation, question sourcing, answer sourcing, and validation. We also applied a post-processing step with answer agreement consolidation and span merging to enhance the usability of the dataset. These steps are detailed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ARTICLE CURATION</head><p>We retrieve articles from CNN using the script created by Hermann et al. (2015) for CNN/Daily Mail. From the returned set of 90,266 articles, we select 12,744 uniformly at random. These cover a wide range of topics that includes politics, economics, and current events. Articles are partitioned at random into a training set (90%), a development set (5%), and a test set (5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">QUESTION SOURCING</head><p>It was important to us to collect challenging questions that could not be answered using straightforward word-or context-matching. Like <ref type="bibr" target="#b15">Richardson et al. (2013)</ref> we want to encourage reasoning in comprehension models. We are also interested in questions that, in some sense, model human curiosity and reflect actual human use-cases of information seeking. Along a similar line, we consider it an important (though as yet overlooked) capacity of a comprehension model to recognize when given information is inadequate, so we are also interested in questions that may not have sufficient evidence in the text. Our question sourcing stage was designed to solicit questions of this nature, and deliberately separated from the answer sourcing stage for the same reason.</p><p>Questioners (a distinct set of crowdworkers) see only a news article's headline and its summary points (also available from CNN); they do not see the full article itself. They are asked to formulate a question from this incomplete information. This encourages curiosity about the contents of the full article and prevents questions that are simple reformulations of sentences in the text. It also increases the likelihood of questions whose answers do not exist in the text. We reject questions that have significant word overlap with the summary points to ensure that crowdworkers do not treat the summaries as mini-articles, and further discouraged this in the instructions. During collection each Questioner is solicited for up to three questions about an article. They are provided with positive and negative examples to prompt and guide them (detailed instructions are shown in <ref type="figure" target="#fig_2">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ANSWER SOURCING</head><p>A second set of crowdworkers (Answerers) provide answers. Although this separation of question and answer increases the overall cognitive load, we hypothesized that unburdening Questioners in this way would encourage more complex questions. Answerers receive a full article along with a crowdsourced question and are tasked with determining the answer. They may also reject the question as nonsensical, or select the null answer if the article contains insufficient information. Answers are submitted by clicking on and highlighting words in the article, while instructions encourage the set of answer words to consist of a single continuous span (again, we give an example prompt in the Appendix). For each question we solicit answers from multiple crowdworkers (avg. 2.73) with the aim of achieving agreement between at least two Answerers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">VALIDATION</head><p>Crowdsourcing is a powerful tool but it is not without peril (collection glitches; uninterested or malicious workers). To obtain a dataset of the highest possible quality we use a validation process that mitigates some of these issues. In validation, a third set of crowdworkers sees the full article, a question, and the set of unique answers to that question. We task these workers with choosing the best answer from the candidate set or rejecting all answers. Each article-question pair is validated by an average of 2.48 crowdworkers. Validation was used on those questions without answer-agreement after the previous stage, amounting to 43.2% of all questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">ANSWER MARKING AND CLEANUP</head><p>After validation, 86.0% of all questions in NewsQA have answers agreed upon by at least two separate crowdworkers-either at the initial answer sourcing stage or in the top-answer selection. This improves the dataset's quality. We choose to include the questions without agreed answers in the corpus also, but they are specially marked. Such questions could be treated as having the null answer and used to train models that are aware of poorly posed questions.</p><p>As a final cleanup step we combine answer spans that are less than 3 words apart (punctuation is discounted). We find that 5.68% of answers consist of multiple spans, while 71.3% of multi-spans are within the 3-word threshold. Looking more closely at the data reveals that the multi-span answers often represent lists. These may present an interesting challenge for comprehension models moving forward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATA ANALYSIS</head><p>We provide a thorough analysis of NewsQA to demonstrate its challenge and its usefulness as a machine comprehension benchmark. The analysis focuses on the types of answers that appear in the dataset and the various forms of reasoning required to solve it. 1  <ref type="formula">2016</ref>, we categorize answers based on their linguistic type (see <ref type="table" target="#tab_0">Table 1</ref>). This categorization relies on Stanford CoreNLP to generate constituency parses, POS tags, and NER tags for answer spans (see <ref type="bibr" target="#b14">Rajpurkar et al. (2016)</ref> for more details). From the table we see that the majority of answers (22.2%) are common noun phrases. Thereafter, answers are fairly evenly spread among the clause phrase (18.3%), person (14.8%), numeric (9.8%), and other (11.2%) types. Clearly, answers in NewsQA are linguistically diverse.</p><p>The proportions in <ref type="table" target="#tab_0">Table 1</ref> only account for cases when an answer span exists. The complement of this set comprises questions with an agreed null answer (9.5% of the full corpus) and answers without agreement after validation (4.5% of the full corpus).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">REASONING TYPES</head><p>The forms of reasoning required to solve NewsQA directly influence the abilities that models will learn from the dataset. We stratified reasoning types using a variation on the taxonomy presented by <ref type="bibr" target="#b3">Chen et al. (2016)</ref> in their analysis of the CNN/Daily Mail dataset. Types are as follows, in ascending order of difficulty:</p><p>1. Word Matching: Important words in the question exactly match words in the immediate context of an answer span, such that a keyword search algorithm could perform well on this subset.</p><p>2. Paraphrasing: A single sentence in the article entails or paraphrases the question. Paraphrase recognition may require synonymy and world knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Inference: The answer must be inferred from incomplete information in the article or by recognizing conceptual overlap. This typically draws on world knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Synthesis:</head><p>The answer can only be inferred by synthesizing information distributed across multiple sentences.</p><p>5. Ambiguous/Insufficient: The question has no answer or no unique answer in the article.</p><p>For both NewsQA and SQuAD, we manually labelled 1,000 examples (drawn randomly from the respective development sets) according to these types and compiled the results in <ref type="table" target="#tab_1">Table 2</ref>. Some examples fall into more than one category, in which case we defaulted to the more challenging type.</p><p>We can see from the table that word matching, the easiest type, makes up the largest subset in both datasets (32.7% for NewsQA and 39.8% for SQuAD). Paraphrasing constitutes a larger proportion in SQuAD than in NewsQA (34.3% vs 27.0%), possibly a result from the explicit encouragement of lexical variety in SQuAD question sourcing. However, NewsQA significantly outnumbers SQuAD on the distribution of the more difficult forms of reasoning: synthesis and inference make up a combined 33.9% of the data in contrast to 20.5% in SQuAD. 6.4 5.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">BASELINE MODELS</head><p>We test the performance of three comprehension systems on NewsQA: human data analysts and two neural models. The first neural model is the match-LSTM (mLSTM) system of <ref type="bibr" target="#b24">Wang &amp; Jiang (2016b)</ref>. The second is a model of our own design that is similar but computationally cheaper. We describe these models below but omit the personal details of our analysts. Implementation details of the models are described in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MATCH-LSTM</head><p>We selected the mLSTM model because it is straightforward to implement and offers strong, though not state-of-the-art, performance on the similar SQuAD dataset. There are three stages involved in the mLSTM. First, LSTM networks encode the document and question (represented by GloVe word embeddings <ref type="bibr" target="#b13">(Pennington et al., 2014)</ref>) as sequences of hidden states. Second, an mLSTM network <ref type="bibr" target="#b23">(Wang &amp; Jiang, 2016a)</ref> compares the document encodings with the question encodings. This network processes the document sequentially and at each token uses an attention mechanism to obtain a weighted vector representation of the question; the weighted combination is concatenated with the encoding of the current token and fed into a standard LSTM. Finally, a Pointer Network uses the hidden states of the mLSTM to select the boundaries of the answer span. We refer the reader to Wang &amp; Jiang (2016a;b) for full details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">THE BILINEAR ANNOTATION RE-ENCODING BOUNDARY (BARB) MODEL</head><p>The match-LSTM is computationally intensive since it computes an attention over the entire question at each document token in the recurrence. To facilitate faster experimentation with NewsQA we developed a lighter-weight model (BARB) that achieves similar results on SQuAD 2 . Our model consists of four stages:</p><p>Encoding All words in the document and question are mapped to real-valued vectors using the GloVe embeddings W ∈ R |V |×d . This yields d 1 , . . . ,</p><formula xml:id="formula_0">d n ∈ R d and q 1 , . . . , q m ∈ R d . A bidirec-</formula><p>With the configurations for the results reported in Section 6.2, one epoch of training on NewsQA takes about 3.9k seconds for BARB and 8.1k seconds for mLSTM.</p><p>tional GRU network <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> encodes d i into contextual states h i ∈ R D1 for the document. The same encoder is applied to q j to derive contextual states k j ∈ R D1 for the question. <ref type="bibr">3</ref> Bilinear Annotation Next we compare the document and question encodings using a set of C bilinear transformations,</p><formula xml:id="formula_1">g ij = h T i T [1:C] k j , T c ∈ R D1×D1</formula><p>, g ij ∈ R C , which we use to produce an (n × m × C)-dimensional tensor of annotation scores, G = [g ij ]. We take the maximum over the question-token (second) dimension and call the columns of the resulting matrix g i ∈ R C . We use this matrix as an annotation over the document word dimension. In contrast with the more typical multiplicative application of attention vectors, this annotation matrix is concatenated to the encoder RNN input in the re-encoding stage.</p><p>Re-encoding For each document word, the input of the re-encoding RNN (another biGRU) consists of three components: the document encodings h i , the annotation vectors g i , and a binary feature q i indicating whether the document word appears in the question. The resulting vectors f i = [h i ; g i ; q i ] are fed into the re-encoding RNN to produce D 2 -dimensional encodings e i for the boundary-pointing stage.</p><p>Boundary pointing Finally, we search for the boundaries of the answer span using a convolutional network (in a process similar to edge detection). Encodings e i are arranged in matrix E ∈ R D2×n . E is convolved with a bank of n f filters, F k ∈ R D2×w , where w is the filter width, k indexes the different filters, and indexes the layer of the convolutional network. Each layer has the same number of filters of the same dimensions. We add a bias term and apply a nonlinearity (ReLU) following each convolution, with the result an (n f × n)-dimensional matrix B .</p><p>We use two convolutional layers in the boundary-pointing stage. Given B 1 and B 2 , the answer span's start-and end-location probabilities are computed using p(s) ∝ exp v T s B 1 + b s and p(e) ∝ exp v T e B 2 + b e , respectively. We also concatenate p(s) to the input of the second convolutional layer (along the n f -dimension) so as to condition the end-boundary pointing on the start-boundary. Vectors v s , v e ∈ R n f and scalars b s , b e ∈ R are trainable parameters.</p><p>We also provide an intermediate level of "guidance" to the annotation mechanism by first reducing the feature dimension C in G with mean-pooling, then maximizing the softmax probabilities in the resulting (n-dimensional) vector corresponding to the answer word positions in each document. This auxiliary task is observed empirically to improve performance. 6 EXPERIMENTS 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">HUMAN EVALUATION</head><p>We tested four English speakers on a total of 1,000 questions from the NewsQA development set. We used four performance measures: F1 and exact match (EM) scores (the same measures used by SQuAD), as well as BLEU and CIDEr 5 . BLEU is a precision-based metric popular in machine translation that uses a weighted average of variable length phrase matches (n-grams) against the reference sentence <ref type="bibr" target="#b11">(Papineni et al., 2002)</ref>. CIDEr was designed to correlate better with human judgements of sentence similarity, and uses tf-idf scores over n-grams <ref type="bibr" target="#b21">(Vedantam et al., 2015)</ref>.</p><p>As given in <ref type="table" target="#tab_3">Table 4</ref>, humans averaged 0.694 F1 on NewsQA. The human EM scores are relatively low at 0.465. These lower scores are a reflection of the fact that, particularly in a dataset as complex as NewsQA, there are multiple ways to select semantically equivalent answers, e.g., "1996" versus "in 1996". Although these answers are equally correct they would be measured at 0.5 F1 and 0.0 EM.</p><p>A bidirectional GRU concatenates the hidden states of two GRU networks running in opposite directions. Each of these has hidden size 1 2 D1.  This suggests that simpler automatic metrics are not equal to the task of complex MC evaluation, a problem that has been noted in other domains <ref type="bibr" target="#b10">(Liu et al., 2016)</ref>. Therefore we also measure according to BLEU and CIDEr: humans score 0.560 and 3.596 on these metrics, respectively.</p><p>The original SQuAD evaluation of human performance compares distinct answers given by crowdworkers according to EM and F1; for a closer comparison with NewsQA, we replicated our human test on the same number of validation data (1,000) with the same humans. We measured human answers against the second group of crowdsourced responses in SQuAD's development set, yielding 0.807 F1, 0.625 BLEU, and 3.998 CIDEr. Note that the F1 score is close to the top single-model performance of 0.778 achieved in <ref type="bibr" target="#b25">Wang et al. (2016)</ref>.</p><p>We finally compared human performance on the answers that had crowdworker agreement with and without validation, finding a difference of only 1.4 percentage points F1. This suggests our validation stage yields good-quality answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">MODEL PERFORMANCE</head><p>Performance of the baseline models and humans is measured by EM and F1 with the official evaluation script from SQuAD and listed in <ref type="table" target="#tab_3">Table 4</ref>. We supplement these with BLEU and CIDEr measures on the 1,000 human-annotated dev questions. Unless otherwise stated, hyperparameters are determined by hyperopt (Appendix A). The gap between human and machine performance on NewsQA is a striking 0.198 points F1 -much larger than the gap on SQuAD (0.098) under the same human evaluation scheme. The gaps suggest a large margin for improvement with machine comprehension methods. <ref type="figure" target="#fig_0">Figure 1</ref> stratifies model (BARB) performance according to answer type (left) and reasoning type (right) as defined in Sections 4.1 and 4.2, respectively. The answer-type stratification suggests that the model is better at pointing to named entities compared to other types of answers. The reasoningtype stratification, on the other hand, shows that questions requiring inference and synthesis are, not surprisingly, more difficult for the model. Consistent with observations in <ref type="table" target="#tab_3">Table 4</ref>, stratified performance on NewsQA is significantly lower than on SQuAD. The difference is smallest on word matching and largest on synthesis. We postulate that the longer stories in NewsQA make synthesizing information from separate sentences more difficult, since the relevant sentences may be farther apart. This requires the model to track longer-term dependencies. It is also interesting to observe that on SQuAD, BARB outperforms human annotators in answering ambiguous questions or those with incomplete information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">SENTENCE-LEVEL SCORING</head><p>We propose a simple sentence-level subtask as an additional quantitative demonstration of the relative difficulty of NewsQA. Given a document and a question, the goal is to find the sentence containing the answer span. We hypothesize that simple techniques like word-matching are inadequate to this task owing to the more involved reasoning required by NewsQA.</p><p>We employ a technique that resembles inverse document frequency (idf ), which we call inverse sentence frequency (isf ). Given a sentence S i from an article and its corresponding question Q, the isf score is given by the sum of the idf scores of the words common to S i and Q (each sentence is treated as a document for the idf computation). The sentence with the highest isf is taken as the answer sentence S * , that is,</p><formula xml:id="formula_2">S * = arg max i w∈Si∩Q isf (w).</formula><p>The isf method achieves an impressive 79.4% sentence-level accuracy on SQuAD's development set but only 35.4% accuracy on NewsQA's development set, highlighting the comparative difficulty of the latter. To eliminate the difference in article length as a possible cause of the performance gap, we also artificially increased the article lengths in SQuAD by concatenating adjacent SQuAD articles from the same Wikipedia article. Accuracy decreases as expected with the increased SQuAD article length, yet remains significantly higher than on NewsQA with comparable or even greater article length (see <ref type="table" target="#tab_4">Table 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We have introduced a challenging new comprehension dataset: NewsQA. We collected the 100,000+ examples of NewsQA using teams of crowdworkers, who variously read CNN articles or highlights, posed questions about them, and determined answers. Our methodology yields diverse answer types and a significant proportion of questions that require some reasoning ability to solve. This makes the corpus challenging, as confirmed by the large performance gap between humans and deep neural models <ref type="bibr">(0.198 F1,</ref><ref type="bibr">0.479 BLEU,</ref><ref type="bibr">1.165 CIDEr)</ref>. By its size and complexity, NewsQA makes a significant extension to the existing body of comprehension datasets. We hope that our corpus will spur further advances in machine comprehension and guide the development of literate artificial intelligence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDICES A IMPLEMENTATION DETAILS</head><p>Both mLSTM and BARB are implemented with the Keras framework <ref type="bibr" target="#b4">(Chollet, 2015)</ref> using the Theano <ref type="bibr" target="#b2">(Bergstra et al., 2010)</ref> backend. Word embeddings are initialized using GloVe vectors <ref type="bibr" target="#b13">(Pennington et al., 2014)</ref> pre-trained on the 840-billion Common Crawl corpus. The word embeddings are not updated during training. Embeddings for out-of-vocabulary words are initialized with zero.</p><p>For both models, the training objective is to maximize the log likelihood of the boundary pointers. Optimization is performed using stochastic gradient descent (with a batch-size of 32) with the ADAM optimizer <ref type="bibr" target="#b9">(Kingma &amp; Ba, 2015)</ref>. The initial learning rate is 0.003 for mLSTM and 0.0005 for BARB. The learning rate is decayed by a factor of 0.7 if validation loss does not decrease at the end of each epoch. Gradient clipping <ref type="bibr" target="#b12">(Pascanu et al., 2013)</ref> is applied with a threshold of 5.</p><p>Parameter tuning is performed on both models using hyperopt 6 . For each model, configurations for the best observed performance are as follows:</p><p>mLSTM Both the pre-processing layer and the answer-pointing layer use bi-directional RNN with a hidden size of 192. These settings are consistent with those used by <ref type="bibr" target="#b24">Wang &amp; Jiang (2016b)</ref>.</p><p>Model parameters are initialized with either the normal distribution (N (0, 0.05)) or the orthogonal initialization <ref type="bibr">(O, Saxe et al. 2013)</ref> in Keras. All weight matrices in the LSTMs are initialized with O.</p><p>In the Match-LSTM layer, W q , W p , and W r are initialized with O, b p and w are initialized with N , and b is initialized as 1.</p><p>In the answer-pointing layer, V and W a are initialized with O, b a and v are initialized with N , and c is initialized as 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARB</head><p>For BARB, the following hyperparameters are used on both SQuAD and NewsQA: d = 300, D 1 = 128, C = 64, D 2 = 256, w = 3, and n f = 128. Weight matrices in the GRU, the bilinear models, as well as the boundary decoder (v s and v e ) are initialized with O. The filter weights in the boundary decoder are initialized with glorot_uniform <ref type="bibr">(Glorot &amp; Bengio 2010, default in Keras)</ref>. The bilinear biases are initialized with N , and the boundary decoder biases are initialized with 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATA COLLECTION USER INTERFACE</head><p>Here we present the user interfaces used in question sourcing, answer sourcing, and question/answer validation.</p><p>https://github.com/hyperopt/hyperopt  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: BARB performance (F1 and EM) stratified by answer type on the full development set of NewsQA. Right: BARB performance (F1) stratified by reasoning type on the human-assessed subset on both NewsQA and SQuAD. Error bars indicate performance differences between BARB and human annotators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Examples of user interfaces for question sourcing, answer sourcing, and validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Question sourcing instructions for the crowdworkers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The variety of answer types appearing in NewsQA, with proportion statistics and examples.</figDesc><table><row><cell>Answer type</cell><cell>Example</cell><cell>Proportion (%)</cell></row><row><cell>Date/Time</cell><cell>March 12, 2008</cell><cell>2.9</cell></row><row><cell>Numeric</cell><cell>24.3 million</cell><cell>9.8</cell></row><row><cell>Person</cell><cell>Ludwig van Beethoven</cell><cell>14.8</cell></row><row><cell>Location</cell><cell>Torrance, California</cell><cell>7.8</cell></row><row><cell>Other Entity</cell><cell>Pew Hispanic Center</cell><cell>5.8</cell></row><row><cell>Common Noun Phr.</cell><cell>federal prosecutors</cell><cell>22.2</cell></row><row><cell>Adjective Phr.</cell><cell>5-hour</cell><cell>1.9</cell></row><row><cell>Verb Phr.</cell><cell>suffered minor damage</cell><cell>1.4</cell></row><row><cell>Clause Phr.</cell><cell>trampling on human rights</cell><cell>18.3</cell></row><row><cell>Prepositional Phr.</cell><cell>in the attack</cell><cell>3.8</cell></row><row><cell>Other</cell><cell>nearly half</cell><cell>11.2</cell></row><row><cell>4.1 ANSWER</cell><cell></cell><cell></cell></row></table><note>TYPES Following Rajpurkar et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Reasoning mechanisms needed to answer questions. For each we show an example question with the sentence that contains the answer span. Words relevant to the reasoning type are in bold. The corresponding proportion in the human-evaluated subset of both NewsQA and SQuAD (1,000 samples each) is also given. When were the findings published?S: Both sets of research findings were published Thursday... Who drew inspiration from presidents? S: Rudy Ruiz says the lives of US presidents can make them positive role models</figDesc><table><row><cell>Reasoning</cell><cell>Example</cell></row></table><note>Q: Whose mother is moving to the White House? S: ... Barack Obama's mother-in-law, Marian Robinson, will join the Obamas at the family's private quarters at 1600 Pennsylvania Avenue. [Michelle is never mentioned]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Model performance on SQuAD and NewsQA datasets. Random are taken from<ref type="bibr" target="#b14">Rajpurkar et al. (2016)</ref>, and mLSTM from<ref type="bibr" target="#b24">Wang &amp; Jiang (2016b)</ref>.</figDesc><table><row><cell>SQuAD</cell><cell cols="2">Exact Match</cell><cell>F1</cell><cell></cell><cell>NewsQA</cell><cell cols="2">Exact Match</cell><cell>F1</cell><cell></cell></row><row><cell>Model</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell><cell>Model</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>Random</cell><cell>0.11</cell><cell>0.13</cell><cell>0.41</cell><cell>0.43</cell><cell>Random</cell><cell>0.00</cell><cell>0.00</cell><cell>0.30</cell><cell>0.30</cell></row><row><cell cols="5">mLSTM 0.591 0.595 0.700 0.703</cell><cell cols="5">mLSTM 0.344 0.349 0.496 0.500</cell></row><row><cell>BARB</cell><cell>0.591</cell><cell>-</cell><cell>0.709</cell><cell>-</cell><cell>BARB</cell><cell cols="4">0.361 0.341 0.496 0.482</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Human performance on SQuAD and NewsQA datasets. The first row is taken from<ref type="bibr" target="#b14">Rajpurkar et al. (2016)</ref>, and the last two rows correspond to machine performance (BARB) on the humanevaluated subsets.</figDesc><table><row><cell>Dataset</cell><cell>Exact Match</cell><cell>F1</cell><cell cols="2">BLEU CIDEr</cell></row><row><cell>SQuAD</cell><cell>0.803</cell><cell>0.905</cell><cell>-</cell><cell>-</cell></row><row><cell>SQuAD (ours)</cell><cell>0.650</cell><cell>0.807</cell><cell>0.625</cell><cell>3.998</cell></row><row><cell>NewsQA</cell><cell>0.465</cell><cell>0.694</cell><cell>0.560</cell><cell>3.596</cell></row><row><cell>SQuADBARB</cell><cell>0.553</cell><cell>0.685</cell><cell>0.366</cell><cell>2.845</cell></row><row><cell>NewsQABARB</cell><cell>0.340</cell><cell>0.501</cell><cell>0.081</cell><cell>2.431</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Sentence-level accuracy on artificially-lengthened SQuAD documents.</figDesc><table><row><cell>SQuAD</cell><cell>NewsQA</cell></row><row><cell># documents 1 3 5 7 9</cell><cell>1</cell></row><row><cell>Avg # sentences 4.9 14.3 23.2 31.8 40.3</cell><cell>30.7</cell></row><row><cell>isf 79.6 74.9 73.0 72.3 71.0</cell><cell>35.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Additional statistics are available at https://datasets.maluuba.com/NewsQA/stats.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">All experiments in this section use the subset of NewsQA dataset with answer agreements (92,549 samples for training, 5,166 for validation, and 5,126 for testing). We leave the challenge of identifying the unanswerable questions for future work.5  We use https://github.com/tylin/coco-caption to calculate these two scores.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Çaglar Gülçehre, Sandeep Subramanian and Saizheng Zhang for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Embracing data abundance: Booktest dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00956</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SciPy</title>
		<meeting>of SciPy</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn / daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Text understanding with the attention sum reader network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01547</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning answerentailing structures for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Iterative alternating neural attention for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02245</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A parallelhierarchical model for machine comprehension on sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural language comprehension with the epireader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Machine comprehension with syntax, frames, and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">700</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning natural language inference with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-lstm and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
