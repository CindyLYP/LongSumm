<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Artistic style transfer for videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-10-19">19 Oct 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Ruder</surname></persName>
							<email>rudera@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
							<email>dosovits@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Artistic style transfer for videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-10-19">19 Oct 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1604.08610v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There have recently been a lot of interesting contributions to the issue of style transfer using deep neural networks. Gatys et al. <ref type="bibr" target="#b2">[3]</ref> proposed a novel approach using neural networks to capture the style of artistic images and transfer it to real world photographs. Their approach uses high-level feature representations of the images from hidden layers of the VGG convolutional network <ref type="bibr" target="#b9">[10]</ref> to separate and reassemble content and style. This is done by formulating an optimization problem that, starting with white noise, searches for a new image showing similar neural activations as the content image and similar feature correlations (expressed by a Gram matrix) as the style image.</p><p>The present paper builds upon the approach from Gatys et al. <ref type="bibr" target="#b2">[3]</ref> and extends style transfer to video sequences. Given an artistic image, we transfer its particular style of painting to the entire video. Processing each frame of the video independently leads to flickering and false discontinuities, since the solution of the style transfer task is not stable. To regularize the transfer and to preserve smooth transition between individual frames of the video, we introduce a temporal constraint that penalizes deviations between two frames. The temporal constraint takes the optical flow from the original video into account: instead of penalizing the deviations from the previous frame, we penalize deviation along the point trajectories. Disoccluded regions as well as motion boundaries are excluded from the penalizer. This allows the process to rebuild disoccluded regions and distorted motion boundaries while preserving the appearance of the rest of the image, see <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>In addition, we present two extensions of our approach. The first one aims on improving the consistency over larger periods of time. When a region that is occluded in some frame and disoccluded later gets rebuilt during the process, most likely this region will have a different appearance than before the occlusion. To solve this, we make use of long term motion estimates. This allows us to enforce consistency of the synthesized frames before and after the occlusion.</p><p>Secondly, the style transfer tends to create artifacts at the image boundaries. For static images, these artifacts are hardly visible, yet for videos with strong camera motion they move towards the center of the image and get amplified. We developed a multi-pass algorithm, which processes the video in alternating directions using both forward and backward flow. This results in a more coherent video.</p><p>We quantitatively evaluated our approach in combination with different optical flow algorithms on the Sintel benchmark. Additionally we show qualitative results on several movie shots. We were able to successfully eliminate most of the temporal artifacts and can create smooth and coherent stylized videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Style transfer using deep networks: Gatys et al. <ref type="bibr" target="#b2">[3]</ref> showed remarkable results by using the VGG-19 deep neural network for style transfer. Their approach was taken up by various follow-up papers that, among other things, proposed different ways to represent the style within the neural network. Li et al. <ref type="bibr" target="#b4">[5]</ref> suggested an approach to preserve local patterns of the style image. Instead of using a global representation of the style, computed as Gram matrix, they used patches of the neural activation from the style image. Nikulin et al. <ref type="bibr" target="#b6">[7]</ref> tried the style transfer algorithm by Gatys et al. on other nets than VGG and proposed several variations in the way the style of the image is represented to archive different goals like illumination or season transfer. However, we are not aware of any work that applies this kind of style transfer to videos.</p><p>Painted animations: One common approach to create video sequences with an artistic style is to generate artificial brush strokes to repaint the scene. Different artistic styles are gained by modifying various parameters of these brush strokes, like thickness, or by using different brush placement methods. To achieve temporal consistency Litwinowicz <ref type="bibr" target="#b5">[6]</ref> was one of the first who used optical flow. In his approach, brush strokes were generated for the first frame and then moved along the flow field. Later, this approach was refined. Hays et al. <ref type="bibr" target="#b3">[4]</ref> proposed new stylistic parameters for the brush strokes to mimic different artistic styles. O'Donovan et al. <ref type="bibr" target="#b7">[8]</ref> formulated an energy optimization problem for an optimal placement and shape of the brush strokes and also integrated a temporal constraint into the optimization problem by penalizing changes in shape and width of the brush strokes compared to the previous frame. These approaches are similar in spirit to what we are doing, but they are only capable of applying a restricted class of artistic styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Style transfer in still images</head><p>In this section, we briefly review the style transfer approach introduced by Gatys et al. <ref type="bibr" target="#b2">[3]</ref>. The aim is to generate a stylized image x showing the content of an image p in the style of an image a. Gatys et al. formulated an energy minimization problem consisting of a content loss and a style loss. The key idea is that features extracted by a convolutional network carry information about the content of the image, while the correlations of these features encode the style.</p><p>We denote by Φ l (•) the function implemented by the part of the convolutional network from input up to the layer l. The feature maps extracted by the network from the original image p, the style image a and the stylized image x we denote by P l = Φ l (p), S l = Φ l (a) and F l = Φ l (x) respectively. The dimensionality of these feature maps we denote by N l × M l , where N l is the number of filters (channels) in the layer, and M l is the spatial dimensionality of the feature map, that is, the product of its width and height.</p><p>The content loss, denoted as L content , is simply the mean squared error between P l ∈ R N l ×M l and F l ∈ R N l ×M l . This loss need not be restricted to only one layer. Let L content be the set of layers to be used for content representation, then we have:</p><formula xml:id="formula_0">L content p, x = l∈Lcontent 1 N l M l i,j F l ij − P l ij 2 .<label>(1)</label></formula><p>The style loss is also a mean squared error, but between the correlations of the filter responses expressed by their Gram matrices A l ∈ R N l ×N l for the style image a and G l ∈ R N l ×N l for the stylized image x. These are computed as</p><formula xml:id="formula_1">A l ij = M l k=1 S l ik S l jk and G l ij = M l k=1 F l ik F l jk .</formula><p>As above, let L style be the set of layers we use to represent the style, then the style loss is given by:</p><formula xml:id="formula_2">L style a, x = l∈L style N 2 l M 2 l i,j G l ij − A l ij 2<label>(2)</label></formula><p>Overall, the loss function is given by</p><formula xml:id="formula_3">L singleimage p, a, x = αL content p, x + βL style a, x ,<label>(3)</label></formula><p>with weighting factors α and β governing the importance of the two components. The stylized image is computed by minimizing this energy with respect to x using gradient-based optimization. Typically it is initialized with random Gaussian noise. However, the loss function is non-convex, therefore the optimization is prone to falling into local minima. This makes the initialization of the stylized image important, especially when applying the method to frames of a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style transfer in videos</head><p>We use the following notation: p (i) is the i th frame of the original video, a is the style image and x (i) are the stylized frames to be generated. Furthermore, we denote by x (i) the initialization of the style optimization algorithm at frame i. By x j we denote the j th component of a vector x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Short-term consistency by initialization</head><p>When the style transfer for consecutive frames is initialized by independent Gaussian noise, two frames of a video converge to very different local minima, resulting in a strong flickering. The most basic way to yield temporal consistency is to initialize the optimization for the frame i + 1 with the stylized frame i. Areas that have not changed between the two frames are then initialized with the desired appearance, while the rest of the image has to be rebuilt through the optimization process.</p><p>If there is motion in the scene, this simple approach does not perform well, since moving objects are initialized incorrectly. Thus, we take the optical flow into account and initialize the optimization for the frame i + 1 with the previous stylized frame warped:</p><formula xml:id="formula_4">x (i+1) = ω i+1 i x (i) .</formula><p>Here ω i+1 i denotes the function that warps a given image using the optical flow field that was estimated between image p (i) and p (i+1) . Clearly, the first frame of the stylized video still has to be initialized randomly.</p><p>We experimented with two state-of-the-art optical flow estimation algorithms: DeepFlow <ref type="bibr" target="#b11">[12]</ref> and EpicFlow <ref type="bibr" target="#b8">[9]</ref>. Both are based on Deep Matching <ref type="bibr" target="#b11">[12]</ref>: Deep-Flow combines it with a variational approach, while EpicFlow relies on edgepreserving sparse-to-dense interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Temporal consistency loss</head><p>To enforce stronger consistency between adjacent frames we additionally introduce an explicit consistency penalty to the loss function. This requires detection of disoccluded regions and motion boundaries. To detect disocclusions, we perform a forward-backward consistency check of the optical flow <ref type="bibr" target="#b10">[11]</ref>. Let w = (u, v) be the optical flow in forward direction andŵ = (û,v) the flow in backward direction. Denote by w the forward flow warped to the second image:</p><formula xml:id="formula_5">w(x, y) = w((x, y) +ŵ(x, y)).<label>(4)</label></formula><p>In areas without disocclusion, this warped flow should be approximately the opposite of the backward flow. Therefore we mark as disocclusions those areas where the following inequality holds:</p><formula xml:id="formula_6">| w +ŵ| 2 &gt; 0.01(| w| 2 + |ŵ| 2 ) + 0.5<label>(5)</label></formula><p>Motion boundaries are detected using the following inequality:</p><formula xml:id="formula_7">|∇û| 2 + |∇v| 2 &gt; 0.01|ŵ| 2 + 0.002<label>(6)</label></formula><p>Coefficients in inequalities (5) and (6) are taken from Sundaram et al. <ref type="bibr" target="#b10">[11]</ref>.</p><p>The temporal consistency loss function penalizes deviations from the warped image in regions where the optical flow is consistent and estimated with high confidence:</p><formula xml:id="formula_8">L temporal (x, ω, c) = 1 D D k=1 c k • (x k − ω k ) 2 .<label>(7)</label></formula><p>Here c ∈ [0, 1] D is per-pixel weighting of the loss and D = W × H × C is the dimensionality of the image. We define the weights c (i−1,i) between frames i−1 and i as follows: 0 in disoccluded regions (as detected by forward-backward consistency) and at the motion boundaries, and 1 everywhere else. Potentially weights between 0 and 1 could be used to incorporate the certainty of the optical flow prediction. The overall loss takes the form:</p><formula xml:id="formula_9">L shortterm p (i) , a, x (i) = αL content p (i) , x (i) + βL style a, x (i) + γL temporal x (i) , ω i i−1 (x (i−1) ), c (i−1,i) .<label>(8)</label></formula><p>We optimize one frame after another, thus x (i−1) refers to the already stylized frame i−1.</p><p>Furthermore we experimented with the more robust absolute error instead of squared error for the temporal consistency loss; results are shown in section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Long-term consistency</head><p>The short-term model has the following limitation: when some areas are occluded in some frame and disoccluded later, these areas will likely change their appearance in the stylized video. This can be counteracted by also making use of long-term motion, i.e. not only penalizing deviations from the previous frame, but also from temporally more distant frames. Let J denote the set of indices each frame should take into account, relative to the frame number. E.g. J = {1, 2, 4} means frame i takes frames i − 1, i − 2 and i − 4 into account. Then, the loss function with long-term consistency is given by:</p><formula xml:id="formula_10">L longterm p (i) , a, x (i) = αL content p (i) , x (i) + βL style a, x (i) + γ j∈J:i−j≥1 L temporal x (i) , ω i i−j (x (i−j) ), c (i−j,i) long<label>(9)</label></formula><p>It is essential how the weights c</p><formula xml:id="formula_11">(i−j,i) long</formula><p>are computed. Let c (i−j,i) be the weights for the flow between image i−j and i, as defined for the short-term model. The long-term weights c</p><formula xml:id="formula_12">(i−j,i) long</formula><p>are computed as follows:</p><formula xml:id="formula_13">c (i−j,i) long = max c (i−j,i) − k∈J:i−k&gt;i−j c (i−k,i) , 0 ,<label>(10)</label></formula><p>where max is taken element-wise. This means, we first apply the usual shortterm constraint. For pixels in disoccluded regions we look into the past until we find a frame in which these have consistent correspondences. An advantage over simply using c (i−j,i) is that each pixel is connected only to the closest possible frame from the past. Since the optical flow computed over more frames is more erroneous than over fewer frames, this results in nicer videos. An empirical comparison of c (i−j,i) and c</p><formula xml:id="formula_14">(i−j,i) long</formula><p>is shown in the supplementary video (see section 8.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-pass algorithm</head><p>We found that the output image tends to have less contrast and is less diverse near image boundaries than in other areas of the image. For mostly static videos this effect is hardly visible. However, in cases of strong camera motion the areas from image boundaries move towards the center of the image, which leads to a lower image quality over time when combined with our temporal constraint. Therefore, we developed a multi-pass algorithm which processes the whole sequence in multiple passes and alternating directions. The basic idea is that we progressively propagate intermediate results in both forward and backward direction, avoiding an one-way information flow from the image boundaries to the center only.</p><p>Every pass consists of a relatively low number of iterations without full convergence. At the beginning, we process every frame independently based on a random initialization. After that, we blend frames with non-disoccluded parts of previous frames warped according to the optical flow, then run the optimization algorithm for some iterations initialized with this blend. The direction in which the sequence is processed is alternated in every pass. We repeat this blending and optimization to convergence.</p><p>Formally, let x (i)(j) be the initialization of frame i in pass j and x (i)(j) the corresponding output after some iterations of the optimization algorithm. When processed in forward direction, the initialization of frame i is created as follows: Analogously, the initialization for a backward direction pass is:</p><formula xml:id="formula_15">x (i)(j) = x (i)(j−1) if i = 1, δc (i−1,i) • ω i i−1 x (i−1)(j) + (δ1 + δc (i−1,i) ) • x (i)(j−1) else.<label>(11)</label></formula><formula xml:id="formula_16">x (i)(j) = x (i)(j−1) if i = N frames δc (i+1,i) • ω i i+1 x (i+1)(j) + (δ1 + δc (i+1,i) ) • x (i)(j−1) else (12)</formula><p>The multi-pass algorithm can be combined with the temporal consistency loss described above. We achieved good results when we disabled the temporal consistency loss in several initial passes and enabled it in later passes only after the images had stabilized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we briefly describe implementation details and present experimental results produced with different versions of our algorithm. While we did our best to make the paper self-contained, it is not possible to demonstrate effects like video flickering in still images. We therefore advise the readers to watch the supplementary video, which is available at https://youtu.be/vQk_Sfl7kSc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation details</head><p>Our implementation 1 is based on the Torch <ref type="bibr" target="#b1">[2]</ref> implementation called neuralstyle 2 . We used the following layers of the VGG-19 network <ref type="bibr" target="#b9">[10]</ref> for computing the losses: relu4 for the content and relu1 1,relu2 1,relu3 1,relu4 1,relu5 for the style. The energy function was minimized using L-BFGS. For precise evaluation we incorporated the following strict stopping criterion: the optimization was considered converged if the loss did not change by more than 0.01% during 50 iterations. This typically resulted in roughly 2000 to 3000 iterations for the first frame and roughly 400 to 800 iterations for subsequent frames when optimizing with our temporal constraint, depending on the amount of motion and the complexity of the style image. Using a convergence threshold of 0.1% cuts the number of iterations and the running time in half, and we found it still produces reasonable results in most cases. However, we used the stronger criterion in our experiments for the sake of accuracy.</p><p>For videos of resolution 350 × 450 we used weights α = 1 and β = 20 for the content and style losses, respectively (default values from neural-style), and weight γ = 200 for the temporal losses. However, the weights should be adjusted if the video resolution is different. We provide the details in section 7.2.</p><p>For our multi-pass algorithm, we used 100 iterations per pass and set δ = 0.5, but we needed at least 10 passes for good results, so this algorithm needs more computation time than our previous approaches.</p><p>We used DeepMatching, DeepFlow and EpicFlow implementations provided by the authors of these methods. We used the "improved-settings" flag in Deep-Matching 1.0.1 and the default settings for DeepFlow 1.0.1 and EpicFlow 1.00.</p><p>Runtime For the relaxed convergence threshold of 0.1% with random initialization the optimization process needed on average roughly eight to ten minutes per frame at a resolution of 1024 × on an Nvidia Titan X GPU. When initialized with the warped previous frame and combined with our temporal loss, the optimization converges 2 to 3 times faster, three minutes on average. Optical flow computation runs on a CPU and takes roughly 3 minutes per frame pair (forward and backward flow together), therefore it can be performed in parallel with the style transfer. Hence, our modified algorithm is roughly 3 times faster than naive per-frame processing, while providing temporally consistent output videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Short-term consistency</head><p>We evaluated our short-term temporal loss on 5 diverse scenes from the MPI Sintel Dataset <ref type="bibr" target="#b0">[1]</ref>, with 20 to 50 frames of resolution 1024 × 436 pixels per scene, and 6 famous paintings (shown in section 7.1) as style images. The Sintel dataset provides ground truth optical flow and ground truth occlusion areas, which allows a quantitative study. We warped each stylized frame i back with the ground truth flow and computed the difference with the stylized frame i − 1 in non-disoccluded regions. We use the mean square of this difference (that is, the mean squared error) as a quantitative performance measure.</p><p>On this benchmark we compared several approaches: our short-term consistency loss with DeepFlow and EpicFlow, as well as three different initializations without the temporal loss: random noise, the previous stylized frame and the previous stylized frame warped with DeepFlow. We set α = 1, β = 100, γ = 400.</p><p>A qualitative comparison is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Quantitative results are in Table 2. The most straightforward approach, processing every frame independently, performed roughly an order of magnitude worse than our more sophisticated methods. In most cases, the temporal penalty significantly improved the results. The ambush scenes are exceptions, since they contain very large motion and the erroneous optical flow impairs the temporal constraint. Interestingly, on average DeepFlow performed slightly better than EpicFlow in our experiments, even through EpicFlow outperforms DeepFlow on the Sintel optical flow benchmark. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Long-term consistency and multi-pass algorithm</head><p>The short-term consistency benchmark presented above cannot evaluate the long-term consistency of videos (since we do not have long-term ground truth flow available) and their visual quality (this can only be judged by humans). We therefore excluded the long-term penalty and the multi-pass approach from the quantitative comparison and present only qualitative results. Please see the supplementary video for more results. <ref type="figure" target="#fig_3">Fig. 3</ref> shows a scene from Miss Marple where a person walks through the scene. Without our long-term consistency model, the background looks very different after the person passes by. The long-term consistency model keeps the background unchanged. <ref type="figure" target="#fig_4">Fig. 4</ref> shows another scene from Miss Marple with fast camera motion. The multi-pass algorithm avoids the artifacts introduced by the basic algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a set of techniques for style transfer in videos: suitable initialization, a loss function that enforces short-term temporal consistency of the stylized video, a loss function for long-term consistency, and a multi-pass approach. As a consequence, we can produce stable and visually appealing stylized videos even in the presence of fast motion and strong occlusion.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Weighting of the loss components</head><p>As mentioned in the main paper, for best results the weights α, β and γ of different components of the loss function have to be adjusted depending on the resolution of the video. The settings we used for different resolutions are shown in <ref type="table" target="#tab_1">Table 2</ref>. -Results of the basic algorithm on different sequences from Sintel with different styles -Additional comparison of the basic and the multi-pass algorithm -Additional comparison of the basic and the long-term algorithm -Comparison of "naive" (c) and "advanced" (c long ) weighting schemes for long-term consistency Another video, showing results of the algorithm on a number of diverse videos with different style images is avalable at https://youtu.be/Khuj4ASldmU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Robust loss function for temporal consistency</head><p>We tried using the more robust absolute error instead of squared error for the temporal consistency loss. The weight for the temporal consistency was doubled in this case. Results are shown in <ref type="figure" target="#fig_7">Figure 6</ref>. While in some cases (left example in the figure) absolute error leads to slightly improved results, in other cases (right example in the figure) it causes large fluctuations. We therefore stick with mean squared error in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Effect of errors in optical flow estimation</head><p>The quality of results produced by our algorithm strongly depends on the quality of optical flow estimation. This is illustrated <ref type="figure" target="#fig_5">Figure 7</ref>. When the optical flow is correct (top right region of the image), the method manages to repair the artifacts introduced by warping in the disoccluded region. However, erroneous optical flow (tip of the sword in the bottom right) leads to degraded performance. Optimization process partially compensates the errors (sword edges get sharp), but cannot fully recover.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Scene from Ice Age (2002) processed in the style of The Starry Night. Comparing independent per-frame processing to our time consistent approach, the latter is clearly preferable. Best observed in the supplemental video, see section 8.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Here • denotes element-wise vector multiplication, δ and δ = 1−δ are the blend factors, 1 is a vector of all ones, and c = 1 − c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Close-up of a scene from Sintel, combined with The Scream painting. a) With temporal constraint b) Initialized with previous image warped, but without the constraint c) Initialized randomly. The marked regions show most visible differences. Error images show the contrast-enhanced absolute difference between frame #1 and frame #2 warped back using ground truth optical flow, as used in our evaluation. The effect of the temporal constraint is very clear in the error images and in the corresponding video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Scene from Miss Marple, combined with The Starry Night painting. a) Shortterm consistency only. b) Long-term consistency with J = {1, 10, 20, 40}. Corresponding video is linked in section 8.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The multi-pass algorithm applied to a scene from Miss Marple. With the default method, the image becomes notably brighter and loses contrast, while the multi-pass algorithm yields a more consistent image quality over time. Corresponding video is linked in section 8.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Supplementary material 7</head><label>7</label><figDesc>Additional details of experimental setup7.1 Style imagesStyle images we used for benchmark experiments on Sintel are shown inFigure 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Styles used for experiments on Sintel. Left to right, top to bottom: "Composition VII" by Wassily Kandinsky (1913), Self-Portrait by Pablo Picasso (1907), "Seated female nude" by Pablo Picasso (1910), "Woman with a Hat" by Henri Matisse (1905), "The Scream" by Edvard Munch (1893), "Shipwreck" by William Turner (1805).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Left: Scene from Ice Age (2002) where an absolute error function works better, because the movement of the bird wasn't captured correctly by the optical flow. Right: Extreme case from Sintel movie where a squared error is far superior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Scene from the Sintel video showing how the algorithm deals with optical flow errors (red rectangle) and disocclusions (blue circle). Both artifacts are somehow repaired in the optimization process due to the exclusion of uncertain areas from our temporal constrain. Still, optical flow errors lead to imperfect results. The third image shows the uncertainty of the flow filed in black and motion boundaries in gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Short-term consistency benchmark results. Mean squared error of different methods on 5 video sequences, averaged over 6 styles, is shown. Pixel values in images were between 0 and 1.</figDesc><table><row><cell></cell><cell>alley</cell><cell cols="4">ambush 5 ambush 6 bandage 2 market 6</cell></row><row><cell>DeepFlow</cell><cell cols="2">0.00061 0.0062</cell><cell>0.012</cell><cell>0.00084</cell><cell>0.0035</cell></row><row><cell>EpicFlow</cell><cell cols="2">0.00073 0.0068</cell><cell>0.014</cell><cell>0.00080</cell><cell>0.0032</cell></row><row><cell cols="2">Init prev warped 0.0016</cell><cell>0.0063</cell><cell>0.012</cell><cell>0.0015</cell><cell>0.0049</cell></row><row><cell>Init prev</cell><cell>0.010</cell><cell>0.018</cell><cell>0.028</cell><cell>0.0041</cell><cell>0.014</cell></row><row><cell>Init random</cell><cell>0.019</cell><cell>0.027</cell><cell>0.037</cell><cell>0.018</cell><cell>0.023</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Weights of the loss function components for different input resolutions.</figDesc><table><row><cell></cell><cell cols="2">× 450 768 × 432 1024 × 436</cell></row><row><cell>α (content)</cell><cell>1</cell><cell>1</cell></row><row><cell>β (style)</cell><cell>40</cell><cell>100</cell></row><row><cell>γ (temporal)</cell><cell>200</cell><cell>400</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">GitHub: https://github.com/manuelruder/artistic-videos 2 GitHub: https://github.com/jcjohnson/neural-style</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV)</title>
		<editor>A. Fitzgibbon et al.</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012-10" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
	<note>Part IV</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno>abs/1508.06576</idno>
		<ptr target="http://arxiv.org/abs/1508.06576" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image and video based painterly animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<idno type="DOI">http://doi.acm.org/10.1145/987657.987676</idno>
		<idno>113-120. NPAR &apos;04</idno>
		<ptr target="http://doi.acm.org/10.1145/987657.987676" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Symposium on Non-photorealistic Animation and Rendering</title>
		<meeting>the 3rd International Symposium on Non-photorealistic Animation and Rendering<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Combining markov random fields and convolutional neural networks for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
		<idno>abs/1601.04589</idno>
		<ptr target="http://arxiv.org/abs/1601.04589" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Processing images and video for an impressionist effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Litwinowicz</surname></persName>
		</author>
		<idno type="DOI">10.1145/258734.258893</idno>
		<ptr target="http://dx.doi.org/10.1145/258734" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 24th Annual Conference on Computer Graphics and Interactive Techniques<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
	<note>SIGGRAPH &apos;97</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exploring the neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nikulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<idno>abs/1602.07188</idno>
		<ptr target="http://arxiv.org/abs/1602.07188" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anipaint: Interactive painterly animation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">475</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-01142656" />
	</analytic>
	<monogr>
		<title level="m">CVPR 2015 -IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<meeting><address><addrLine>Boston, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<ptr target="http://lmb.informatik.uni-freiburg.de//Publications/2010/Bro10e" />
		<imprint>
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-00873592" />
	</analytic>
	<monogr>
		<title level="m">ICCV 2013 -IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
