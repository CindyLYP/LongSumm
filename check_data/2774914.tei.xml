<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HARP: Hierarchical Representation Learning for Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-16">16 Nov 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Chen</surname></persName>
							<email>haocchen@cs.stonybrook.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@acm.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Hu</surname></persName>
							<email>yifanhu@oath.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
							<email>skiena@cs.stonybrook.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HARP: Hierarchical Representation Learning for Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-16">16 Nov 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1706.07845v2[cs.SI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We present HARP, a novel method for learning low dimensional embeddings of a graph&apos;s nodes which preserves higherorder structural features. Our proposed method achieves this by compressing the input graph prior to embedding it, effectively avoiding troublesome embedding configurations (i.e. local minima) which can pose problems to non-convex optimization. HARP works by finding a smaller graph which approximates the global structure of its input. This simplified graph is used to learn a set of initial representations, which serve as good initializations for learning representations in the original, detailed graph. We inductively extend this idea, by decomposing a graph in a series of levels, and then embed the hierarchy of graphs from the coarsest one to the original graph. HARP is a general meta-strategy to improve all of the stateof-the-art neural algorithms for embedding graphs, including DeepWalk, LINE, and Node2vec. Indeed, we demonstrate that applying HARP&apos;s hierarchical paradigm yields improved implementations for all three of these methods, as evaluated on classification tasks on real-world graphs such as DBLP, Blog-Catalog, and CiteSeer, where we achieve a performance gain over the original implementations by up to 14% Macro F1.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>From social networks to the World Wide Web, graphs are a ubiquitous way to organize a diverse set of real-world information. Given a network's structure, it is often desirable to predict missing information (frequently called attributes or labels) associated with each node in the graph. This missing information can represent a variety of aspects of the datafor example, on a social network they could represent the communities a person belongs to, or the categories of a document's content on the web. Because many information networks can contain billions of nodes and edges, it can be intractable to perform complex inference procedures on the entire network. One technique which has been proposed to address this problem is dimensionality reduction. The central idea is to find a mapping function which converts each node in the graph to a lowdimensional latent representation. These representations can Copyright c 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Comparison of two-dimensional embeddings from LINE and our proposed method, for two distinct graphs. Observe how HARP's embedding better preserves the higher order structure of a ring and a plane. then be used as features for common tasks on graphs such as multi-label classification, clustering, and link prediction. Traditional methods for graph dimensionality reduction (Belkin and Niyogi 2001; <ref type="bibr" target="#b8">Roweis and Saul 2000;</ref><ref type="bibr" target="#b9">Tenenbaum, De Silva, and Langford 2000)</ref> perform well on small graphs. However, the time complexity of these methods are at least quadratic in the number of graph nodes, makes them impossible to run on large-scale networks.</p><p>A recent advancement in graph representation learning, DeepWalk <ref type="bibr" target="#b6">(Perozzi, Al-Rfou, and Skiena 2014)</ref> proposed online learning methods using neural networks to address this scalability limitation. Much work has since followed <ref type="bibr" target="#b1">(Cao, Lu, and Xu 2015;</ref><ref type="bibr" target="#b3">Grover and Leskovec 2016;</ref><ref type="bibr" target="#b6">Perozzi et al. 2017;</ref><ref type="bibr" target="#b9">Tang et al. 2015)</ref>. These neural network-based methods have proven both highly scalable and performant, achieving strong results on classification and link prediction tasks in large networks.</p><p>Despite their success, all these methods have several shared weaknesses. Firstly, they are all local approacheslimited to the structure immediately around a node. Deep-Walk <ref type="bibr" target="#b6">(Perozzi, Al-Rfou, and Skiena 2014)</ref> and Node2vec (Grover and Leskovec 2016) adopt short random walks to explore the local neighborhoods of nodes, while <ref type="bibr">LINE (Tang et al. 2015</ref>) is concerned with even closer relationships (nodes at most two hops away). This focus on local structure implicitly ignores long-distance global relationships, and the learned representations can fail to uncover important global structural patterns. Secondly, they all rely on a non-convex optimization goal solved using stochastic gradient descent <ref type="bibr" target="#b3">(Goldberg and Levy 2014;</ref><ref type="bibr" target="#b5">Mikolov et al. 2013)</ref> which can become stuck in a local minima (e.g. perhaps as a result of a poor initialization). In other words, all previously proposed techniques for graph representation learning can accidentally learn embedding configurations which disregard important structural features of their input graph.</p><p>In this work, we propose HARP, a meta strategy for embedding graph datasets which preserves higher-order structural features. HARP recursively coalesces the nodes and edges in the original graph to get a series of successively smaller graphs with similar structure. These coalesced graphs, each with a different granularity, provide us a view of the original graph's global structure. Starting from the most simplified form, each graph is used to learn a set of initial representations which serve as good initializations for embedding the next, more detailed graph. This process is repeated until we get an embedding for each node in the original graph.</p><p>We illustrate the effectiveness of this multilevel paradigm in <ref type="figure">Figure 1</ref>, by visualizing the two-dimension embeddings from an existing method (LINE (Tang et al. 2015)) and our improvement to it, HARP(LINE). Each of the small graphs we consider has an obvious global structure (that of a ring (1a) and a grid (1d)) which is easily exposed by a force direced layout <ref type="bibr" target="#b3">(Hu 2005)</ref>. The center figures represent the two-dimensional embedding obtained by LINE for the ring (1b) and grid (1e). In these embeddings, the global structure is lost (i.e. that is, the ring and plane are unidentifiable). However, the embeddings produced by using our meta-strategy to improve LINE (right) clearly capture both the local and global structure of the given graphs (1c, 1f).</p><p>Our contributions are the following: • New Representation Learning Paradigm. We propose HARP, a novel multilevel paradigm for graph representation which seamlessly blends ideas from the graph drawing (Fruchterman and Reingold 1991) and graph representation learning <ref type="bibr" target="#b6">(Perozzi, Al-Rfou, and Skiena 2014;</ref><ref type="bibr" target="#b9">Tang et al. 2015</ref>; Grover and Leskovec 2016) communities to build substantially better graph embeddings. • Improved Optimization Primitives. We demonstrate that our approach leads to improved implementations of all state-of-the-art graph representation learning methods, namely DeepWalk (DW), LINE and Node2vec (N2V). Our improvements on these popular methods for learning latent representations illustrate the broad applicability of our hierarchical approach. • Better Embeddings for Downstream Tasks. We demonstrate that HARP(DW), HARP(LINE) and HARP(N2V) embeddings consistently outperform the originals on clas-sification tasks on several real-world networks, with improvements as large as 14% Macro F 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Formulation</head><p>We desire to learn latent representations of nodes in a graph. Formally, let G = (V, E) be a graph, where V is the set of nodes and E is the set of edges. The goal of graph representation learning is to develop a mapping func-</p><formula xml:id="formula_0">tion Φ : V → R |V |×d , d</formula><p>|V |. This mapping Φ defines the latent representation (or embedding) of each node v ∈ V . Popular methods for learning the parameters of Φ <ref type="bibr" target="#b6">(Perozzi, Al-Rfou, and Skiena 2014;</ref><ref type="bibr" target="#b9">Tang et al. 2015</ref>; Grover and Leskovec 2016) suffer from two main disadvantages: (1) higher-order graph structural information is not modeled, and (2) their stochastic optimization can fall victim to poor initialization.</p><p>In light of these difficulties, we introduce the hierarchical representation learning problem for graphs. At its core, we seek to find a graph, G s = (V s , E s ) which captures the essential structure of G, but is smaller than our original (i.e.</p><formula xml:id="formula_1">|V s | &lt;&lt; |V |, |E s | &lt;&lt; |E|).</formula><p>It is likely that G s will be easier to embed for two reasons. First, there are many less pairwise relationships (|V s | 2 versus |V | 2 ) which can be expressed in the space. As the sample space shrinks, there is less variation in training examples -this can yield a smoother objective function which is easier to optimize. Second, the diameter of G s may be smaller than G, so algorithms with a local focus can exploit the graph's global structure.</p><p>In summary, we define the hierarchical representation learning problem in graphs as follows:</p><p>Given a large graph G(V, E) and a function f , which embeds G using initialization θ, f :</p><formula xml:id="formula_2">G × θ → Φ G , Simplify G to a series of successively smaller graphs G 0 . . . G L , Learn a coarse embedding Φ G L = f (G L , ∅),</formula><p>Refine the coarse embedding into Φ G by iteratively applying</p><formula xml:id="formula_3">Φ Gi = f (G i , Φ Gi+1 ), 0 ≤ i &lt; L.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Here we present our hierarchical paradigm for graph representation learning. After discussing the method in general, we present a structure-preserving algorithm for its most crucial step, graph coarsening.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm: HARP</head><p>Our method for multi-level graph representation learning, HARP, is presented in Algorithm 1. It consists of three parts -graph coarsening, graph embedding, and representation refinement -which we detail below:</p><p>1. Graph Coarsening (line 1): Given a graph G, graph coarsening algorithms create a hierarchy of successively smaller graphs</p><formula xml:id="formula_4">G 0 , G 1 , • • • , G L , where G 0 = G.</formula><p>The coarser (smaller) graphs preserve the global structure of the original graph, yet have significantly fewer nodes and edges. Algorithms for generating this hierarchy of graphs will be discussed in detail below. Figure 2: Illustration of graph coarsening algorithms. 2a: Edge collapsing on a graph snippet. 2b: How edge collapsing fails to coalesce star-like structures. 2c: How star collapsing scheme coalesces the same graph snippet efficiently.</p><p>Algorithm 1 HARP(G, Embed()) Input:</p><formula xml:id="formula_5">graph G(V, E) arbitrary graph embedding algorithm EMBED() Output: matrix of vertex representations Φ ∈ R |V |×d 1: G 0 , G 1 , • • • , G L ← GRAPHCOARSENING(G) 2: Initialize Φ G L by assigning zeros 3: Φ G L ← EMBED(G L , Φ G L ) 4: for i = L − 1 to 0 do 5: Φ Gi ← PROLONGATE(Φ Gi+1 , G i+1 , G i ) 6: Φ Gi ← EMBED(G i , Φ Gi ) 7: end for 8: return Φ G0</formula><p>2. Graph Embedding on the Coarsest Graph (line 2-3): The graph embedding is obtained on the coarsest graph G L with the provided graph embedding algorithm. As the size of G L is usually very small, it is much easier to get a highquality graph representation.</p><p>3. Graph Representation Prolongation and Refinement (line 4-7): We prolong and refine the graph representation from the coarsest to the finest graph. For each graph G i , we prolong the graph representation of G i+1 as its initial embedding Φ Gi . Then, the embedding algorithm Embed() is applied to (G i , Φ Gi ) to further refine Φ Gi , resulting in the refined embedding Φ Gi . We discuss this step in the embedding prolongation section below.</p><p>4. Graph Embedding of the Original Graph (line 8): We return Φ G0 , which is the graph embedding of the original graph.</p><p>We can easily see that this paradigm is algorithm independent, relying only on the provided functions Embed(). Thus, with minimum effort, this paradigm can be incorporated into any existing graph representation learning methods, yielding a multilevel version of that method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Coarsening</head><p>In Algorithm 2, we develop a hybrid graph coarsening scheme which preserves global graph structural information at different scales. Its two key parts, namely edge collapsing and star collapsing, preserve first-order proximity and second-order proximity (Tang et al. 2015) respectively. Firstorder proximity is concerned with preserving the observed edges in the input graph, while second-order proximity is</p><formula xml:id="formula_6">Algorithm 2 GraphCoarsening(G) Input: graph G(V, E) Output: Series of Coarsened Graphs G 0 , G 1 , • • • , G L 1: L ← 0 2: G 0 ← G 3: while |V L | ≥ threshold do 4: L ← L + 1 5: G L ← EDGECOLLAPSE(STARCOLLAPSE(G)) 6: end while 7: return G 0 , G 1 , • • • , G L</formula><p>based on the shared neighborhood structure of the nodes.</p><p>Edge Collapsing. Edge collapsing (Hu 2005) is an efficient algorithm for preserving first-order proximity. It selects E ⊆ E, such that no two edges in E are incident to the same vertex. Then, for each</p><formula xml:id="formula_7">(u i , v i ) ∈ E , it merges (u i , v i )</formula><p>into a single node w i , and merge the edges incident to u i and v i . The number of nodes in the coarser graph is therefore at least half of that in the original graph. As illustrated in <ref type="figure">Figure 2a</ref>, the edge collapsing algorithm merges node pairs</p><formula xml:id="formula_8">(v 1 , v 2 ) and (v 3 , v 4 ) into supernodes v 1,2 and v 3,4</formula><p>respectively, resulting in a coarser graph with 2 nodes and 1 edge. The order of merging is arbitrary; we find different merging orders result in very similar node embeddings in practice.</p><p>Star Collapsing. Real world graphs are often scale-free, which means they contain a large number of star-like structures. A star consists of a popular central node (sometimes referred to as hubs) connected to many peripheral nodes. Although the edge collapsing algorithm is simple and efficient, it cannot sufficiently compress the star-like structures in a graph. Consider the graph snippet in <ref type="figure">Figure 2b</ref>, where the only central node v 7 connects to all the other nodes. Assume the degree of the central node is k, it is clear that the edge collapsing scheme can only compress this graph into a coarsened graph with k − 1 nodes. Therefore when k is large, the coarsening process could be arbitrarily slow, takes O(k) steps instead of O(log k) steps.</p><p>One observation on the star structure is that there are strong second-order similarities between the peripheral nodes since they share the same neighborhood. This leads to our star collapsing scheme, which merges nodes with the same neighbors into supernodes since they are similar to each other. As shown in <ref type="figure">Figure 2c</ref>,</p><formula xml:id="formula_9">(v 1 , v 2 ), (v 3 , v 4 ) and (v 5 , v 6 )</formula><p>are merged into supernodes as they share the same neighbors (v 7 ), generating a coarsened graph with only k/2 nodes.</p><p>Hybrid Coarsening Scheme. By combining edge collapsing and star collapsing, we present a hybrid scheme for graph coarsening in Algorithm 2, which is adopted on all test graphs. In each coarsening step, the hybrid coarsening scheme first decomposes the input graph with star collapsing, then adopts the edge collapsing scheme to generate the coalesced graph. We repeat this process until a small enough graph (with less than 100 vertices) is obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Prolongation</head><p>After the graph representation for G i+1 is learned, we prolong it into the initial representation for G i . We observe that each node v ∈ G i+1 is either a member of the finer representation (v ∈ G i ), or the result of a merger,</p><formula xml:id="formula_10">(v 1 , v 2 , • • • , v k ) ∈ G i .</formula><p>In both cases, we can simply reuse the representation of the parent node v ∈ G i -the children are quickly separated by gradient updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity Analysis</head><p>In this section, we discuss the time complexity of HARP(DW) and HARP(LINE) and compare with the time complexity of DeepWalk and LINE respectively. HARP(N2V) has the same time complexity as HARP(DW), thus it is not included in the discussion below. HARP(DW): Given the number of random walks γ, walk length t, window size w and representation size d, the time complexity of DeepWalk is dominated by the training time of the Skip-gram model, which is O(γ|V |tw(d + dlog|V |)). For HARP(DW), coarsening a graph with |V | nodes produces a coarser graph with about |V |/2 nodes. The total number of nodes in all levels is approximately</p><formula xml:id="formula_11">|V | log2|V | i=0</formula><p>( 1 ) i = 2|V |. Therefore, the time complexity of HARP(DW) is O(|V |) for copying binary tree and O(γ|V |tw(d+dlog|V |)) for model training. Thus, the overall time complexity of HARP(DW) is also O(γ|V |tw(d + dlog|V |)). HARP(LINE): The time complexity of LINE is linear to the number of edges in the graph and the number of iterations r over edges, which is O(r|E|). For HARP(LINE), coarsening a graph with |E| nodes produces a coarsened graph with about |E|/2 edges. The total number edges in all levels is approximately |E| log2|E| i=0</p><p>( 1 2 ) i = 2|E|. Thus, the time complexity of HARP(LINE) is also O(r|E|).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>In this section, we provide an overview of the datasets and methods used for experiments and evaluate the effectiveness of our method on challenging multi-label classification tasks in several real-life networks. We further illustrate the scalability of our method and discuss its performance with regard to several important parameters. <ref type="table" target="#tab_1">Table 1</ref> gives an overview of the datasets used in our experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Methods</head><p>We compare our model with the following graph embedding methods:</p><p>• DeepWalk -DeepWalk is a two-phase method for embedding graphs. Firstly, DeepWalk generates random walks of fixed length from all the vertices of a graph. Then, the walks are treated as sentences in a language model and the Skip-Gram model for learning word embeddings is utilized to obtain graph embeddings. Deep-Walk uses hierarchical softmax for Skip-gram model optimization. • LINE -LINE is a method for embedding large-scale networks. The objective function of LINE is designed for preserving both first-order and second-order proximities, and we use first-order LINE for comparison. Skip-gram with negative sampling is used to solve the objective function. • Node2vec -Node2vec proposes an improvement to the random walk phase of DeepWalk. By introducing the return parameter p and the in-out parameter q, Node2vec combines DFS-like and BFS-like neighborhood exploration. Node2vec also uses negative sampling for optimizing the Skip-gram model.</p><p>For each baseline method, we combine it with HARP and compare their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Settings</head><p>Here we discuss the parameter settings for our models and baseline models. Since DeepWalk, LINE and Node2vec are all sampling based algorithms, we always ensure that the total number of samples seen by the baseline algorithm is the same as that of the corresponding HARP enhanced algorithm.  <ref type="figure">Figure 3</ref>: The ratio of nodes/edges of the coarsened graphs to that of the original test graphs. For disconnected graphs, the graph coarsening result on the largest connected component is shown.</p><p>DeepWalk. For DeepWalk and HARP(DW), we need to set the following parameters: the number of random walks γ, walk length t, window size w for the Skip-gram model and representation size d. In HARP(DW), the parameter setting is γ = 40, t = 10, w = 10, d = 128. For DeepWalk, all the parameters except γ are the same as in HARP(DW). Specifically, to ensure a fair comparison, we increase the value of γ for DeepWalk. This gives DeepWalk a larger training dataset (as large as all of the levels of HARP(DW) combined). We note that failure to increase γ in this way resulted in substantially worse DeepWalk (and Node2vec) models.</p><p>LINE. For HARP(LINE), we run 50 iterations on all graph edges on all coarsening levels. For LINE, we increase the number of iterations over graph edges accordingly, so that the amount of training data for both models remain the same. The representation size d is set to 64 for both LINE and HARP(LINE).</p><p>Node2vec. For HARP(N2V), the parameter setting is γ = 40, t = 10, w = 10, d = 128. Similar to DeepWalk, we increase the value of γ in Node2vec to ensure a fair comparison. Both in-out and return hyperparameters are set to 1.0. For all models, the initial learning rate and final learning rate are set to 0.025 and 0.001 respectively. <ref type="figure">Figure 3</ref> demonstrates the effect of our hybrid coarsening method on all test graphs. The first step of graph coarsening for each graph eliminates about half the nodes, but the number of edges only reduce by about 10% for BlogCatalog. This illustrates the difficulty of coarsening real-world graphs. However, as the graph coarsening process continues, the scale of all graphs drastically decrease. At level 8, all graphs have less than 10% nodes and edges left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Coarsening</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization</head><p>To show the intuition of the HARP paradigm, we set d = 2, and visualize the graph representation generated by HARP(LINE) at each level. <ref type="figure" target="#fig_3">Figure 4</ref> shows the level-wise 2D graph embeddings obtained with HARP(LINE) on Poisson 2D. The graph layout of level 5 (which has only 21 nodes) already highly resembles the layout of the original graph. The graph layout on each subsequent level is initialized with the prolongation of the previous graph layout, thus the global structure is kept. Level 7 denotes the smallest graph, while level 0 denotes the original graph. The last subfigure is the graph layout generated by a force-direct graph drawing algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-label Classification</head><p>We evaluate our method using the same experimental procedure in <ref type="bibr" target="#b6">(Perozzi, Al-Rfou, and Skiena 2014)</ref>. Firstly, we obtain the graph embeddings of the input graph. Then, a portion (T R ) of nodes along with their labels are randomly sampled from the graph as training data, and the task is to predict the labels for the remaining nodes. We train a one-vs-rest logistic regression model with L2 regularization on the graph embeddings for prediction. The logistic regression model is implemented by LibLinear <ref type="bibr" target="#b2">(Fan et al. 2008)</ref>. To ensure the reliability of our experiment, the above process is repeated for 10 times, and the average Macro F 1 score is reported. The other evaluation metrics such as Micro F 1 score and accuracy follow the same trend as Macro F 1 score, thus are not shown. <ref type="table">Table 2</ref> reports the Macro F 1 scores achieved on DBLP, BlogCatalog, and CiteSeer with 5%, 50%, and 5% labeled nodes respectively. The number of class labels of BlogCatalog is about 10 times that of the other two graphs, thus we use a larger portion of labeled nodes. We can see that our method improves all existing neural embedding techniques on all test graphs. In DBLP, the improvements introduced by HARP(DW), HARP(LINE) and HARP(N2V) are 7.8%, 3.0% and 0.3% respectively. Given the scale-free nature of BlogCatalog, graph coarsening is much harder due to a large amount of star-like structures in it. Still, HARP(DW), HARP(LINE) and HARP(N2V) achieve gains of 4.0%, 4.6% and 4.7% over the corresponding baseline methods respectively. For CiteSeer, the performance improvement is also striking: HARP(DW), HARP(LINE) and HARP(N2V) outperforms the baseline methods by 4.8%, 13.6%, and 2.8%.</p><p>To have a detailed comparison between HARP and the baseline methods, we vary the portion of labeled nodes for classification, and present the macro F 1 scores in <ref type="figure">Fig-0</ref>  <ref type="table">Table 2</ref>: Macro F 1 scores and performance gain of HARP on DBLP, BlogCatalog, and CiteSeer in percentage. * indicates statistically superior performance to the corresponding baseline method at level of 0.001 using a standard paired t-test. Our method improves all existing neural embedding techniques.</p><p>ure 5. We can observe that HARP(DW), HARP(LINE) and HARP(N2V) consistently perform better than the corresponding baseline methods. DBLP. For DBLP, the relative gain of HARP(DW) is over 9% with 4% labeled data. With only 2% labeled data, HARP(DW) achieves higher macro F 1 score than Deep-Walk with 8% label data. HARP(LINE) also consistently outperforms LINE given any amount of training data, with macro F 1 score gain between 1% and 3%. HARP(N2V) and Node2vec have comparable performance with less than 5% labeled data, but as the ratio of labeled data increases, HARP(N2V) eventually distances itself to a 0.7% improvement over Node2vec. We can also see that Node2vec generally has better performance when compared to DeepWalk, and the same holds for HARP(N2V) and HARP(DW). The difference in optimization method for Skip-gram (negative sampling for Node2vec and hierarchical softmax for Deep-Walk) may account for this difference.</p><p>BlogCatalog. As a scale-free network with complex structure, BlogCatalog is challenging for graph coarsening. Still, by considering both first-order proximity and secondorder proximity, our hybrid coarsening algorithm generates an appropriate hierarchy of coarsened graphs. With the same amount of training data, HARP(DW) always leads by at least 3.0%. For HARP(LINE), it achieves a relative gain of 4.8% with 80% labeled data. For HARP(N2V), its gain over Node2vec reaches 4.7% given 50% labeled nodes.</p><p>Citeseer. For CiteSeer, the lead of HARP(DW) on Macro F 1 score varies between 5.7% and 7.8%. For HARP(LINE), its improvement over LINE with 4% labeled data is an impressive 24.4%. HARP(N2V) also performs better than Node2vec on any ratio of labeled nodes. 10 10 2 10 3 10 4 10 5 10 6</p><p>Number of Nodes  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalability</head><p>We already shown that introducing HARP does not affect the time complexity of the underlying graph embedding algorithms. Here, we compare the actual run time of HARP enhanced embedding algorithms with the corresponding baseline methods on all test graphs. All models run on a single machine with 128GB memory, 24 CPU cores at 2.0GHZ with 20 threads. As shown in <ref type="figure" target="#fig_5">Figure 6a</ref>, applying HARP typically only introduces an overhead of less than 10% total running time. The time spent on sampling and training the Skip-gram model dominates the overall running time. Additionally, we learn graph embeddings on Erdos-Renyi graphs with node count ranging from 100 to 100,000 and constant average degree of 10. In <ref type="figure" target="#fig_5">Figure 6b</ref>, we can observe that the running time of HARP increases linearly with the number of nodes in the graph. Also, when compared to the corresponding baseline method, the overhead introduces by the graph coarsening and prolongation process in HARP is negligible, especially on large-scale graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>The related work is in the areas of graph representation learning and graph drawing, which we briefly describe here. Graph Representation Learning. Most early methods treated representation learning as performing dimension reduction on the Laplacian and adjacency matrices <ref type="bibr" target="#b0">(Belkin and Niyogi 2001;</ref><ref type="bibr" target="#b1">Cox and Cox 2000;</ref><ref type="bibr" target="#b9">Tenenbaum, De Silva, and Langford 2000)</ref>. These methods work well on small graphs, but the time complexity of these algorithms is too high for the large-scale graphs commonly encountered today.</p><p>Recently, neural network-based methods have been proposed for constructing node representation in large-scale graphs. Deepwalk <ref type="bibr" target="#b6">(Perozzi, Al-Rfou, and Skiena 2014)</ref> presents a two-phase algorithm for graph representation learning. In the first phase, Deepwalk samples sequences of neighboring nodes of each node by random walking on the graph. Then, the node representation is learned by training a Skip-gram model <ref type="bibr" target="#b5">(Mikolov et al. 2013</ref>) on the random walks. A number of methods have been proposed which extend this idea. First, several methods use different strategies for sampling neighboring nodes. <ref type="bibr">LINE (Tang et al. 2015)</ref> learns graph embeddings which preserve both the first-order and second-order proximities in a graph. Walklets <ref type="bibr" target="#b6">(Perozzi et al. 2017)</ref> captures multiscale node representation on graphs by sampling edges from higher powers of the graph adjacency matrix. Node2vec (Grover and Leskovec 2016) combines DFS-like and BFS-like exploration within the random walk framework. Second, matrix factorization methods and deep neural networks have also been proposed <ref type="bibr" target="#b1">(Cao, Lu, and Xu 2015;</ref><ref type="bibr">Ou et al. 2016;</ref><ref type="bibr" target="#b11">Wang, Cui, and Zhu 2016;</ref><ref type="bibr" target="#b0">Abu-El-Haija, Perozzi, and Al-Rfou 2017)</ref> as alternatives to the Skip-gram model for learning the latent representations.</p><p>Although these methods are highly scalable, they all rely on optimizing a non-convex objective function. With no prior knowledge of the graph, the latent representations are usually initialized with random numbers or zero. With such an initialization scheme, these methods are at risk of converging to a poor local minima. HARP overcomes this problem by introducing a multilevel paradigm for graph representation learning. Graph Drawing. Multilevel layout algorithms are popular methods in the graph drawing community, where a hierarchy of approximations is used to solve the original layout problem <ref type="bibr" target="#b2">(Fruchterman and Reingold 1991;</ref><ref type="bibr" target="#b3">Hu 2005;</ref><ref type="bibr">Walshaw 2003)</ref>. Using an approximation of the original graph has two advantages -not only is the approximation usually simpler to solve, it can also be extended as a good initialization for solving the original problem. In addition to force-directed graph drawing, the multilevel framework <ref type="bibr" target="#b10">(Walshaw 2004)</ref> has been proved successful in various graph theory problems, including the traveling salesman problem <ref type="bibr">(Walshaw 2001)</ref>, and graph partitioning <ref type="bibr" target="#b4">(Karypis and Kumar 1998)</ref>.</p><p>HARP extends the idea of the multilevel layout to neural representation learning methods. We illustrate the utility of this paradigm by combining HARP with three state-of-theart representation learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Recent literature on graph representation learning aims at optimizing a non-convex function. With no prior knowledge of the graph, these methods could easily get stuck at a bad local minima as the result of poor initialization. Moreover, these methods mostly aim to preserve local proximities in a graph but neglect its global structure. In this paper, we propose a multilevel graph representation learning paradigm to address these issues. By recursively coalescing the input graph into smaller but structurally similar graphs, HARP captures the global structure of the input graph. By learning graph representation on these smaller graphs, a good initialization scheme for the input graph is derived. This multilevel paradigm is further combined with the state-ofthe-art graph embedding methods, namely DeepWalk, LINE, and Node2vec. Experimental results on various real-world graphs show that introducing HARP yields graph embeddings of higher quality for all these three methods.</p><p>In the future, we would like to combine HARP with other graph representation learning methods. Specifically, as Skipgram is a shallow method for representation learning, it would be interesting to see if HARP also works well with deep representation learning methods. On the other hand, our method could also be applied to language networks, possibly yielding better word embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Two-dimensional embeddings generated with HARP(LINE) on different coarsening levels on Poisson 2D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Runtime analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the graphs used in our experiments. • DBLP (Perozzi et al. 2017) -DBLP is a co-author graph of researchers in computer science. The labels indicate the research areas a researcher publishes his work in. The 4 research areas included in this dataset are DB, DM, IR, and ML.</figDesc><table><row><cell>• BlogCatalog (Tang and Liu 2009) -BlogCatalog is a net-</cell></row><row><cell>work of social relationships between users on the Blog-</cell></row><row><cell>Catalog website. The labels represent the categories a</cell></row><row><cell>blogger publishes in.</cell></row></table><note>• CiteSeer (Sen et al. 2008) -CiteSeer is a citation network between publications in computer science. The labels in- dicate the research areas a paper belongs to. The papers are classified into 6 categories: Agents, AI, DB, IR, ML, and HCI.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Detailed multi-label classification result on DBLP, BlogCatalog, and CiteSeer.</figDesc><table><row><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.64</cell><cell></cell></row><row><cell>DBLP</cell><cell>0.52 0.54 0.56 0.58 0.60 0.62</cell><cell></cell><cell cols="2">HARP(DeepWalk) DeepWalk</cell><cell></cell><cell>0.52 0.54 0.56 0.58 0.60 0.62</cell><cell></cell><cell cols="2">HARP(LINE) LINE</cell><cell></cell><cell>0.52 0.54 0.56 0.60 0.62 0.58</cell><cell></cell><cell>HARP(Node2vec) Node2vec</cell><cell>1 Score Macro F</cell></row><row><cell></cell><cell>.00 0.50</cell><cell cols="3">0.02 Fraction of Labeled Data 0.04 0.06 0.08</cell><cell>0.10</cell><cell>0.00 0.50</cell><cell>0.02</cell><cell>0.04 Fraction of Labeled Data 0.06</cell><cell>0.08</cell><cell>0.10</cell><cell>0.00 0.50</cell><cell cols="2">0.02 Fraction of Labeled Data 0.04 0.06 0.08</cell><cell>0.10</cell></row><row><cell></cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.30</cell><cell></cell></row><row><cell>BlogCatalog</cell><cell>0.16 0.18 0.20 0.22 0.24 0.26 0.28</cell><cell></cell><cell cols="3">HARP(DeepWalk) DeepWalk</cell><cell>0.24 0.26 0.28 0.16 0.18 0.20 0.22</cell><cell></cell><cell cols="3">HARP(LINE) LINE</cell><cell>0.24 0.26 0.28 0.16 0.18 0.20 0.22</cell><cell></cell><cell>HARP(Node2vec) Node2vec</cell><cell>1 Score Macro F</cell></row><row><cell></cell><cell>0.0</cell><cell cols="3">0.2 Fraction of Labeled Data 0.4 0.6 0.8</cell><cell>1.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4 Fraction of Labeled Data 0.6</cell><cell>0.8</cell><cell>1.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4 Fraction of Labeled Data 0.6</cell><cell>0.8</cell><cell>1.0</cell></row><row><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.50</cell><cell></cell></row><row><cell>CiteSeer</cell><cell>0.30 0.35 0.40 0.45</cell><cell></cell><cell cols="2">HARP(DeepWalk) DeepWalk</cell><cell></cell><cell>0.40 0.45 0.30 0.35</cell><cell></cell><cell cols="2">HARP(LINE) LINE</cell><cell></cell><cell>0.40 0.45 0.30 0.35</cell><cell></cell><cell>HARP(Node2vec) Node2vec</cell><cell>1 Score Macro F</cell></row><row><cell></cell><cell>0.00 0.25</cell><cell cols="3">0.02 Fraction of Labeled Data 0.04 0.06 0.08</cell><cell>0.10</cell><cell>0.00 0.25</cell><cell>0.02</cell><cell>0.04 Fraction of Labeled Data 0.06</cell><cell>0.08</cell><cell>0.10</cell><cell>0.00 0.25</cell><cell cols="2">0.02 Fraction of Labeled Data 0.04 0.06 0.08</cell><cell>0.10</cell></row><row><cell cols="4">Figure 5: Algorithm</cell><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>DBLP</cell><cell cols="5">BlogCatalog CiteSeer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DeepWalk</cell><cell></cell><cell>57.29</cell><cell>24.88</cell><cell></cell><cell cols="2">42.72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">HARP(DW)</cell><cell cols="3">61.76  *  25.90  *</cell><cell cols="3">44.78  *</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Gain of HARP[%] 7.8</cell><cell>4.0</cell><cell></cell><cell>4.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">LINE</cell><cell></cell><cell>57.76</cell><cell>22.43</cell><cell></cell><cell cols="2">37.11</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">HARP(LINE)</cell><cell cols="3">59.51  *  23.47  *</cell><cell cols="3">42.95  *</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Gain of HARP[%] 3.0</cell><cell>4.6</cell><cell></cell><cell cols="2">13.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Node2vec</cell><cell></cell><cell>62.64</cell><cell>23.55</cell><cell></cell><cell cols="2">44.84</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">HARP(N2V)</cell><cell>62.80</cell><cell cols="2">24.66  *</cell><cell cols="3">46.08  *</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Gain of HARP[%] 0.3</cell><cell>4.7</cell><cell></cell><cell>2.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is partially supported by NSF grants IIS-1546113 and DBI-1355990.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perozzi</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Rfou ;</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05615</idno>
	</analytic>
	<monogr>
		<title level="m">Learning edge representations via low-rank asymmetric projections</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Grarep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu ; Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="891" to="900" />
		</imprint>
	</monogr>
	<note>Multidimensional scaling</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1129" to="1164" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note>Software: Practice and experience</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">word2vec explained: deriving mikolov et al.&apos;s negativesampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Leskovec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="37" to="71" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Efficient, high-quality force-directed graph drawing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A parallel algorithm for multilevel graph partitioning and sparse matrix ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="95" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>Karypis and Kumar</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Don&apos;t walk, skip!: Online learning of multiscale network embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Rfou</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2017</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="258" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Roweis and Saul</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multilevel Lin-Kernighan-Helsgaun algorithm for the travelling salesman problem. Citeseer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sen</surname></persName>
		</author>
		<idno>Tang et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>Walshaw; Walshaw, C</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="253" to="285" />
		</imprint>
	</monogr>
	<note>AI Magazine</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multilevel refinement for combinatorial optimisation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Walshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="325" to="372" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
