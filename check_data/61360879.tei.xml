<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-11-25">25 Nov 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
							<email>cbfinn@eecs.berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Indicates equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Christiano</surname></persName>
							<email>paulfchristiano@eecs.berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Indicates equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
							<email>pabbeel@eecs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<email>svlevine@eecs.berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-11-25">25 Nov 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1611.03852v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning the cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator&apos;s density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator <ref type="bibr" target="#b7">[8]</ref>. While the idea of learning objectives is relatively new to the field of generative modeling, learning cost or reward functions has long been studied in control <ref type="bibr" target="#b4">[5]</ref> and was popularized in 2000 for reinforcement learning problems <ref type="bibr" target="#b14">[15]</ref>. In these fields, learning the cost function underlying demonstrated behavior is referred to as inverse reinforcement learning (IRL) or inverse optimal control (IOC). At first glance, the connection between cost learning in RL and cost learning for generative models may appear to be superficial; however, if we apply GANs to a setting where the generator density can be efficiently evaluated, the result is exactly equivalent to a sample-based algorithm for maximum entropy (MaxEnt) IRL. Interestingly, as MaxEnt IRL is an energy-based model, this connection suggests a method for using GANs to train a broader class of energy-based models.</p><p>MaxEnt IRL is a widely-used objective for IRL, proposed by Ziebart et al. <ref type="bibr" target="#b26">[27]</ref>. Sample-based algorithms for performing maximum entropy (MaxEnt) IRL have scaled cost learning to scenarios with unknown dynamics, using nonlinear function classes, such as neural networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>. We show that the gradient updates for the cost and the policy in these methods can be viewed as the updates for the discriminator and generator in GANs, under a specific form of the discriminator. The key difference to a generic discriminator is that we need to be able evaluate the density of the generator, which we integrate into the discriminator in a natural way.</p><p>Traditionally, GANs are used to train generative models for which it is not possible to evaluate the density. When it is possible to evaluate the density, for example in an autoregressive model, it is typical to maximize the likelihood of the data directly. By considering the connection to IRL, we find that GAN training may be appropriate even when density values are available. For example, suppose we are interested in modeling a complex multimodal distribution, but our model does not have enough capacity to represent the distribution. Then maximizing likelihood will lead to a distribution which "covers" all of the modes, but puts most of its mass in parts of the space that have negligible density under the data distribution. These might be images that look extremely unrealistic, nonsensical sentences, or suboptimal robot behavior. A generator trained adversarially will instead try to "fill in" as many of modes as it can, without putting much mass in the space between modes. This results in lower diversity, but ensures that samples "look like" they could have been from the original data.</p><p>By drawing an exact correspondence between adaptive, sample-based algorithms for MaxEnt IRL and GAN training, we show that this phenomenon occurs and is practically important: GAN training can significantly improve the quality of samples even when the generator density can be exactly evaluated. This is precisely analogous to the observed ability of inverse reinforcement learning to imitate behaviors that cannot be successfully learned through behavioral cloning <ref type="bibr" target="#b20">[21]</ref>, direct maximum likelihood regression to the demonstrated behavior.</p><p>Interestingly, the maximum entropy formulation of IRL is a special case of an energy-based model (EBM) <ref type="bibr" target="#b25">[26]</ref>. The learned cost in MaxEnt IRL corresponds to the energy function, and is trained via maximum likelihood. Hence, we can also show how a particular form of GANs can be used to train EBMs. Recent works have recognized a connection between EBMs and GANs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref>. In this work, we particularly focus on EBMs trained with maximum likelihood, and expand upon the connection recognized by Kim &amp; Bengio <ref type="bibr" target="#b11">[12]</ref> for the case where the generator's density can be computed. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three areas can better identify and apply transferable ideas from one domain to another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we formally define generative adversarial networks (GANs), energy-based models (EBMs), and inverse reinforcement learning (IRL), and introduce notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Adversarial Networks</head><p>Generative adversarial networks are an approach to generative modeling where two models are trained simultaneously: a generator G and a discriminator D. The discriminator is tasked with classifying its inputs as either the output of the generator, or actual samples from the underlying data distribution p(x). The goal of the generator is to produce outputs that are classified by the discriminator as coming from the underlying data distribution <ref type="bibr" target="#b7">[8]</ref>.</p><p>Formally, the generator takes noise as input and outputs a sample x ∼ G, while the discriminator takes as input a sample x and outputs the probability D(x) that the sample was from the data distribution. The discriminator's loss is the average log probability it assigns to the correct classification, evaluated on an equal mixture of real samples and outputs from the generator:</p><formula xml:id="formula_0">L discriminator (D) = E x∼p [− log D(x)] + E x∼G [− log(1 − D(x))].</formula><p>The generator's loss can be defined one of several similar ways. The simplest definition, originally proposed in <ref type="bibr" target="#b7">[8]</ref>, is simply the opposite of the discriminator's loss. However, this provides very little training signal if the generator's output can be easily distinguished from the real samples. It is common to instead use the log of the discriminator's confusion <ref type="bibr" target="#b7">[8]</ref>. We will define the generator's loss as the sum of these two variants:</p><formula xml:id="formula_1">L generator (G) = E x∼G [− log D(x)] + E x∼G [log(1 − D(x))].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Energy-Based Models</head><p>Energy-based models <ref type="bibr" target="#b13">[14]</ref> associate an energy value E θ (x) with a sample x, modeling the data as a Boltzmann distribution:</p><formula xml:id="formula_2">p θ (x) = 1 Z exp(−E θ (x))<label>(1)</label></formula><p>The energy function parameters θ are often chosen to maximize the likelihood of the data; the main challenge in this optimization is evaluating the partition function Z, which is an intractable sum or integral for most high-dimensional problems. A common approach to estimating Z requires sampling from the Boltzmann distribution p θ (x) within the inner loop of learning.</p><p>Sampling from p θ (x) can be approximated by using Markov chain Monte Carlo (MCMC) methods; however, these methods face issues when there are several distinct modes of the distribution and, as a result, can take arbitrarily large amounts of time to produce a diverse set of samples. Approximate inference methods can also be used during training, though the energy function may incorrectly assign low energy to some modes if the approximate inference method cannot find them <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inverse Reinforcement Learning</head><p>The goal of inverse reinforcement learning is to infer the cost function underlying demonstrated behavior <ref type="bibr" target="#b14">[15]</ref>. It is typically assumed that the demonstrations come from an expert who is behaving near-optimally under some unknown cost. In this section, we discuss MaxEnt IRL and guided cost learning, an algorithm for MaxEnt IRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Maximum entropy inverse reinforcement learning</head><p>Maximum entropy inverse reinforcement learning models the demonstrations using a Boltzmann distribution, where the energy is given by the cost function c θ :</p><formula xml:id="formula_3">p θ (τ) = 1 Z exp(−c θ (τ)),</formula><p>Here</p><formula xml:id="formula_4">, τ = {x 1 , u 1 , . . . , x T , u T } is a trajectory; c θ (τ) = ∑ t c θ (x t , u t )</formula><p>is a learned cost function parametrized by θ ; x t and u t are the state and action at time step t; and the partition function Z is the integral of exp(−c θ (τ)) over all trajectories that are consistent with the environment dynamics. <ref type="bibr" target="#b1">2</ref> Under this model, the optimal trajectories have the highest likelihood, and the expert can generate suboptimal trajectories with a probability that decreases exponentially as the trajectories become more costly. As in other energy-based models, the parameters θ are optimized to maximize the likelihood of the demonstrations. Estimating the partition function Z is difficult for large or continuous domains, and presents the main computational challenge. The first applications of this model computed Z exactly with dynamic programming <ref type="bibr" target="#b26">[27]</ref>. However, this is only practical in small, discrete domains, and is impossible in domains where the system dynamics p(x t+1 |x t , u t ) are unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Guided cost learning</head><p>Guided cost learning introduces an iterative sample-based method for estimating Z in the Max-Ent IRL formulation, and can scale to high-dimensional state and action spaces and nonlinear cost functions <ref type="bibr" target="#b6">[7]</ref>. The algorithm estimates Z by training a new sampling distribution q(τ) and using importance sampling:</p><formula xml:id="formula_5">L cost (θ ) = E τ∼p [− log p θ (τ)] = E τ∼p [c θ (τ)] + log Z = E τ∼p [c θ (τ)] + log E τ∼q exp(−c θ (τ)) q(τ) .</formula><p>Guided cost learning alternates between optimizing c θ using this estimate, and optimizing q(τ) to minimize the variance of the importance sampling estimate.</p><p>The optimal importance sampling distribution for estimating the partition function exp(−c</p><formula xml:id="formula_6">θ (τ))dτ is q(τ) ∝ | exp(−c θ (τ))| = exp(−c θ (τ)).</formula><p>During guided cost learning, the sampling policy q(τ) is updated to match this distribution by minimizing the KL divergence between q(τ) and 1 Z exp(−c θ (τ)), or equivalently minimizing the learned cost and maximizing entropy.</p><formula xml:id="formula_7">L sampler (q) = E τ∼q [c θ (τ)] + E τ∼q [log q(τ)]<label>(2)</label></formula><p>Conveniently, this optimal sampling distribution is the demonstration distribution for the true cost function. Thus, this training procedure results in both a learned cost function, characterizing the demonstration distribution, and a learned policy q(τ), capable of generating samples from the demonstration distribution.</p><p>This importance sampling estimate can have very high variance if the sampling distribution q fails to cover some trajectories τ with high values of exp(−c θ (τ)). Since the demonstrations will have low cost (as a result of the IRL objective), we can address this coverage problem by mixing the demonstration data samples with the generated samples. Let µ = 1 2 p + 1 2 q be the mixture distribution over trajectory roll-outs. Let p(τ) be a rough estimate for the density of the demonstrations; for example we could use the current model p θ , or we could use a simpler density model trained using another method. Guided cost learning uses µ for importance sampling 3 , with <ref type="bibr" target="#b0">1</ref> 2 p(τ) + 1 2 q(τ) as the importance weights:</p><formula xml:id="formula_8">L cost (θ ) = E τ∼p [c θ (τ)] + log E τ∼µ exp(−c θ (τ)) 1 2 p(τ) + 1 2 q(τ) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Direct Maximum Likelihood and Behavioral Cloning</head><p>A simple approach to imitation learning and generative modeling is to train a generator or policy to output a distribution over the data, without learning a discriminator or energy function. For tractability, the data distribution is typically factorized using a directed graphical model or Bayesian network. In the field of generative modeling, this approach has most commonly been applied to speech and language generation tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b17">18]</ref>, but has also been applied to image generation <ref type="bibr" target="#b21">[22]</ref>. Like most EBMs, these models are trained by maximizing the likelihood of the observed data points. When a generative model does not have the capacity to represent the entire data distribution, maximizing likelihood directly will lead to a moment-matching distribution that tries to "cover" all of the modes, leading to a solution that puts much of its mass in parts of the space that have negligible probability under the true distribution. In many scenarios, it is preferable to instead produce only realistic, highly probable samples, by "filling in" as many modes as possible, at the trade-off of lower diversity. Since EBMs are also trained with maximum likelihood, the energy function in an EBM will exhibit the same moment-matching behavior when it has limited capacity. However, designing a flexible energy function to represent a distribution's density function is generally much easier than designing a tractable generator with the same flexibility, that can to generate samples without a complex iterative inference procedure. Moreover, once we have a trained energy function, the generator is trained to be mode-seeking, by minimizing the KL divergence between the generator's distribution and the distribution induced by the energy function. As a result, even if the generator has the same capacity as a generative model trained with direct maximum likelihood, the generator trained with an EBM will exhibit mode-seeking behavior as long as the energy function is more flexible than the generator. Of course, this phenomenon is often achieved at the cost of tractability, as generating samples from an energy function requires training a generator which, in the case of IRL, is forward policy optimization.</p><p>In sequential decision-making domains, using direct maximum likelihood is known as behavioral cloning, where the policy is trained with supervised learning to match the actions of the demonstrating agent, conditioned on the corresponding observations. While this approach is simple and often effective for small problems, the moment-matching behavior of direct maximum likelihood can produce particularly ineffective trajectories because of compounding errors. When the policy makes a small mistake, it deviates from the state distribution seen during training, making it more likely to make a mistake again. This issue compounds and eventually, the agent reaches a state far from the training distribution and makes a catastrophic error <ref type="bibr" target="#b20">[21]</ref>. Generative modeling also faces this issue when generating variables sequentially. A popular approach for handling this involves incrementally sampling more from the model and drawing less from the data distribution during training <ref type="bibr" target="#b20">[21]</ref>. This requires that the true data distribution can be sampled from during training, corresponding to a human or algorithmic expert. Bengio et al. proposed an approximate solution, termed scheduled sampling, that does not require querying the data distribution <ref type="bibr" target="#b2">[3]</ref>. However, while these approaches alleviate the issue, they do not solve it completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GANs and IRL</head><p>We now show how generative adversarial modeling has implicitly been applied to the setting of inverse reinforcement learning, where the data-to-be-modeled is a set of expert demonstrations. The derivation requires a particular form of discriminator, which we discuss first in Section 3.1. After making this modification to the discriminator, we obtain an algorithm for IRL, as we show in Section 3.2, where the discriminator involves the learned cost and the generator represents the policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A special form of discriminator</head><p>For a fixed generator with a [typically unknown] density q(τ), the optimal discriminator is the following <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_9">D * (τ) = p(τ) p(τ) + q(τ) ,<label>(3)</label></formula><p>where p(τ) is the actual distribution of the data.</p><p>In the traditional GAN algorithm, the discriminator is trained to directly output this value. When the generator density q(τ) can be evaluated, the traditional GAN discriminator can be modified to incorporate this density information. Instead of having the discriminator estimate the value of Equation 3 directly, it can be used to estimate p(τ), filling in the value of q(τ) with its known value. In this case, the new form of the discriminator D θ with parameters θ is</p><formula xml:id="formula_10">D θ (τ) =p θ (τ) p θ (τ) + q(τ)</formula><p>.</p><p>In order to make the connection to MaxEnt IRL, we also replace the estimated data density with the Boltzmann distribution. As in MaxEnt IRL, we write the energy function as c θ to designate the learned cost. Now the discriminator's output is:</p><formula xml:id="formula_11">D θ (τ) = 1 Z exp(−c θ (τ)) 1 Z exp(−c θ (τ)) + q(τ) .</formula><p>The resulting architecture for the discriminator is very similar to a typical model for binary classification, with a sigmoid as the final layer and log Z as the bias of the sigmoid. We have adjusted the architecture only by subtracting log q(τ) from the input to the sigmoid. This modest change allows the optimal discriminator to be completely independent of the generator: the discriminator is optimal when Z exp(−c θ (τ)) = p(τ). Independence between the generator and the optimal discriminator may significantly improve the stability of training.</p><p>This change is very simple to implement and is applicable in any setting where the density q(τ) can be cheaply evaluated. Of course this is precisely the case where we could directly maximize likelihood, and we might wonder whether it is worth the additional complexity of GAN training. But the experience of researchers in IRL has shown that maximizing log likelihood directly is not always the most effective way to learn complex behaviors, even when it is possible to implement. As we will show, there is a precise equivalence between MaxEnt IRL and this type of GAN, suggesting that the same phenomenon may occur in other domains: GAN training may provide advantages even when it would be possible to maximize likelihood directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Equivalence between generative adversarial networks and guided cost learning</head><p>In this section, we show that GANs, when applied to IRL problems, optimize the same objective as MaxEnt IRL, and in fact the variant of GANs described in the previous section is precisely equivalent to guided cost learning.</p><p>Recall that the discriminator's loss is equal to</p><formula xml:id="formula_12">L discriminator (D θ ) = E τ∼p [− log D θ (τ)] + E τ∼q [− log(1 − D θ (τ))] = E τ∼p − log 1 Z exp(−c θ (τ)) 1 Z exp(−c θ (τ)) + q(τ) + E τ∼q − log q(τ) 1 Z exp(−c θ (τ)) + q(τ)</formula><p>In maximum entropy IRL, the log-likelihood objective is:</p><formula xml:id="formula_13">L cost (θ ) = E τ∼p [c θ (τ)] + log E τ∼ 1 2 p+ 1 2 q exp(−c θ (τ)) 1 2 p(τ) + 1 2 q(τ) (4) = E τ∼p [c θ (τ)] + log E τ∼µ exp(−c θ (τ)) 1 2Z exp(−c θ (τ)) + 1 2 q(τ) ,<label>(5)</label></formula><p>where we have substituted p(τ) = p θ (τ) = 1 Z exp(−c θ (τ)), i.e. we are using the current model to estimate the importance weights.</p><p>We will establish the following facts, which together imply that GANs optimize precisely the Max-Ent IRL problem:</p><p>1. The value of Z which minimizes the discriminator's loss is an importance-sampling estimator for the partition function, as described in Section 2.3.2.</p><p>2. For this value of Z, the derivative of the discriminator's loss with respect to θ is equal to the derivative of the MaxEnt IRL objective. Recall that µ is the mixture distribution between p and q. Write µ(τ) = 1 2Z exp(−c θ (τ)) + 1 2 q(τ). Note that when θ and Z are optimized, <ref type="bibr" target="#b0">1</ref> Z exp(−c θ (τ)) is an estimate for the density of p(τ), and hence µ(τ) is an estimate for the density of µ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Z estimates the partition function</head><p>We can compute the discriminator's loss:</p><formula xml:id="formula_14">L discriminator (D θ ) =E τ∼p − log 1 Z exp(−c θ (τ)) µ(τ) + E τ∼q − log q(τ) µ(τ) (6) = log Z + E τ∼p [c θ (τ)] + E τ∼p [log µ(τ)] − E τ∼q [log q(τ)] + E τ∼q [log µ(τ)] (7) = log Z + E τ∼p [c θ (τ)] − E τ∼q [log q(τ)] + 2E τ∼µ [log µ(τ)].<label>(8)</label></formula><p>Only the first and last terms depend on Z. At the minimizing value of Z, the derivative of these term with respect to Z will be zero:</p><formula xml:id="formula_15">∂ Z L discriminator (D θ ) = 0 1 Z = E τ∼µ 1 Z 2 exp(−c θ (τ)) µ(τ) Z = E τ∼µ exp(−c θ (τ)) µ(τ) .</formula><p>Thus the minimizing Z is precisely the importance sampling estimate of the partition function in Equation 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">c θ optimizes the IRL objective</head><p>We return to the discriminator's loss as computed in Equation 8, and consider the derivative with respect to the parameters θ . We will show that this is exactly the same as the derivative of the IRL objective.</p><p>Only the second and fourth terms in the sum depend on θ . When we differentiate those terms we obtain:</p><formula xml:id="formula_16">∂ θ L discriminator (D θ ) = E τ∼p [∂ θ c θ (τ)] − E τ∼µ 1 Z exp(−c θ (τ))∂ θ c θ (τ) µ(τ)</formula><p>On the other hand, when we differentiate the MaxEnt IRL objective, we obtain:</p><formula xml:id="formula_17">∂ θ L cost (θ ) = E τ∼p [∂ θ c θ (τ)] + ∂ θ log E τ∼µ exp(−c θ (τ)) µ(τ) = E τ∼p [∂ θ c θ (τ)] + E τ∼µ − exp(−c θ (τ))∂ θ c θ (τ) µ(τ) E τ∼µ exp(−c θ (τ)) µ(τ) = E τ∼p [∂ θ c θ (τ)] − E τ∼µ 1 Z exp(−c θ (τ))∂ θ c θ (τ) µ(τ) = ∂ θ L discriminator (D θ ).</formula><p>In the third equality, we used the definition of Z as an importance sampling estimate. Note that in the second equality, we have treated µ(τ) as a constant rather than as a quantity that depends on θ . This is because the IRL optimization is minimizing log Z = log ∑ τ exp(−c θ (τ)) and using µ(τ) as the weights for an importance sampling estimator of Z. For this purpose we do not want to differentiate through the importance weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The generator optimizes the MaxEnt IRL objective</head><p>Finally, we compute the generator's loss:</p><formula xml:id="formula_18">L generator (q) = E τ∼q [log(1 − D(τ)) − log(D(τ))] = E τ∼q log q(τ) µ(τ) − log 1 Z exp(−c θ (τ)) µ(τ) = E τ∼q [log q(τ) + log Z + c θ (τ)] = log Z + E τ∼q [c θ (τ)] + E τ∼q [log q(τ)] = log Z + L sampler (q).</formula><p>The term log Z is a parameter of the discriminator that is held fixed while optimizing the generator, this loss is exactly equivalent the sampler loss from MaxEnt IRL, defined in Equation 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>There are many apparent differences between MaxEnt IRL and the GAN optimization problem. But, we have shown that after making a single key change-using a generator q(τ) for which densities can be evaluated efficiently, and incorporating this information into the discriminator in a natural way-generative adversarial networks can be viewed as a sample-based algorithm for the MaxEnt IRL problem. By connecting GANs to the empirical literature on inverse reinforcement learning <ref type="bibr" target="#b6">[7]</ref>, this demonstrates that GAN training can improve the quality of samples even when the generator's density can be evaluated exactly. By generalizing this connection, we can derive a new adversarial training strategy for energy-based models, which we discuss in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GANs for training EBMs</head><p>Now that we have highlighted the connection between GANs and guided cost learning, the application of GANs to EBMs follows directly. As discussed in Section 2.2, the primary challenge in training EBMs is estimating the partition function, which is done by approximately sampling from the distribution induced by the energy E θ . Two recent papers have proposed to use adversarial training to derive fast estimates of the partition function <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref>. In particular, these methods alternate between training a generator to produce samples with minimal energy E θ (x), and optimizing the parameters of the energy function using the samples to estimate the partition function.</p><p>When the density of the generator is available, however, we can derive an unbiased estimate of the partition function as</p><formula xml:id="formula_19">Z = E x∼µ exp(−E θ (x)) 1 2 p(x) + 1 2 q(x)</formula><p>where µ denotes an equal mixture of generated and real data points, q(x) denotes the density under the generator, and p(x) denotes an estimate for the data density.</p><p>This gives a loss function</p><formula xml:id="formula_20">L energy (θ ) = E x∼p [− log p θ (x)] = E x∼p [−E θ (x)] − log E x∼µ exp(−E θ (x)) 1 2 p(x) + 1 q(x)</formula><p>.</p><p>As before, the generator is updated to minimize energy and maximize entropy:</p><formula xml:id="formula_21">L generator (q) = E x∼q [E θ (x)] + E x∼q [log q(x)]</formula><p>If we set p(x) = p θ (x), the resulting model is a special case of a GAN which is straightforward to implement. The discriminator's output is σ (E θ (x) − logq(x)), where σ is a sigmoid with a trainable bias. The discriminator's loss is the log probability and the generator's loss is the discriminator's log odds, as defined in Section 2.1.</p><p>Kim &amp; Bengio proposed a similar energy-based model for generative image modeling, but did not assume they could compute the generator's density <ref type="bibr" target="#b11">[12]</ref>. As a result, they do not use importance weights, and work with a biased estimator of the partition function which converges to the true partition function when the generator correctly samples from the energy-based model. In contrast, by using the generator density, we can get an unbiased estimate of the partition function that does not rely on any assumptions about the generator. Thus, even if the generator cannot learn to sample exactly from the data distribution, our training procedure is consistent.</p><p>Zhao et al. also proposed an energy-based GAN model with an autoencoder discriminator where the energy is given by the mean-squared error between the data example (generated or real) and the discriminator's reconstruction <ref type="bibr" target="#b24">[25]</ref>. The energy function is optimized with a margin loss, and the generator is trained to minimize energy. This method also did not use the form of discriminator presented above. An interesting direction for future exploration is to consider combining the GAN training algorithm discussed here with an objective other than log-likelihood, such as one used with EBMs <ref type="bibr" target="#b13">[14]</ref> or different f -divergences used with GANs <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Ho et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> previously presented a GAN-like algorithm for imitation learning, where the goal is to recover a policy that matches the expert demonstrations. The proposed algorithm, called generative adversarial imitation learning (GAIL), has an adversarial structure. The analysis in this paper provides additional insight into what GAIL is doing. As discussed above, GANs are optimizing the same objective as MaxEnt IRL. Thus, the GAIL policy is being trained to optimize a cost learned through MaxEnt IRL. Unlike guided cost learning <ref type="bibr" target="#b6">[7]</ref>, however, Ho &amp; Ermon use the typical unconstrained form of the discriminator <ref type="bibr" target="#b8">[9]</ref> and do not use the generator's density. In this case, the cost function remains implicit within the discriminator and cannot be recovered. Hence, in GAIL, the discriminator is discarded and the policy is the end result.</p><p>Bachman &amp; Precup <ref type="bibr" target="#b0">[1]</ref> suggested that data generation can be converted into a sequential decisionmaking problem and solved with a reinforcement learning method. Several recent works have proposed methods for merging maximum likelihood objectives and known reward functions for training sequential language generation models and rely on surrogate reward function such as BLEU score or edit distance <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b1">2]</ref>. In this work, we assume that the reward function is unknown.</p><p>Yu et al. proposed to learn a cost function for sequential data generation using GANs, where the cost is defined as the probability of the discriminator classifying the generated sequence as coming from the data distribution <ref type="bibr" target="#b23">[24]</ref>. The discriminator does not take advantage of the policy's density values, despite the fact that they are known (and are used during pre-training). Their experiments also find that max-likelihood pre-training is crucial for good performance, suggesting that recurrent generators that can't afford such pre-training (e.g. because they don't have densities) are less practical to train.</p><p>Pfau &amp; Vinyals drew a connection between the optimization problems in GANs and actor-critic methods in reinforcement learning, suggesting how ideas for stabilizing training in one domain could be beneficial for the other <ref type="bibr" target="#b18">[19]</ref>. As the authors point out, these optimization tricks could also be useful for imitation learning algorithms with the same two-level optimization structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this work, we showed an equivalence between generative adversarial modeling and an algorithm for performing maximum entropy inverse reinforcement learning. Our derivation used a special form of discriminator that leverages likelihood values from the generator, leading to an unbiased estimate of the underlying energy function. A natural direction for future work is to experiment with combining deep generators that can provide densities, such as autoregressive models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref> or models that use invertible transformations <ref type="bibr" target="#b5">[6]</ref>, with generative adversarial modeling. Such an approach may provide more stable training, better generators, and wider applicability to discrete problems such as language.</p><p>This work also suggests a new algorithm for training energy-based models using generative adversarial networks, that trains a neural network model to sample from the distribution induced by the current energy. This method could reduce the computational challenges of existing MCMC-based solutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 .</head><label>3</label><figDesc>The generator's loss is exactly equal to the cost c θ minus the entropy of q(τ), i.e. the MaxEnt policy loss defined in Equation 2 in Section 2.3.2.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This formula assumes that x t+1 is a deterministic function of the previous history. A more general form of this equation can be derived for stochastic dynamics<ref type="bibr" target="#b25">[26]</ref>. However, the analysis largely remains the same: the probability of a trajectory can be written as the product of conditional probabilities, but the conditional probabilities of the states x t are not affected by θ and so factor out of all likelihood ratios.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In RL settings, where generating samples requires executing a policy in the real world, such as in robotics, old samples from old generators are typically retained for efficiency. In this case, the density q can be computed using a fusion distribution over the past generator densities.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Ian Goodfellow and Joan Bruna for insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data generation as sequential decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07086</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relative entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boularias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Density estimation using real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Guided cost learning: Deep inverse optimal control via policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-free imitation learning with policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning objective functions for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Righetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep directed generative models with energy-based probability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Track</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting structured data</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithms for inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reward augmented maximum likelihood for neural structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01945</idno>
		<title level="m">Connecting generative adversarial networks and actor-critic methods</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Sequence level training with recurrent neural networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel recurrent neural networks. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seqgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05473</idno>
		<title level="m">Sequence generative adversarial nets with policy gradient</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<title level="m">Energy-based generative adversarial network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Modeling purposeful adaptive behavior with the principle of maximum causal entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ziebart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
