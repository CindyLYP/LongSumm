<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-09-30">30 Sep 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kye-Hyeon</forename><surname>Kim</surname></persName>
							<email>kye-hyeon.kim@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Imaging and Camera Technology</orgName>
								<address>
									<addrLine>21 Teheran-ro 52-gil, Gangnam-gu</addrLine>
									<postCode>06212</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Hong</surname></persName>
							<email>sanghoon.hong@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Imaging and Camera Technology</orgName>
								<address>
									<addrLine>21 Teheran-ro 52-gil, Gangnam-gu</addrLine>
									<postCode>06212</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Intel Imaging and Camera Technology</orgName>
								<address>
									<addrLine>21 Teheran-ro 52-gil, Gangnam-gu</addrLine>
									<postCode>06212</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungseok</forename><surname>Roh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Imaging and Camera Technology</orgName>
								<address>
									<addrLine>21 Teheran-ro 52-gil, Gangnam-gu</addrLine>
									<postCode>06212</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeongjae</forename><surname>Cheon</surname></persName>
							<email>yeongjae.cheon@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Imaging and Camera Technology</orgName>
								<address>
									<addrLine>21 Teheran-ro 52-gil, Gangnam-gu</addrLine>
									<postCode>06212</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Park</surname></persName>
							<email>minje.park@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Imaging and Camera Technology</orgName>
								<address>
									<addrLine>21 Teheran-ro 52-gil, Gangnam-gu</addrLine>
									<postCode>06212</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-09-30">30 Sep 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1608.08021v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents how we can achieve the state-of-the-art accuracy in multicategory object detection task while minimizing the computational cost by adapting and combining recent technical innovations. Following the common pipeline of &quot;CNN feature extraction + region proposal + RoI classification&quot;, we mainly redesign the feature extraction part, since region proposal part is not computationally expensive and classification part can be efficiently compressed with common techniques like truncated SVD. Our design principle is &quot;less channels with more layers&quot; and adoption of some building blocks including concatenated ReLU, Inception, and HyperNet. The designed network is deep and thin and trained with the help of batch normalization, residual connections, and learning rate scheduling based on plateau detection. We obtained solid results on well-known object detection benchmarks: 83.8% mAP (mean average precision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only 750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA Titan X GPU. Theoretically, our network requires only 12.3% of the computational cost compared to ResNet-101, the winner on VOC2012.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNNs) have made impressive improvements in object detection for several years. Thanks to many innovative work, recent object detection systems have met acceptable accuracies for commercialization in a broad range of markets like automotive and surveillance. In terms of detection speed, however, even the best algorithms are still suffering from heavy computational cost. Although recent work on network compression and quantization shows promising result, it is important to reduce the computational cost in the network design stage. This paper presents our lightweight feature extraction network architecture for object detection, named PVANET <ref type="bibr" target="#b0">1</ref> , which achieves real-time object detection performance without losing accuracy compared to the other state-of-the-art systems:</p><p>• Computational cost: 7.9GMAC for feature extraction with 1065x640 input (cf. ResNet-101 <ref type="bibr" target="#b0">[1]</ref>: 80. Scale / Shift applies trainable weight and bias to each channel, allowing activations in the negated part to be adaptive.</p><p>• Runtime performance: 750ms/image (1.3FPS) on Intel i7-6700K CPU with a single core; 46ms/image (21.7FPS) on NVIDIA Titan X GPU</p><p>• Accuracy: 83.8% mAP on VOC-2007; 82.5% mAP on VOC-2012 (2nd place)</p><p>The key design principle is "less channels with more layers". Additionally, our networks adopted some recent building blocks while some of them have not been verified their effectiveness on object detection tasks:</p><p>• Concatenated rectified linear unit (C.ReLU) <ref type="bibr" target="#b1">[2]</ref> is applied to the early stage of our CNNs (i.e., first several layers from the network input) to reduce the number of computations by half without losing accuracy.</p><p>• Inception <ref type="bibr" target="#b2">[3]</ref> is applied to the remaining of our feature generation sub-network. An Inception module produces output activations of different sizes of receptive fields, so that increases the variety of receptive field sizes in the previous layer. We observed that stacking up Inception modules can capture widely varying-sized objects more effectively than a linear chain of convolutions.</p><p>• We adopted the idea of multi-scale representation like HyperNet <ref type="bibr" target="#b3">[4]</ref> that combines several intermediate outputs so that multiple levels of details and non-linearities can be considered simultaneously.</p><p>We will show that our thin but deep network can be trained effectively with batch normalization <ref type="bibr" target="#b4">[5]</ref>, residual connections <ref type="bibr" target="#b0">[1]</ref>, and learning rate scheduling based on plateau detection <ref type="bibr" target="#b0">[1]</ref>.</p><p>In the remaining of the paper, we describe our network design briefly (Section 2) and summarize the detailed structure of PVANET (Section 3). Finally we provide some experimental results on VOC-2007 and VOC-2012 benchmarks, with detailed settings for training and testing (Section 4). In the early stage, output nodes tend to be "paired" such that one node's activation is the opposite side of another's. From this observation, C.ReLU reduces the number of output channels by half, and doubles it by simply concatenating the same outputs with negation, which leads to 2x speed-up of the early stage without losing accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inception: Remaining building blocks in feature generation</head><p>For object detection tasks, Inception has neither been widely applied to existing work, nor been verified its effectiveness. We found that Inception can be one of the most cost-effective building block for capturing both small and large objects in an input image. To Learn visual patterns for capturing large object, output features of CNNs should correspond to sufficiently large receptive fields, which can be easily fulfilled by stacking up convolutions of 3x3 or larger kernels. On the other hand, for capturing small-sized objects, output features should correspond to sufficiently small receptive fields to localize small regions of interest precisely. <ref type="figure">Figure 2</ref> clearly shows that Inception can fulfill both requirements. 1x1 convolution plays the key role to this end, by preserving the receptive field of the previous layer. Just increasing the nonlinearity of input patterns, it slows down the growth of receptive fields for some output features so that small-sized objects can be captured precisely. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates our Inception implementation. 5x5 convolution is replaced with a sequence of two 3x3 convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">HyperNet: Concatenation of multi-scale intermediate outputs</head><p>Multi-scale representation and its combination are proven to be effective in many recent deep learning tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Combining fine-grained details with highly-abstracted information in feature extraction layer helps the following region proposal network and classification network to detect objects of different scales. However, since the direct concatenation of all abstraction layers may produce redundant information with much higher compute requirement we need to design the number of different abstraction layers and the layer numbers of abstraction carefully. If you choose the layers which are too early for object proposal and classification, it would be little help when we consider additional compute complexity.   <ref type="figure" target="#fig_0">Figure 1</ref>. conv1 1 has no 1x1 conv layer. "C.ReLU" column shows the number of output channels of each conv layer. For Residual, 1x1 conv is applied for projecting pool1 1 into conv2 1, conv2 into conv3 1, conv3 4 into conv4 1, and conv4 into conv5 1. Inception consists of four sub-sequences: 1x1 conv (#1x1); "1x1 -3x3" conv (#3x3); "1x1 -3x3 -3x3" conv (#5x5); "3x3 max-pool -1x1 conv" (#pool, only for stride 2). "#out" refers to 1x1 conv after concatenating those sub-sequences. The number of output channels of each conv layer is shown. Multi-scale features are obtained by four steps: conv3 4 is down-scaled into "downscale" by 3x3 max-pool with stride 2; conv5 4 is up-scaled into "upscale" by 4x4 channel-wise deconvolution whose weights are fixed as bilinear interpolation; "downscale", conv4 and "upscale" are combined into "concat" by channel-wise concatenation; after 1x1 conv, the final output is obtained (convf).</p><p>Our design choice is not different from the observation from ION <ref type="bibr" target="#b5">[6]</ref> and HyperNet <ref type="bibr" target="#b3">[4]</ref>, which combines 1) the last layer and 2) two intermediate layers whose scales are 2x and 4x of the last layer, respectively. We choose the middle-sized layer as a reference scale (= 2x), and concatenate the 4x-scaled layer and the last layer with down-scaling (pooling) and up-scaling (linear interpolation), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Deep network training</head><p>It is widely accepted that as network goes deeper and deeper, the training of network becomes more troublesome. We solve this issue by adopting residual structures <ref type="bibr" target="#b0">[1]</ref>. Unlike the original residual training idea, we add residual connections onto inception layers as well to stabilize the later part of our deep network architecture.</p><p>We also add Batch normalization <ref type="bibr" target="#b4">[5]</ref> layers before all ReLU activation layers. Mini-batch sample statistics are used during pre-training, and moving-averaged statistics are used afterwards as fixed scale-and-shift parameters.</p><p>Learning rate policy is also important to train network successfully. Our policy is to control the learning rate dynamically, based on plateau detection <ref type="bibr" target="#b0">[1]</ref>. We measure the moving average of loss, and decide it to be on-plateau if its improvement is below a threshold during a certain period of iterations. Whenever the plateau is detected, the learning rate is decreased by a constant factor. In experiments, our learning rate policy gave a significant gain of accuracy.</p><p>3 Faster R-CNN with our feature extraction network <ref type="table" target="#tab_2">Table 1</ref> shows the whole structure of PVANET. In the early stage (conv1 1, ..., conv3 4), C.ReLU is adapted to convolutional layers to reduce the computational cost of KxK conv by half. 1x1 conv layers are added before and after the KxK conv, in order to reduce the input size and then enlarge the representation capacity, respectively.</p><p>Three intermediate outputs from conv3 (with down-scaling), conv4 4, and conv5 (with upscaling) are combined into the 512-channel multi-scale output features (convf), which are fed into the Faster R-CNN modules:</p><p>• For computational efficiency, only the first 128 channels in convf are fed into the region proposal network (RPN). Our RPN is a sequence of "3x3 conv (384 channels) -1x1 conv (25x(2+4) = 150 channels 3 )" layers to generate regions of interest (RoIs) from</p><p>• R-CNN takes all 512 channels in convf. For each RoI, 6x6x512 tensor is generated by RoI pooling, and then passed through a sequence of fully-connected layers of "4096 -4096 -(21+84)" output nodes. <ref type="bibr" target="#b3">4</ref> 4 Experimental results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and testing</head><p>PVANET was pre-trained with ILSVRC2012 training images for 1000-class image classification. <ref type="bibr" target="#b4">5</ref> All images were resized into 256x256, and 192x192 patches were randomly cropped and used as the network input. The learning rate was initially set to 0.1, and then decreased by a factor of 1/ √ ≈ 0.3165 whenever a plateau is detected. Pre-training terminated if the learning rate drops below 1e − 4, which usually requires about 2M iterations.</p><p>Then PVANET was trained with the union set of MS COCO 6 trainval, VOC2007 7 trainval and VOC2012 8 trainval. Fine-tuning with VOC2007 trainval and VOC2012 trainval was also required afterwards, since the class definitions in MS COCO and VOC competitions are slightly different. Training images were resized randomly such that a shorter edge of an image to be between 416 and 864.</p><p>For PASCAL VOC evaluations, each input image was resized such that its shorter edge to be 640. All parameters related to Faster R-CNN were set as in the original work <ref type="bibr" target="#b7">[8]</ref> except for the number of proposal boxes before non-maximum suppression (NMS) (= 12000) and the NMS threshold (= 0.4). All evaluations were done on Intel i7-6700K CPU with a single core and NVIDIA Titan X GPU. <ref type="table">Table 2</ref> shows the accuracy of our models in different configurations. <ref type="bibr" target="#b8">9</ref> Thanks to Inception (Section 2.2) and multi-scale features (Section 2.3), our RPN generated initial proposals very accurately. Since the results imply that more than 200 proposals does not give notable benefits to detection accuracy, we fixed the number of proposals to 200 in the remaining experiments. We also measured the performance with bounding-box voting <ref type="bibr" target="#b9">[10]</ref>, while iterative regression was not applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VOC2007</head><p>RPN produces 2 predicted scores (foreground and background) and 4 predicted values of the bounding box for each anchor. Our RPN uses 25 anchors of 5 scales (3, 6, 9, 16, 25) and 5 aspect ratios (0.5, 0.667, 1.0, 1.5, 2.0). <ref type="bibr" target="#b3">4</ref> For 20-class object detection, R-CNN produces 21 predicted scores (20 classes + 1 background) and 21x4 predicted values of 21 bounding boxes. <ref type="bibr" target="#b4">5</ref>   <ref type="table">Table 3</ref>: Comparisons between our network and some state-of-the-arts in the PASCAL VOC2012 leaderboard. PVANET+ denotes PVANET with bounding-box voting. We assume that PVANET takes a 1056x640 image and the number of proposals is 200. Competitors' MAC are estimated from their Caffe prototxt files which are publicly available. All testing-time configurations are the same with the original articles <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref>. Competitors' runtime performances are also therein, while we projected the original values with assuming that NVIDIA Titan X is 1.5x faster than NVIDIA K40.</p><p>Faster R-CNN consists of fully-connected layers, which can be compressed easily without a significant drop of accuracy <ref type="bibr" target="#b10">[11]</ref>. We compressed the fully-connected layers of "4096 -4096" into to "512 -4096 -512 -4096" by the truncated singular value decomposition (SVD), with some fine-tuning after that. The compressed network achieved 82.9% mAP (-0.9%) and ran in 31.3 FPS (+9.6 FPS). <ref type="table">Table 3</ref> summarizes comparisons between PVANET+ and some state-of-the-art networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref> from the PASCAL VOC2012 leaderboard. <ref type="bibr" target="#b9">10</ref> Our PVANET+ achieved 82.5% mAP, the 2nd place on the leaderboard, outperforming all other competitors except for "Faster R-CNN + ResNet-101". However, the top-performer uses ResNet-101 which is much heavier than PVANET, as well as several time-consuming techniques such as global contexts and multi-scale testing, leading to 40x (or more) slower than ours. In <ref type="table">Table 3</ref>, we also compare mAP with respect to the computational cost. Among the networks performing over 80% mAP, PVANET+ is the only network running ≤ 50ms. Taking its accuracy and computational cost into account, our PVANET+ is the most efficient network in the leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">VOC2012</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we showed that the current networks are highly redundant and we can design a thin and light network which is capable enough for complex vision tasks. Elaborate adoption and combination of recent technical innovations on deep learning makes us possible to re-design the feature extraction part of the Faster R-CNN framework to maximize the computational efficiency. Even though the proposed network is designed for object detection, we believe our design principle can be widely applicable to other tasks like face recognition and semantic analysis.</p><p>Our network design is completely independent of network compression and quantization. All kinds of recent compression and quantization techniques are applicable to our network as well to further increase the actual performance in real applications. As an example, we showed that a simple technique like truncated SVD could achieve a notable improvement in the runtime performance based on our network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our C.ReLU building block. Negation simply multiplies −1 to the output of Convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 Figure 2 :</head><label>12</label><figDesc>illustrates our C.ReLU implementation. Compared to the original C.ReLU, we append scaling and shifting after concatenation to allow that each channel's slope and activation threshold can be different from those of its opposite channel. Example of a distribution of (expected) receptive field sizes of intermediate outputs in a chain of 3 Inception modules. Each module concatenates 3 convolutional layers of different kernel sizes, 1x1, 3x3 and 5x5, respectively. The number of output channels in each module is set to {1/2, 1/4, 1/4} of the number of channels from the previous module, respectively. A latter Inception module can learn visual patterns of wider range of sizes, as well as having higher level of nonlinearity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(Left) Our Inception building block. 5x5 convolution is replaced with two 3x3 convolutional layers for efficiency. (Right) Inception for reducing feature map size by half.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The detailed structure of PVANET. All conv layers are combined with batch normaliza- tion, channel-wise scaling and shifting, and ReLU activation layers. Theoretical computational cost is given as the number of adds and multiplications (MAC), assuming that the input image size is 1056x640. KxK C.ReLU refers to a sequence of "1x1 -KxK -1x1" conv layers, where KxK is a C.ReLU block as in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>On Sep. 19, 2016, we updated the mAP numbers according to the latest version of the evaluation code in py-faster-rcnn.</figDesc><table><row><cell>Model</cell><cell cols="6">Proposals Recall (%) mAP (%) Time (ms) FPS</cell></row><row><cell>PVANET</cell><cell></cell><cell></cell><cell>98.9</cell><cell>83.6</cell><cell></cell><cell>48.5 20.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>98.3</cell><cell>83.5</cell><cell></cell><cell>42.2 23.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>97.0</cell><cell>83.2</cell><cell></cell><cell>40.0 25.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell>94.7</cell><cell>82.1</cell><cell></cell><cell>26.8 37.3</cell></row><row><cell>PVANET+</cell><cell></cell><cell></cell><cell>98.3</cell><cell>83.8</cell><cell></cell><cell>46.1 21.7</cell></row><row><cell>PVANET+ (compressed)</cell><cell></cell><cell></cell><cell>98.3</cell><cell>82.9</cell><cell></cell><cell>31.9 31.3</cell></row><row><cell cols="8">Table 2: Performance on VOC2007-test benchmark data. "Recall" refers to a ratio of "true positive</cell></row><row><cell cols="8">(TP)" boxes among the proposals, considering a box as TP if the intersection-over-union (IoU) score</cell></row><row><cell cols="8">with its maximally-overlapped ground-truth box is ≥ 0.5. PVANET+ denotes that bounding-box</cell></row><row><cell cols="8">voting is applied, and PVANET+ (compressed) denotes that fully-connected layers in R-CNN are</cell></row><row><cell>compressed.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Computation cost (MAC)</cell><cell></cell><cell cols="2">Running time</cell><cell>mAP</cell></row><row><cell cols="4">Shared CNN RPN Classifier</cell><cell>Total</cell><cell cols="2">ms x(PVANET)</cell><cell>(%)</cell></row><row><cell>PVANET+</cell><cell>7.9</cell><cell>1.3</cell><cell>27.7</cell><cell>37.0</cell><cell>46</cell><cell cols="2">1.0 82.5</cell></row><row><cell>Faster R-CNN + ResNet-101</cell><cell>80.5</cell><cell>N/A</cell><cell cols="3">219.6 300.1 2240</cell><cell cols="2">48.6 83.8</cell></row><row><cell>Faster R-CNN + VGG-16</cell><cell>183.2</cell><cell>5.5</cell><cell cols="2">27.7 216.4</cell><cell>110</cell><cell cols="2">2.4 75.9</cell></row><row><cell>R-FCN + ResNet-101</cell><cell>122.9</cell><cell>0</cell><cell cols="2">0 122.9</cell><cell>133</cell><cell cols="2">2.9 82.0</cell></row></table><note>http://www.image-net.org/challenges/LSVRC/2012/6 http://mscoco.org/dataset/7 http://host.robots.ox.ac.uk/pascal/VOC/voc2007/8 http://host.robots.ox.ac.uk/pascal/VOC/voc2012/9</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php? challengeid=11&amp;compid=4</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and improving convolutional neural networks via concatenated rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenling</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HyperNet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.09308</idno>
		<title level="m">Fast algorithms for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection via a multi-region &amp; semantic segmentationaware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. R-fcn : Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
