<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Independently Controllable Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-03-22">22 Mar 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
							<email>ebengi@cs.mcgill.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Thomaś</surname></persName>
							<email>valentin.thomas@epfl.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
							<email>jpineau@cs.mcgill.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
							<email>dprecup@cs.mcgill.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<email>yoshua.bengio@umontreal.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">CIFAR Senior Fellow</orgName>
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Independently Controllable Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-03-22">22 Mar 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1703.07718v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>representation learning, controllable features</keywords>
			</textClass>
			<abstract>
				<p>Finding features that disentangle the different causes of variation in real data is a difficult task, that has nonetheless received considerable attention in static domains like natural images. Interactive environments, in which an agent can deliberately take actions, offer an opportunity to tackle this task better, because the agent can experiment with different actions and observe their effects. We introduce the idea that in interactive environments, latent factors that control the variation in observed data can be identified by figuring out what the agent can control. We propose a naive method to find factors that explain or measure the effect of the actions of a learner, and test it in illustrative experiments.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Whether in static or dynamic environments, decision making for real world problems is often confronted with the hard challenge of finding a "good" representation of the problem. In the context of supervised or semi-supervised learning, it has been argued <ref type="bibr">(Bengio, 2009)</ref> that good representations separate out underlying explanatory factors, which may be causes of the observed data. In such problems, feature learning often involves mechanisms such as autoencoders <ref type="bibr" target="#b5">(Hinton &amp; Salakhutdinov, 2006)</ref>, which find latent features that explain the observed data. In interactive environments, the temporal dependency between successive observations creates a new opportunity to notice structure in data which may not be apparent using only observational studies. The need to experiment in order to discover causal structures has already been well explored in psychology (e.g. <ref type="bibr">Gopnik &amp; Wellman (in press)</ref>). In reinforcement learning, several approaches explore mechanisms that push the internal representations of learned models to be "good" in the sense that they provide better control (see Sec. 4).</p><p>We propose and explore a more direct mechanism, which explicitly links an agent's control over its environment with its internal feature representations. Specifically, we hypothesize that some of the factors explaining variations in the data correspond to aspects of the world which can be controlled by the agent. For example, an object could be pushed around or picked up independently of others. In such a case, our approach aims to extract object features from the raw data while learning a policy that controls precisely these features of the data. In Sec. 2 we explain this mechanism and show experimental results in its simplest instantiation. In Sec. 3 we discuss how this principle could be applied more generally, and what are the research challenges that emerge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Independently controllable features</head><p>To make the above intuitions concrete, assume that there are factors of variation underlying the observations coming from an interactive environment that are "independently controllable". That is, for each of these factors of variation, there exists a policy which will modify that factor only, and not the others. For example, the object behind a set of pixels could be acted on independently from other objects, which would explain variations in its pose and scale when we move it around. The object in this case is a "factor of variation". What makes discovering and mapping such factors into features tricky is that the factors are not explicitly observed. Our goal is to learn these factors, which we call independently controllable features, along with policies that control them. While these may seem strong assumptions about the nature of the environment, our point of view is that they are similar to regularizers meant to make a difficult learning problem better constrained.</p><p>There are many possible ways to express the desire to learn independently controllable features as an objective. Section 2.2 proposes such an objective for a simple scenario. Section 2.3 illustrates the effect of this objective when all the features of the environment are simple and controllable by the agent. Section 2.4 explores a slightly harder scenario in which there is redundancy and policies are learned through a reinforcement learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Autoencoders</head><p>Our approach builds on the familiar framework of autoencoders <ref type="bibr" target="#b5">(Hinton &amp; Salakhutdinov, 2006)</ref>, which are defined as a pair of function approximators f, g with parameters θ such that f : X → H maps the input space to some latent space H, and g : H → X maps back to the input space X ⊂ R d . Autoencoders are trained to minimize the discrepancy between x and g(f (x)), a.k.a. the reconstruction error, e.g.,:</p><formula xml:id="formula_0">min θ 1 x − g(f (x)) 2 2</formula><p>We call f (x) = h ∈ H ⊂ R n the latent feature representation of x, with n features.</p><p>It is common to assume that n d. This causes f and g to perform dimensionality reduction of X, i.e. compression, since there is a dimension bottleneck through which information about the input data must pass. Often, this bottleneck forces the optimization procedure to uncover principal factors of variation of the data on which they are trained. However, this does not necessarily imply that the different dimensions of h = f (x) are individually meaningful. In fact, note that for any bijective function r, we could obtain the same reconstruction error by replacing f by r • f and g by r −1 • g, so we should not expect any form of disentangling of the factors of variation unless some additional constraints or penalties are imposed on h. This motivates the approach we are about to present. Specifically, we will look for policies which can separately influence one of the dimensions of h, and we will prefer representations which make such policies possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Policy Selectivity</head><p>Consider the following simple scenario: we train an autoencoder f, g producing n latent features, f k , k = 1, . . . n. In tandem with these features we train n policies, denoted π k . Autoencoders can learn relatively arbitrary feature representations, but we would like these features to correspond to controllable factors in the learner's environment. Specifically, we would like policy π k to cause a change only in f k and not in any other features. We think of f k and π k as a feature-policy pair.</p><p>In order to quantify the change in f k when actions are taken according to π k , we define the selectivity of a feature as:</p><formula xml:id="formula_1">sel(s, a, k) = E s ∼P a ss |f k (s ) − f k (s)| k |f k (s ) − f k (s)| (1)</formula><p>where s,s are successive raw state representations (e.g. pixels), a is the action, P a ss is the environment's transition distribution from s to s under action a. The normalization by the change in all features means that the selectivity of f k is maximal when only that single feature changes as a result of some action.</p><p>By having an objective that maximizes selectivity and minimizes the autoencoder objective, we can ensure that the features learned can both reconstruct the data and recover independently controllable factors. Hence, we define the following objective, which can be minimized via stochastic gradient descent:</p><formula xml:id="formula_2">E s [ 1 ||s − g(f (s))|| 2 2 ] reconstruction error − λ k E s [ a π k (a|s) log sel(s, a, k)] disentanglement objective<label>(2)</label></formula><p>Here one can think of log sel(s, a, k) as the reward signal R k (s, a) of a control problem, and the expected reward</p><formula xml:id="formula_3">E a∼π k [R k ]</formula><p>is maximized by finding the optimal set of policies π k .</p><p>Note that it is also possible to have directed selectivity: by not taking the absolute value of the numerator of (1) (and replacing log sel with log(1 + sel) in (2)), the policies must learn to increase the learned latent feature rather than simply change it. This may be useful if the policy to gradually increase a feature is distinct from the policy that decreases it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A first toy problem</head><p>Consider the simple environment described in <ref type="figure" target="#fig_0">Figure 1(a)</ref>: the agent sees a 2 × 2 square of adjacent cells in the environment, and has 4 actions that move it up, down, left or right. An autoencoder with directed selectivity (see <ref type="figure" target="#fig_0">Figure 1(c,d)</ref>) learns latent features that map to the (x, y) position of the square in the input space, without ever having explicitly access to these values, and while reconstructing the input properly. An autoencoder without selectivity also reconstructs the input properly but without learning these two latent (x, y) features explicitly.</p><p>In this setting f , g and π share some of their parameters. We use the following architecture: f has two 16 × × ReLU convolutional layers, followed by a fully connected ReLU layer of 32 units, and a tanh layer of n = 4 features; g is the transpose architecture of f ; π k is a softmax policy over 4 actions, computed from the output of the ReLU fully connected layer. White is no correlation, blue and red indicate strong negative or positive slopes respectively. We can see that features 0 and 1 recover y and features 2 and 3 recover x. (d) Representation of the learned policies. Each row is a policy π k , each column corresponds to an action (left/right/up/down). Each cell (k, i) represents the probability of action i in policy π k ; We can see that features 0 and 1 correspond to going down and up (−y/+y) and features 2 and 3 correspond to going right and left (+x/−x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">A slightly harder toy problem</head><p>In the next experiments, we aim to generalize the model above in a slightly more complex environment. Instead of having f and π k parametrized by the same parameters, we now introduce a different set of parameters for each policy and for the encoder, so that each policy can be learned separately.</p><p>Additionally, we use a richer action space, in which we added a "move down and to the right" action as well as an "increase/decrease color of square" action. Note also that the first two actions ("move down") are redundant.</p><p>Thus, in this setting, f, g and π k are all parametrized by different variables, as follows: </p><formula xml:id="formula_4">• f (s) = f (s; W f )</formula><formula xml:id="formula_5">W f ← W f − η f ∇ W f [ 1 ||s − g(f (s))|| 2 2 ] 4: W g ← W g − η g ∇ Wg [ 1 ||s − g(f (s))|| 2 2 ] 5:</formula><p>for k = 1..K do 6:</p><formula xml:id="formula_6">W f ← W f + η f λ ∇ W f E a∼π k (•|s) [log sel(s, a, k)]</formula><p>7:</p><formula xml:id="formula_7">θ k ← θ k + η k λ ∇ θ k E a∼π k (•|s) [log sel(s, a, k)]</formula><p>The gradients on lines 3 and 4 are computed exactly via backpropagation. In our experiments, gradients on lines 6 and 7 are also computed by backpropagation, but in a more general case in which the environment also provides a reward r t (so the total reward to maximize becomes r t + log sel k ), they can be estimated via Monte-Carlo. Note that, in any case, <ref type="bibr">s, a, k)</ref>] can be computed with the REINFORCE <ref type="bibr" target="#b12">(Williams, 1992)</ref> estimator:</p><formula xml:id="formula_8">∇ θ k E a∼π k (•|s) [log sel(</formula><formula xml:id="formula_9">∇ θ k E a∼π k (•|s) [log sel(s, a, k)] = E a∼π k (•|s) [log sel(s, a, k) • ∇ θ k log π k (a|s)] (a) (b)<label>(c)</label></formula><p>Figure 2: Policy π k , selectivity sel k and objective −π k log sel k for a random s and feature/policy k after optimizing (2). On the y-axis is the index k of the feature, and on the x-axis, the 8 different actions that the agent can select. Initially, π k was initialized to be uniform. (a): As in the previous toy problem, each π k quickly concentrates on a specific action. (b): However, in this example, the encoder is also simultaneously shaped so that each feature f k (s) reacts to one specific action with maximal selectivity (L6 of algorithm 1) (c): −π k log sel k . The autoencoder was able to learn independently controllable features. Moreover, in (a), we observe that, as the two first actions are redundant, they have the exact same selectivity and therefore π 3 , the policy associated to the "move down" feature, can either choose the first or second action.</p><p>3 Scaling to general environments: controllability and the binding problem</p><p>In the previous section we used problems in which the environment is made of a static set of objects. In this case, if the objective posited in section 2.2 is learned correctly, we can assume that feature k of the representation can unambiguously refer to some controllable property of some specific object in the environment. For example, the agent's world might contain only a red circle and a green rectangle, which are only affected bu the actions of the agent (they do not move on their own) and we only change the positions and colours of these objects from one trial to the next. Hence, a specific feature f k can learn to unambiguously refer to the position or the colour of one of these two objects.</p><p>In reality, environments are stochastic, and the set of objects in a given scene is drawn from some distribution. The number of objects may vary and their types may be different. It then becomes less obvious how feature k could refer in a clear way to some feature of one of the objects in a particular scene. If we have instances of objects of different types, some addressing or naming scheme is required to refer to the particular objects (instances) present in the scene, so as to match the policy with a particular attribute of a particular object to selectively modify. In our simple environments, this task was trivial because we could simply use the integer k to achieve this coordination between the policy π k and the representation elements (feature f k (s)). In the more general case, this is not possible, which gives rise to a representational problem.</p><p>This is connected to the binding problem in neuro-cognitive science: how to represent a set of objects, each having different attributes, so that we don't confuse, for example, the set {red circle, blue square} with {red square, blue circle}. It is not enough to have a red-detector feature and a blue-detector feature, a square-detector feature and a circle-detector feature. The binding problem has seen some attention in the representation learning literature <ref type="bibr" target="#b8">(Minin et al., 2012;</ref><ref type="bibr" target="#b4">Greff et al., 2016)</ref>, but still remains mostly unsolved. Jointly considering this problem and larning controllable features may prove fruitful.</p><p>These ideas may also lead to interesting ways of performing exploration. How do humans choose with which object to play? We are attracted to objects which we do not know yet (i.e., if and how we can control them). The RL exploration process could be driven by a notion of controllability, predicting the interestingness of objects in a scene and choosing features and associated policies with which to attempt control them. Such ideas have only been explored briefly in the literature, e.g. <ref type="bibr" target="#b10">Ratitch &amp; Precup (2003)</ref> 4 Discussion</p><p>There is a large body of work on learning features in RL focusing on indirectly learning good internal representations. In <ref type="bibr" target="#b6">Jaderberg et al. (2016)</ref>, agents learn off-policy to control their pixel inputs, forcing them to learn features that, intuitively, help control the environment (at least at the pixel level). <ref type="bibr" target="#b9">Oh et al. (2015)</ref> propose models that learn to predict the future, conditioned on action sequences, which push the agent to capture temporal features. There are many more works that go in this direction, such as (deep) successor feature representations <ref type="bibr" target="#b2">(Dayan, 1993;</ref><ref type="bibr" target="#b7">Kulkarni et al., 2016)</ref> or the options framework <ref type="bibr" target="#b11">(Sutton et al., 1999;</ref><ref type="bibr">Precup, 2000)</ref> when used in conjunction with neural networks <ref type="bibr" target="#b0">(Bacon et al., 2016)</ref>.</p><p>Our approach is similar in spirit to the Horde architecture <ref type="bibr">(Sutton, 2011)</ref>. In that scenarion, agents learn policies that maximize specific inputs. The predictions for all these policies then become features for the agent. Our objective is defined specifically in the context of autoencoders. Unlike recent work on the predictron (David Silver, 2017), our approach is not focused on solving a planning task, and the goal is simply to learn how the world works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) &amp; (b) A simple environment with 4 actions that push a square left, right, up or down. (a) is an example ground truth, (b) is the reconstruction of the model trained with selectivity. (c) The slope of a linear regression of the true features (the real x and y position of the agent) as a function of each latent feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>a neural net with a tanh output activation • g(h) = g(h; W g ) a neural net with a ReLU output activation • π k (a|s) = π(a|s; θ k ) so that π k (•|s) = softmax(θ k • s)</figDesc><table><row><cell cols="2">Algorithm 1 Training an autoencoder with disentangled factors</cell></row><row><cell cols="2">1: for t = 1..T do</cell></row><row><cell>2:</cell><cell>Sample s</cell></row><row><cell>3:</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors gratefully acknowledge financial support for this work by the Samsung Advanced Institute of Technology (SAIT), the Canadian Institute For Advanced Research (CIFAR), as well as the Natural Sciences and Engineering Research Council of Canada (NSERC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05140</idno>
		<title level="m">Yoshua. Learning deep architectures for AI</title>
		<imprint>
			<publisher>Now Publishers</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The option-critic architecture</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hado Van Hasselt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08810</idno>
		<title level="m">The predictron: End-to-end learning and planning. arXiv</title>
		<editor>Matteo Hessel Tom Schaul Arthur Guez Tim Harley Gabriel Dulac-Arnold David Reichert Neil Rabinowitz Andre Barreto Thomas Degris</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving generalization for temporal difference learning: The successor representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="613" to="624" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reconstructing constructivism: Causal models, bayesian learning mechanisms and the theory theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gopnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep unsupervised perceptual grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tagger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4484" to="4492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Reducing the dimensionality of data with neural networks. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05397</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ardavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simanta</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Samuel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02396</idno>
		<title level="m">Deep successor reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Complex valued artificial recurrent neural network as a novel approach to model the perceptual binding problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Minin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hans-Georg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Siemens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llc</forename><surname>Siemens</surname></persName>
		</author>
		<editor>ESANN. Citeseer</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiaoxiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Honglak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Precup, Doina. Temporal abstraction in reinforcement learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="2863" to="2871" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ratitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS</title>
		<editor>Sutton, R. S., Modayil J. Delp M. Degris T. Pilarski P. M. White A. Precup-D.</editor>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="313" to="324" />
		</imprint>
	</monogr>
	<note>ECML</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satinder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
