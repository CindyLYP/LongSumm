<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Concurrent Log-Structured Data Stores</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Golan-Gueta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Labs Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Bortnikov</surname></persName>
							<email>ebortnik@yahoo-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Yahoo Labs Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eshcar</forename><surname>Hillel</surname></persName>
							<email>eshcar@yahoo-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Yahoo Labs Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idit</forename><surname>Keidar</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">Technion, Yahoo Labs Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling Concurrent Log-Structured Data Stores</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2741948.2741973</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Log-structured data stores (LSM-DSs) are widely accepted as the state-of-the-art implementation of key-value stores. They replace random disk writes with sequential I/O, by accumulating large batches of updates in an in-memory data structure and merging it with the on-disk store in the background. While LSM-DS implementations proved to be highly successful at masking the I/O bottleneck, scaling them up on multicore CPUs remains a challenge. This is nontrivial due to their often rich APIs, as well as the need to coordinate the RAM access with the background I/O. We present cLSM, an algorithm for scalable concurrency in LSM-DS, which exploits multiprocessor-friendly data structures and non-blocking synchronization. cLSM supports a rich API, including consistent snapshot scans and general non-blocking read-modify-write operations. We implement cLSM based on the popular LevelDB keyvalue store, and evaluate it using intensive synthetic workloads as well as ones from production web-serving applications. Our algorithm outperforms state of the art LSM-DS implementations, improving throughput by 1.5x to 2.5x. Moreover, cLSM demonstrates superior scalability with the number of cores (successfully exploiting twice as many cores as the competition).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the last decade, key-value stores have become prevalent for real-time serving of Internet-scale data <ref type="bibr" target="#b13">[16]</ref>. Gigantic stores managing billions of items serve Web search indexing <ref type="bibr" target="#b30">[33]</ref>, messaging <ref type="bibr" target="#b9">[12]</ref>, personalized media, and advertising <ref type="bibr" target="#b15">[18]</ref>. A key-value store is essentially a persistent map with atomic get and put operations used to access data items identified by unique keys. Modern stores also support consistent snapshot scans and range queries for online analytics.</p><p>In write-intensive environments, key-value stores are commonly implemented as Log-Structured Merge Data Stores (LSM-DSs) <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b33">36]</ref> (see <ref type="bibr">Section 2)</ref>. The main centerpiece behind such data stores is absorbing large batches of writes in a RAM data structure that is merged into a (substantially larger) persistent data store upon spillover. This approach masks persistent storage latencies from the end user, and increases throughput by performing I/O sequentially. A major bottleneck of such data stores is their limited in-memory concurrency, which, as we show in Section 5, restricts their vertical scalability on multicore servers. In the past, this was not a serious limitation, as large Web-scale servers did not harness high-end multicore hardware. Nowadays, however, servers with more cores have become cheaper, and 16-core machines commonplace in production settings.</p><p>Our goal in this work is to improve the scalability of stateof the art key-value stores on multicore servers. We focus on a data store that runs on a single multicore machine, which is often the basic building block for a distributed database that runs on multiple machines (e.g., <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b15">18]</ref>). Although it is possible to scale up by further partitioning the data and running multiple LSM-DS's on the same machine, there are significant advantages to consolidation <ref type="bibr" target="#b8">[11]</ref>; see more detailed discussion in Section 2. We therefore strive to scale up a single LSM-DS by maximizing its parallelism.</p><p>We present (in Section 3) cLSM, a scalable LSM-DS algorithm optimized for multi-core machines. We implement cLSM in the framework of the popular LevelDB <ref type="bibr" target="#b2">[4]</ref> library (Section 4), and evaluate it extensively (Section 5), showing better scalability and 1.5x to 2.5x performance improvements over the state-of-the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>This paper makes the following contributions: Non-blocking synchronization. cLSM overcomes the scalability bottlenecks incurred in previous works <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b18">21]</ref> by eliminating blocking during normal operation. It never explicitly blocks get operations, and only blocks puts for short periods of time before and after batch I/Os. Rich API. Beyond atomic put and get operations, cLSM also supports consistent snapshot scans, which can be used to provide range queries. These are important for applications such as online analytics <ref type="bibr" target="#b13">[16]</ref>, and multi-object transactions <ref type="bibr" target="#b38">[41]</ref>. In addition, cLSM supports fully-general nonblocking atomic read-modify-write (RMW) operations. We are not aware of any existing lock-free support for such operations in today's key-value stores. Such operations are useful, e.g., for multisite update reconciliation <ref type="bibr" target="#b15">[18,</ref><ref type="bibr" target="#b16">19]</ref>. Generic algorithm. Our algorithm for supporting puts, gets, snapshot scans, and range queries is decoupled from any specific implementation of the LSM-DS's main building blocks, namely the in-memory component (a map data structure), the disk store, and the merge process that integrates the former into the latter. Only our support for atomic read-modifywrite requires a specific implementation of the in-memory component as a skip-list data structure. This allows one to readily benefit from numerous optimizations of other components (e.g., disk management <ref type="bibr" target="#b6">[8]</ref>) which are orthogonal to our contribution. Implementation. We implement a working prototype of cLSM based on LevelDB <ref type="bibr" target="#b2">[4]</ref>, a state-of-the-art key-value store. Our implementation supports the full functionality of LevelDB and inherits its core modules (including disk and cache management), and therefore benefits from the same optimizations. Evaluation. We compare cLSM's performance to LevelDB and three additional open-source key-value stores, Hyper-LevelDB <ref type="bibr" target="#b18">[21]</ref>, bLSM <ref type="bibr" target="#b33">[36]</ref>, and RocksDB <ref type="bibr" target="#b6">[8]</ref>, on productiongrade multi-core hardware. We evaluate the systems under large-scale intensive synthetic workloads as well as production workloads from a web-scale system serving personalized content and ad recommendation products.</p><p>In our experiments, cLSM achieves performance improvements ranging between 1.5x and 2.5x over the best competitor, on a variety of workloads. cLSM's RMW operations are also twice as fast as a popular implementation based on lock striping <ref type="bibr" target="#b19">[22]</ref>. Furthermore, cLSM exhibits superior scalability, successfully utilizing at least twice as many threads, and also benefits more from a larger RAM allocation to the in-memory component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Architecture Principles</head><p>We overview our design choices, as motivated by today's leading key-value store implementations. We discuss their API, approaches to scaling them, and the LSM approach to data management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Model and API</head><p>In key-value stores <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b15">18]</ref>, the data is comprised of items (rows) identified by unique keys. A row value is a (sparse) bag of attributes called columns. The internal structure of data items is largely opaque for the rest of our discussion.</p><p>The basic API of a key-value store includes put and get operations to store and retrieve values by their keys. Updating an item is cast into putting an existing key with a new value, and deleting one is performed by putting a deletion marker, ⊥, as the key's value.</p><p>To cater to the demands of online analytics applications (e.g., <ref type="bibr" target="#b30">[33]</ref>), key-value stores typically support snapshot scans, which provide consistent read-only views of the data. A scan allows the user to acquire a snapshot of the data (getSnap), from which the user can iterate over items in lexicographical order of their keys by applying next operations.</p><p>Geo-replication scenarios drive the need to reconcile conflicting replicas. This is often done through vector clocks <ref type="bibr" target="#b16">[19]</ref>, which require the key-value store to support conditional updates, namely, atomic read-modify-write operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scalability in Distributed Key-Value Stores</head><p>Distributed key-value stores achieve scalability by sharding data into units called partitions (also referred to as tablets <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b15">18]</ref> or regions <ref type="bibr" target="#b0">[2]</ref>). Partitioning provides horizontal scalability -stretching the service across multiple servers. Nevertheless, there are penalties associated with having many partitions, as argued in <ref type="bibr" target="#b8">[11]</ref>: First, the data store's consistent snapshot scans do not span multiple partitions. Analytics applications that require large consistent scans are forced to use costly transactions across shards. Second, this requires a system-level mechanism for managing partitions <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b15">18]</ref>, whose meta-data size depends on the number of partitions, and can become a scalability bottleneck.</p><p>The complementary approach of increasing the serving capacity of each individual partition is called vertical scalability. First, this necessitates optimizing the speed of I/Obound operations. The leading approach to do so, especially in write-intensive settings, is LSM (discussed in Section 2.3), which effectively eliminates the disk bottleneck. Once this is achieved, the rate of in-memory operations becomes paramount (as we show in Section 5). Increasing this rate is the challenge we focus on in this paper.</p><p>We argue here that we can improve performance while reducing the number of partitions, which in turn allows for larger snapshot scans and reduces meta-data size. To illustrate this point, <ref type="figure" target="#fig_0">Figure 1</ref> shows sample results from our experiments (the experiment setup is detailed in Section 5).</p><p>In this example, we evaluate cLSM with one big partition versus LevelDB and HyperLevelDB with four small partitions, where each small partition's workload is based on a distinct production log, and the big partition is the union thereof. Each of the small partitions is served by a dedicated one quarter of the thread pool (resource separation), whereas the big partition is served by all worker threads (resource sharing). We see that cLSM's improved concurrency control scales better than partitioning, achieving a peak throughput of above 1 million operations/sec -approximately 25% above the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Log-Structured Merge</head><p>Disk access is a principal bottleneck in storage systems, and remains a bottleneck even with today's SSDs <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b37">40]</ref>. Since reads are often effectively masked by caching, significant emphasis is placed on improving write throughput and latency <ref type="bibr" target="#b35">[38]</ref>. It is therefore not surprising that log-structured merge solutions <ref type="bibr" target="#b28">[31]</ref>, which batch writes in memory and merge them with on-disk storage in the background, have become the de facto choice for today's leading key-value stores <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b9">12,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr" target="#b33">36]</ref>. An LSM data store organizes data in a series of components of increasing sizes, as illustrated in <ref type="figure" target="#fig_4">Figure 2a</ref> A put operation inserts an item into the main memory component C m , and logs it in a sequential file for recovery purposes. Logging can be configured to be synchronous (blocking) or asynchronous (non-blocking). The common default is asynchronous logging, which avoids waiting for disk access, at the risk of losing some recent writes in case of a crash.</p><p>When C m reaches its size limit, which can be hard or soft, it is merged with component C d , in a way reminiscent of merge sort: The items of both C m and C d , are scanned and merged. The new merged component is then migrated to disk in bulk fashion, replacing the old component. When considering multiple disk components, C m is merged with component C 1 . Similarly, once a disk component C i becomes full its data is migrated to the next component C i+1 . Component merges are executed in the background as an automatic maintenance service.</p><p>The get operation may require going through multiple components until the key is found. But when get operations are applied mostly to recently inserted keys, the search is completed in C m . Moreover, the disk component utilizes a large RAM cache. Thus, in workloads that exhibit locality, most requests that do access C d are satisfied from RAM as well.</p><p>During a merge, the memory component becomes immutable, at which point it is denoted as C m . To allow put operations to be executed while rolling the merge, a new memory component C m then becomes available for updates (see <ref type="figure" target="#fig_4">Figure 2b)</ref>. The put and get operations access the components through three global pointers: pointers P m and P m to the current (mutable) and previous (immutable) memory components, and pointer P d to the disk component. When the merge is complete, the previous memory component is discarded. Allowing multiple puts and gets to be executed in parallel is discussed in the sequel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">cLSM Algorithm</head><p>We now present cLSM, our algorithm for concurrency support in an LSM-DS. Section 3.1 presents our basic approach for providing scalable concurrent get and put operations; this solution is generic, and can be integrated with many LSM-DS implementations. In Section 3.2, we extend the functionality with snapshot scans, which are implemented in state-of-the-art key-value stores (e.g., <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b18">21]</ref>). This extension assumes that the in-memory data structure supports ordered iterated access with weak consistency (explained below), as various in-memory data structures do (e.g., <ref type="bibr">[1,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b12">15]</ref>). Finally, in Section 3.3, we provide general-purpose non-blocking atomic read-modify-write operations. These are supported in the context of a specific implementation of the in-memory store as a skip list data structure (or any collection of sorted linked lists).</p><p>cLSM optimizes in-memory access in the LSM-DS, while ensuring correctness of the entire data store. Specifi-   cally, if the in-memory component's operations ensure serializability <ref type="bibr" target="#b29">[32]</ref>, then the same is guaranteed by the resulting LSM-DS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Put and Get Operations</head><p>We assume a thread-safe map data structure for the inmemory component, i.e., its operations can be executed by multiple threads concurrently. Numerous data structure implementations, (e.g., see [1, 20, 23]), provide this functionality in a non-blocking and atomic manner. In order to differentiate the interface of the internal map data structure from that of the entire LSM-DS, we refer to the corresponding functions of the in-memory data structure as insert and find:</p><p>insert(k,v) -inserts the key-value pair (k, v) into the map. If k exists, the value associated with it is overwritten.</p><p>find(k) -returns a value v such that the map contains an item (k, v), or ⊥ if no such value exists.</p><p>The disk component and merge function are implemented in an arbitrary way.</p><p>We implement our concurrency support in two hooks, beforeMerge and afterMerge, which are executed immediately before and immediately after the merge process, respectively. The merge function returns a pointer to the new disk component, N d , which is passed as a parameter to afterMerge. The global pointers P m , P m to the memory components, and P d to the disk component, are updated during beforeMerge and afterMerge.</p><p>Puts and gets access the in-memory component directly. Get operations that fail to find the requested key in the current in-memory component search the previous one (if it exists) and then the disk store. Recall that insert and find are thread-safe, so we do not need to synchronize put and get with respect to each other. However, synchronizing between the update of global pointers and normal operation is a subtle issue.</p><p>We observe that for get operations, no blocking synchronization is needed. This is because the access to each of the pointers is atomic (as it is a single-word variable). The order in which components are traversed in search of a key follows the direction in which the data flows (from P m to P m and from there to P d ) and is the opposite of the order in which the pointers are updated in beforeMerge and afterMerge. Therefore, if the pointers change after get has searched the component pointed by P m or P m , then it will search the same data twice, which may be inefficient, but does not violate safety.</p><p>We use reference counters to avoid freeing a memory component while it is being read. In addition, we apply an RCU-like mechanism to protect the pointers to memory components from being switched while an operation is in the middle of the (short) critical section in which the pointer is read and its reference counter is increased. As we only use reference counters per component (and not per row), their overhead is negligible.</p><p>For put operations, a little more care is needed to avoid insertion to obsolete in-memory components. This is because such insertions may be lost in case the merge process has already traversed the section of the data structure where the data is inserted. To this end, we use a shared-exclusive lock (sometimes called readers-writer lock <ref type="bibr" target="#b17">[20]</ref>), Lock, in order to synchronize between put operations and the global pointers' update in beforeMerge and afterMerge. (Such a lock does not block shared lockers as long as no exclusive locks are requested.) The lock is acquired in shared mode during the put procedure, and in exclusive mode during beforeMerge and afterMerge. In order to avoid starvation of the merge process, the lock implementation should prefer exclusive locking over shared locking. Such a lock implementation is given, e.g., in <ref type="bibr">[1]</ref>.</p><p>The basic algorithm is implemented by the four procedures in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Snapshot Scans</head><p>We implement serializable snapshot scans using the common approach of multi-versioning: each key-value pair is stored in the map together with a unique, monotonically increasing, timestamp. That is, the elements stored in the Algorithm 1 Basic cLSM algorithm.</p><p>1:</p><formula xml:id="formula_0">procedure PUT(Key k, Value v) 2:</formula><p>Lock.lockSharedMode()</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Pm.insert(k, t)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Lock.unlock()</p><formula xml:id="formula_1">5: procedure GET(Key k) 6: v ← find k in Pm, P m , or P d , in this order 7: return v 8: procedure BEFOREMERGE 9:</formula><p>Lock.lockExclusiveMode() 10:</p><formula xml:id="formula_2">P m ← Pm 11:</formula><p>Pm ← new in-memory component <ref type="bibr" target="#b9">12</ref>:</p><formula xml:id="formula_3">Lock.unlock() 13: procedure AFTERMERGE(DiskComp N d ) 14:</formula><p>Lock.lockExclusiveMode() 15:</p><formula xml:id="formula_4">P d ← N d 16: P m ← ⊥ 17:</formula><p>Lock.unlock() underlying map are now key-timestamp-value triples. The timestamps are internal, and are not exposed to the LSM-DS's application.</p><p>Here, we assume the underlying map is sorted in lexicographical order of the key-timestamp pair. Thus, find operations can return the value associated with the highest timestamp for a given key. We further assume that the underlying map provides iterators with the so-called weak consistency property, which guarantees that if an element is included in the data structure for the entire duration of a complete snapshot scan, this element is returned by the scan. Several map data structures and data stores support such sorted access and iterators with weak consistency (see <ref type="bibr">[7,</ref><ref type="bibr" target="#b12">15]</ref>).</p><p>To support multi-versioning, a put operation acquires a timestamp before inserting a value into the in-memory component. This is done by atomically incrementing and reading a global counter, timeCounter; there are non-blocking implementations of such counters (e.g., see <ref type="bibr" target="#b17">[20]</ref>). A get operation now returns the highest timestamped value for the given key.</p><p>Our support for snapshots and full scans thereof is explained in Section 3.2.1. We discuss other snapshot-based operations (like range queries) in Section 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Snapshot Management Mechanism</head><p>A snapshot is associated with a timestamp, and contains, for each key, the latest value updated up to this timestamp. Thus, although a snapshot scan spans multiple operations, it reflects the state of the data at a unique point in time.</p><p>The getSnap operation returns a snapshot handle s, over which subsequent operations may iterate. In cLSM, a snapshot handle is simply a timestamp ts. A scan iterates over all live components (one or two memory components and the disk component) and filters out items that do not belong to the snapshot: for each key k, the next operation filters out items that have higher timestamps than the snapshot time, or are older than the latest timestamp (of key k) that does not exceed the snapshot time. When there are no more items in the snapshot, next returns ⊥.</p><p>Our snapshot management algorithm appears in Algorithm 2. Determining the timestamp of a snapshot is a bit subtle. In the absence of concurrent operations, one could simply read the current value of the global counter. However, in the presence of concurrency, this approach may lead to inconsistent scans, as illustrated in <ref type="figure" target="#fig_5">Figure 3</ref>. In this example, next operations executed in snapshot s 2 , which reads from timeCounter, filter out a key written with timestamp 99, while next operations executed in snapshot s 1 , which reads timestamp 99, read this key, but miss a key written with timestamp 98. The latter is missed because the put operation writing it updates timeCounter before the getSnap operation, and inserts the key into the underlying map after the next operation is completed. This violates serializability as there is no way to serialize the two scans.</p><p>We remedy this problem by tracking timestamps that were obtained but possibly not yet written. These are kept in a set data structure, Active, which can be implemented in a non-blocking manner. The getSnap operation chooses a timestamp that is earlier than all active ones. In the above example, since both 98 and 99 are active at the time s 1 and s 2 are invoked, they choose 97 as their snapshot time.</p><p>Note that a race can be introduced between obtaining a timestamp and inserting it into Active as depicted in <ref type="figure" target="#fig_6">Figure 4</ref>. In this example, a put operation reads timestamp 98 from timeCounter, and before it updates the Active set to include it, a getSnap operation reads timestamp 98 from timeCounter and finds the Active set empty. The snapshot timestamp is therefore set to 98. The value later written by the put operation is not filtered out by the scan, which may lead to inconsistencies, as in the previous example. To overcome this race, the put operation verifies that its chosen timestamp exceeds the latest snapshot's timestamp (tracked in the snapTime variable), and re-starts if it does not, while getSnap waits until all active put operations have timestamps greater than snapTime.</p><p>We note that our scan is serializable but not linearizable <ref type="bibr" target="#b21">[24]</ref>, in the sense that it can read a consistent state "in the past". That is, it may miss some recent updates, (including ones written by the thread executing the scan). To preserve linearizability, the getSnap operation could be modified to wait until it is able to acquire a snapTime value greater than the timeCounter value at the time the operation started. This can be done by omitting lines 10-11 in Algorithm 2.</p><p>Since puts are implemented as insertions with a new timestamp, the key-value store potentially holds many versions for a given key. Following standard practice in LSM-DS, old versions are not removed from the memory component, i.e., they exist at least until the component is discarded following its merge into disk. Obsolete versions are removed during a merge once they are no longer needed for any snapshot. In other words, for every key and every snapshot, the latest version of the key that does not exceed the snapshot's timestamp is kept.</p><p>To consolidate with the merge operation, getSnap installs the snapshot handle in a list that captures all active snapshots. Ensuing merge operations query the list to identify the maximal timestamp before which versions can be removed. To avoid a race between installing a snapshot handle and it being observed by a merge, the data structure is accessed while holding the lock. The getSnap operation acquires the lock in shared mode while updating the list, and beforeMerge queries the list while holding the lock in exclusive mode. The timestamp returned by beforeMerge is then used by the merge operation to determine which elements can be discarded. As in levelDB, we assume handles of unused snapshots are removed from the list either by the application (through an API call), or based on TTL; failing to do so may reduce the amount of available memory for useful data.</p><p>Because more than one getSnap operation can be executed concurrently, we have to update snapTime with care, to ensure that it does not move backward in time. We therefore atomically advance snapTime to ts (e.g., using a CAS 1 ) <ref type="bibr">1</ref> Compare and Swap operation <ref type="bibr" target="#b20">[23]</ref>. Lock.lockSharedMode()</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>ts ← getTS()</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Pm.insert(k, ts, v)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Active.remove(ts) Lock.unlock()</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>31:</head><p>return ts in line 12. The rollback loop in getTS may cause the starvation of a put operation. We note, however, that each repeated attempt to acquire a timestamp implies the progress of some other put and getSnap operations, as expected in non-blocking implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Partial Scans and Snapshot Reads</head><p>A full snapshot scan traverses all keys starting with the lowest and ending with the highest one. More common are partial scans, (e.g., range queries), in which the application only traverses a small consecutive range of the keys, or even simple reads of a single key from the snapshot. Given our snapshot management mechanism, it is straightforward to support these by using a find function to locate the first entry to be retrieved (like finding a key in a get operation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Atomic Read-Modify-Write</head><p>We now introduce a general read-modify-write operation, RMW(k,f), which atomically applies an arbitrary function f to the current value v associated with key k and stores f (v) in its place. Such operations are useful for many appli-Algorithm 3 RMW algorithm for linked list memory component. if succ.key = k then continue conflict 9:</p><p>tsn ← getTS() Active.remove(tsn)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>16:</head><p>Lock.unlock() cations, ranging from simple vector clock update and validation to implementing full-scale transactions.</p><p>Our solution is efficient and avoids blocking. It is given in the context of a specific implementation of the in-memory data store as a linked list or any collection thereof, e.g., a skip-list. Each entry in the linked list contains a keytimestamp-value tuple, and the linked list is sorted in lexicographical order. In a non-blocking implementation of such a data structure, put updates the next pointer of the predecessor of the inserted node using a CAS operation <ref type="bibr" target="#b20">[23]</ref>.</p><p>The pseudo-code for read-modify-write on an in-memory linked-list appears in Algorithm 3. The idea is to use optimistic concurrency control -having read v as the latest value of key k, our attempt to insert f (v) fails (and restarts) in case a new value has been inserted for k after v. This situation is called a conflict, and it means that some concurrent operation has interfered between our read step in line 4 and our update step in line 12.</p><p>The challenge is to detect conflicts efficiently. Here, we take advantage of the fact that all updates occur in RAM, ensuring that all conflicts will be manifested in the in-memory component. We further exploit the linked list structure of this component. In line 5, we locate, and store in prev, the insertion point for the new node. If prev is a node holding key k and a timestamp higher than ts, then it means that another thread has inserted a new node for k between lines 4 and 5this conflict is detected in line 6. In line 8, we detect a conflict that occurs when another thread inserts a new node for k between lines 5 and 7 -this conflict is observed when succ is a node holding key k. If the conflict occurs after line 7, it is detected by failure of the CAS in line 12.</p><p>When the data store consists of multiple linked lists, as libcds's lock-free skip-list does [1], items are inserted to the lists one at a time, from the bottom up <ref type="bibr" target="#b20">[23]</ref>. Only the bottom list is required for correctness, while the others ensure the logarithmic search complexity. Our implementation thus first inserts the new item to the bottom list atomically using Algorithm 3. It then adds the item to each higher list using a CAS as in line 12, but with no need for a new timestamp 9 or conflict detection as in lines 6 and 8.</p><p>We note that the lock-free skip-list [1] (which is based on the skip-list algorithm in <ref type="bibr" target="#b20">[23]</ref>) satisfies the requirements specified in Section 3.2 -weak consistency is guaranteed as long as items are not removed from the skip-list, as is the case in cLSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head><p>We implement cLSM in C++ based on the popular open source LevelDB LSM-DS library <ref type="bibr" target="#b2">[4]</ref>. LevelDB is used by numerous applications including Google Chrome and Facebook's embeddable key-value store <ref type="bibr" target="#b6">[8]</ref>.</p><p>LevelDB implements a rich API that includes read (get), write (put), and various snapshot operations. Its memory component is implemented as a skip list with custom concurrency control. Every write is logged to a sequential file following the LSM-DS update. Typically, the data store is configured to perform logging asynchronously, which allows writes to occur at memory speed; hence, a write only queues the request for logging and a handful of writes may be lost due to a crash. LevelDB features a number of optimizations, including multilevel merge, custom memory allocation, caching via memory-mapped I/O, Bloom filters <ref type="bibr" target="#b11">[14]</ref> to speed up reads, etc.</p><p>The original LevelDB acquires a global exclusive lock to protect critical sections at the beginning and the end of each read and write. The bulk of the code is guarded by a mechanism that allows a single writer thread and multiple reader threads to execute at any given time. Snapshots are implemented using timestamps -the timestamp management is simpler than ours (i.e., no need for Active set) since concurrent write operations are not permitted. LevelDB supports an atomic batch of write operations that is implemented using coarse-grained synchronization of simple write operations.</p><p>cLSM supports the full functionality of LevelDB's API. Its implementation inherits the core of LevelDB's modules (disk component, cache, merge function, etc), and benefits from the same optimizations. It implements the algorithm described in Section 3, which eliminates the blocking parts of the LevelDB code. Our support for atomic batches of write operations continues to block (similarly to the original LevelDB) -its synchronization is implemented by holding the shared-exclusive lock in exclusive mode.</p><p>We harness the libcds concurrent data structures' library [1] to implement the in-memory store and the logging queue (via the non-blocking skip list and queue implementations, respectively). We also implement multiple custom tools based on atomic hardware instructions: a sharedexclusive lock, and a non-blocking memory allocator <ref type="bibr" target="#b26">[29]</ref>. All accesses we add to shared memory are protected by memory fences, whereas the libraries we use include fences where deemed necessary by their developers.</p><p>Relaxing LevelDB's single-writer constraint implies that writes might get logged out of order. Since all the log records bear cLSM-generated timestamps, the correct order is easily restored upon recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>We evaluate our cLSM implementation versus a number of open source competitors. In Section 5.1, our experiments are based on synthetic CPU-bound workloads. In Section 5.2 we use real web-scale application workloads. Finally, in Section 5.3, we use a synthetic disk-bound benchmark from RocksDB's benchmarks suite <ref type="bibr" target="#b7">[10]</ref>.</p><p>Our platform is a Xeon E5620 machine with 2 quad-core CPUs, each core with two hardware threads (16 hardware threads overall). The server has 48GB of RAM and 720GB SSD storage <ref type="bibr" target="#b0">2</ref> .</p><p>We vary the concurrency degree in our experiments from one to sixteen worker threads performing operations; these are run in addition to the maintenance compaction thread (or threads in Section 5.3).</p><p>We compare cLSM with four open-source LSM data stores: LevelDB <ref type="bibr" target="#b2">[4]</ref>, HyperLevelDB <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b18">21]</ref>, RocksDB <ref type="bibr" target="#b6">[8]</ref>, and bLSM <ref type="bibr" target="#b33">[36]</ref>. HyperLevelDB and RocksDB are extensions of LevelDB that employ specialized synchronization to improve parallelism (see <ref type="bibr" target="#b1">[3]</ref>), and bLSM is a single-writer prototype that capitalizes on careful scheduling of merges. Unless stated otherwise, each LSM store is configured to employ an in-memory component of 128MB (this is the standard value in key-value stores like HBase); we use the default values of all other configurable parameters.</p><p>Recall that in LSM-DS, component merges occur as a background process, which is often called compaction. All systems except RocksDB use a single background thread for compaction. RocksDB has a configurable parameter determining the maximum number of compaction threads, which we set to one <ref type="bibr" target="#b1">3</ref> , except in Section 5.3. We note that in experiments that involve writes (i.e., put operations), the compaction thread is working a significant portion of the time -in the CPU-bound experiments reported in Sections 5.1 and 5.2, we found that it runs roughly between a quarter and three-quarters of the time, in all systems. In Section 5.3 we consider disk-bound workloads, where compaction runs virtually all the time, and creates a bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Workloads</head><p>We start with a set of benchmarks that exercise the systems in a variety of controlled settings. Our experiment harnesses a 150GB dataset (100x the size of the collection used to compare HyperLevelDB to LevelDB in the publicly avail-Composed of four 240GB SSD SATA/300 OCZ Deneva MLC, configured as RAID-5. <ref type="bibr" target="#b1">3</ref> This is the default value in RocksDB. able benchmark <ref type="bibr" target="#b4">[6]</ref>). The key-value pairs have 8-byte keys, and the value size is 256 bytes.</p><p>Write performance. We start by exploring a write-only workload. The keys are drawn uniformly at random from the entire range. (Different distributions lead to similar resultsrecall that the write performance in LSM stores is localityinsensitive.) <ref type="figure">Figure 5a</ref> depicts the results in terms of throughput. Lev-elDB, HyperLevelDB, and cLSM start from approximately the same point, but they behave differently as we increase the number of threads. LevelDB, bLSM and RocksDB are bounded by their single-writer architectures, and do not scale at all. Moreover, having multiple threads contending for a single synchronization point (e.g., a writers queue) causes the throughput to decrease. HyperLevelDB achieves a 33% throughput gain with 4 workers, and deteriorates beyond that point. cLSM's throughput scales 2.5x and becomes saturated at 8 threads. The degragation in write performance can be explained by cross-chip latency and cache invalidations, since only the 16 threads experiment spans more than one chip. cLSM's peak rate exceeds 430K writes/sec, in contrast with 240K for HyperLevelDB, 160K for LevelDB and 65K for RocksDB. <ref type="figure">Figure 5b</ref> refines the results by presenting the throughputlatency perspective, where the latency is computed for the 90-th percentile; other percentiles exhibit similar trends. For better readability we delineate improvement trends and omit points exhibiting decreasing throughput. This figure marks the point in which each implementation saturates, namely, either achieves a slight throughput gain while increasing the latency by a factor of 2x-3x or achieves no gain at all. It is clear that cLSM scales better than all competitors.</p><p>Read performance. We turn to evaluate performance in a read-only scenario. In this context, uniformly distributed reads would not be indicative, since the system would spend most of the time in disk seeks, devoiding the concurrency control optimizations of any meaning. Hence, we employ a skewed distribution that generates a CPU-intensive workload: 90% of the keys are selected randomly from "popular" blocks that comprise 10% of the database. The rest are drawn u.a.r. from the whole range. This workload is both dispersed and amenable to caching. Its locality is similar to that of production workloads analyzed in Section 5.2. All the following experiments exercise this distribution. <ref type="figure" target="#fig_8">Figure 6a</ref> demonstrates throughput scalability. LevelDB and HyperLevelDB exhibit similar performance. Neither scales beyond 8 threads, reflecting the limitations of Lev-elDB's concurrency control, namely, read operations blocking even when data is available in memory. On the other hand, cLSM and RocksDB scale all the way to 128 threads, far beyond the hardware parallelism (more threads than cores are utilized, since some threads block when reading data from disk). In all cases, RocksDB is not only slower than cLSM, but even slower than LevelDB. In this exper-  <ref type="figure">Figure 5</ref>: Write performance -a 100% write scenario, with the keys uniformly distributed across the domain. cLSM scales to 8 threads and achieves 80% throughput advantage over the closest competitor, which only scales to 4.</p><p>iment, the peak throughput of cLSM is almost 1.8 million reads/sec -2.3x as much as the peak competitor rate. Again, <ref type="figure" target="#fig_8">Figure 6b</ref> shows the throughput-latency (90-th percentile) perspective. This figure emphasizes the scalability advantage of cLSM: it shows that while RocksDB scales all the way, this comes at a very high latency cost, an order of magnitude higher than other LevelDB-based solutions with the same throughput (800K reads/sec).</p><p>Mixed workloads. <ref type="figure">Figure 7a</ref> depicts the throughput achieved by the different systems under a 1:1 read-write mix. The original LevelDB fails to scale, even though the writes are now only 50% of the workload. HyperLevelDB slightly improves upon that result, whereas cLSM fully exploits the software parallelism, scaling beyond 730K operations/sec with 16 workers.</p><p>We note that while under cLSM and HyperLevelDB the reads and the writes scale independently (and the throughput numbers are roughly the avarage of the 100% writes and 100% reads scenarios), in LevelDB and RocksDB the writes impede the reads' progress, and therefore the absolute numbers are lower than the average of the 100% writes and 100% reads scenarios. <ref type="figure">Figure 7b</ref> repeats the same experiment with reads replaced by range scans. (bLSM is not part of this evaluation because it does not directly support consistent scans). The size of each range is picked uniformly between 10 and 20 keys. The number of scan operations is therefore smaller than the number of writes by an order of magnitude, to maintain the balance between the number of keys written and scanned. The cumulative throughput is measured as the overall number of accessed keys. Similarly to the previous cases, the competitors are slower than cLSM by more than 60%. Note that scans are faster than read operations since in each scan operation, the scanned items are located close to the first item, which results in write operations running substantially more than 50% of the time, and the cross-chip effect causes a small degragation in cLSM's throughput with 16 worker threads.</p><p>We next evaluate how the system may benefit from additional RAM. <ref type="figure">Figure 8</ref> compares LevelDB's and cLSM's benefit from larger memory components, under the read-write workload, with 8 working threads. LevelDB performs nearly the same for all sizes beyond 16MB, whereas cLSM keeps improving with the memory buffer growing to 512MB. In general, LSM data stores may gain from increasing the inmemory component size thanks to better batching of disk accesses <ref type="bibr" target="#b8">[11]</ref>. However, this also entails slower in-memory operations. We see that cLSM successfully masks this added latency via its high degree of parallelism, which the less scalable alternatives fail to do.</p><p>Read-Modify-Write. We now explore the performance of atomic RMW operations (put-if-absent flavor <ref type="bibr" target="#b34">[37]</ref>). To establish a comparison baseline, we augment LevelDB with a textbook RMW implementation based on lock striping <ref type="bibr" target="#b19">[22]</ref>. The algorithm protects each RMW and write operation with an exclusive granular lock to the accessed key. The basic read and write implementations remain the same.</p><p>We compare the lock-striped LevelDB with cLSM. The first workload under study is comprised solely of RMW operations. As shown in <ref type="figure">Figure 9</ref>, cLSM scales to almost 400K operations/sec -a 2.5x throughput gain compared to the standard implementation. This volume is almost identical to the peak write load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Production Workloads</head><p>We study a set of 20 workloads logged in a production keyvalue store that serves some of the major personalized content and advertising systems on the web.   workloads are read-dominated (85% to 95% reads). The key distributions are heavy-tail, all with similar locality properties. In most settings, 10% of the keys stand for more than 75% of the requests, while the 1-2% most popular keys account for more than 50%. Approximately 10% of the keys are only encountered once. <ref type="figure" target="#fig_0">Figure 10</ref> depicts the evaluation results for 4 representative workloads. Although cLSM is slower than the alternatives with a small number of threads, its scalability is much better. These results are similar to the results shown in 7a. However, our advantage over the competitors is reduced, because, with larger keys and values, the synchronization overhead is less pronounced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Workloads with Heavy Disk-Compaction</head><p>The above experiments demonstrate situations in which the in-memory access is the main performance bottleneck. Recently, the RocksDB project has shown that in some scenarios, the main performance bottleneck is disk-compaction <ref type="bibr" target="#b7">[10]</ref>. In these scenarios, a huge number of items is inserted (at once) into the LSM store, leading to many heavy diskcompactions. As a result of the high disk activity, the C m component frequently becomes full before the C m component has been merged into the disk. This causes client operations to wait until the merge process completes.</p><p>We use a benchmark from <ref type="bibr" target="#b7">[10]</ref> to demonstrate this situation. In this benchmark, the initial database is created by sequentially inserting 1 billion items. During the benchmark, 1 billion update operations are invoked by the worker threads.</p><p>As in <ref type="bibr" target="#b7">[10]</ref>, each key is of size 10 bytes; however, each value is of size 400 bytes (instead of 800) to ensure that our 720GB disk is sufficient.</p><p>We compare cLSM with RocksDB following the configuration in <ref type="bibr" target="#b7">[10]</ref>. RocksDB is configured to use multi-threaded compactions so that multiple threads can simultaneously compact non-overlapping key ranges in multiple levels. For each parameter that appears both in cLSM and RocksDB, we configure the systems to use the same values. Specifically, these parameters are: size of in-memory component (128MB), total number of levels (6 levels), target file size at level-1 (64MB), and number of bytes in a block (64KB). <ref type="figure" target="#fig_0">Figure 11</ref> depicts the results of this benchmark. The results show that both cLSM and RocksDB scale all the way to 16 worker threads (despite the fact that disk-compaction is running most of the time). At 16 threads, cLSM becomes equivalent to RocksDB. Notice that RocksDB uses an optimized compaction algorithm that utilizes several background threads, whereas cLSM uses a simpler compaction algorithm executed by a single background thread. It should be noted that RocksDB's compaction optimizations are orthogonal to our improved parallelism among worker threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>The basis for LSM data structures is the logarithmic method <ref type="bibr" target="#b10">[13]</ref>. It was initially proposed as a way to efficiently transform static search structures into dynamic ones. This method inspired the original work on LSM-trees <ref type="bibr" target="#b28">[31]</ref> and its variant for multi-versioned data stores <ref type="bibr" target="#b27">[30]</ref>. LSMtrees provide low-cost indexing for key-value stores with high rates of put operations, by deferring in-place random writes and batching them into sequential writes. The LSMtree indexing approach employs B + -tree-like structures as its disk components, and for the main memory component, an efficient key-lookup structure similar to a (2-3)-tree ormore common in recent implementations-a skip-list <ref type="bibr" target="#b31">[34]</ref>.</p><p>Nowadays, key-value stores are commonly implemented as LSM data stores <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b22">25]</ref>. Google's LevelDB <ref type="bibr" target="#b2">[4]</ref> is the state-of-the-art implementation of a single machine LSM that serves as the backbone in many of such key-value stores. It applies coarse-grained synchronization that forces all puts to be executed sequentially, and a single threaded merge process. These two design choices significantly reduce the system throughput in multicore environment. This effect is mitigated by HyperLevelDB <ref type="bibr" target="#b3">[5]</ref>, the data storage engine that powers HyperDex <ref type="bibr" target="#b18">[21]</ref>. It improves on LevelDB in two key ways: (1) by using fine-grained locking to increase concurrency, and (2) by using a different merging strategy. Our evaluations show that cLSM outperforms both of them.</p><p>Facebook's key-value store, RocksDB <ref type="bibr" target="#b6">[8]</ref> also builds on LevelDB. Much effort is done in order to reduce critical sections in the memory component <ref type="bibr" target="#b1">[3,</ref><ref type="bibr">9]</ref>. Specifically, readers avoid locks by caching metadata in their thread local storage. Only when a newer version becomes available readers use locks to get hold of a reference to it. In addition, the merge process of disk components is executed by multiple threads concurrently, and some thread is always reserved for flushing the memory component to the disk.</p><p>In the same vein, bLSM <ref type="bibr" target="#b33">[36]</ref> introduces a new merge scheduler, which bounds the time a merge can block write operations. As bLSM optimizations focus on the merging process and disk access, it is orthogonal to our work on memory optimizations.</p><p>Several approaches for optimizing the performance of the general logarithmic method have been proposed in re-cent years. One such approach suggests adopting a new treeindexing data structure, FD-tree <ref type="bibr" target="#b25">[28]</ref>, to better facilitate the properties of contemporary flash disks and solid state drives (SSDs). Like components in LSM-trees, FD-trees maintain multiple levels with cross-level pointers. This approach applies the fractional cascading <ref type="bibr" target="#b14">[17]</ref> technique to speed up search in the logarithmic structure. A follow-up work <ref type="bibr" target="#b36">[39]</ref> further refines FD-trees to support concurrency, allowing concurrent reads and writes during ongoing index reorganizations.</p><p>With a similar goal of exploiting flash storage as well as the caches of modern multi-core processors, Bw-tree <ref type="bibr" target="#b24">[27]</ref> is a new form of a B-tree, used as an index for a persistent keyvalue store. The implementation is non-blocking, allowing for better scalability (throughput). It also avoids cache line invalidation thus improving cache performance (latency). Instead of locks, their implementation, which bares similarity to B-link design <ref type="bibr" target="#b23">[26]</ref>, uses CAS instructions, and therefore blocks only rarely, when fetching a page from disk. At its storage layer, Bw-tree uses log structuring <ref type="bibr" target="#b32">[35]</ref>.</p><p>None of these new approaches support consistent scans or an atomic RMW operation (as cLSM does). In addition, each of these algorithms builds upon a specific data structure as its main memory component, whereas our work can employ any implementation of a concurrent sorted map to support the basic API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>Leading key-value stores today rely on LSM-DS methodology for serving requests mostly from RAM. With this approach, the implementation of in-memory building blocks is critical for performance, as we have demonstrated in Section 5. The primary challenge such systems face is scaling up with the available hardware resources most notably, the number of CPU cores. In this context, the concurrency control that protects shared data structures can be a major performance roadblock. Our work overcomes this roadblock and presents cLSM, an efficient concurrent LSM-DS implementation. Scalability is achieved by eliminating blocking in scenarios that do not involve physical access to disk.</p><p>In addition to atomic reads and writes, cLSM supports consistent snapshot scans, range queries, and atomic readmodify-write operations. Our algorithm is generic, and can be applied to a range of implementations. Such decoupling allows our solution to be combined with other optimization applied to the disk components and merge utility.</p><p>Our evaluation versus state-of-the-art LSM implementations shows performance improvements and superior scalability, even when the competitors utilize smaller partitions. The latter, along with other disadvantages of partitioning discussed in Section 2, suggests that our approach can potentially serve as an alternative for vertical scalability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Comparing two approaches to scalability with production workload. The resource-isolated configuration exercises LevelDB and HyperLevelDB with 4 separate partitions, whereas the resource-shared configuration evaluates cLSM with one big partition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. The first component, C m , is an in-memory sorted map that contains most recent data. The rest of the components C 1 , . . . , C n reside on disk. For simplicity, in the context of this work, they are perceived as a single component, C d . An additional important building block is the merge procedure, (sometimes called compaction), which incorporates the contents of the memory component into the disk, and the contents of each component into the next one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>LSM-DS consists of a small memory component, and a large disk component comprised of a series of components of increasing sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Global pointers Pm to current (mutable) memory component Cm, P m to previous (immutable) memory component C m , and P d to disk component C d . Merge incorporates C m into C d , while new items are added to Cm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>LSM-DS architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Snapshots s and s 2 cannot use the current values of timeCounter, 99 and 98 respectively, since a next operation pertaining to snapshot s 1 may miss the concurrently written key a with timestamp 98, while a next operation pertaining to snapshot s 2 filters out the key b with timestamp 99. The snapshot time should instead be 97, which excludes the concurrently inserted keys.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>The put operation cannot use the value since a snapshot operation already assumes there are no active put operations before timestamp 99. Using the timestamp 98 may lead to the problem depicted inFigure 3. The put operation should instead acquire a new timestamp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 2</head><label>2</label><figDesc>cLSM snapshot algorithm. 1: procedure PUT(Key k, Value v) 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1: 6 :</head><label>6</label><figDesc>procedure RMW(Key k, Function f ) ts, v) with highest ts in Pm, P m , or P d 5: prev ← Pm node with max(k , ts ) ≤ (k, ∞) if prev.key = k and prev.time &gt; ts then continue conflict 7:succ ← prev.next 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Throughput versus latency; each data point is labeled with the number of threads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Each log captures the history of operations applied to an individual partition server. The average log consists of approximately 5 million operations. Operations have variable key and value sizes, averaging 40-bytes per key, and 1KB values. The captured Throughput versus latency; each data point is labeled with the number of threads Read performance -a 100% read scenario with locality (90% of keys picked from 10% popular blocks). Throughput in mixed workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Mixed reads and writes benefit from memory component size with 8 threads. cLSM successfully exploits RAM buffers of up to 512MB, whereas LevelDB can only exploit 16MB. Read-modify-write (RMW) throughput -a 100% put-if-absent scenario with locality. cLSM improves upon lock-striping by 150%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Throughput in workloads collected from a production web-scale system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Workload with heavy disk-compaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>if ts ≤ snapTime then Active.remove(ts)</figDesc><table><row><cell>6:</cell><cell>Lock.unlock()</cell></row><row><cell cols="2">7: procedure GETSNAP</cell></row><row><cell>8:</cell><cell>Lock.lockSharedMode()</cell></row><row><cell>9:</cell><cell>ts ← timeCounter.get()</cell></row><row><cell>10:</cell><cell>tsa ← Active.findMin()</cell></row><row><cell>11:</cell><cell>if tsa = ⊥ then ts ← tsa − 1</cell></row><row><cell>12:</cell><cell>atomically assign max(ts, snapT ime) to snapTime</cell></row><row><cell>13:</cell><cell>while Active.findMin() &lt; snapTime do nop</cell></row><row><cell>14:</cell><cell>ts b ← snapT ime</cell></row><row><cell>15:</cell><cell>install ts b in the active snapshot list</cell></row><row><cell>16:</cell><cell>Lock.unlock()</cell></row><row><cell>17:</cell><cell>return ts b</cell></row><row><cell cols="2">18: procedure GETTS</cell></row><row><cell>19:</cell><cell>while true do</cell></row><row><cell>20:</cell><cell>ts ← timeCounter.incAndGet()</cell></row><row><cell>21:</cell><cell>Active.add(ts)</cell></row><row><cell>22:</cell><cell></cell></row><row><cell>23:</cell><cell>else break</cell></row><row><cell>24:</cell><cell>return ts</cell></row><row><cell cols="2">25: procedure BEFOREMERGE</cell></row><row><cell>26:</cell><cell>Lock.lockExclusiveMode()</cell></row><row><cell>27:</cell><cell>P m ← Pm</cell></row><row><cell>28:</cell><cell>Pm ← new in-memory component</cell></row><row><cell>29:</cell><cell>ts ← find minimal active snapshot timestamp</cell></row><row><cell>30:</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Apache hbase, a distributed, scalable, big data store</title>
		<ptr target="http://hbase.apache.org/" />
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">expensive-locks-in-get</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A fast and lightweight key/value database library by google</title>
		<ptr target="http://code.google.com/p/leveldb" />
		<imprint>
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyperleveldb</surname></persName>
		</author>
		<ptr target="https://github.com/rescrv/HyperLevelDB" />
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://hyperdex.org/performance/leveldb" />
		<title level="m">Hyperleveldb performance benchmarks</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Java's Concurrentskiplistmap</surname></persName>
		</author>
		<ptr target="http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ConcurrentSkipListMap.html" />
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A persistent key-value store for fast storage environments</title>
		<ptr target="http://rocksdb.org/" />
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="https://github.com/facebook/rocksdb/wiki/Performance-Benchmarks" />
		<title level="m">RocksDB performance benchmarks</title>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Why cannot I have too many regions?</title>
		<ptr target="https://hbase.apache.org/book/regions.arch.html" />
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Storage Infrastructure Behind Facebook Messages: Using HBase at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aiyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bautin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Damania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khe-Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Muthukkaruppan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Spiegelberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaidya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4" to="13" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Decomposable searching problems i. static-to-dynamic transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Saxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="301" to="358" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="422" to="426" />
			<date type="published" when="1970-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A practical concurrent binary search tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Bronson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oluko-Tun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="257" to="268" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bigtable: A distributed storage system for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fractional cascading: I. a data structuring technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="133" to="162" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pnuts: Yahoo!&apos;s hosted data serving platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bohannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-A</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Puz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yerneni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1277" to="1288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s highly available key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kaku-Lapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivasubra-Manian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vosshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Concurrent Programming on Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duffy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hyperdex: a distributed, searchable key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Escriva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Sirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Transaction Processing: Concepts and Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reuters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Art of Multiprocessor Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herlihy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Linearizability: A correctness condition for concurrent objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Herlihy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOPLAS</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A decentralized structured storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="35" to="40" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient locking for concurrent operations on b-trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="650" to="670" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Bw-tree: A B-tree for new hardware platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Levandoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th IEEE International Conference on Data Engineering (ICDE) (2013)</title>
		<imprint>
			<biblScope unit="page" from="302" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tree indexing on solid state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1195" to="1206" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scalable lock-free dynamic memory allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="35" to="46" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Design, implementation, and performance of the lham logstructured history data access method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24rd International Conference on Very Large Data Bases</title>
		<meeting>the 24rd International Conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="452" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The log-structured merge-tree (LSM-tree)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gawlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Inf</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="385" />
			<date type="published" when="1996-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The serializability of concurrent database updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="631" to="653" />
			<date type="published" when="1979-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-scale incremental processing using distributed transactions and notifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Skip lists: A probabilistic alternative to balanced trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="668" to="676" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The design and implementation of a log-structured file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="26" to="52" />
			<date type="published" when="1992-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">bLSM: A general purpose log structured merge tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Verifying atomicity via data independence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Golan-Gueta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bronson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sagiv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vechev</surname></persName>
		</author>
		<editor>ISSTA</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Modern Operating Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Tanenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Prentice Hall Press</publisher>
			<pubPlace>Upper Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
	<note>4th ed</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A practical concurrent index for solid-state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thonangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1332" to="1341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An adaptive write buffer management scheme for flash-based ssds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eckart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lock-free transactional support for distributed data stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yabandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gomez-Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jun-Queira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE &apos;14</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
