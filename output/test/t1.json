{"1000": "to this end, we construct a bilingual graph over word types to establish a connection between the two languages (3), and then use graph label propagation to project syntactic information from english to the foreign language (4).\nas discussed in more detail in 3, we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the english side are individual word types.\nto establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (3.3).3 since we have no labeled foreign data, our goal is to project syntactic information from the english side to the foreign side.\nand their foreign language translations df label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence ( 0.9) alignments def based on these high-confidence alignments we can extract tuples of the form u v, where u is a foreign trigram type, whose middle word aligns to an english word type v.\ngiven the bilingual graph described in the previous section, we can use label propagation to project the english pos labels to the foreign language.\nthis stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui vf aligns to english words vy tagged with label y: ri(y) vy ui vy y vy ui vy (1) the second stage consists of running traditional label propagation to propagate labels from these peripheral vertices v lf to all foreign language vertices in the graph, optimizing the following objective: c(q) uivf\\v lf ,ujn (ui) wijqi qj2 uivf\\v lf qi u2 s.t. y qi(y) ui qi(y) ui, y qi ri ui v lf (2) where the qi (i 1, , vf ) are the label distributions over the foreign language vertices and and are hyperparameters that we discuss in we use a squared loss to penalize neighboring vertices that have different label distributions: qi qj2 y(qi(y)qj(y))2, and additionally regularize the label distributions towards the uniform distribution u over all possible labels y it can be shown that this objective is convex in q.\nafter running label propagation (lp), we compute tag probabilities for foreign word types x by marginalizing the pos tag distributions of foreign trigrams ui x x x over the left and right context words: p(yx) x,x qi(y) x,x,y qi(y ) (6) we then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : tx(y) if p(yx) otherwise (7) we describe how we choose in this vector tx is constructed for every word in the foreign vocabulary and will be used to provide features for the unsupervised foreign language pos tagger.", "1001": "the rnn is trained to predict the next element of a sequence given the previous elements.\nxn) in this paper, we propose a new method to train a convolutional neural network ( rnn) to predict the next word of a sequence of words.\nthe rnn is trained to predict the next word of the sentence, given the previous words w0, ...\n, r ( wn) xn the rnn is trained to predict the next word of the sentence, given the previous words w0, ...\na sentence, being an ordered sequence of words, can be represented as a vector using the rnn-fv scheme.\nfisher vectors have been shown to provide a significant performance gain on many different applications in the domain of computer vision 39, 33, 2, in the domain of video action recognition, fisher vectors and stacked fisher vectors have recently outperformed state-of-theart methods on multiple datasets 33, fisher vectors (fv) have also recently been applied to word embedding (e.g. word2vec 30) and have been shown to provide state of the art results on a variety of nlp tasks 24, as well as on image annotation and image search tasks in all of these contributions, the fv of a set of local descriptors is obtained as a sum of gradients of the loglikelihood of the descriptors in the set with respect to the parameters of a probabilistic mixture model that was fitted on a training set in an unsupervised manner.\nthe rnn is trained to predict the next element of a sequence given the previous elements.\nseveral recent works have proposed to use an rnn for sentence representation 44, 1, 31, the recurrent neural network fisher vector (rnn-fv) method differs from these works in that a sequence is represented by using derived gradient from the rnn as features, instead of using a hidden or an output layer of the rnn.\nin this section, we show that such a model can be obtained by training an rnn to predict the next element in a sequence, given the previous elements.\nthe rnn is trained to predict, at each time step i, the next element xi1 of the sequence, given the previous elements x0, ..., xi.\ngiven a sequence of input vectors x , the regression rnn is trained to predict the next vector in the sequence s, i.e., the sequence y the output layer of the network is a fully-connected layer, the size of which would be d, i.e., the dimension of the input vector space.\nwe train the rnn to predict the next element in our video representation sequence, given the previous elements, as described in sec.\na sentence, being an ordered sequence of words, can be represented as a vector using the rnn-fv scheme.\nan rnn is trained to predict, at each time step i, the next word wi1 of the sentence, given the previous words w0, ..., wi.", "1002": "based on this idea, in this paper, we propose a new method, named talksumm ( acronym for talk-based summarization), to automatically generate extractive content-based summaries for scientific papers based on video talks.\nto our knowledge, this is the first approach to automatically create extractive summaries for scientific papers by utilizing the videos of conference talks.\nthis paper proposes a novel automatic method to generate training data for scientific papers summarization, based on conference talks given by authors.\nwe propose a novel automatic method to generate training data for scientific papers summarization, based on conference talks given by authors.\nbased on this idea, in this paper, we propose a new method, named talksumm (acronym for talk- based summarization), to automatically generate extractive content-based summaries for scientific papers based on video talks.\nour main contributions are as follows: (1) we propose a new approach to automatically gener- ate summaries for scientific papers based on video talks; (2) we create a new dataset, that contains summaries for papers from several computer science conferences, that can be used as training data; (3) we show both automatic and human eval- uations for our approach.\nthus, to create an extractive paper summary based on the transcript, we model the alignment between spo- ken words and sentences in the paper, assuming the following generative process: during the talk, the speaker generates words for describing ver- bally sentences from the paper, one word at each time step.", "1003": "while the aforementioned approach utilizes bow representation and linear classi ers, neural network methods are based on dense vector representations of text samples (word embedding) and are nonlinear.\nclassi er weights (class): in this approach we calculated a weight function, w (t , e ) for each term t in the training data which indicates its importance in classifying a document as expressing emotion e we did this by rst representing the documents in a bow binary vector representation where we only extracted unigram features.\nthe ensemble methods we experimented with, follow the notation: men (d ) (mbow (d )) (1 ) (mwe (d )) , (4) wheremen (d ) is the output probability vector for the ensemble classi er given a test document d ,mbow (d ) andmwe (d ) are the output probability vectors of the bow and the word embedding based classi ers respectively, and is a parameter which corresponds to the speci c ensemble method used.\ntable depicts the macro f1-scores for each dataset and each document representation method that is based on word vectors, as detailed in section results show that for all datasets and representation methods, word vectors trained by glove achieved higher performance than using word2vec based vectors.", "1004": "in this paper we show that, in addition to text based turn features, dialogue features can significantly improve detection of emotions in social media customer service dialogues and help predict emotional techniques used by customer service agents.\n(2) this is the first research using unique dialogue features (e.g., emotions expressed in previous dialogue turns by the agent and customer, time between dialogue turns) to improve emotion detection.\n(3) this is the first research studying the prediction of the agent emotional techniques to be used in the response to customer turns.\nthe first objective of our work is to detect emotions expressed in customer turns and the second is to predict the emotional technique in agent turns.\nagent essence: a set of local binary features that represent the action used by the agent to address the last customer turn, independently of any emotional technique expressed.\nthe emotional family of features includes agent emotion and customer emotion: these two sets of local binary features represent emotions predicted for previous turns.\nour model generates predictions of emotions for each customer and agent turn, and uses these predictions as features to classify a later customer or agent turn with emotion expression.\nsince emotions expressed in customer and agent turns are different, we treated them as different classification tasks (like in our previous approach) and trained a separate classifier for each emotion.\nfor assessing the performance of our predictions of agent turns emotion techniques, we first note that we tested with history range, since we assume that the minimal information needed for agent turn classification is the information extracted from the last customer turn.\nthis indicates that for agent emotion technique prediction the last customer turn is the most informative one.\nspecifically, we described two classification tasks, one for detecting customer emotions and the other for predicting the emotional technique used by support service agent.", "1005": "in this paper , we propose an adaptive network architecture for few-shot tasks .\nour goal is to learn a neural network where connections are controllable and adapt to the few-shot task with novel categories.\nto summarize, our contributions in this work are as follows: (1) we show that darts-like bi-level iterative optimization of layer weights and network connections performs well for few-shot classification without suffering from overfitting due to over-parameterization; (2) we show that adding small neural networks, metadapt controllers, that adapt the connections in the main network according to the given task further (and significantly) improves performance; (3) using the proposed method, we obtain improvements over fsc state-of-the-art on two popular fsc benchmarks: miniimagenet and fc100 the major approaches to few-shot learning include: metric learning, generative (or augmentation) based methods, and meta learning (or learning-to-learn).\nthese methods are trained on a set of few-shot tasks (also known as episodes\u2019) instead of a set of object instances, with the motivation to learn a learning strategy that will allow effective adaptation to new such (few-shot) tasks using one or few examples.\nin matching networks 60, a non-parametric k-nn classifier is meta-learned such that for each few-shot task the learned model generates an adaptive embedding space for which the task can be better solved.\nanother family of meta-learning approaches is the so-called gradient based approaches\u2019, that try to maximize the adaptability\u2019, or speed of convergence, of the networks they train to new (few-shot) tasks (usually assuming an sgd optimizer).\nin other words, the meta-learned classifiers are optimized to be easily fine-tuned on new few-shot tasks using small training data.\nnotably, in all previous meta-learning methods, only the parameters of a (fixed) neural network are optimized through meta-learning in order to become adaptable (or partially adaptable) to novel few-shot tasks.\nfor the initial training we use the sgd optimizer with intial learning rate 0.1, momentum and weight decay decreasing the learning rate to at epoch 20, at epoch and at epoch for weights optimization during the search and meta adaptation phases we use the sgd optimizer with learning rate 0.001, momentum and weight decay for the architecture optimization we use adam optimizer with learning rate 104, 0.5, 0.99, weight decay and the cosine annealing learning rate scheduler with min following previous works, e.g. 13,5, we perform test time augmentations and fine-tuning.\nin this work we have proposed metadapt, a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks.\nthe proposed approach effectively applies tools from the neural architecture search (nas) literature, extended with the concept of metadapt controllers\u2019, in order to learn adaptive architectures.\nsome interesting future work directions include extending the proposed approach to progressively searching the full network architecture (instead of just the last block), applying the approach to other few-shot tasks such as detection and segmentation, and researching into different variants of task-adaptivity including global connections modifiers and inter block adaptive wiring.", "1006": "to perform egregious conversation detection , features from both customer inputs and agent responses are extracted, together with features related to the combination of specific inputs and agent responses are extracted.\nusing this set of features for detecting egregious conversations is novel, and as our experimental results show, improves performance compared to a model based solely on features extracted from the conversation text alone.\nthe main contributions of this paper are twofold: (1) this is the first research focusing on detecting egregious conversations in conversational agent (chatbot) setting and (2) this is the first research using unique agent, customer, and customer-agent interaction features to detect egregiousness.\nthe objective of this work is to reliably detect egregious conversations between a human and a virtual agent.\nto perform egregious conversation detection, features from both customer inputs and agent responses are extracted, together with features related to the combination of specific inputs and responses.\nusually, high positive emotions capture different styles of thanking the agent, or indicate that the customer is somewhat satisfied (rychalski and hudson, 2017), thus, the conversation is less likely to become egregious.\nwe also looked at features across conversation utterance-response pairs in order to capture a more complete picture of the interac- tion between the customer and the virtual agent.\na model that was trained to predict egregiousness given the conversation\u2019s text (all customer and agent\u2019s text dur- 8judges that are hci experts and have experience in designing conversational agents systems.\nin this paper, we have shown how it is possible to detect egregious conversations using a combination of customer utterances, agent responses, and customer-agent interactional features.", "1007": "this bias is a single number (per filter) which is added to the sum of slot activations to arrive at the ngram activation which is passed to the max-pooling layer.\ntop-scoring natural ngrams almost never fully activate all slots in a filter.\ntable zooms in on one of the filters and shows its top7 naturally occurring ngrams and top-7 most activated words in each slot.\nwe consider two hypotheses to explain this behavior: (i) each filter captures multiple semantic classes of ngrams, and each class has some dominating slots and some non-dominating slots (which we define as a slot activation pattern).\nto summarize, by discarding noisy ngrams which do not pass the filter\u2019s threshold and then clustering those that remain according to their slot activation patterns, we arrived at a clearer image 6intuitively, we can think of the sampling noise as the ngram embeddings, and the probability distribution as defined by a function of the filter weights.\nour second theory to explain the discrepancy between the activations of naturally occurring and possible ngrams is that certain filter slots are not used to detect a class of highly activating words, but rather to rule out a class of highly negative words.\nidentifying negative slot activations would be very useful for understanding the semantics captured by a filter and the reasoning behind the dismissal of an ngram, as we discuss in sections and respectively.\nfinally, we can also mark cases of negative-ngrams (section 5.4), where an ngram has high slot activations for some words, but these are negated by a highly-negative slot and as a consequence are not selected by max-pooling, or are selected but do not pass the filter\u2019s threshold.\nwe also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words.", "1008": "a summary generated by editnet may include sentences that were either extracted, abstracted or of both types.\nmoreover, on average, per summary, editnet keeps only of the original ( extracted) sentences, while the rest ( 67) are abstracted ones.\na summary generated by editnet may include sentences that were either extracted, abstracted or of both types.\nrecall that, for each sentence si s (in order) the editor makes one of the three possible decisions: extract, abstract or reject si.\nyet, while neusum applies an extraction-only approach, summaries generated by editnet include a mixture of sentences that have been either extracted or abstracted.", "1009": "this paper presents dimsim, a learned ndimensional encoding for chinese along with a phonetic similarity algorithm to generate and rank phonetically similar words.\ndimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final , and tone components.\ndimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components.\nthis paper presents dimsim, a learned ndimensional phonetic encoding for chinese along with a phonetic similarity algorithm, which uses the encoding to generate and rank phonetically similar words.\nthe latter consists of word pairs, with specific pairs of initials or finals manually annotated for phonetic similarity.\nthe set of annotated pairs between initials and finals are then used to learn the n-dimensional encodings of initials and finals, which will in turn be used for generating phonetically similar candidates.\nlearning pinyin encodings therefore, the next task is to generate an accurate representation of phonetic similarity for every pair of initials, finals, and tones.\nthe training data sets consist of word pairs that highlight a pair of initials (or finals), and are used as the context for an annotator-provided phonetic similarity score.\ngenerating similar word pairs phonetically similar word pairs are used to create annotations representing the phonetic similarity of a pair of initials, or finals.\nhaving determined the phonetic encodings and the mechanism to compute the phonetic similarity using learned phonetic encodings, we now describe how to generate and rank similar candidates in algorithm given a word w, a similarity threshold th, and a chinese pinyin dictionary dict, we retrieve the pinyin py of w from dict.\ndimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components.", "1010": "this paper proposes a hierarchical residual bilstm ( hr-bilstm) model for the relation detection task.\nthe hr-bilstm is a hierarchical residual matching (hrm) model for the relation detection task.\nwe propose a novel kb relation detection model, hr-bilstm, that performs hierarchical matching between questions and kb relations.\nthe kbqa system in the figure performs two key tasks: (1) entity linking, which links n-grams in questions to kb entities, and (2) relation detection, which identifies the kb relation(s) a question refers to.\nthird, we use deep bidirectional lstms (bilstms) to learn different levels of question representations in order to match the different levels of relation information.\ngiven an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the kbqa process: (1) re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model.\nour main contributions include: (i) an improved relation detection model by hierarchical matching between questions and relations with residual learning; (ii) we demonstrate that the improved relation detector enables our simple kbqa system to achieve state-of-the-art results on both single-relation and multi-relation kbqa tasks.\nfor relation detection in kbqa, such information is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one kb entity could have multiple types (type vocabulary size larger than 1,500).\nsince both these levels of granularity have their own pros and cons, we propose a hierarchical matching approach for kb relation detection: for a candidate relation, our approach matches the input question to both word-level and relation-level representations to get the final ranking score.\nthen we generate the kb queries for q following the four steps illustrated in algorithm algorithm 1: kbqa with two-step relation detection input : question q, knowledge base kb, the initial top-k entity candidates elk(q) output: top query tuple (e, r, (c, rc)) entity re-ranking (first-step relation detection): use the raw question text as input for a relation detector to score all relations in the kb that are associated to the entities in elk(q); use the relation scores to re-rank elk(q) and generate a shorter list el0k0(q) containing the top-k0 entity candidates (section 5.1) relation detection: detect relation(s) using the reformatted question text in which the topic entity is replaced by a special token e (section 5.2) query generation: combine the scores from step and 2, and select the top pair (e, r) (section 5.3) constraint detection (optional): compute similarity between q and any neighbor entity c of the entities along r (connecting by a relation rc) , add the high scoring c and rc to the query (section 5.4).\n(2016), we use s-mart (yang and chang, 2015) entity-linking outputs.7 in order to evaluate the relation detection models, we create a new relation detection task from the webqsp data set.8 for each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length 2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples.\nin order to highlight the effect of different relation detection models on the kbqa end-task, we also implemented another baseline that uses our kbqa system but replaces hr-bilstm with our implementation of ampcnn (for simplequestions) or the char-3-gram bicnn (for webqsp) relation detectors (second block in table 3).\nwe propose a novel kb relation detection model, hr-bilstm, that performs hierarchical matching between questions and kb relations.", "1011": "the approach is based on the explore/exploit paradigm to effectively discover new instances ( explore) from the text corpus as well as predict new unseen terms not currently in the corpus using the accepted dictionary entries ( exploit) this paper proposes an interactive dictionary expansion tool using a lightweight neural language model.\ngiven an input text corpus and a set of seed examples, the proposed approach runs in two phases, explore and exploit, to identify new potential dictionary entries.\nthe explore phase tries to identify similar instances to the dictionary entries that are present in the input text corpus, using term vectors from the neural language model to calculate a similarity score.\nin the exploit phase, the approach generates new phrases by analyzing the single terms of the instances in the input dictionary.\nfor example, given the instance clotting problems in the input dictionary the approach first tries to identify related terms in the text corpus for clotting.", "1012": "based on those image concepts\u2019 which are represented as score maps, we formulate textual grounding as a search over all possible bounding boxes.\nwe obtain a bounding box prediction y given our data x, by solving the energy minimization y arg min yy e(x, y, w), (1) to global optimality.\nenergy function details: our energy function e(x, y, w) is based on a set of image concepts,\u2019 such as semantic segmentation of object categories, detections, or word priors, all of which we subsume in the set c.\nbased on this definition, we formulate the energy function as follows: e(x, y, w) ss:s1 cc ws,cc(x, y, wr), (2) where ws,c is a parameter connecting a word s s to an image concept c c.\ngiven such a repetitive decomposition strategy for the output space, and since the energy e(x, y, w) for a bounding box y is obtained using a linear combination of word priors and accumulated segmentation masks, we can design an efficient branch and bound based search algorithm to exactly solve the inference problem specified in eq.\nfor each subspace, the algorithm computes a lower bound e(x,yj , w) for the energy of all possible bounding boxes within the respective subspace.\nfor the latter, we note that two conditions are required to ensure convergence to the optimum: (i) the bound of the considered product space has to lower-bound the true energy for each of its bounding box hypothesis y y , i.e., y y , e(x, y, w) e(x, y, w); (ii) the bound has to be exact for all possible bounding boxes y y , i.e., e(x, y, w) e(x, y, w).\nintuitively, if the weight is positive and the feature mask contains only positive values, we obtain the smallest lower bound e(x, y, w) by considering the content within the smallest possible bounding box.\nobserve that, e(x, y, w), is a weighted sum of the feature maps over the region specified by a hypothesized bounding box.", "1013": "we investigate here the use of pfs in the context of learning implicit generative models through moment matching ( mm) this paper proposes a new approach to training implicit generative models by using pretrained feature extractors .\nin this approach , the generator is trained with a dcgan-like architecture and then used as a feature extractor to perform moment matching in a pf space induced by a non - linear kernel function ( a dcnn) that is orders of magnitude larger than the ae latent code, and that we argued is universal in the image domain.\ngfmn is also related to the recent body of work on mmd and moment matching based generative models 22, 8, 21, 3, the closest to our method is the generative moment matching network autoencoder ( gmmnae) proposed in in gmmnae, the objective is to train a generator g that maps from a prior uniform distribution to the latent code learned by a pretrained ae, and then uses the frozen pretrained decoder to map back to image space.\nthe main contributions of this work can be summarized as follows: (1) we propose a new effective moment matchingbased approach to train implicit generative models that does not use adversarial or online learning of kernel functions, provides stable training, and achieves state-of-the-art results; (2) we show theoretical results that demonstrate gfmn convergence under the assumption of the universality of perceptual features; (3) we propose an adam-based moving average method that allows effective training with small minibatches; (4) our extensive quantitative and qualitative experimental results demonstrate that pretrained autoencoders and dcnn classifiers can be effectively used as (cross-domain) feature extractors for gfmn training.\nthe main advantage of ama over simple moving average (ma) is in its adaptive first and second order moments that ensure stable estimation of the moving averages vj in fact, this is a non-stationary estimation since the mean of the generated data changes in the training, and it is well known that adam works well for such online non-stationary losses in section we provide experimental results supporting: (1) the memory advantage that the ama formulation of feature matching offers over the naive implementation; (2) the stability advantage and improved generation results that ama allows compared to the naive implementation.\ngfmn is related to the recent body of work on mmd and moment matching based generative models 22, 8, 21, 3, the closest to our method is the generative moment matching network autoencoder (gmmnae) proposed in in gmmnae, the objective is to train a generator g that maps from a prior uniform distribution to the latent code learned by a pretrained ae, and then uses the frozen pretrained decoder to map back to image space.\nmmd-gans, discussed in section 3, demonstrated competitive results with the use of adversarial learning by learning a feature map in conjuction with a gaussian kernel 21, recently proposed a method to perform online learning of the moments while training the generator.\npresents samples from gfmnvgg19 trained with celeba dataset with resolution 128128, which shows that gfmn can achieve good performance with image resolutions larger than these results also demonstrate that: (1) the same classifier (vgg19 trained on imagenet) can be successfully applied to train gfmn models across different domains; (2) perceptual features from dcnns encapsulate enough statistics to allow the learning of good generative models through moment matching.\nthis feature extractor is smaller than vgg19/resnet18 allowing for minibatches of size up to for image size shows generated images from gfmn trained with either ma or our proposed ama.\nwe achieve successful non-adversarial training of implicit generative models by introducing different key ingredients: (1) moment matching on perceptual features from all layers of pretrained neural networks; (2) a more robust way to compute the moving average of the mean features by using adam optimizer, which allows us to use small minibatches; and (3) the use of perceptual features from multiple neural networks at the same time (vgg19 resnet18).\nwe are confident that gfmn can achieve state-ofthe-art results with features from classifiers trained with unsupervised methods such as in conclusion, this work presents important theoretical and practical contributions that shed light on the effectiveness of perceptual features for training implicit generative models through moment matching.\nwe show results for both simple moving average (ma) and adam moving average (ama), for both cases we use a minibatch size of 11, we show generated images from gfmn trained with either vgg19 features (top row) or autoencoder (ae) features (bottom row).", "1014": "in this paper, we examine the problem of demand-aware recommendation in settings when inter-purchase duration within item categories affects users duration intention in combination with intrinsic properties of the items themselves.\ngiven purchase triplets (user, item, time) and item categories, the objective is to make recommendations based on users\u2019 overall predicted combination of form utility and time utility.\nspecifically, we model a user\u2019s time utility for an item by comparing the time t since her most recent purchase within the item\u2019s category and the item category\u2019s underlying inter-purchase duration d; the larger the value of d t, the less likely she needs this item.\nthis observation allows us to cast demand-aware recommendation as the problem of learning users\u2019 form utility tensor x and items\u2019 inter-purchase durations vector d given the binary tensor p although the learning problem can be naturally formulated as a tensor nuclear norm minimization problem, the high computational cost significantly limits its application to large-scale recommendation problems.\nto overcome this limitation, we develop an efficient alternating minimization algorithm and show that its time complexity is only approximately proportional to the number of nonzero elements in the purchase records tensor p since p is usually very sparse, our algorithm is extremely efficient and can solve problems with millions of users and items.\ncompared to existing recommender systems, our work has the following contributions and advantages: (i) to the best of our knowledge, this is the first work that makes demand-aware recommendation by considering inter-purchase durations for durable and nondurable goods; (ii) the proposed algorithm is able to simultaneously infer items\u2019 inter-purchase durations and users\u2019 real-time purchase intentions, which can help e-retailers make more informed decisions on inventory planning and marketing strategy; (iii) by effectively exploiting sparsity, the proposed algorithm is extremely efficient and able to handle large-scale recommendation problems.\ngiven p and c, we further generate a tensor t rmrl where ticjk denotes the number of time slots between user i\u2019s most recent purchase within item category cj until time k.\nif user i has not purchased within item category cj until time k, ticjk is set to in this work, we formulate users\u2019 utility as a combined effect of form utility and time utility.\nlet dcj be the inter-purchase duration time of item j\u2019s category cj , and let ticjk be the time gap of user i\u2019s most recent purchase within item category cj until time k.", "1015": "to our knowledge, this work is the first to train a neural response generation model that encodes target personality traits.\nin this section we present our personality-based model (figure 2) which generates responses conditioned on a target set of personality traits values which the responses should express.\nthus, our model\u2019s response is conditioned on generation parameters which are based on personality traits.\nthis evaluation measures whether the responses generated by our model are correlated with the target personality traits.", "1016": "we introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-neumann computation model, where no frames, arrays, or any other such data-structures are used.\nthis is the first time that an end-to-end stereo pipeline from image acquisition and rectification , multi- scale spatiotemporal stereo correspondence , winner-take-all, to disparity regularization is implemented fully on event-based hardware.\nusing a cluster of truenorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by dynamic vision sensors ( dvs) , at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct , at low power , the depth of events produced from rapidly changing scenes.\nwe have developed a fully event based neuromorphic stereo system capable of running on live input event streams, using a fully graph-based computation model, where no frames, arrays or other such data-structures are used.\nthe implemented neuromorphic stereo disparity system achieves these advantages, while consuming less power per pixel per disparity map compared to the stateof-the-art the homogeneous computational substrate provides the first example of a fully end-to-end low-power , high throughput fully event-based neuromorphic stereo system capable of running on live input event streams, using a fully graph-based computation model, where no frames, arrays or other such data-structures are used.\nthe proposed event-based disparity method is implemented using a stereo pair of davis sensors (a version of dvs) and nine truenorth ns1e boards however, the method is applicable to other spiking neuromorphic architectures, and it is also tested offline on larger models using a truenorth simulator.\n60, propose dsp implementation of a spatiotemporal similarity method using two live event sensors use a rotating pair of event-based line (vertical) sensors in static scenes and render events from each rotation to an edge map 33, which is subsequently processed using a frame-based panoramic stereo algorithm time-based methods utilize event timestamps for matching.\nthen for any 0, 1, 2, 0, 1, ..., b, we define the conversion of candidate disparity level d 0, ..., d to a qt-coded membrane potential v cnv,,d (t) as 2for notational simplicity we henceforth drop the , superscripts: the left and right sensors could produce distinct event streams based on event polarity, or could merge events in a single polarity-agnostic stream.\nthe implemented neuromorphic stereo disparity system achieves these advantages, while consuming less power per pixel per disparity map compared to the stateof-the-art the homogeneous computational substrate provides the first example of a fully end-to-end low-power, high throughput fully event-based neuromorphic stereo system capable of running on live input event streams, using a fully graph-based computation model, where no frames, arrays or other such data-structures are used.", "1017": "we call two causal graphs d1,d2 i-markov equivalent if the set of distributions that are i-markov tod1 andd2 are the same.\nour contributions can be summarized as follows: we propose a characterization of i-markov equivalence between two causal graphs with latent variables for a given intervention set i that is based on a generalization of do-calculus rules to arbitrary subsets of interventions.\na pag, which represents a markov equivalence class of a mag, is learnable from the independence model over the observed variables, and the fci algorithm is a standard sound and complete method to learn such an object related work: learning causal graphs from a combination of observational and interventional data has been studied in the literature 3, 11, 7, 20, 8, 12, for causally sufficient systems, the notion and characterization of interventional markov equivalence has been introduced in 9, more recently, showed that the same characterization can be used for both hard and soft interventions.\nto witness, consider the causal graph d x y, x cd y, and suppose we have the observational and interventional distributions p(y, x) and px(y, x), respectively.\nthe two conditions used in the definition correspond to rule of theorem and that of corollary notice that the traditional markov definition only considers the first condition over the observational distribution p(v); a case included in the i-markov whenever i accordingly, two causal graphs are said to be i-markov equivalent if they license the same set of distribution tuples.\nthe augmented graph of d with respect to i , denoted as augi (d), is the graph constructed as follows: augi (d) (v f ,e e) where f b fiik and e (fi, j)ik, jsi the significance of the augmented graph construction is illustrated by proposition 1, which provides criteria to test the d-separation statements in definition equivalently from the corresponding augmented graph of a causal graph.\nconsider a causal graphd (v l,e) and the corresponding augmented graph augi (d) (v l f ,e e) with respect to an intervention set i , where f fiik.\nbelow, we derive a characterization for two causal graphs to be i-markov equivalent two causal graphs are i-markov equivalent if their corresponding augmented mags satisfy the three conditions given in theorem for example, the two augmented mags in figures 2c and 2d satisfy the three conditions, hence the original causal graphs are in the same i-markov equivalence class.\ntwo causal graphs d1 (v l1,e1) and d2 (v l2,e2) are i-markov equivalent for a set of controlled experiments i if and only if for m1 mag(augi(d1)) and m2 mag(augi (d2)): m1 andm2 have the same skeleton; m1 andm2 have the same unshielded colliders; if a path p is a discriminating path for a node y in bothm1 andm2, then y is a collider on the path in one graph if and only if it is a collider on the path in the other.\ngiven a causal graph d and an intervention set i , letm mag(augi(d)) and let m be the set of augmented mags corresponding to all the causal graphs that are i-markov equivalent tod.\nrule (inducing paths): if fk f is adjacent to a node y sk and sk 1, e.g., sk x, then orient x y out of x, i.e., x y the intuition for this rule is as follows: if fk is adjacent to a node y sk in g, then there is an inducing path p between fk and y in augi(d), where d is any causal graph in the equivalence class.\nconsider a set of interventional distributions (pi)ii c-faithful to a causal graph d (v l), where i is a set of controlled experiments.", "1018": "we present complex imperative program induction from terminal rewards ( cipitr) , an advanced neural programmer that mitigates sparsity with auxiliary rewards, and uses high-level constraints, kb schema, and inferred answer type.\nthe model takes as input the natural language question, the kb, and the prepopulated variable memory tables to generate a program.\nhandling complex queries by expanding the operator set and generating longer programs ( numop ( maxvar) m this, in absence of gold programs, poses serious training challenges for the programmer.\nmain contributions we present complex imperative program induction from terminal rewards\u2019\u2019 (cipitr),2 an advanced neural program induction (npi) system that is able to answer complex logical, quantitative, and comparative queries by inducing programs of length up to 7, using atomic operators and variable types.\ncipitr reduces the combinatorial program space to only semantically correct programs by (i) incorporating symbolic constraints guided by kb schema and inferred answer type, and (ii) adopting pragmatic programming techniques by decomposing the final goal into a hierarchy of sub-goals, thereby mitigating the sparse reward problem by considering additional auxiliary rewards in a generic, task-independent way.\ncsqa is particularly suited to study the complex program induction (cpi) challenge over other kbqa data sets because: it contains large-scale training data of question-answer pairs across diverse classes of complex queries, each requiring different inference tools over large kb sub-graphs.\noperator prototype matrices: these matrices store the argument variable type information for the m arguments of every operator in mop arg 0, 1, , numvartypesnum opm and the output variable type created by it in mop out 0, 1, memory matrices: this is the query-specific scratch memory for storing new program variables as they get created by cipitr.\ncipitr consists of three components: the preprocessor takes the input query and the kb and performs the task of entity, relation, and type linking which acts as input to the program induction.\nthe programmer model takes as input the natural language question, the kb, and the pre-populated variable memory tables to generate a program (i.e., a sequence of operators invoked with past instantiated variables as their arguments and generating new variables in memory).\nprocedure: argvarsampler(ht, v typep , v feasp , nv) forj 1, 2, ,m do vattp,j softmax(m var attv typep,j ) fvtype(ht) vdistp,j lookup(vattp,j , fvar,mvar keyv type p,j ) v distp v dist p,0 vdistp,1 vdistp,m , joint distribution v distp , vp feassampling(v distp , v feasp , nv) output: vp end-to-end cipitr training: cipitr takes a natural language query and generates an output program in a number of steps.\nhandling complex queries by expanding the operator set and generating longer programs blows up the program space to a huge size of (numop (maxvar) m)t this, in absence of gold programs, poses serious training challenges for the programmer.\nfor this work, we limit our effort on kbqa to the setting where the query is annotated with the gold kb-artifacts, which standardizes the input to the program induction for the competing models.", "1019": "contact author: haggaiil.ibm.com to illustrate the effect of summary length on this tradeoff, using the duc dataset, figure reports the summarization quality which was obtained by the cross entropy summarizer (ces) a state of the art unsupervised query-focused multidocument extractive summarizer saliency was measured according to cosine similarity between the summary\u2019s bigram representation and that of the input documents.\naiming at better handling the saliency versus focus tradeoff, in this work, we propose dual-ces an extended ces summarizer similar to ces, dual-ces is an unsupervised query-focused, multi-document, extractive summarizer.\nto this end, like ces, dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents, whose combination is predicted to produce a good summary.\nusing an evaluation with the duc 2005, and benchmarks, we show that, dual-ces generates a focused (and shorter) summary which has much higher saliency (and hence a better tradeoff handling).\ninstead, following 6, q(sq,d) is surrogated by several summary quality prediction measures qi(sq,d) (i 1, 2, each predictor qi(sq,d) is designed to estimate the level of saliency or focus of a given candidate summary s and is presumed to correlate (up to some extent) with actual summarization quality, e.g., rouge for simplicity, similar to ces, various predictions are assumed to be independent and are combined into a single optimization objective by taking their product, i.e. the ce-method provides a generic monte-carlo optimization framework for solving hard combinatorial problems previously, it was utilized for solving the sentence subset selection problem to this end, the ce-method gets as an input q(q,d), a constraint on maximum summary length l and an optional pseudo-reference summary sl, whose usage will be explained later on.\nyet, at the second step, similar to ces, the primary goal is actually to produce a focused summary (with maximum length limit lmax).\nto this end, on the first saliency-driven step, for dual-ces, we fixed the (strict) upper bound limit on summary length to l dual-ces-a, on the other hand, adaptively adjusts such length limit and was initialized with lt0 both variants were further set with a summary limit lmax for their second focus-driven step.\ndual-ces then utilizes such salient words for better selection of salient sentences within its second step of focused summary production.\ndual-ces was shown to better handle the tradeoff between saliency and focus, providing the best summarization quality compared to other alternative stateof-the-art unsupervised summarizers.", "1020": "we tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems.\nin recent years we are experiencing a dramatic improvement of the synthesized speech quality in tts systems, with the introduction of systems that are based on neural networks (nn).\na major improvement in quality was achieved by using attention based models such as tacotron and by replacing vocoders with a nn based waveform generators such as wavenet a useful feature of systems with trainable models is the ability to adapt the tts to an unseen voice using a small amount of training data (from a few seconds to an hour of speech).\nin this paper we show that we can get a considerable quality improvement by modifying a tts system that produced the world vocoder parameters to predict parameters for lpcnet as in the previous work 6, we conduct multiple adaptation experiments, applied on multiple vctk voices and show that the new system has much better quality and similarity to the target voices but can still run much faster than real-time in a single-cpu mode.\nthe input features, derived from the tts front end, are comprised of 1-hot coded categorical features and standard positional features in this architecture the prosody adaptation to unseen speaker is based on a variational auto encoder (vae) utterance prosody embedding, averaged over all the speaker utterances 6, as presented on figure in the current work we used multi-speaker baseline models for prosody adaptation to unseen voices, as it resulted in better quality than the single speaker models.\nthe final parameters which we use as input for the lpcnet are found by solving the maximum likelihood parameter generation (mlpg) equations we also apply a formant enhancement filter on the cepstral coefficients ck, k1n to compensate for the nn averaging and to improve the speech quality similar to the enhancement starts by multiplication of the high-order coefficients: c k c k kk ck kk (1) we choose and k2.\nthe networks for the acoustic features and the lpcnet where adapted from the corresponding, same gender networks that where trained in section the prosody network was adapted from a multi-speaker baseline model (that was originally trained on high-quality voices and vctk voices).\nwe tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems.", "1021": "is a standard three-layer relu dropout network with no biases, i.e. for instance, the hilbert-schmidt independence criterion (hsic) uses the maximum mean discrepancy (mmd) to assess the dependency between two variables, i.e. hsic(x,y ) mmd(pxy, pxpy), which can be easily estimated from samples via kernel mean embeddings in a reproducing kernel hilbert space (rkhs) in this paper we introduce the sobolev independence criterion (sic), a form of gradient regularized integral probability metric (ipm) between the joint distribution and the product of marginals.\ninstead of the usual kl divergence, the metric d with its witness function, or critic, f(x, y) measures the distance between the joint pxy and the product of marginals pxpy with this generalized definition of mutual information, the feature selection problem can be formalized as finding a sparse selector or gate w rdx such thatd(pw x,y, pw xpy) is maximal , i.e. supw,w0sd(pw x,y, pw xpy), where is a pointwise multiplication and w0 jwj this problem can be written in the following penalized form: (p) : sup w sup ff epxyf(w x, y) epxpyf(w x, y) w0 we can relabel f(x, y) f(w x, y) and write (p) as: supff epxy f(x, y)epxpy f(x, y), where f f f(x, y) f(w x, y)f f , w0 s.\nhence, we can reformulate the problem (p) as follows: (sic): sup ff epxyf(x, y) epxpyf(x, y) ps(f), where ps(f) is a penalty that controls the sparsity of the gradient of the witness function f on the support of the measures.\nthen, given samples (xi, yi), i 1, , n from the joint probability distribution pxy and iid samples (xi, yi), i 1, , n from pxpy , sic can be estimated as follows: sic(l1)2(pxy, pxpy) sup ff n n i1 f(xi, yi) n n i1 f(xi, yi) ( s(f) )2 n n i1 f2(xi, yi), where s(f) dx j1 n n i1 f(xi,yi)xj throughout this paper we consider feature selection only on x since y is thought of as the response.\nlet (u, ) be the limit defined in theorem we have that sic(l1)2(pxy, pxpy) ( epxyf(x, y) epxpyf(x, y) ) dx j1 epxpy f(x, y) xj epxpyf,2(x, y) f2f moreover, epxpy f(x,y) xj js,l1(f) and dx j1 j the terms j can be seen as quantifying how much dependency as measured by sic can be explained by a coordinate j.\nwe apply hrt-sic on a shortlist of pre-selected features per their ranking of j knockoffs work by finding control variables called knockoffs x that mimic the behavior of the real features x and provably control the fdr we use here gaussian knockoffs and train sic on the concatenation of x, x, i.e we train sic(x; x, y ) and obtain that has now twice the dimension dx, i.e for each real feature j, there is the real importance score j and the knockoff importance score jdx knockoffs-sic consists in using the statistics wj j jdx and the knockoff filter to select features based on the sign of wj (see alg.\nsimilarly block coordinate descent (bcd) using first order methods as given in algorithms and (in appendix): gradient descent on u and mirror descent on (in order to satisfy the simplex constraint 22) are also known to be globally convergent (theo in 41.) algorithm alternating optimization inputs: ,, , , initialize j 1dx ,j, (pxy) (pxpy) for i .maxiter do u( dx j1 dj(pxpy) j c(pxpy) im )1 j u,dj(pxpy)udx k1 u,dk(pxpy)u end for output: u, algorithm block coordinate descent inputs: ,, , , , (learning rates), initialize j 1dx ,j , softmax(z) ez/ dx j1 e zj for i .maxiter do gradient step u: u u l(u,)u mirror descent : logit log() l(u,) softmax(logit) stable implementation of softmax end for output: u, algorithm (non convex) neural sic(x,y ) (stochastic bcd ) inputs: x,y dataset x rndx , y rndy , such that (xi xi,., yi yi,.) pxy hyperparameters: ,, , , , (learning rates) initialize j 1dx ,j , softmax(z) ez/ dx j1 e zj for iter .maxiter do fetch a minibatch of size n (xi, yi) pxy fetch a minibatch of size n (xi, yi) pxpy yi obtained by permuting rows of y stochastic gradient step on : l(f,) we use adam mirror descent : logit log() l(f,) softmax(logit) stable implementation of softmax end for output: f, algorithm hrt with sic (x,y ) inputs: dtrain (xtr, ytr) , a heldout set dholdout (x,y ), features cutoff k sic: (f , ) sic(dtrain) alg.\nsj,r mean(f(x, y )) score of witness function on the null end for pj r1 ( r r1 1srjs ) end for discoveries bh(p,targetfdr) benjaminihochberg procedure output: discoveries algorithm model-x knockoffs fdr control with sic inputs: dtrain (xtr, ytr) , model-x knockoff features x modelx(xtr), target fdr q train sic: (f , ) sic(xtr, x, y ), alg.\nit follows that j epxpy f (x,y)xj k epxpy f (x,y)xk note that we have epxyf (x, y) epxpyf (x, y) , u u, dx j1 dj(pxpy) j, c(pxpy) im u dx j1 epxpy f (x, y) xj epxpyf,2 (x, y) f 2f sic(l1)2, epxyf (x, y) epxpyf (x, y) ( dx j1 epxpy f (x, y) xj epxpyf,2 (x, y) f 2f ) dx j1 epxpy f (x, y) xj epxpyf,2 (x, y) f 2f ( epxyf (x, y) epxpyf (x, y) ) we conclude by taking the holdout randomization test (hrt) is a principled method to produce valid p-values for each feature, that enables the control over the false discovery of a predictive model the p-value associated to each feature xj essentially quantifies the result of a conditional independence test with the null hypothesis stating that xj is independent of the output y, conditioned on all the remaining features xj (x1, this in practice requires the availability of an estimate of the complete conditional of each feature xj , i.e. hrt then samples the values of xj from this conditional distribution to obtain the p-value associated to it."}