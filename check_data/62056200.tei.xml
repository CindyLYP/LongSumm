<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interaction Networks for Learning about Objects, Relations and Physics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-12-01">1 Dec 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
							<email>peterbattaglia@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google DeepMind London</orgName>
								<address>
									<postCode>N1C 4AG</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind London</orgName>
								<address>
									<postCode>N1C 4AG</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
							<email>matthewlai@google.com</email>
							<affiliation key="aff2">
								<orgName type="department">Google DeepMind London</orgName>
								<address>
									<postCode>N1C 4AG</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
							<email>danilor@google.com</email>
							<affiliation key="aff3">
								<orgName type="department">Google DeepMind London</orgName>
								<address>
									<postCode>N1C 4AG</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
							<email>korayk@google.com</email>
							<affiliation key="aff4">
								<orgName type="department">Google DeepMind London</orgName>
								<address>
									<postCode>N1C 4AG</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interaction Networks for Learning about Objects, Relations and Physics</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-12-01">1 Dec 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1612.00222v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object-and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representing and reasoning about objects, relations and physics is a "core" domain of human common sense knowledge <ref type="bibr" target="#b25">[26]</ref>, and among the most basic and important aspects of intelligence <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15]</ref>. Many everyday problems, such as predicting what will happen next in physical environments or inferring underlying properties of complex scenes, are challenging because their elements can be composed in combinatorially many possible arrangements. People can nevertheless solve such problems by decomposing the scenario into distinct objects and relations, and reasoning about the consequences of their interactions and dynamics. Here we introduce the interaction network -a model that can perform an analogous form of reasoning about objects and relations in complex systems.</p><p>Interaction networks combine three powerful approaches: structured models, simulation, and deep learning. Structured models <ref type="bibr" target="#b6">[7]</ref> can exploit rich, explicit knowledge of relations among objects, independent of the objects themselves, which supports general-purpose reasoning across diverse contexts. Simulation is an effective method for approximating dynamical systems, predicting how the elements in a complex system are influenced by interactions with one another, and by the dynamics of the system. Deep learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16]</ref> couples generic architectures with efficient optimization algorithms to provide highly scalable learning and inference in challenging real-world settings.  as input, reasons about their interactions, and applies the effects and physical dynamics to predict new states. b. For more complex systems, the model takes as input a graph that represents a system of objects, oj, and relations, i, j, r k k , instantiates the pairwise interaction terms, b k , and computes their effects, e k , via a relational model, fR <ref type="bibr">(•)</ref>. The e k are then aggregated and combined with the oj and external effects, xj, to generate input (as cj), for an object model, fO(•), which predicts how the interactions and dynamics influence the objects, p.</p><p>Interaction networks explicitly separate how they reason about relations from how they reason about objects, assigning each task to distinct models which are: fundamentally object-and relation-centric; and independent of the observation modality and task specification (see Model section 2 below and <ref type="figure" target="#fig_1">Fig. 1a</ref>). This lets interaction networks automatically generalize their learning across variable numbers of arbitrarily ordered objects and relations, and also recompose their knowledge of entities and interactions in novel and combinatorially many ways. They take relations as explicit input, allowing them to selectively process different potential interactions for different input data, rather than being forced to consider every possible interaction or those imposed by a fixed architecture.</p><p>We evaluate interaction networks by testing their ability to make predictions and inferences about various physical systems, including n-body problems, and rigid-body collision, and non-rigid dynamics. Our interaction networks learn to capture the complex interactions that can be used to predict future states and abstract physical properties, such as energy. We show that they can roll out thousands of realistic future state predictions, even when trained only on single-step predictions. We also explore how they generalize to novel systems with different numbers and configurations of elements. Though they are not restricted to physical reasoning, the interaction networks used here represent the first general-purpose learnable physics engine, and even have the potential to learn novel physical systems for which no physics engines currently exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Our model draws inspiration from previous work that reasons about graphs and relations using neural networks. The "graph neural network" <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b18">19]</ref> is a framework that shares learning across nodes and edges, the "recursive autoencoder" <ref type="bibr" target="#b24">[25]</ref> adapts its processing architecture to exploit an input parse tree, the "neural programmer-interpreter" <ref type="bibr" target="#b21">[22]</ref> is a composable network that mimics the execution trace of a program, and the "spatial transformer" <ref type="bibr" target="#b10">[11]</ref> learns to dynamically modify network connectivity to capture certain types of interactions. Others have explored deep learning of logical and arithmetic relations <ref type="bibr" target="#b26">[27]</ref>, and relations suitable for visual question-answering <ref type="bibr" target="#b0">[1]</ref>.</p><p>The behavior of our model is similar in spirit to a physical simulation engine <ref type="bibr" target="#b1">[2]</ref>, which generates sequences of states by repeatedly applying rules that approximate the effects of physical interactions and dynamics on objects over time. The interaction rules are relation-centric, operating on two or more objects that are interacting, and the dynamics rules are object-centric, operating on individual objects and the aggregated effects of the interactions they participate in. A similar approach has been developed independently by Chang et al. <ref type="bibr" target="#b3">[4]</ref>.</p><p>Previous AI work on physical reasoning explored commonsense knowledge, qualitative representations, and simulation techniques for approximating physical prediction and inference <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9]</ref> . The "NeuroAnimator" <ref type="bibr" target="#b7">[8]</ref> was perhaps the first quantitative approach to learning physical dynamics, by training neural networks to predict and control the state of articulated bodies. Ladický et al. <ref type="bibr" target="#b13">[14]</ref> recently used regression forests to learn fluid dynamics. Recent advances in convolutional neural networks (CNNs) have led to efforts that learn to predict coarse-grained physical dynamics from images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Notably, Fragkiadaki et al. <ref type="bibr" target="#b5">[6]</ref> used CNNs to predict and control a moving ball from an image centered at its coordinates. Mottaghi et al. <ref type="bibr" target="#b20">[21]</ref> trained CNNs to predict the 3D trajectory of an object after an external impulse is applied. Wu et al. <ref type="bibr" target="#b29">[30]</ref> used CNNs to parse objects from images, which were then input to a physics engine that supported prediction and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Definition To describe our model, we use physical reasoning as an example <ref type="figure" target="#fig_1">(Fig. 1a)</ref>, and build from a simple model to the full interaction network <ref type="bibr">(</ref> </p><formula xml:id="formula_0">e t+1 = f R (o 1,t , o 2,t , r) o 2,t+1 = f O (o 2,t , e t+1 )</formula><p>The above formulation can be expanded to larger and more complex systems by representing them as a graph, G = O, R , where the nodes, O, correspond to the objects, and the edges, R, to the relations (see <ref type="figure" target="#fig_1">Fig. 1b</ref>). We assume an attributed, directed multigraph because the relations have attributes, and there can be multiple distinct relations between two objects (e.g., rigid and magnetic interactions). For a system with N O objects and N R relations, the inputs to the IN are,</p><formula xml:id="formula_1">O = {o j } j=1...N O , R = { i, j, r k k } k=1...N R where i = j, 1 ≤ i, j ≤ N O , X = {x j } j=1...N O</formula><p>The O represents the states of each object. The triplet, i, j, r k k , represents the k-th relation in the system, from sender, o i , to receiver, o j , with relation attribute, r k . The X represents external effects, such as active control inputs or gravitational acceleration, which we define as not being part of the system, and which are applied to each object separately.</p><p>The basic IN is defined as,</p><formula xml:id="formula_2">IN(G) = φ O (a(G, X, φ R ( m(G) ) )) (1) m(G) = B = {b k } k=1...N R f R (b k ) = e k φ R (B) = E = {e k } k=1...N R a(G, X, E) = C = {c j } j=1...N O f O (c j ) = p j φ O (C) = P = {p j } j=1...N O<label>(2)</label></formula><p>The marshalling function, m, rearranges the objects and relations into interaction terms, b k = o i , o j , r k ∈ B, one per relation, which correspond to each interaction's receiver, sender, and relation attributes. The relational model, φ R , predicts the effect of each interaction, e k ∈ E, by applying f R to each b k . The aggregation function, a, collects all effects, e k ∈ E, that apply to each receiver object, merges them, and combines them with O and X to form a set of object model inputs, c j ∈ C, one per object. The object model, φ O , predicts how the interactions and dynamics influence the objects by applying f O to each c j , and returning the results, p j ∈ P . This basic IN can predict the evolution of states in a dynamical system -for physical simulation, P may equal the future states of the objects, O t+1 .</p><p>The IN can also be augmented with an additional component to make abstract inferences about the system. The p j ∈ P , rather than serving as output, can be combined by another aggregation function, g, and input to an abstraction model, φ A , which returns a single output, q, for the whole system. We explore this variant in our final experiments that use the IN to predict potential energy.</p><p>An IN applies the same f R and f O to every b k and c j , respectively, which makes their relational and object reasoning able to handle variable numbers of arbitrarily ordered objects and relations. But one additional constraint must be satisfied to maintain this: the a function must be commutative and associative over the objects and relations. Using summation within a to merge the elements of E into C satisfies this, but division would not.</p><p>Here we focus on binary relations, which means there is one interaction term per relation, but another option is to have the interactions correspond to n-th order relations by combining n senders in each b k . The interactions could even have variable order, where each b k includes all sender objects that interact with a receiver, but would require a f R than can handle variable-length inputs. These possibilities are beyond the scope of this work, but are interesting future directions.</p><p>Implementation The general definition of the IN in the previous section is agnostic to the choice of functions and algorithms, but we now outline a learnable implementation capable of reasoning about complex systems with nonlinear relations and dynamics. We use standard deep neural network building blocks, multilayer perceptrons (MLP), matrix operations, etc., which can be trained efficiently from data using gradient-based optimization, such as stochastic gradient descent.</p><p>We and</p><formula xml:id="formula_3">R s = 1 0 0 0 0 1 . The X is a D X × N O matrix,</formula><p>whose columns are D X -length vectors that represent the external effect applied each of the N O objects.</p><p>The marshalling function, m, computes the matrix products, OR r and OR s , and concatenates them with</p><formula xml:id="formula_4">R a : m(G) = [OR r ; OR s ; R a ] = B . The resulting B is a (2D S + D R ) × N R matrix,</formula><p>whose columns represent the interaction terms, b k , for the N R relations (we denote vertical and horizontal matrix concatenation with a semicolon and comma, respectively). The way m constructs interaction terms can be modified, as described in our Experiments section (3).</p><p>The B is input to φ R , which applies f R , an MLP, to each column. The output of f R is a D E -length vector, e k , a distributed representation of the effects. The φ R concatenates the N R effects to form the</p><formula xml:id="formula_5">D E × N R effect matrix, E.</formula><p>The G, X, and E are input to a, which computes the D E × N O matrix product,Ē = ER T r , whose j-th column is equivalent to the elementwise sum across all e k whose corresponding relation has receiver object, j. TheĒ is concatenated with O and X: a(G, X, E) = [O; X;Ē] = C. The resulting C is a (D S + D X + D E ) × N O matrix, whose N O columns represent the object states, external effects, and per-object aggregate interaction effects.</p><p>The C is input to φ O , which applies f O , another MLP, to each of the N O columns. The output of f O is a D P -length vector, p j , and φ O concatenates them to form the output matrix, P .</p><p>To infer abstract properties of a system, an additional φ A is appended and takes P as input. The g aggregation function performs an elementwise sum across the columns of P to return a D P -length vector,P . TheP is input to φ A , another MLP, which returns a D A -length vector, q, that represents an abstract, global property of the system.</p><p>Training an IN requires optimizing an objective function over the learnable parameters of φ R and φ O . Note, m and a involve matrix operations that do not contain learnable parameters.</p><p>Because φ R and φ O are shared across all relations and objects, respectively, training them is statistically efficient. This is similar to CNNs, which are very efficient due to their weight-sharing scheme. A CNN treats a local neighborhood of pixels as related, interacting entities: each pixel is effectively a receiver object and its neighboring pixels are senders. The convolution operator is analogous to φ R , where f R is the local linear/nonlinear kernel applied to each neighborhood. Skip connections, recently popularized by residual networks, are loosely analogous to how the IN inputs O to both φ R and φ O , though in CNNs relation-and object-centric reasoning are not delineated. But because CNNs exploit local interactions in a fixed way which is well-suited to the specific topology of images, capturing longer-range dependencies requires either broad, insensitive convolution kernels, or deep stacks of layers, in order to implement sufficiently large receptive fields. The IN avoids this restriction by being able to process arbitrary neighborhoods that are explicitly specified by the R input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Physical reasoning tasks Our experiments explored two types of physical reasoning tasks: predicting future states of a system, and estimating their abstract properties, specifically potential energy. We evaluated the IN's ability to learn to make these judgments in three complex physical domains: n-body systems; balls bouncing in a box; and strings composed of springs that collide with rigid objects. We simulated the 2D trajectories of the elements of these systems with a physics engine, and recorded their sequences of states. See the Supplementary Material for full details.</p><p>In the n-body domain, such as solar systems, all n bodies exert distance-and mass-dependent gravitational forces on each other, so there were n(n − 1) relations input to our model. Across simulations, the objects' masses varied, while all other fixed attributes were held constant. The training scenes always included 6 bodies, and for testing we used 3, 6, and 12 bodies. In half of the systems, bodies were initialized with velocities that would cause stable orbits, if not for the interactions with other objects; the other half had random velocities.</p><p>In the bouncing balls domain, moving balls could collide with each other and with static walls. The walls were represented as objects whose shape attribute represented a rectangle, and whose inverse-mass was 0. The relations input to the model were between the n objects (which included the walls), for (n(n − 1) relations). Collisions are more difficult to simulate than gravitational forces, and the data distribution was much more challenging: each ball participated in a collision on less than 1% of the steps, following straight-line motion at all other times. The model thus had to learn that despite there being a rigid relation between two objects, they only had meaningful collision interactions when they were in contact. We also varied more of the object attributes -shape, scale and mass (as before) -as well as the coefficient of restitution, which was a relation attribute. Training scenes contained 6 balls inside a box with 4 variably sized walls, and test scenes contained either 3, 6, or 9 balls.</p><p>The string domain used two types of relations (indicated in r k ), relation structures that were more sparse and specific than all-to-all, as well as variable external effects. Each scene contained a string, comprised of masses connected by springs, and a static, rigid circle positioned below the string. The n masses had spring relations with their immediate neighbors (2(n − 1)), and all masses had rigid relations with the rigid object (2n). Gravitational acceleration, with a magnitude that was varied across simulation runs, was applied so that the string always fell, usually colliding with the static object. The gravitational acceleration was an external input (not to be confused with the gravitational attraction relations in the n-body experiments). Each training scene contained a string with 15 point masses, and test scenes contained either 5, 15, or 30 mass strings. In training, one of the point masses at the end of the string, chosen at random, was always held static, as if pinned to the wall, while the other masses were free to move. In the test conditions, we also included strings that had both ends pinned, and no ends pinned, to evaluate generalization.</p><p>Our model takes as input the state of each system, G, decomposed into the objects, O (e.g., n-body objects, balls, walls, points masses that represented string elements), and their physical relations, R (e.g., gravitational attraction, collisions, springs), as well as the external effects, X (e.g., gravitational acceleration). Each object state, o j , could be further divided into a dynamic state component (e.g., position and velocity) and a static attribute component (e.g., mass, size, shape). The relation attributes, R a , represented quantities such as the coefficient of restitution, and spring constant. The input represented the system at the current time. The prediction experiment's target outputs were the velocities of the objects on the subsequent time step, and the energy estimation experiment's targets were the potential energies of the system on the current time step. We also generated multi-step rollouts for the prediction experiments <ref type="figure" target="#fig_3">(Fig. 2)</ref>, to assess the model's effectiveness at creating visually realistic simulations. The output velocity, v t , on time step t became the input velocity on t + 1, and the position at t + 1 was updated by the predicted velocity at t.</p><p>Data Each of the training, validation, test data sets were generated by simulating 2000 scenes over 1000 time steps, and randomly sampling 1 million, 200k, and 200k one-step input/target pairs, respectively. The model was trained for 2000 epochs, randomly shuffling the data indices between each. We used mini-batches of 100, and balanced their data distributions so the targets had similar per-element statistics. The performance reported in the Results was measured on held-out test data.</p><p>We explored adding a small amount of Gaussian noise to 20% of the data's input positions and velocities during the initial phase of training, which was reduced to 0% from epochs 50 to 250. The noise std. dev. was 0.05× the std. dev. of each element's values across the dataset. It allowed the model to experience physically impossible states which could not have been generated by the physics engine, and learn to project them back to nearby, possible states. Our error measure did not reflect clear differences with or without noise, but rollouts from models trained with noise were slightly more visually realistic, and static objects were less subject to drift over many steps.</p><p>Model architecture The f R and f O MLPs contained multiple hidden layers of linear transforms plus biases, followed by rectified linear units (ReLUs), and an output layer that was a linear transform plus bias. The best model architecture was selected by a grid search over layer sizes and depths. All inputs (except R r and R s ) were normalized by centering at the median and rescaling the 5th and 95th percentiles to -1 and 1. All training objectives and test measures used mean squared error (MSE) between the model's prediction and the ground truth target.</p><p>All prediction experiments used the same architecture, with parameters selected by a hyperparameter search. The f R MLP had four, 150-length hidden layers, and output length D E = 50. The f O MLP had one, 100-length hidden layer, and output length D P = 2, which targeted the x, y-velocity. The m and a were customized so that the model was invariant to the absolute positions of objects in the scene. The m concatenated three terms for each b k : the difference vector between the dynamic states of the receiver and sender, the concatenated receiver and sender attribute vectors, and the relation attribute vector. The a only outputs the velocities, not the positions, for input to φ O .</p><p>The energy estimation experiments used the IN from the prediction experiments with an additional φ A MLP which had one, 25-length hidden layer. Its P inputs' columns were length D P = 10, and its output length was D A = 1.</p><p>We optimized the parameters using Adam <ref type="bibr" target="#b12">[13]</ref>, with a waterfall schedule that began with a learning rate of 0.001 and down-scaled the learning rate by 0.8 each time the validation error, estimated over a window of 40 epochs, stopped decreasing.</p><p>Two forms of L2 regularization were explored: one applied to the effects, E, and another to the model parameters. Regularizing E improved generalization to different numbers of objects and reduced drift over many rollout steps. It likely incentivizes sparser communication between the φ R and φ O , prompting them to operate more independently. Regularizing the parameters generally improved performance and reduced overfitting. Both penalty factors were selected by a grid search.</p><p>Few competing models are available in the literature to compare our model against, but we considered several alternatives: a constant velocity baseline which output the input velocity; an MLP baseline, with two 300-length hidden layers, which took as input a flattened vector of all of the input data; and a variant of the IN with the φ R component removed (the interaction effects, E, was set to a 0-matrix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Prediction experiments Our results show that the IN can predict the next-step dynamics of our task domains very accurately after training, with orders of magnitude lower test error than the alternative models ( <ref type="figure">Fig. 3a, d</ref> and g, and <ref type="table" target="#tab_1">Table 2</ref>). Because the dynamics of each domain depended crucially on interactions among objects, the IN was able to learn to exploit these relationships for its predictions. The dynamics-only IN had no mechanism for processing interactions, and performed similarly to the constant velocity model. The baseline MLP's connectivity makes it possible, in principle, for it to learn the interactions, but that would require learning how to use the relation indices to selectively process the interactions. It would also not benefit from sharing its learning across relations and objects, instead being forced to approximate the interactive dynamics in parallel for each objects.</p><p>The IN also generalized well to systems with fewer and greater numbers of objects <ref type="figure">(Figs. 3b-c</ref>, e-f and h-k, and <ref type="table">Table SM1</ref> in Supp. Mat.). For each domain, we selected the best IN model from the system size on which it was trained, and evaluated its MSE on a different system size. When tested on smaller n-body and spring systems from those on which it was trained, its performance actually exceeded a model trained on the smaller system. This may be due to the model's ability to exploit its greater experience with how objects and relations behave, available in the more complex system.</p><p>We also found that the IN trained on single-step predictions can be used to simulate trajectories over thousands of steps very effectively, often tracking the ground truth closely, especially in the n-body and string domains. When rendered into images and videos, the model-generated trajectories are each spanning 1000 rollout steps. Columns 1-2 are ground truth and model predictions for n-body systems, 3-4 are bouncing balls, and 5-6 are strings. Each model column was generated by a single model, trained on the underlying states of a system of the size in the top panel. The middle and bottom panels show its generalization to systems of different sizes and structure. For n-body, the training was on 6 bodies, and generalization was to 3 and 12 bodies. For balls, the training was on 6 balls, and generalization was to 3 and 9 balls. For strings, the training was on 15 masses with 1 end pinned, and generalization was to 30 masses with 0 and 2 ends pinned. The URLs to the full videos of each rollout are in <ref type="table">Table 1.</ref> usually visually indistinguishable from those of the ground truth physics engine ( <ref type="figure" target="#fig_3">Fig. 2</ref>; see <ref type="table">Table 1</ref> for URLs to the full videos of each rollout). This is not to say that given the same initial conditions, they cohere perfectly: the dynamics are highly nonlinear and imperceptible prediction errors by the model can rapidly lead to large differences in the systems' states. But the incoherent rollouts do not violate people's expectations, and might be roughly on par with people's understanding of these domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating abstract properties</head><p>We trained an abstract-estimation variant of our model to predict potential energies in the n-body and string domains (the ball domain's potential energies were always 0), and found it was much more accurate (n-body MSE 1.4, string MSE 1.1) than the MLP baseline (n-body MSE 19, string MSE 425). The IN presumably learns the gravitational and spring potential energy functions, applies them to the relations in their respective domains, and combines the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We introduced interaction networks as a flexible and efficient model for explicit reasoning about objects and relations in complex systems. Our results provide surprisingly strong evidence of their ability to learn accurate physical simulations and generalize their training to novel systems with different numbers and configurations of objects and relations. They could also learn to infer abstract properties of physical systems, such as potential energy. The alternative models we tested performed much more poorly, with orders of magnitude greater error. Simulation over rich mental models is thought to be a crucial mechanism of how humans reason about physics and other complex domains <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10]</ref>, and Battaglia et al. <ref type="bibr" target="#b2">[3]</ref> recently posited a simulation-based "intuitive physics engine" model to explain human physical scene understanding. Our interaction network implementation is the first learnable physics engine that can scale up to real-world problems, and is a promising template for new AI approaches to reasoning about other physical and mechanical systems, scene understanding, social perception, hierarchical planning, and analogical reasoning.</p><p>In the future, it will be important to develop techniques that allow interaction networks to handle very large systems with many interactions, such as by culling interaction computations that will have negligible effects. The interaction network may also serve as a powerful model for model-predictive control inputting active control signals as external effects -because it is differentiable, it naturally supports gradient-based planning. It will also be important to prepend a perceptual front-end that can infer from objects and relations raw observations, which can then be provided as input to an interaction network that can reason about the underlying structure of a scene. By adapting the interaction network into a recurrent neural network, even more accurate long-term predictions might be possible, though preliminary tests found little benefit beyond its already-strong performance. By modifying the interaction network to be a probabilistic generative model, it may also support probabilistic inference over unknown object properties and relations.</p><p>By combining three powerful tools from the modern machine learning toolkit -relational reasoning over structured knowledge, simulation, and deep learning -interaction networks offer flexible, accurate, and efficient learning and inference in challenging domains. Decomposing complex systems into objects and relations, and reasoning about them explicitly, provides for combinatorial generalization to novel contexts, one of the most important future challenges for AI, and a crucial step toward closing the gap between how humans and machines think.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary material</head><p>A.1 Experimental details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Physics engine details</head><p>Every simulated trajectory contained the states of the objects in the system on each frame of a sequence of 1000 one-millisecond time steps. The parameters of each system were chosen so that a diverse set of dynamics could unfold within the trajectory. On each step, the physics engine took as input the current state of the system, calculated the forces associated with each inter-entity interaction, and applied them to the individual entities as accelerations by dividing by the entity's mass, parameterized as inverse-mass, a = F m −1 in both the engine and model input. This also allows static objects to be represented as having m −1 = 0. The previous positions and velocities, and newly computed accelerations, were input to an Euler integrator to update the current velocities and positions of the entities. By using a custom engine we were able to have perfect control over the details of the simulation, and use one engine for all physical domains. It produced trajectory rollouts that were indistinguishable from those of an off-the-shelf simulation engine, but was highly efficient because it could perform thousands of runs in parallel on a GPU, allowing millions of simulated steps to be generated each second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Physical domains</head><p>n-body All objects in n-body systems exerted gravitational forces on each other, which were a function of the objects' pairwise distances and masses, giving rise to very complex dynamics that are highly sensitive to initial conditions. Across simulation runs, the objects' masses varied, while all other non-dynamic variables were held constant. The gravitational constant was set to ensure that objects could move across several hundred meters within the 1000 step rollout. The training scenes always included 6 objects, and for testing we used 3, 6, and 12 objects. The masses were uniformly sampled from [0.02, 9] kg, their shapes were points, and their initial positions were randomly sampled from all angles, with a distance in [10, 100] m. We included two classes of scenes. The first, orbit systems, had one object (the star), initialized at position (0, 0), with zero velocity and a mass of 100 kg. The planets' velocities were initialized such that they would have stable orbits around the star, if not for their interactions with other planets. The second, non-orbit systems, sampled initial xand y-velocity components from [−3, 3] m/s. The objects would typically begin attracting, and gave rise to complex semi-periodic behavior.</p><p>An n-body system is a highly nonlinear (chaotic) dynamical system, which means they are highly sensitive to initial conditions and extended predictions of their states are not possible under even small perturbations. The relations between the objects corresponded to gravitational attraction. Between simulation runs, the masses of the objects were varied, while all other non-dynamic variables were held constant (e.g., gravitational constant) or were not meaningful (e.g., object scales and shapes). The gravitational force from object i to j was computed as, Fij =</p><formula xml:id="formula_6">Gm i m j (x i −x j ) x i −x j 3</formula><p>, where G is the gravitational constant. The denominator was clipped so that forces could not go too high as the distances between objects went to zero. All forces applied to each object were summed to yield the per-object total forces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bouncing balls</head><p>The bouncing balls domain still had all-to-all object relations-collisions-and any object could collide with any other, including the walls. But colliding objects are more difficult to simulate than the gravitational interactions in n-body systems, and our bouncing balls domain also included more variability in object attributes, such as shape, scale, and mass (as before), as well as relation attributes, such as the coefficient of restitution. The data distribution was much more challenging: for more than 99% of the time steps, a ball was not in contact with any others, and its next-step velocity equaled its current velocity. For the remaining steps, however, collisions caused next-step velocities that was a complex function of its state and the state of the object it collides with. The training scene contained 6 balls inside a box with 4 walls, and test scenes contained either 3, 6, or 9 balls. The balls' radii were sampled uniformly from [0.1, 0.3] m, and masses from [0.75, 1.25] kg. The walls were static rectangular boxes, positioned so that the horizontal and vertical lengths of area they enclosed varied independently between <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>  Rigid body collision systems are highly nonlinear (chaotic) dynamical systems, as well. Collision forces were applied between objects when they began to interpenetrate, using two-step process: collision detection between all objects, and resolution of detected collisions. A detected collision between two objects meant that their shapes overlapped and their velocities were causing them to approach each other. To resolve a partially inelastic collision, the post-collision velocities of each object were computed, and forces appropriate to effect these velocities on the subsequent time step were then calculated and applied the the objects. This resulted in realistic bouncing ball trajectories. String Our string domain used multiple types of relations (springs and collisions), relation structures that were more sparse and specific than all-to-all, and variable external effects. Each scene contained one string, comprised of point masses connected by springs, and one static, rigid circle positioned below the string. Gravitational acceleration, varied across simulation runs, was applied so that the string always fell, usually colliding with the static object. Each training scene contained a string with 15 point masses, and test scenes contained either 5, 15, or 30 mass strings. In training, one of the point masses at the end of the string, chosen at random, was always held static, as if pinned to the wall, while the other masses were free to move. In the test conditions, we also included strings that had both ends pinned, and no ends pinned, to evaluate generalization. </p><formula xml:id="formula_7">= Cs(1 − L x i −x j )(xi − xj),</formula><p>where Cs is the spring constant and L is the spring's rest length. A damping factor, which was proportional to the difference in the objects' velocities, was also applied. Collision forces between the string's masses and the static, rigid object were calculated as described in the bouncing balls section above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Results details</head><p>The prediction experiment's generalization performance MSEs are shown <ref type="table" target="#tab_2">Table 3</ref>. The URL links for the videos associated with all still frames in <ref type="figure" target="#fig_3">Figure 2</ref> are in <ref type="table">Table 1.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Schematic of an interaction network. a. For physical reasoning, the model takes objects and relations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>define O as a D S × N O matrix, whose columns correspond to the objects' D S -length state vectors. The relations are a triplet, R = R r , R s , R a , where R r and R s are N O × N R binary matrices which index the receiver and sender objects, respectively, and R a is a D R × N R matrix whose D R -length columns represent the N R relations' attributes. The j-th column of R r is a one-hot vector which indicates the receiver object's index; R s indicates the sender similarly. For the graph inFig. 1b,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Prediction rollouts. Each column contains three panels of three video frames (with motion blur),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>m. The balls' initial positions were randomly sampled so that all balls fit within the box and did not interpenetrate any other object, with initial xand y-velocity components sampled uniformly from [−5, 5] m/s. The restitutions, an attribute of their collision relations, was sampled uniformly from [0.4, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The string's masses were sampled from [0.05, 0.15] kg. The springs' rest lengths were set to 0.2 m, spring constants to 100, and damping factors to 0.001. The static object's x-position was sampled from [−0.5, 0.5] m, y-position was sampled from [−1, −0.5] m, and radius from [0.2, 0.4] m. The string mass-rigid object coefficient of restitution was sampled from [0, 1]. The gravitational acceleration applied to all non-static objects was sampled from [−30, −5] m/s 2 .The spring force from object i to j was computed using Hooke's law as, F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Prediction experiment MSEs</figDesc><table><row><cell cols="4">Domain Constant velocity Baseline Dynamics-only IN</cell><cell>IN</cell></row><row><cell cols="2">n-body 82</cell><cell>79</cell><cell>76</cell><cell>0.25</cell></row><row><cell>Balls</cell><cell>0.074</cell><cell>0.072</cell><cell>0.074</cell><cell>0.0020</cell></row><row><cell>String</cell><cell>0.018</cell><cell>0.016</cell><cell>0.017</cell><cell>0.0011</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Prediction experiments -Generalization MSEs .0053 0.0019 0.0030 0.00066 0.0010 0.00097 IN (transfer) 0.052 7.8 0.0053 0.004 0.00052 0.00074 0.0014 0.00097</figDesc><table><row><cell></cell><cell cols="2">n-body</cell><cell cols="2">Balls</cell><cell></cell><cell cols="2">String</cell><cell></cell></row><row><cell>Model</cell><cell>3</cell><cell>12</cell><cell>3</cell><cell>9</cell><cell>5, 1</cell><cell>30, 1</cell><cell>15, 0</cell><cell>15, 2</cell></row><row><cell>Const. vel.</cell><cell>45</cell><cell cols="2">166 0.054</cell><cell>0.084</cell><cell>0.0080</cell><cell>0.010</cell><cell>0.020</cell><cell>0.012</cell></row><row><cell>IN (within)</cell><cell>0.21</cell><cell>5.2 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Physically based modeling: Rigid body simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baraff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Course Notes, ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pw Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="18327" to="18332" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Under Review</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The nature of explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J W</forename><surname>Craik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1943" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning visual predictive models of physics for playing billiards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic machine learning and artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="452" to="459" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neuroanimator: Fast neural network emulation and control of physics-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 25th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The naive physics manifesto</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Institut pour les études s é mantiques et cognitives</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
		<respStmt>
			<orgName>Université de Genève</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mechanical reasoning by mental simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hegarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TICS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="280" to="285" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mental models: towards a cognitive science of language, inference, and consciousness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Johnson-Laird</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data-driven fluid simulations using regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Solenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">199</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Building machines that learn and think like people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gershman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00289</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning physical intuition of block towers by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01312</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">To fall or not to fall: A visual approach to physical stability prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00066</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Newtonian image understanding: Unfolding the dynamics of objects in static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04048</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05600</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>what happens if...&quot; learning to predict the effect of forces in images</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Se Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Freitas</surname></persName>
		</author>
		<title level="m">Neural programmer-interpreters. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Origins of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Spelke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breinlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macomber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="605" to="632" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using matrices to model symbolic relationship</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<editor>D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou</editor>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1593" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How to grow a mind: Statistics, structure, and abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="issue">6022</biblScope>
			<biblScope unit="page">1279</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The psychology of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Winston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<publisher>McGraw-Hill</publisher>
			<biblScope unit="volume">73</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Galileo: Perceiving physical object properties by integrating a physics engine with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
