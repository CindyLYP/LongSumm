{"1005": "few-shot learning ( fsl) is a topic of rapidly growing interest. typically, in fsl a model is trained on a dataset consisting of many small tasks ( meta-tasks) and learns to adapt to novel tasks that it will encounter during test time. however, little attention has been given to explicitly optimizing the architectures for fsl, nor to an adaptation of the architecture at test time to particular novel tasks. in this work, we propose to employ tools inspired by the differentiable neural architecture search ( d-nas) literature in order to optimize the architecture for fsl without over-fitting. additionally, to make the architecture task adaptive, we propose the concept of met controller. these modules are added to the model and predict optimal network connections for a given novel task. using the proposed approach we observe state-of-art results on two popular few-shot benchmarks: miniimagenet and fc100. the architecture in task dependent manner to accommodate for novel tasks also has not been explored. in this work, we build our few-shot task-adaptive architecture search upon a technique from d-nas ( adversarial darts). our goal is to learn a neural network where connections are controllable and adapt to the few-shot task with novel categories. to make our architecture task adaptive so it would be able to quickly rewire for each new target task. to this end, we employ a set of small neural networks, met controllers, responsible for controlling the connections in the dag given the current task. the metadapt controllers adjust the weights of the different operations, such that if some operations are better for the current task they will get higher weights, thus, effectively modifying the architecture and adapting it to the task. in this work, we show how to learn a specialized backbone architecture that would facilitate this adaptability, as well as meta-learn to become capable of adapting that architecture itself to the task, thus going beyond the only adaptation of all previous metalearning approaches. in bf3s auxiliary self-supervision tasks are added, such as predicting image rotation or patch location. in robust-dist first an ensemble of up to models is learned, so each model by itself can not overfit the data. then, the the the final model is distilled from all those models. notably, our method which also deals with training large architecture without over-fitting is orthogonal to these two approaches. it is likely that further improvement can be achieved by combining these methods with our previous meta-learning methods. notably, in all previous meta-learning approaches, only the parameters of a ( fixed ) neural network are optimized in order to become adaptable ( or partially adaptable) to novel few-shots. by that, they managed to accelerate the search process. metadapt is a set of controllers that predict the task-adapted coefficients as a function of the current task. they are responsible for predicting , given a few-shot task, the best way of adapting the mixing coefficients ( ( i, j) o let ( i, j) be the vector of all ( i, j) o we use resnet9 followed by a single task-adaptable block with nodes ( v 4) in our experiments, resulting in about times more parameters compared with the original resnet12 ( due to large set of operations on all connections combined). the list of search space operations used in our experiments is provided in table this list includes the zero-operation and identity-operation that can fully or partially ( depending on the corresponding ( i, j) o) cut the connection or make it a residual one ( skip-connection) . each feature map xi in the block is connected to all previous maps by setting it to be : xi ji o ( j, i) ( xj) . in this paper, we propose a new method to train a large neural network with weighted connections that are then pruned to form the final chosen architecture. it has been shown, in the case of architecture search, that it is possible to learn an architecture on a smaller dataset, where the objective is to train a large neural network with weighted connections that are then pruned to form the final chosen architecture. in this paper , we propose a new approach to few-shot architecture search called metadapt. it is based on stochastic neural architecture search ( snas) where each edge is a weighted-sum of its operations according to i, j contrarily, in snas i, j are treated as probabilities of a multinomial distribution and at each iteration a single operation is sampled accordingly. so at each iteration only a single operation per edge affects the classification outcome and only this operation is be updated in the gradient descent backward step. a recent approach suggested for architecture search is stochastic neural architecture search ( snas 65) . metadapt is a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks. the proposed approach effectively applies tools from neural architecture search ( nas) literature in order to learn adaptive architectures. these tools help over-fitting to extremely small data of the few-shot tasks and mitigate the domain shift between the training set and the test set. we demonstrate that the proposed approach successfully improves state-of-the-art results on two popular few-shot benchmarks, miniimagenet and fc100, and carefully ablate the different optimization steps and design choices of the proposed approach. some interesting future work directions include extending the proposed approach to progressively searching the full network architecture ( instead of just the last block), applying the approach to other few-shot tasks such as detection and segmentation, and researching into different variants of task-adaptivity including global connections modifiers and inter block adaptive wiring. this is also referred to as meta-learning. another topic closely related to meta-learning with a lot of interest in the community is neural architecture search ( nas) , automatically finding optimal architecture instead of engineering it manually. additionally, to make the architecture task adaptive, we propose the concept of metadapt controllers. this paper proposes a new approach for few-shot classification that adapts the architecture at test-time to a specific novel task. in this approach, the architecture of the block is built from feature maps v xi that are linked by mixtures of operations. the input feature map to the block is x0 and its output is xv a mixed operation, o ( i, j) o ( i, j) o ( x) oo exp ( ( ( i, j) o) o ( x) oo exp ( ( ( i, j) o) o ( x) oo exp ( ( ( i, j) o) o ( x) oo exp ( ( i, j) o) o ( x) oo exp ( ( i, j) o) o ( x) oo exp ( ( ( i, j) o) o ( x) oo exp ( ( ( i, j) o) o ( x) oo exp ( ( i, j) o) o ( x) oo exp ( ( i, j) o) o ( x) oo exp ( ( i, j) o ( x) oo exp ( ( i, j) o ( x) oo exp ( in this work we have proposed metadapt, a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks. the proposed approach effectively applies tools from the neural architecture search ( nas) literature, extended with the concept of metadapt controllers, in order to learn adaptive architectures.", "1000": "###", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1006": "###", "1007": "###", "1008": "###", "1009": "###", "1010": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1018": "###", "1019": "###", "1020": "###", "1021": "###"}