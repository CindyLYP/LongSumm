<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Firmament: fast, centralized cluster scheduling at scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ionel</forename><surname>Gog</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge Computer Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gleave</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge Computer Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">N M</forename><surname>Watson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge Computer Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hand</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename><surname>Mit Csail</surname></persName>
						</author>
						<title level="a" type="main">Firmament: fast, centralized cluster scheduling at scale</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Centralized datacenter schedulers can make high-quality placement decisions when scheduling tasks in a cluster. Today, however, high-quality placements come at the cost of high latency at scale, which degrades response time for interactive tasks and reduces cluster utilization. This paper describes Firmament, a centralized scheduler that scales to over ten thousand machines at subsecond placement latency even though it continuously reschedules all tasks via a min-cost max-flow (MCMF) optimization. Firmament achieves low latency by using multiple MCMF algorithms, by solving the problem incrementally, and via problem-specific optimizations. Experiments with a Google workload trace from a 12,500-machine cluster show that Firmament improves placement latency by 20× over Quincy [22], a prior centralized scheduler using the same MCMF optimization. Moreover, even though Firmament is centralized, it matches the placement latency of distributed schedulers for workloads of short tasks. Finally, Firmament exceeds the placement quality of four widely-used centralized and distributed schedulers on a real-world cluster, and hence improves batch task response time by 6×. 100 12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many applications today run on large datacenter clusters <ref type="bibr" target="#b2">[3]</ref>. These clusters are shared by applications of many organizations and users <ref type="bibr">[6; 21; 35]</ref>. Users execute jobs, which each consist of one or more parallel tasks. The cluster scheduler decides how to place these tasks on cluster machines, where they are instantiated as processes, containers, or VMs.</p><p>Better task placements by the cluster scheduler lead to higher machine utilization <ref type="bibr" target="#b34">[35]</ref>, shorter batch job runtime, improved load balancing, more predictable application performance <ref type="bibr">[12; 36]</ref>, and increased fault tolerance <ref type="bibr" target="#b31">[32]</ref>. Achieving high task placement quality is hard: it requires algorithmically complex optimization in multiple dimensions. This goal conflicts with the need for a low placement latency, the time it takes the scheduler to place a new task. A low placement latency is required both to meet user expectations and to avoid idle cluster resources while there are waiting tasks. Shorter batch task runtimes and increasing cluster scale make it difficult to meet both conflicting goals <ref type="bibr">[9; 10; 13; 23; 29]</ref>. Current schedulers thus choose one to prioritize.</p><p>Three different cluster scheduler architectures exist today. First, centralized schedulers use elaborate algorithms to find high-quality placements <ref type="bibr">[11; 12; 35]</ref>, but have latencies of seconds or minutes <ref type="bibr">[13; 32]</ref>. Second, distributed schedulers use simple algorithms that allow for high throughput, low latency parallel task placement at scale <ref type="bibr">[13; 28; 29]</ref>. However, their uncoordinated decisions based on partial, stale state can result in poor placements. Third, hybrid schedulers split the workload across a centralized and a distributed component. They use sophisticated algorithms for long-running tasks, but rely on distributed placement for short tasks <ref type="bibr">[9; 10; 23]</ref>.</p><p>In this paper, we show that a centralized scheduler based on sophisticated algorithms can be fast and scalable for both current and future workloads. We built Firmament, a centralized scheduler that meets three goals:</p><p>1. to maintain the same high placement quality as an existing centralized scheduler (viz. Quincy <ref type="bibr" target="#b21">[22]</ref>); 2. to achieve sub-second task placement latency for all workloads in the common case; and 3. to cope well with demanding situations such as cluster oversubscription or large incoming jobs. Our key insight is that even centralized sophisticated algorithms for the scheduling problem can be fast (i) if they match the problem structure well, and (ii) if few changes to cluster state occur while the algorithm runs.</p><p>Firmament generalizes Quincy <ref type="bibr" target="#b21">[22]</ref>, which represents the scheduling problem as a min-cost max-flow (MCMF) optimization over a graph ( §3) and continuously reschedules the entire workload. Quincy's original MCMF algorithm results in task placement latencies of minutes on a large cluster. Firmament, however, achieves placement latencies of hundreds of milliseconds in the common case and reaches the same placement quality as Quincy.</p><p>To achieve this, we studied several MCMF optimization algorithms and their performance ( §4). Surprisingly, we found that relaxation <ref type="bibr" target="#b3">[4]</ref>, a seemingly inefficient MCMF algorithm, outperforms other algorithms on the graphs generated by the scheduling problem. However, relaxation can be slow in crucial edge cases, and we thus investigated three techniques to reduce Firmament's placement latency across different algorithms ( §5):</p><p>1. Terminating the MCMF algorithms early to find approximate solutions generates unacceptably poor and volatile placements, and we reject the idea. 2. Incremental re-optimization improves the runtime of Quincy's original MCMF algorithm (cost scaling <ref type="bibr" target="#b16">[17]</ref>), and makes it an acceptable fallback. 3. Problem-specific heuristics aid some MCMF algorithms to run faster on graphs of specific structure. We combined these algorithmic insights with several implementation-level techniques to further reduce Firmament's placement latency ( §6). Firmament runs two MCMF algorithms concurrently to avoid slowdown in edge cases; it implements an efficient graph update algorithm to handle cluster state changes; and it quickly extracts task placements from the computed optimal flow.</p><p>Our evaluation compares Firmament to existing distributed and centralized schedulers, both in simulation (using a Google workload trace) and on a local 40machine cluster ( §7). In our experiments, we find that Firmament scales well: even with 12,500 machines and 150,000 live tasks eligible for rescheduling, Firmament makes sub-second placements. This task placement latency is comparable to those of distributed schedulers, even though Firmament is centralized. When scheduling workloads that consist exclusively of short, sub-second tasks, Firmament scales to over 1,000 machines, but suffers overheads for task runtimes below 5s at 10,000 machines. Yet, we find that Firmament copes well with realistic, mixed workloads that combine long-running services and short tasks even at this scale: Firmament keeps up with a 250× accelerated Google workload. Finally, we show that Firmament's improved placement quality reduces short batch tasks' runtime by up to 6× compared to other schedulers on a real-world cluster.</p><p>Firmament is available as open-source software ( §9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Cluster managers such as Mesos <ref type="bibr" target="#b20">[21]</ref>, YARN <ref type="bibr" target="#b33">[34]</ref>, Borg <ref type="bibr" target="#b34">[35]</ref>, and Kubernetes <ref type="bibr" target="#b13">[14]</ref> automatically share and manage physical datacenter resources. Each one has a scheduler, which is responsible for placing tasks on machines. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the lifecycle of a task in a cluster manager: after the user submits the task, it waits until the scheduler places it on a machine where it sub- sequently runs. The time between submission and task placement is the task placement latency, and to the total time between the task's submission and its completion is the task response time. <ref type="bibr" target="#b0">1</ref> The time a task spends being actively scheduled is the scheduler's algorithm runtime.</p><p>For each task, the scheduling algorithm typically first performs a feasibility check to identify suitable machines, then scores them according to a preference order, and finally places the task on the best-scoring machine. Scoring, i.e., rating the different placement choices for a task, can be expensive. Google's Borg, for example, relies on several batching, caching, and approximation optimizations to keep scoring tractable <ref type="bibr">[35, §3.4]</ref>.</p><p>High placement quality increases cluster utilization and avoids performance degradation due to overcommit. Poor placement quality, by contrast, increases task response time (for batch tasks), or decreases applicationlevel performance (for long-running services).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task-by-task placement</head><p>Most cluster schedulers, whether centralized or distributed, are queue-based and process one task at a time (per scheduler). <ref type="figure" target="#fig_2">Figure 2a</ref> illustrates how such a queuebased scheduler processes a new task. The task first waits in a queue of unscheduled tasks until it is dequeued and processed by the scheduler. In a busy cluster, a task may spend substantial time enqueued. Some schedulers also have tasks wait in a per-machine "worker-side" queue <ref type="bibr" target="#b28">[29]</ref>, which allows for pipelined parallelism. Task-by-task placement has the advantage of being amenable to uncoordinated, parallel decisions in distributed schedulers <ref type="bibr">[9; 10; 13; 28]</ref>. On the other hand, processing one task at a time also has two crucial downsides: first, the scheduler commits to a placement early and restricts its choices for further waiting tasks, and second, there is limited opportunity to amortize work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Batching placement</head><p>Both downsides of task-by-task placement can be addressed by batching. Processing several tasks in a batch Task response time is primarily meaningful for batch tasks; longrunning service tasks' response times are conceptually infinite, and in practice are determined by failures and operational decisions.    <ref type="bibr" target="#b21">[22]</ref>'s approach scales poorly as cluster size grows. Simulation on subsets of the Google trace; boxes are 25 th , 50 th , and 75 th percentile delays, whiskers 1 st and 99 th , and a star indicates the maximum.</p><p>allows the scheduler to jointly consider their placement, and thus to find the best trade-off for the whole batch. A natural extension of this idea is to reconsider the entire existing workload ("rescheduling"), and to preempt and migrate running tasks if prudent.</p><p>Flow-based scheduling, introduced by Quincy <ref type="bibr" target="#b21">[22]</ref>, is an efficient batching technique. Flow-based scheduling uses a placement mechanism -min-cost max-flow (MCMF) optimization -with an attractive property: it guarantees overall optimal task placements for a given scheduling policy. <ref type="figure" target="#fig_2">Figure 2b</ref> illustrates how it proceeds. If a change to cluster state happens (e.g., task submission), the scheduler updates an internal graph representation of the scheduling problem. It waits for any running optimization to finish, and then runs a MCMF solver on the graph. This yields an optimal flow from which the scheduler extracts the task assignments.</p><p>However, <ref type="figure" target="#fig_3">Figure 3</ref> illustrates that Quincy, the current state-of-the-art flow-based scheduler, is too slow to meet our placement latency goal at scale. In this experiment, we replayed subsets of the public Google trace <ref type="bibr" target="#b29">[30]</ref>, which we augmented with locality preferences for batch  <ref type="figure">Figure 4</ref>: Firmament's scheduling policy modifies the flow network according to workload, cluster, and monitoring data; the network is passed to the MCMF solver, whose computed optimal flow yields task placements.</p><p>processing jobs 2 against our faithful reimplementation of Quincy's approach. We measured the scheduler algorithm runtime for clusters of increasing size with proportional workload growth. The algorithm runtime increases with scale, up to a median of 64s and a 99 th percentile of 83s for the full Google cluster (12,500 machines). During this time, the scheduler must wait for the solver to finish, and cannot choose any placements for new tasks. The goal of this paper is to build a flow-based scheduler that achieves equal placement quality to Quincy, but which does so at sub-second placement latency. As our experiment illustrates, we must achieve at least an orderof-magnitude speedup over Quincy to meet this goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Firmament approach</head><p>We chose to develop Firmament as a flow-based scheduler for three reasons. First, flow-based scheduling considers the entire workload, allowing us to support rescheduling and priority preemption. Second, flowbased scheduling achieves high placement quality and, consequently, low task response times <ref type="bibr">[22, §6]</ref>. Third, as a batching approach, flow-based scheduling amortizes work well over many tasks and placement decisions, and hence achieves high task throughput -albeit at a high placement latency that we aim to improve.  <ref type="figure">Figure 4</ref> gives an overview of the Firmament scheduler architecture. Firmament, like Quincy, models the scheduling problem as a min-cost max-flow (MCMF) optimization over a flow network. The flow network is a directed graph whose structure is defined by the scheduling policy. In response to events and monitoring information, the flow network is modified according to the scheduling policy, and submitted to an MCMF solver to find an optimal (i.e., min-cost) flow. Once the solver completes, it returns the optimal flow, from which Firmament extracts the implied task placements. In the following, we first explain the basic structure of the flow network, and then discuss how to make the solver fast.</p><formula xml:id="formula_0">T 0,0 T 0,1 T 0,2 T 1,0 T 1,1 M 0 M 1 M 2 M 3 S U 0 U 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Flow network structure</head><p>A flow network is a directed graph whose arcs carry flow from source nodes to a sink node. A cost and capacity associated with each arc constrain the flow, and specify preferential routes for it. <ref type="figure" target="#fig_5">Figure 5</ref> shows an example of a flow network that expresses a simple cluster scheduling problem. Each task node T j,i on the left hand side, representing the i th task of job j, is a source of one unit of flow. All such flow must be drained into the sink node (S) for a feasible solution to the optimization problem. To reach S, flow from T j,i can proceed through a machine node (M m ), which schedules the task on machine m (e.g., T 0,2 on M 1 ). Alternatively, the flow may proceed to the sink through an unscheduled aggregator node (U j for job j), which leaves the task unscheduled (as with T 0,1 ) or preempts it if running.</p><p>In the example, a task's placement preferences are expressed as costs on direct arcs to machines. The cost to leave the task unscheduled, or to preempt it when run-ning, is the cost on its arc to the unscheduled aggregator (e.g., 7 for T 1,1 ). Given this flow network, an MCMF solver finds a globally optimal (i.e., minimum-cost) flow (shown in red in <ref type="figure" target="#fig_5">Figure 5</ref>). This optimal flow expresses the best trade-off between the tasks' unscheduled costs and their placement preferences. Task placements are extracted by tracing flow from the machines back to tasks.</p><p>In our example, tasks had only direct arcs to machines. The solver finds the best solution if every task has an arc to each machine scored according to the scheduling policy, but this requires thousands of arcs per task on a large cluster. Policy-defined aggregator nodes, similar to the unscheduled aggregators, reduce the number of arcs required to express a scheduling policy. Such aggregators group, e.g., machines in a rack, tasks with similar resource needs, or machines with similar capabilities. With aggregators, the cost of a task placement is the sum of all costs on the path from the task node to the sink.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Scheduling policies</head><p>Firmament generalizes flow-based scheduling over the single, batch-oriented policy proposed by Quincy. Cluster administrators use a policy API to configure Firmament's scheduling policy, which may incorporate e.g., multi-dimensional resources, fairness, and priority preemption <ref type="bibr" target="#b30">[31,</ref>. This paper focuses on Firmament's scalability, and we therefore use only three simplified, illustrative policies explained in the following: (i) a simple load-spreading policy, (ii) Quincy's slot-based, locality-oriented policy, and (iii) a network-aware policy that avoids overloading machines' network connections.</p><p>Load-spreading policy. <ref type="figure" target="#fig_8">Figure 6a</ref> shows a trivial use of an aggregator: all tasks have arcs to a cluster-wide aggregator (X). The cost on the outgoing arc from X to each machine node is proportional to the number of tasks already running on the machine (e.g., one task on M 3 ). The effect is that the number of tasks on a machine only increases once all other machines have at least as many tasks (as e.g., in Docker SwarmKit). This policy neither requires or nor uses the full sophistication of flow-based scheduling. We use it to highlight specific edge cases in MCMF algorithms (see §4.3).</p><p>Quincy policy. <ref type="figure" target="#fig_8">Figure 6b</ref> depicts Quincy's original locality-oriented policy <ref type="bibr">[22, §4.2]</ref>, which uses rack aggregators (R r ) and a cluster aggregator (X) to express data locality for batch jobs. Tasks have low-cost preference arcs to machines and racks on which they have local data, but fall back to scheduling via the cluster aggregator if their preferences are unavailable (e.g., T 0,2 ). This policy is suitable for batch jobs, and optimizes for a tradeoff between data locality, task wait time, and preemption cost. We use it to illustrate MCMF algorithm performance and for head-to-head comparison with Quincy.</p><formula xml:id="formula_1">T 0,0 T 0,1 T 0,2 T 1,0 T 1,1 X M 0 M 1 M 2 M 3 S U 0 U 1 5 5 5 7 7 1 1 1 2</formula><p>r u n n i n g : 0 (a) Load-spreading policy with a single cluster aggregator (X) and costs proprtional to number of tasks per machine. (b) Quincy policy with cluster (X) and rack (R) aggregators, and data locality preference arcs (PA).  Network-aware policy. <ref type="figure" target="#fig_8">Figure 6c</ref> illustrates a policy which avoids overcommitting machines' network bandwidth (which degrades task response time). Each task connects to a request aggregator (RA) for its network bandwidth request. The RAs have one arc for each task that fits on each machine with sufficient spare bandwidth (e.g., 650 MB/s of 1.25 GB/s on M 2 's 10G link). These arcs are dynamically adapted as the observed bandwidth use changes. Costs on the arcs to machines are the sum of the request and the currently used bandwidth, which incentivizes balanced utilization. We use this policy to illustrate Firmament's potential to make high-quality decisions, but a production policy would be more complex and extend it with a priority notion and additional resource dimensions (e.g., CPU/RAM) <ref type="bibr">[31, §7.3</ref>].</p><formula xml:id="formula_2">T 0,0 T 0,1 T 0,2 T 1,0 T 1,1 X R 0 M 0 M 1 R 1 M 2 M 3 S U 0 U 1</formula><formula xml:id="formula_3">T 0,0 T 0,1 T 0,2 T 1,0 T 1,1 RA 400 M 0 (850) M 1 (850) RA 150 M 2 (650) M 3 (1050) S U 0 U</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Min-cost max-flow algorithms</head><p>A flow-based scheduler can use any MCMF algorithm, but some algorithms are better suited to the scheduling problem than others. In this section, we explain the MCMF algorithms that we implemented for Firmament, compare them empirically, and explain their sometimes unexpected performance. A min-cost max-flow algorithm takes a directed flow network G = (N, A) as input. Each arc (i, j) ∈ A has a cost c i j and a maximum capacity u i j . Each node i ∈ N also has an associated supply b(i); nodes with positive supply are sources, those with negative supply are sinks.</p><p>Informally, MCMF algorithms must optimally route the flow from all sources (e.g., task nodes T i, j ) to sinks (e.g., the sink node S) without exceeding the capacity constraint on any arc. To understand the differences between MCMF algorithms, we need a slightly more formal definition: the goal is to find a flow f that minimizes Eq. 1, while respecting the flow feasibility constraints of mass balance (Eq. 2) and capacity (Eq. 3):</p><formula xml:id="formula_4">Minimize ∑ (i, j)∈A c i j f i j subject to (1) ∑ k:( j,k)∈A f jk − ∑ i:(i, j)∈A f i j = b( j), ∀ j ∈ N<label>(2)</label></formula><p>and 0</p><formula xml:id="formula_5">≤ f i j ≤ u i j , ∀(i, j) ∈ A<label>(3)</label></formula><p>Some algorithms use an equivalent definition of the flow network, the residual network. In the residual network, each arc (i, j) ∈ A with cost c i j and maximum capacity u i j is replaced by two arcs: (i, j) and ( j, i). Arc (i, j) has cost c i j and a residual capacity of r i j = u i j − f i j , while arc ( j, i) has cost −c i j and a residual capacity r ji = f i j . The feasibility constraints also apply in the residual network. The primal minimization problem (Eq. 1) also has an associated dual problem, which some algorithms solve more efficiently. In the dual min-cost max-flow problem, each node i ∈ N has an associated dual variable π(i) called the potential. The potentials are adjusted in different, algorithm-specific ways to meet optimality conditions. Moreover, each arc has a reduced cost with respect to the node potentials, defined as:</p><formula xml:id="formula_6">c π i j = c i j − π(i) + π( j)<label>(4)</label></formula><p>A feasible flow is optimal if and only if at least one of three optimality conditions is met:</p><p>1. Negative cycle optimality: no directed negativecost cycles exist in the residual network.</p><p>2. Reduced cost optimality: there is a set of node potentials π such that there are no arcs in the residual network with negative reduced cost (c π i j ). <ref type="table">Table 1</ref>: Worst-case time complexities for min-cost max-flow algorithms. N is the number of nodes, M the number of arcs, C the largest arc cost and U the largest arc capacity. In our problem, M &gt; N &gt; C &gt; U.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm Worst-case complexity</head><formula xml:id="formula_7">Relaxation O(M 3 CU 2 ) Cycle canceling O(NM 2 CU) Cost scaling O(N 2 M log(NC)) Successive shortest path O(N 2 U log(N))</formula><p>3. Complementary slackness optimality: there is a set of node potentials π such that the flow on arcs with c π i j &gt; 0 is zero, and there are no arcs with both c π i j &lt; 0 and available capacity.</p><p>Algorithms. The simplest MCMF algorithm is cycle canceling <ref type="bibr" target="#b24">[25]</ref>. The algorithm first computes a maxflow solution, and then performs a series of iterations in which it augments flow along negative-cost directed cycles in the residual network. Pushing flow along such a cycle guarantees that the overall solution cost decreases. The algorithm finishes with an optimal solution once no negative-cost cycles remain (i.e., the negative cycle optimality condition is met). Cycle canceling always maintains feasibility and attempts to achieve optimality. Unlike cycle canceling, the successive shortest path algorithm [2, p. 320] maintains reduced cost optimality at every step and tries to achieve feasibility. It repeatedly selects a source node (i.e., b(i) &gt; 0) and sends flow from it to the sink along the shortest path.</p><p>The relaxation algorithm <ref type="bibr">[4; 5]</ref>, like successive shortest path, augments flow from source nodes along the shortest path to the sink. However, unlike successive shortest path, relaxation optimizes the dual problem by applying one of two changes when possible:</p><p>1. Keeping π unchanged, the algorithm modifies the flow, f , to f such that f still respects the reduced cost optimality condition and the total supply decreases (i.e., feasibility improves). 2. It modifies π to π and f to f such that f is still a reduced cost-optimal solution and the cost of that solution decreases (i.e., total cost decreases). This allows relaxation to decouple the improvements in feasibility from reductions in total cost. When relaxation can reduce cost or improve feasibility, it reduces cost.</p><p>Cost scaling <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> iterates to reduce cost while maintaining feasibility, and uses a relaxed complementary slackness condition called ε-optimality. A flow is εoptimal if the flow on arcs with c π i j &gt; ε is zero and there are no arcs with c π i j &lt; −ε on which flow can be sent. Initially, ε is equal to the maximum arc cost, but ε rapidly decreases as it is divided by a constant factor after every iteration that achieves ε-optimality. Cost scaling finishes Cycle canceling Succ. shortest Cost scaling Relaxation <ref type="figure">Figure 7</ref>: Average runtime for MCMF algorithms on clusters of different sizes, subsampled from the Google trace. We use the Quincy policy and slot utilization is about 50%. Relaxation performs best, despite having the highest time complexity.</p><p>[N.B.: log 10 -scale y-axis.]</p><p>when 1 n -optimality is achieved, since this is equivalent to the complementary slackness optimality condition <ref type="bibr" target="#b16">[17]</ref>. <ref type="table">Table 1</ref> summarizes the worst-case complexities of the algorithms discussed. The complexities suggest that successive shortest path ought to work best, as long as U log(N) &lt; M log(NC), which is the case as U M and C ≥ 1. However, since MCMF algorithms are known to have variable runtimes depending on the input graph <ref type="bibr">[15; 24; 26]</ref>, we decided to directly measure performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Algorithmic performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Measured performance</head><p>As in the experiment in <ref type="figure" target="#fig_3">Figure 3</ref>, we subsample the Google trace and replay it for simulated clusters of different sizes. We use the Quincy scheduling policy for batch jobs and prioritize service jobs over batch ones. <ref type="figure">Figure 7</ref> shows the average runtime for each MCMF algorithm considered. Even though it has the best worst-case time complexity, successive shortest path outperforms only cycle canceling, and even on a modest cluster of 1,250 machines its algorithm runtime exceeds 100 seconds.</p><p>Moreover, the relaxation algorithm, which has the highest worst-case time complexity, actually performs best in practice. It outperforms cost scaling (used in Quincy) by two orders of magnitude: on average, relaxation completes in under 200ms even on a cluster of 12,500 machines. One key reason for this perhaps surprising performance is that relaxation does minimal work when most scheduling choices are straightforward. This happens if the destinations for tasks' flow are uncontested, i.e., few new tasks have arcs to the same location and attempt to schedule there. In this situation, relaxation routes most of the flow in a single pass over the graph.  <ref type="figure">Figure 9</ref>: Contention slows down the relaxation algorithm: on cluster with a load-spreading scheduling policy, relaxation runtime exceeds that of cost scaling at just under 3,000 concurrently arriving tasks (e.g., a large job).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Edge cases for relaxation</head><p>Yet, relaxation is not always the right choice. For example, it can perform poorly under the high load and oversubscription common in batch analytics clusters <ref type="bibr" target="#b28">[29]</ref>. <ref type="figure" target="#fig_10">Figure 8</ref> illustrates this: here, we push the simulated Google cluster closer to oversubscription. We take a snapshot of the cluster and then submit increasingly larger jobs. The relaxation runtime increases rapidly, and at about 93% cluster utilization, it exceeds that of cost scaling, growing to over 400s in the oversubscribed case.</p><p>Moreover, some scheduling policies inherently create contention between tasks. Consider, for example, our load-spreading policy that balances the task count on each machine. This policy makes "under-populated" machines a popular destination for tasks' flow, and thus creates contention. We illustrate this with an experi-  <ref type="figure" target="#fig_0">Figure 10</ref>: Approximate min-cost max-flow yields poor solutions, since many tasks are misplaced until shortly before the algorithms reach the optimal solution. ment: we submit a single job with an increasing number of tasks to a cluster using the load-spreading policy. This corresponds to the rare-but-important arrival of very large jobs: for example, 1.2% of jobs in the Google trace have over 1,000 tasks, and some even over 20,000. <ref type="figure">Figure 9</ref> shows that relaxation's runtime increases linearly in the number of tasks, and that it exceeds the runtime of cost scaling once the new job has over 3,000 tasks.</p><p>To make matters worse, a single overlong relaxation run can have a devastating effect on long-term placement latency. If many new tasks arrive during such a long run, the scheduler might again be faced with many unscheduled tasks when it next runs. Hence, relaxation may take a long time again, accumulate many changes, and in the worst case fail to ever recover to low placement latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MCMF optimizations for scheduling</head><p>Relaxation has promising common-case performance at scale for typical workloads. However, its edge-case behavior makes it necessary either (i) to fall back to other algorithms in these cases, or (ii) to reduce runtime in other ways. In the following, we use challenging graphs to investigate optimizations that either improve relaxation or the best "fallback" algorithm, cost scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Approximate min-cost max-flow</head><p>MCMF algorithms return an optimal solution. For the cluster scheduling problem, however, an approximate solution may well suffice. For example, TetriSched <ref type="bibr" target="#b32">[33]</ref> (based on an MILP solver), as well as Paragon <ref type="bibr" target="#b10">[11]</ref> and Quasar <ref type="bibr" target="#b11">[12]</ref> (based on collaborative filtering), terminate their solution search after a set time. We therefore investigated the solution quality of cost scaling and relaxation when they are terminated early. This would work well if the algorithms spent a long time on minor solution refinements with little impact on the overall outcome.  In our experiment, we use a highly-utilized cluster (cf. <ref type="figure" target="#fig_10">Figure 8</ref>) to investigate relaxation and cost scaling, but the results generalize. <ref type="figure" target="#fig_0">Figure 10</ref> shows the number of "misplaced" tasks as a function of how early we terminate the algorithms. We treat any task as misplaced if it is (i) preempted in the approximate solution but keeps running in the optimal one; (ii) scheduled on a different machine to where it is scheduled in the optimal solution. Both cost scaling and relaxation misplace thousands of tasks when terminated early, and tasks are still misplaced even in the final iteration before completion. Hence, early termination appears not to be a viable placement latency optimization for flow-based schedulers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Incremental min-cost max-flow</head><p>Since cluster state does not change dramatically between subsequent scheduling runs, the MCMF algorithm might be able to reuse its previous state. In this section, we describe what changes are required to make MCMF algorithms work incrementally, and provide some intuition for which algorithms are suitable for incremental use.</p><p>All cluster events (e.g., task submissions, machine failures) ultimately reduce to three different types of graph change in the flow network:</p><p>1. Supply changes at nodes when arcs or nodes which previously carried flow are removed (e.g., due to machine failure), or when nodes with supply are added to the graph (e.g., at task submission).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Capacity changes on arcs if machines fail or</head><p>(re)join the cluster. Note that arc additions and removals can also be modeled as capacity changes from and to zero-capacity arcs. 3. Cost changes on an arc when the desirability of routing flow via that arc changes; when these happen exactly depends on the scheduling policy. Changes to the supply of a node, an arc's capacity, or its cost can invalidate the feasibility and optimality of an existing flow. Some MCMF algorithms require the flow to be feasible at every step and improve ε-optimality, while others require optimality to always hold and improve feasibility ( <ref type="table" target="#tab_1">Table 2)</ref>. A solution must be optimal and feasible because an infeasible solution fails to route all flow, which leaves tasks unscheduled or erroneously preempts them, while a non-optimal solution misplaces tasks.  <ref type="figure" target="#fig_0">Figure 11</ref>: Incremental cost scaling is 25% faster compared to from-scratch cost scaling for the Quincy policy and 50% faster for the load-spreading policy.</p><p>Reduced cost on arc from i to j Change type We implemented incremental versions of the cost scaling and relaxation algorithms. Incremental cost scaling is up to 50% faster than running cost scaling from scratch ( <ref type="figure" target="#fig_0">Figure 11</ref>). Incremental cost scaling's potential gains are limited because cost scaling requires the flow to be feasible and ε-optimal before each intermediate iteration <ref type="table" target="#tab_1">(Table 2)</ref>. Graph changes can cause the flow to violate one or both requirements: for example, any addition or removal of task nodes adds supply and breaks feasibility. <ref type="table" target="#tab_2">Table 3</ref> shows the effect of different arc changes on the feasibility and optimality of the flow. A change that modifies the cost of an arc (i, j) from c π i j &lt; 0 to c π i j &gt; 0, for example, breaks optimality. Many changes break optimality and cause cost scaling to fall back to a higher ε-optimality to compensate. To bring ε back down, cost scaling must do a substantial part of the work it would do from scratch. However, the limited improvement still helps reduce our fallback algorithm's runtime.</p><formula xml:id="formula_8">c π i j &lt; 0 c π i j = 0 c π i j &gt; 0 Increasing arc cap. Decreasing arc cap. f i j &gt; u i j Increasing arc cost c π i j &gt; 0 f i j &gt; 0 Decreasing arc cost c π i j &lt; 0</formula><p>Incremental relaxation ought to work better than incremental cost scaling, since the relaxation algorithm only needs to maintain reduced cost optimality <ref type="table" target="#tab_1">(Table 2)</ref>. In practice, however, it turns out not to work well. While the algorithm can be incrementalized with relative ease and often runs faster, it -counter-intuitively -can also be  slower incrementally than when running from scratch.</p><p>Relaxation requires reduced cost optimality to hold at every step of the algorithm and tries to achieve feasibility by pushing flow on zero-reduced cost arcs from source nodes to nodes with demand. The algorithm builds a tree of zero-reduced cost arcs from each source node in order to find such demand nodes. The tree is expanded by adding zero-reduced cost arcs to it. When running from scratch, the likelihood of zero-reduced cost arcs connecting two zero-reduced cost trees is low, as there are few such trees initially. Only when the solution is close to optimality, trees are joined into larger ones. Incremental relaxation, however, works with the existing, closeto-optimal state, which already contains large trees that must be extended for each source. Having to traverse these large trees many times, incremental relaxation can run slower than from scratch. This happens especially for graphs that relaxation already struggles with, e.g. ones that contain nodes with a lot of potential incoming flow. In practice, we found that incremental relaxation performs well only if tasks are not typically connected to a large zero-reduced cost tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Problem-specific heuristics</head><p>Our scheduler runs min-cost max-flow on a graph with specific properties, rather than the more general graphs typically used to evaluate MCMF algorithms <ref type="bibr">[24, §4]</ref>. For example, our graph has a single sink; it is a directed acyclic graph; and flow must always traverse unscheduled aggregators or machine nodes. Hence, problemspecific heuristics might help the algorithms find solutions more quickly. We investigated several such heuristics, and found two beneficial ones: (i) prioritization of promising arcs, and (ii) efficient task node removal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Arc prioritization</head><p>The relaxation algorithm builds a tree of zero-reduced cost arcs for every source node (see §5.2) in order to locate zero-reduced cost paths (i.e., paths that do not break reduced cost optimality) to nodes with demand. When this tree must be extended, any arc of zero reduced cost that connects a node inside the tree to a node outside the tree can be used. However, some arcs are better choices for extension than others. The quicker we can find paths to nodes with demand, the sooner we can route the supply. We therefore prioritize arcs that lead to nodes with demand when extending the cut, adding them to the front of a priority queue to ensure they are visited sooner.</p><p>In effect, this heuristic implements a hybrid graph traversal that biases towards depth-first exploration when demand nodes can be reached, but uses breadth first exploration otherwise. <ref type="figure" target="#fig_0">Figure 12a</ref> shows that applying this heuristic reduces relaxation runtime by 45% when running over a graph with contended nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Efficient task removal</head><p>Our second heuristic helps incremental cost scaling. It is based on the insight that removal of a running task is common (e.g., due to completion, preemption, or a machine failure), but breaks feasibility. This happens because the task node is removed, which creates demand at the machine node where the task ran, since the machine node still has outgoing flow in the intermediate solution.</p><p>Breaking feasibility is expensive for cost scaling ( §5.2).</p><p>However, we can reconstruct the task's flow through the graph, remove it, and drain the machine node's flow at the single sink node. This creates demand in a single place only (the sink), which accelerates the incremental solution. However, <ref type="figure" target="#fig_0">Figure 12b</ref> shows that this heuristic offers only modest gains: it improves runtime by 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Firmament implementation</head><p>We implemented a new MCMF solver for Firmament. It supports the four algorithms discussed earlier ( §4) and incremental cost scaling. The solver consists of about 8,000 lines of C++. Firmament's cluster manager and our simulator are implemented in about 24,000 lines of C++, and are available at http://firmament.io.</p><p>In this section, we discuss implementation-level techniques that, in addition to our prior algorithmic insights, help Firmament achieve low task placement latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Algorithm choice</head><p>In §4, we saw that the practical performance of MCMF algorithms varies. Relaxation often works best, but scales poorly in specific edge cases. Cost scaling, by contrast, scales well and can be incrementalized ( §5.2), but is usually substantially slower than relaxation.</p><p>Firmament's MCMF solver always speculatively executes cost scaling and relaxation, and picks the solution offered by whichever algorithm finishes first. In the common case, this is relaxation; having cost scaling as well guarantees that Firmament's placement latency does not grow unreasonably large in challenging situations. We run both algorithms instead of developing a heuristic to choose the right one for two reasons: first, it is cheap, as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX Association</head><p>12th USENIX Symposium on Operating Systems Design and Implementation 107  <ref type="figure" target="#fig_0">Figure 13</ref>: Incremental cost scaling runs 4× faster if we apply the price refine heuristic to a graph from relaxation. the algorithms are single-threaded and do not parallelize; second, predicting the right algorithm is hard and the heuristic would depend on both scheduling policy and cluster utilization (cf. §4), making it brittle and complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Efficient algorithm switching</head><p>Firmament also applies an optimization that helps it efficiently transition state from relaxation to incremental cost scaling. Firmament's MCMF solver uses incremental cost scaling as it is faster than running cost scaling from scratch ( §5.2). Typically, however, the (fromscratch) relaxation algorithm finishes first. The next incremental cost scaling run must therefore use the solution from a prior relaxation as a starting point. Since relaxation and cost scaling use different reduced cost graph representations, this can be slow. Specifically, relaxation may converge on node potentials that fit poorly into cost scaling's complementary slackness requirement, since relaxation only requires reduced cost optimality.</p><p>We found that price refine <ref type="bibr" target="#b16">[17]</ref>, a heuristic originally developed for use within cost scaling, helps with this transition. Price refine reduces the node potentials without affecting solution optimality, and thus simplifies the problem for cost scaling. <ref type="figure" target="#fig_0">Figure 13</ref> shows that applying price refine to the prior relaxation solution graph speeds up incremental cost scaling by 4× in 90% of cases.</p><p>We apply price refine on the previous solution before we apply the latest cluster changes. This guarantees that price refine is able to find node potentials that satisfy complementary slackness optimality without modifying the flow. Consequently, cost scaling must start only at a value of ε equal to the costliest arc graph change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Efficient solver interaction</head><p>So far, we have primarily focused on reducing the MCMF solver's algorithm runtime. To achieve low task placement latency, we must make two steps that fall out-1 to_visit = machine_nodes # list of machine nodes 2 node_flow_destinations = {} # auxiliary remember set 3 mappings = {} # final task mappings 4 while not to_visit.empty(): Listing 1: Our efficient algorithm for extracting task placements from the optimal flow returned by the solver. side the solver runtime efficient as well. First, Firmament must efficiently update the flow network's nodes, arcs, costs, and capacities before every MCMF optimization to reflect the chosen scheduling policy. Second, Firmament must quickly extract task placements out of the flow network after the optimization finishes. We improve over the prior work on flow-based scheduling in Quincy for both aspects, as explained in the following.</p><p>Flow network updates. Firmament does two breadthfirst traversals of the flow network to update it for a new solver run. The first traversal updates resource statistics associated with every entity, such as the memory available on a machine, its current load, or a task's resource request. The traversal starts from the nodes adjacent to the sink (usually machine nodes), and propagates statistics along each node's incoming arcs. Upon the first traversal's completion, Firmament runs a second traversal that starts at the task nodes. This pass allows the scheduling policy to update the flow network's nodes, arcs, costs and capacities using the statistics gathered in the first traversal. Hence, only two passes over the large graph must be made to prepare the next solver run. Their overhead is negligible compared to the solver runtime.</p><p>Task placement extraction. At the end of a run, the solver returns an optimal flow through the given network and Firmament must extract the task placements implied by this flow. Since Firmament allows arbitrary aggregators in the flow network, paths from tasks to machines may be longer than in Quincy, where arcs necessarily pointed to machines or racks. Hence, we had to gen-eralize Quincy's approach to this extraction <ref type="bibr">[22, p. 275]</ref>. To extract task assignments efficiently, we devised the graph traversal algorithm shown in Listing 1. The algorithm starts from machine nodes and propagates a list of machines to which each node has sent flow via its incoming arcs. In the common case, the algorithm extracts the task placements in a single pass over the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>We now evaluate how well Firmament meets its goals:</p><p>1. How do Firmament's task placement quality and placement latency compare to Quincy's? ( §7.2) 2. How does Firmament cope with demanding situations such as an overloaded cluster? ( §7.3) 3. At what operating points does Firmament fail to achieve sub-second placement latency? ( §7.4) 4. How does Firmament's placement quality compare to other cluster schedulers on a physical cluster running a mixed batch/service workload? ( §7.5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Methodology</head><p>Our experiments combine scale-up simulations with experiments on a local testbed cluster.</p><p>In simulations, we replay a public production workload trace from 12,500-machine Google cluster <ref type="bibr" target="#b29">[30]</ref> against Firmament's implementation. Our simulator is similar to Borg's "Fauxmaster" [35, §3.1]: it runs Firmament's real code and scheduling logic against simulated machines, merely stubbing out RPCs and task execution. However, there are three important limitations to note. First, the Google trace contains multi-dimensional resource requests for each task. Firmament supports multidimensional feasibility checking (as in Borg <ref type="bibr">[35, §3.2]</ref>), but in order to fairly compare to Quincy, we use slotbased assignment. Second, we do not enforce task constraints for the same reason, even though they typically help Firmament's MCMF solver. Third, the Google trace lacks information about job types and input sizes. We use Omega's priority-based job type classification [32, §2.1], and estimate batch task input sizes as a function of the known runtime using typical industry distributions <ref type="bibr" target="#b7">[8]</ref>.</p><p>In local cluster experiments, we use a homogeneous 40-machine cluster. Each machine has a Xeon E5-2430Lv2 CPU (12× 2.40GHz), 64 GB RAM, and uses a 1TB magnetic disk for storage. The machines are connected via 10 Gbps, full-bisection bandwidth Ethernet.</p><p>When we compare with Quincy, we run Firmament with Quincy's scheduling policy and restrict the solver to use only cost scaling (as Quincy's cs2 solver does).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Scalability vs. Quincy</head><p>In <ref type="figure" target="#fig_3">Figure 3</ref>, we illustrated that Quincy fails to scale to clusters of thousands of machines at an acceptable placement latency. We now repeat the same experiment using Firmament on the full-scale simulated Google clus-  <ref type="figure" target="#fig_0">Figure 14</ref>: Firmament has a 20× lower task placement latency than Quincy on a simulated 12,500-machine cluster at 90% slot utilization, replaying the Google trace. The placement quality is identical to Quincy's.</p><p>ter. However, we increase the cluster slot utilization from the earlier experiment's 50% to 90% to make the setup more challenging for Firmament, and also tune the cost scaling-based MCMF solver for its best performance. 3 <ref type="figure" target="#fig_0">Figure 14</ref> shows the results as a CDF of task placement latency, i.e., the time between a task being submitted to the cluster manager and the time when it has been placed ( §2). While Quincy takes between 25 and 60 seconds to place tasks, Firmament typically places tasks in hundreds of milliseconds and only exceeds a sub-second placement latency in the 90 th percentile. Therefore, Firmament improves task placement latency by more than a 20× over Quincy, but maintains the same placement quality as it also finds an optimal flow.</p><p>Firmament's low placement latency comes because relaxation scales well even for large flow networks with the Google trace workload. This scalability allows us to afford scheduling policies with many arcs. As an illustrative example, we vary the data locality threshold in the Quincy scheduling policy. This threshold decides what fraction of a task's input data must reside on a machine or within a rack in order for the former to receive a preference arc to the latter. Quincy originally picked a threshold of a maximum of ten arcs per task. However, <ref type="figure" target="#fig_0">Figure 15a</ref> shows that even a lower threshold of 14% local data, which corresponds to at most seven preference arcs, yields algorithm runtimes of 20-40 seconds for Quincy's cost scaling. A low threshold allows the scheduler to exploit more fine-grained locality, but increases the number of arcs in the graph. Consequently, if we lower the threshold to 2% local data, <ref type="bibr" target="#b3">4</ref> the cost scaling runtime in-Specifically, we found that an α-factor parameter value of 9, rather than the default of 2 used in Quincy, improves runtime by ≈30%.</p><p>4 2% is a somewhat extreme value used for exposition here. The benefit of such a low threshold in a real cluster would likely be limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX Association</head><p>12th USENIX Symposium on Operating Systems Design and Implementation 109  creases to well over 40 seconds. Firmament, on the other hand, still achieves sub-second algorithm even with a 2% threshold. This threshold yields an increase in data locality from 56% to 71% of total input data <ref type="table">(Table 15b)</ref>, which saves 4 TB of network traffic per simulated hour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Coping with demanding situations</head><p>In the previous experiments, Firmament had a lower placement latency than Quincy because relaxation handles the Google workload well. As explained in §4, there are situations in which this is not the case. In those situations, Firmament picks incremental cost scaling's solution as it finishes first ( §6). We now demonstrate the benefits of running two algorithms rather than just one. In this experiment, we shrink the number of slots per cluster machine to reach 97% average utilization. Consequently, the cluster experiences transient periods of oversubscription. <ref type="figure" target="#fig_0">Figure 16</ref> compares Firmament's automatic use of the fastest algorithm against using only one algorithm, either relaxation or cost scaling. During oversubscription, relaxation alone takes hundreds of seconds per run, while cost scaling alone completes in ≈30 seconds independent of cluster load. Firmament's incremental cost scaling finishes first in this situation, taking 10-15 seconds, which is about 2× faster than using cost scaling only (as Quincy does). Firmament also recovers earlier from the overload situation starting at 2,200s: while the relaxation-only runtime returns to sub-second level only around 3,700s, Firmament recovers at 3,200s. Relaxation on its own takes longer to recover because  <ref type="figure" target="#fig_0">Figure 17</ref>: Firmament's breaking point is at tasks are shorter than ≈5ms at 100-machine scale, and ≈375ms at 1,000-machine scale, with 80% cluster slot utilization. many tasks complete and free up slots during the long solver runs. These slots cannot be re-used until the next solver run completes, even though new, waiting tasks accumulate. Hence, Firmament's combination of algorithms outperforms either algorithm running alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Scalability to sub-second tasks</head><p>In the absence of oversubscription, we now investigate the scalability limit of Firmament's sub-second relaxation-based MCMF. To find Firmament's breaking point, we subject it to a worst-case workload consisting entirely of short tasks. This experiment is similar to Sparrow's breaking-point experiment for the centralized Spark scheduler <ref type="bibr" target="#b27">[28,</ref><ref type="bibr">Fig. 12]</ref>. We submit jobs of 10 tasks at an interarrival time that keeps the cluster at a constant load of 80% if there is no scheduler overhead. We measure job response time, which is the maximum of the ten task response times for a job. In <ref type="figure" target="#fig_0">Figure 17</ref>, we plot job response time as a function of decreasing task duration. As  <ref type="figure" target="#fig_0">Figure 18</ref>: Firmament, unlike relaxation alone, keeps up with a 300× accelerated Google workload (1 st , 25 th , 50 th , 75 th , 99 th percentiles and maximum).</p><p>we reduce task duration, we also reduce task interarrival time to keep the load constant, hence increasing the task throughput faced by the scheduler. With an ideal scheduler, job response time would be equal to task runtime as the scheduler would take no time to choose placements. Hence, the breaking point occurs when job response time deviates from the diagonal. For example, Spark's centralized task scheduler in 2013 had its breaking point on 100 machines at a 1.35 second task duration <ref type="bibr">[28, §7.6]</ref>. By contrast, even though Firmament runs MCMF over the entire workload every time, <ref type="figure" target="#fig_0">Figure 17</ref> shows that it achieves near-ideal job response time down to task durations as low as 5ms (100 machines) or 375ms (1,000 machines). This makes Firmament's response time competitive with distributed schedulers on medium-sized clusters that only run short tasks. At 10,000 machines, Firmament keeps up with task durations ≥5s. However, such large clusters usually run a mix of long-running and short tasks, rather than short tasks only <ref type="bibr">[7; 10; 23; 35]</ref>.</p><p>We therefore investigate Firmament's performance on a mixed workload. We speed up the Google trace by dividing all task runtimes and interarrival times by a speedup factor. This simulates a future workload of shorter batch tasks <ref type="bibr" target="#b26">[27]</ref>, while service jobs are still longrunning. For example, at a 200× speedup, the median batch task takes 2.1 seconds, and the 90 th and 99 th percentile batch tasks take 18 and 92 seconds. We measure Firmament's placement latency across all tasks, and plot the distributions in <ref type="figure" target="#fig_0">Figure 18</ref>. Even at a speedup of 300×, Firmament keeps up and places 75% of the tasks at with sub-second latency. As before, a single MCMF algorithm does not scale: cost scaling's placement latency already exceeds 10s even without any speedup, and relaxation sees tail latencies well above 10 seconds beyond a 150× speedup, while Firmament scales further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Placement quality on a local cluster</head><p>We deployed Firmament on a local 40-machine cluster to evaluate its real-world performance. We run a workload of short batch analytics tasks that take 3.5-5 seconds to complete on an otherwise idle cluster. Each task reads inputs of 4-8 GB from a cluster-wide HDFS installation in this experiment, and Firmament uses the network-aware scheduling policy. This policy reflects current network bandwidth reservations and observed actual bandwidth use in the flow network, and strives to place tasks on machines with lightly-loaded network connections. In <ref type="figure" target="#fig_0">Figure 19a</ref>, we show CDFs of task response times obtained using different cluster managers' schedulers. We measure task response time, and compare to a baseline that runs each task in isolation on an otherwise idle network. Firmament's task response time comes closest to the baseline above the 80 th percentile as it successfully avoids overcommitting machines' network bandwidth. Other schedulers make random assignments (Sparrow), perform simple load-spreading (SwarmKit), or do not consider network bandwidth (Mesos, Kubernetes). Since our cluster is small, Firmament's task placement latency is inconsequential at around 5ms in this experiment.</p><p>Real-world clusters, however, run a mix of short, interactive tasks and long-running service and batch processing tasks. We therefore extend our workload with new long-running batch and service jobs to represent a similar mix. The long-running batch workloads are generated by fourteen iperf clients who communicate using UDP with seven iperf servers. Each iperf client generates 4 Gbps of sustained network traffic and simulates a batch job in a higher-priority network service class <ref type="bibr" target="#b19">[20]</ref> than the short batch tasks (e.g., a TensorFlow [1] parameter server). Finally, we deploy three nginx web servers and seven HTTP clients as long-running service jobs. We run the cluster at about 80% network utilization, and again measure the task response time for the short batch analytics tasks. <ref type="figure" target="#fig_0">Figure 19b</ref> shows that Firmament's networkaware scheduling policy substantially improves the tail of the task response time distribution of short batch tasks. For example, Firmament's 99 th percentile response time is 3.4× better than the SwarmKit and Kubernetes ones, and 6.2× better than Sparrow's. The tail matters, since the last task's response time often determines a batch job's overall response time (the "straggler" problem).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related work</head><p>Many cluster schedulers exist, but Firmament is the first centralized one to offer high placement quality at subsecond placement latency on large clusters. We now briefly compare Firmament to existing schedulers.</p><p>Optimization-based schedulers. Firmament retains the same optimality as Quincy <ref type="bibr" target="#b21">[22]</ref>, but achieves much USENIX Association 12th USENIX Symposium on Operating Systems Design and Implementation 111  lower placement latency. TetriSched <ref type="bibr" target="#b32">[33]</ref> uses a mixed integer-linear programming (MILP) optimization and applies techniques similar to Firmament's (e.g., incremental restart from a prior solution) to reduce placement latency. Its placement quality degrades gracefully when terminated early (as required at scale), while Firmament always returns optimal solutions. Paragon <ref type="bibr" target="#b10">[11]</ref>, Quasar <ref type="bibr" target="#b11">[12]</ref>, and Bistro <ref type="bibr" target="#b15">[16]</ref> also run expensive scoring computations (collaborative filtering, path selection), but scale the task placement by using greedy algorithms.</p><p>Centralized schedulers. Mesos <ref type="bibr" target="#b20">[21]</ref> and Borg <ref type="bibr" target="#b34">[35]</ref> match tasks to resources greedily; Borg's scoring uses random sampling with early termination <ref type="bibr">[35, §3.4]</ref>, which improves latency at the expense of placement quality. Omega <ref type="bibr" target="#b31">[32]</ref> and Apollo <ref type="bibr" target="#b6">[7]</ref> support multiple parallel schedulers to simplify their engineering and to improve scalability. Firmament shows that a single scheduler can attain scalability, but its MCMF optimization does not trivially admit multiple independent schedulers.</p><p>Distributed schedulers. Sparrow <ref type="bibr" target="#b27">[28]</ref> and Tarcil <ref type="bibr" target="#b12">[13]</ref> are distributed schedulers developed for clusters that see a high throughput of very short, sub-second tasks. In §7.4, we demonstrated that Firmament offers similarly low placement latency as Sparrow on clusters up to 1,000 machines, and beyond if only a part of the workload consists of short tasks. Mercury <ref type="bibr" target="#b22">[23]</ref> is a hybrid scheduler that makes centralized, high-quality assignments for long tasks, and distributedly places short ones. With Firmament, we have shown that a centralized scheduler can scale even to short tasks, and that they benefit from the improved placement quality. Hawk <ref type="bibr" target="#b9">[10]</ref> and Eagle <ref type="bibr" target="#b8">[9]</ref> extend the hybrid approach with work-stealing and state gossiping techniques that improve placement quality; Yaq-d <ref type="bibr" target="#b28">[29]</ref>, by contrast, reorders tasks in workerside queues to a similar end. Firmament shows that even a centralized scheduler can quickly schedule short tasks in large clusters with mixed workloads, rendering such complex compensation mechanisms largely unnecessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>Firmament demonstrates that centralized cluster schedulers can scale to large clusters at low placement latencies. It chooses the same high-quality placements as an advanced centralized scheduler, at the speed and scale typically associated with distributed schedulers. Firmament, our simulator, and our data sets are opensource and available from http://firmament.io. A Firmament scheduler plugin for Kubernetes <ref type="bibr" target="#b13">[14]</ref> is currently under development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Task lifecycle phases, state transition events (bottom) and the time ranges used in this paper (top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Queue-based schedulers ( * : optional worker-side queue). Flow-based schedulers (Quincy<ref type="bibr" target="#b21">[22]</ref>, Firmament).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Tasks wait to be placed individually in queue-based schedulers (a), while flow-based schedulers (b) reschedule the whole workload in a long solver run, which makes it essential to minimize algorithm runtime at scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Quincy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Example flow network for a four-machine cluster with two jobs of three and two tasks. All tasks except T 0,1 are scheduled on machines. Arc labels show non-zero costs, and all arcs have unit capacity apart from those between unscheduled aggregators and the sink. The red arcs carry flow and form the min-cost solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Network-aware policy with request aggregators (RA) and dynamic arcs to machines with spare network bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Different aggregators, arcs, and costs help Firmament express the scheduling policies used in this paper; costs are example values consistent with each policy. Firmament also supports other policies via an API [31, Ch. 6-7].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Close to full cluster utilization, relaxation runtime increases dramatically, while cost scaling is unaffected: the x-axis shows the utilization after scheduling jobs of increasing size to a 90%-utilized cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Eff. task removal (TR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Problem-specific heuristics reduce runtime by 45% (AP, relaxation) and 10% (TR, inc. cost scaling).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>5 7 # 9 moved_machines = 0 10 # 11 # 15 moved_machines += 1 16 #</head><label>7910111516</label><figDesc>node = to_visit.pop() 6 if node.type is not TASK_NODE: Visit the incoming arcs 8 for arc in node.incoming_arcs(): Move as many machines to the incoming arc's source node as there is flow on the arc 12 while assigned_machines &lt; arc.flow: 13 node_flow_destinations[arc.source].append( 14 node_flow_destinations[node].pop()) (Re)visit the incoming arc's source node 17 if arc.source not in to_visit: 18 to_visit.append(arc.source) 19 else: # node.type is TASK_NODE 20 mappings[node.task_id] = 21 node_flow_destinations[node].pop() 22 return mappings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Low preference thresholds see subs-second runtimes in Firmament, while Quincy (with cost scaling) takes over 40s.Pref. threshold [local data] Input data locality14% 56% 2% 71%(b) A lower preference threshold improves data locality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 15 :</head><label>15</label><figDesc>Firmament scales to many arcs, and thus supports a lower preference arc threshold than Quincy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 16 :</head><label>16</label><figDesc>At times of high utilization (gray), Firmament outperforms relaxation and Quincy's cost scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>Short batch analytics tasks running on a cluster with an otherwise idle network. Overhead over "idle" due to contention. Short batch analytics tasks running on a cluster with background traffic from long-running batch and service tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 19 :</head><label>19</label><figDesc>On a local 40-node cluster, Firmament improves task response time of short batch tasks in the tail using a network-aware scheduling policy, both (a) without and (b) with background traffic. Note the different x-axis scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Algorithms have different preconditions for each internal iteration. Cost scaling expects feasibility and ε-optimality, making it difficult to incrementalize.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Arc changes requiring solution reoptimization.</figDesc><table /><note>Green: stays optimal and feasible; red: breaks feasibility or optimality; orange: breaks feasibility or optimality if condition in cell holds. Decreasing arc capacity can de- stroy feasibility; all other changes affect optimality only.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Details of our simulation are in §7; in the steady-state, the 12,500machine cluster runs about 150,000 tasks comprising about 1,800 jobs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="104">12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="112">12th USENIX Symposium on Operating Systems Design and Implementation USENIX Association</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to M. Frans Kaashoek, Frank McSherry, Derek G. Murray, Rebecca Isaacs, Andrew Warfield, Robert Morris, and Pamela Delgado, as well as Jon Gjengset, Srivatsa Bhat, and the rest of MIT PDOS group for comments on drafts of this paper. We also thank Phil Gibbons, our shepherd, and the OSDI 2016 reviewers for their feedback. Their input much improved this paper.</p><p>This work was supported by a Google European Doctoral Fellowship, by NSF award CNS-1413920, and by the Defense Advanced Research Projects Agency (DARPA) and Air Force Research Laboratory (AFRL), under contract FA8750-11-C-0249. The views, opinions, and/or findings contained in this paper are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of DARPA or the Department of Defense.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12 th USENIX Symposium on Operating Systems Design and Implementation (OSDI). Savannah</title>
		<meeting>the 12 th USENIX Symposium on Operating Systems Design and Implementation (OSDI). Savannah<address><addrLine>Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Network flows: theory, algorithms, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ravindra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">B</forename><surname>Magnanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Orlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Luiz André Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Clidaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hölzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Architecture</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="154" />
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
	<note>Second edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relaxation Methods for Minimum Cost Ordinary and Generalized Network Flow Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="114" />
			<date type="published" when="1988-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Relax codes for linear minimum cost network flow problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="190" />
			<date type="published" when="1988-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical Scheduling for Diverse Datacenter Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Culler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4 th Annual Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 4 th Annual Symposium on Cloud Computing (SoCC)<address><addrLine>Santa Clara, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Apollo: Scalable and Coordinated Scheduling for Cloud-Scale Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaliya</forename><surname>Ekanayake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengping</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11 th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 11 th USENIX Symposium on Operating Systems Design and Implementation (OSDI)<address><addrLine>Broomfield, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="285" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive Analytical Processing in Big Data Systems: A Cross-industry Study of MapReduce Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Alspaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment 5</title>
		<meeting>the VLDB Endowment 5</meeting>
		<imprint>
			<date type="published" when="2012-08" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Job-Aware Scheduling in Eagle: Divide and Stick to Your Probes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Didona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florin</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7 th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 7 th ACM Symposium on Cloud Computing (SoCC)<address><addrLine>Santa Clara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hawk: Hybrid Datacenter Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florin</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Marie</forename><surname>Kermarrec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference</title>
		<meeting>the USENIX Annual Technical Conference<address><addrLine>Santa Clara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="499" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Paragon: QoS-aware Scheduling for Heterogeneous Datacenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18 th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 18 th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)<address><addrLine>Houston, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-03" />
			<biblScope unit="page" from="77" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quasar: Resource-Efficient and QoS-Aware Cluster Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18 th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 18 th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)<address><addrLine>Salt Lake City, Utah, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tarcil: Reconciling Scheduling Speed and Quality in Large Shared Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6 th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 6 th ACM Symposium on Cloud Computing (SoCC)<address><addrLine>Kohala Coast, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-08" />
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cloud Native Computing Foundation</title>
		<ptr target="http://k8s.io;accessed14/11/2015" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Computational Study of Cost Reoptimization for Min-Cost Flow Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frangioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Manca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bistro: Scheduling Data-Parallel Jobs Against Live Production Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Goder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference</title>
		<meeting>the USENIX Annual Technical Conference<address><addrLine>Santa Clara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="459" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An Efficient Implementation of a Scaling Minimum-Cost Flow Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">V</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On Implementing Scaling Push-Relabel Algorithms for the Minimum-Cost Flow Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kharitonov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network Flows and Matching: First DIMACS Implementation Challenge</title>
		<editor>D.S. Johnson and C.C. McGeoch.</editor>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>DIMACS series in discrete mathematics and theoretical computer science</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding Minimum-Cost Circulations by Successive Approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="430" to="466" />
			<date type="published" when="1990-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Queues don&apos;t matter when you can JUMP them!</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Grosvenor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ionel</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crowcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12 th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 12 th USENIX Symposium on Networked Systems Design and Implementation (NSDI)<address><addrLine>Oakland, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mesos: A platform for fine-grained resource sharing in the data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8 th USENIX Conference on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 8 th USENIX Conference on Networked Systems Design and Implementation (NSDI)<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-03" />
			<biblScope unit="page" from="295" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quincy: fair scheduling for distributed computing clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayan</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udi</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22 nd ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 22 nd ACM Symposium on Operating Systems Principles (SOSP)<address><addrLine>Big Sky, Montana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10" />
			<biblScope unit="page" from="261" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mercury: Hybrid Centralized and Distributed Scheduling in Large Shared Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Karanasos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Chaliparambil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Matteo</forename><surname>Fumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solom</forename><surname>Heddaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference</title>
		<meeting>the USENIX Annual Technical Conference<address><addrLine>Santa Clara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="485" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient implementations of minimum-cost flow algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltán</forename><surname>Király</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kovács</surname></persName>
		</author>
		<idno>CoRR abs/1207.6381</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Primal Method for Minimal Cost Flows with Applications to the Assignment and Transportation Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morton</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="205" to="220" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Solving Large-Scale Real-World Minimum-Cost Flow Problems by a Network Simplex Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Löbel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zentrum für Informationstechnik Berlin (ZIB)</title>
		<imprint>
			<date type="published" when="1996-02" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep. SC-96-07</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The case for tiny tasks in compute clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reynold</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14 th USENIX Workshop on Hot Topics in Operating Systems (HotOS)</title>
		<meeting>the 14 th USENIX Workshop on Hot Topics in Operating Systems (HotOS)<address><addrLine>Santa Ana Pueblo, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparrow: Distributed, Low Latency Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24 th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 24 th ACM Symposium on Operating Systems Principles (SOSP)<address><addrLine>Nemacolin Woodlands, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-11" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient Queue Management for Cluster Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Karanasos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Vojnovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11 th ACM European Conference on Computer Systems (EuroSys)</title>
		<meeting>the 11 th ACM European Conference on Computer Systems (EuroSys)<address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Heterogeneity and dynamicity of clouds at scale: Google trace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3 rd ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 3 rd ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Operating system support for warehouse-scale computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-10" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Omega: flexible, scalable schedulers for large compute clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Abd-El-Malek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8 th ACM European Conference on Computer Systems (EuroSys)</title>
		<meeting>the 8 th ACM European Conference on Computer Systems (EuroSys)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">TetriSched: global rescheduling with adaptive plan-ahead in dynamic heterogeneous clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Woo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11 th ACM European Conference on Computer Systems (EuroSys)</title>
		<meeting>the 11 th ACM European Conference on Computer Systems (EuroSys)<address><addrLine>London, England, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Apache Hadoop YARN: Yet Another Resource Negotiator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><forename type="middle">Kumar</forename><surname>Vavilapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><forename type="middle">C</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharad</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahadev</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4 th Annual Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 4 th Annual Symposium on Cloud Computing (SoCC)<address><addrLine>Santa Clara, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Large scale cluster management at Google</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">David</forename><surname>Pedrosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhukar</forename><surname>Korupolu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10 th ACM European Conference on Computer Systems (EuroSys)</title>
		<meeting>the 10 th ACM European Conference on Computer Systems (EuroSys)<address><addrLine>Bordeaux, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CPI 2 : CPU Performance Isolation for Shared Compute Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Jnagal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vrigo</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8 th ACM European Conference on Computer Systems (EuroSys)</title>
		<meeting>the 8 th ACM European Conference on Computer Systems (EuroSys)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
