<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Why and How of Nonnegative Matrix Factorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-03-07">7 Mar 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Gillis</surname></persName>
							<email>nicolas.gillis@umons.ac.be</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics and Operational Research Faculté Polytechnique</orgName>
								<orgName type="institution">Université de Mons Rue de Houdain 9</orgName>
								<address>
									<postCode>7000</postCode>
									<settlement>Mons</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Why and How of Nonnegative Matrix Factorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-03-07">7 Mar 2014</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1401.5226v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Nonnegative matrix factorization</term>
					<term>applications</term>
					<term>algorithms</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Nonnegative matrix factorization (NMF) has become a widely used tool for the analysis of high-dimensional data as it automatically extracts sparse and meaningful features from a set of nonnegative data vectors. We first illustrate this property of NMF on three applications, in image processing, text mining and hyperspectral imaging-this is the why. Then we address the problem of solving NMF, which is NP-hard in general. We review some standard NMF algorithms, and also present a recent subclass of NMF problems, referred to as near-separable NMF, that can be solved efficiently (that is, in polynomial time), even in the presence of noise-this is the how. Finally, we briefly describe some problems in mathematics and computer science closely related to NMF via the nonnegative rank.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Linear dimensionality reduction (LDR) techniques are a key tool in data analysis, and are widely used for example for compression, visualization, feature selection and noise filtering. Given a set of data points x j ∈ R p for 1 ≤ j ≤ n and a dimension r &lt; min(p, n), LDR amounts to computing a set of r basis elements w k ∈ R p for 1 ≤ k ≤ r such that the linear space spanned by the w k 's approximates the data points as closely as possible, that is, such that we have for all j</p><formula xml:id="formula_0">x j ≈ r k=1 w k h j (k),</formula><p>for some weights h j ∈ R r .</p><p>In other words, the p-dimensional data points are represented in a r-dimensional linear subspace spanned by the basis elements w k 's and whose coordinates are given by the vectors h j 's. LDR is equivalent to low-rank matrix approximation: in fact, constructing</p><p>• the matrix X ∈ R p×n such that each column is a data point, that is, X(:, j) = x j for 1 ≤ j ≤ n,</p><p>• the matrix W ∈ R p×r such that each column is a basis element, that is, W (:, k) = w k for 1 ≤ k ≤ r, and</p><p>• the matrix H ∈ R r×n such that each column of H gives the coordinates of a data point X(:, j) in the basis W , that is, H(:, j) = h j for 1 ≤ j ≤ n, the above LDR model (1) is equivalent to X ≈ W H, that is, to approximate the data matrix X with a low-rank matrix W H.</p><p>A first key aspect of LDR is the choice of the measure to assess the quality of the approximation. It should be chosen depending on the noise model. The most widely used measure is the Frobenius norm of the error, that is, ||X − W H|| 2 F = i,j (X − W H) 2 ij . The reason for the popularity of the Frobenius norm is two-fold. First, it implicitly assumes the noise N present in the matrix X = W H + N to be Gaussian, which is reasonable in many practical situations (see also the introduction of Section 3). Second, an optimal approximation can be computed efficiently through the truncated singular value decomposition (SVD); see <ref type="bibr" target="#b56">[57]</ref> and the references therein. Note that the SVD is equivalent to principal component analysis (PCA) after mean centering of the data points (that is, after shifting all data points so that their mean is on the origin).</p><p>A second key aspect of LDR is the assumption on the structure of the factors W and H. The truncated SVD and PCA do not make any assumption on W and H. For example, assuming independence of the columns of W leads to independent component analysis (ICA) <ref type="bibr" target="#b28">[29]</ref>, or assuming sparsity of W (and/or H) leads to sparse low-rank matrix decompositions, such as sparse PCA <ref type="bibr" target="#b31">[32]</ref>. Nonnegative matrix factorization (NMF) is an LDR where both the basis elements w k 's and the weights h j 's are assumed to be component-wise nonnegative. Hence NMF aims at decomposing a given nonnegative data matrix X as X ≈ W H where W ≥ 0 and H ≥ 0 (meaning that W and H are component-wise nonnegative). NMF was first introduced in 1994 by Paatero and Tapper <ref type="bibr" target="#b96">[97]</ref> and gathered more and more interest after an article by Lee and Seung <ref type="bibr" target="#b78">[79]</ref> in 1999.</p><p>In this paper, we explain why NMF has been so popular in different data mining applications, and how one can compute NMF's. The aim of this paper is not to give a comprehensive overview of all NMF applications and algorithms -and we apologize for not mentioning many relevant contributionsbut rather to serve as an introduction to NMF, describing three applications and several standard algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Why -NMF Generates Sparse and Meaningful Features</head><p>The reason why NMF has become so popular is because of its ability to automatically extract sparse and easily interpretable factors. In this section, we illustrate this property of NMF through three applications, in image processing, text mining and hyperspectral imaging. Other applications include air emission control <ref type="bibr" target="#b96">[97]</ref>, computational biology <ref type="bibr" target="#b33">[34]</ref>, blind source separation <ref type="bibr" target="#b21">[22]</ref>, single-channel source separation <ref type="bibr" target="#b81">[82]</ref>, clustering <ref type="bibr" target="#b34">[35]</ref>, music analysis <ref type="bibr" target="#b41">[42]</ref>, collaborative filtering <ref type="bibr" target="#b91">[92]</ref>, and community detection <ref type="bibr" target="#b105">[106]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Processing -Facial Feature Extraction</head><p>Let each column of the data matrix X ∈ R p×n + be a vectorized gray-level image of a face, with the (i, j)th entry of matrix X being the intensity of the ith pixel in the jth face. NMF generates two factors (W, H) so that each image X(:, j) is approximated using a linear combination of the columns of W ; see Equation <ref type="bibr" target="#b0">(1)</ref>, and <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration. Since W is nonnegative, the columns of W can be interpreted as images (that is, vectors of pixel intensities) which we refer to as the basis images. As the weights in the linear combinations are nonnegative (H ≥ 0), these basis images can only be summed up to reconstruct each original image. Moreover, the large number of images in the data set must be reconstructed approximately with only a few basis images (in fact, r is in general much smaller than n), hence the latter should be localized features (hence sparse) found simultaneously in several images. In the case of facial images, the basis images are features such as eyes, noses, mustaches, and lips (see <ref type="figure" target="#fig_0">Figure 1</ref>) while the columns of H indicate which feature is present in which image (see also <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b60">61]</ref>).</p><p>A potential application of NMF is in face recognition. It has for example been observed that NMF is more robust to occlusion than PCA (which generates dense factors): in fact, if a new occluded face (e.g., with sun glasses) has to be mapped into the NMF basis, the non-occluded parts (e.g., the mustache or the lips) can still be well approximated <ref type="bibr" target="#b60">[61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text Mining -Topic Recovery and Document Classification</head><p>Let each column of the nonnegative data matrix X correspond to a document and each row to a word. The (i, j)th entry of the matrix X could for example be equal to the number of times the ith word appears in the jth document in which case each column of X is the vector of word counts of a document; in practice, more sophisticated constructions are used, e.g., the term frequency -inverse document frequency (tf-idf). This is the so-called bag-of-words model: each document is associated with a set of words with different weights, while the ordering of the words in the documents is not taken into account (see, e.g., the survey <ref type="bibr" target="#b9">[10]</ref> for a discussion). Note that such a matrix X is in general rather sparse as most documents only use a small subset of the dictionary. Given such a matrix X and a factorization rank r, NMF generates two factors (W, H) such that, for all 1 ≤ j ≤ n, we have</p><formula xml:id="formula_2">X(:, j) jth document ≈ r k=1 W (:, k) kth topic H(k, j) importance of kth topic in jth document ,</formula><p>with W ≥ 0 and H ≥ 0.</p><p>This decomposition can be interpreted as follows (see, also, e.g., <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b2">3]</ref>):</p><p>• Because W is nonnegative, each column of W can be interpreted as a document, that is, as a bag of words.</p><p>• Because the weights in the linear combinations are nonnegative (H ≥ 0), one can only take the union of the sets of words defined by the columns of W to reconstruct all the original documents.</p><p>• Moreover, because the number of documents in the data set is much larger than the number of basis elements (that is, the number of columns of W ), the latter should be set of words found simultaneously in several documents. Hence the basis elements can be interpreted as topics, that is, set of words found simultaneously in different documents, while the weights in the linear combinations (that is, the matrix H) assign the documents to the different topics, that is, identify which document discusses which topic.</p><p>Therefore, given a set of documents, NMF identifies topics and simultaneously classifies the documents among these different topics. Note that NMF is closely related to existing topic models, in particular probabilistic latent semantic analysis and indexing (PLSA and PLSI) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b36">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hyperspectral Unmixing -Identify Endmembers and Classify Pixels</head><p>Let the columns of the nonnegative data matrix X be the spectral signatures of the pixels in a scene being imaged. The spectral signature of a pixel is the fraction of incident light being reflected by that pixel at different wavelengths, and is therefore nonnegative. For a hyperspectral image, there are usually between 100 and 200 wavelength-indexed bands, observed in much broader spectrum than the visible light. This allows for more accurate analysis of the scene under study.</p><p>Given a hyperspectral image (see <ref type="figure" target="#fig_1">Figure 2</ref> for an illustration), the goal of blind hyperspectral unmixing (blind HU) is two-fold:</p><p>1. Identify the constitutive materials present in the image; for example, it could be grass, roads, or metallic surfaces. These are referred to as the endmembers.</p><p>2. Classify the pixels, that is, identify which pixel contains which endmember and in which proportion. (In fact, pixels are in general mixture of several endmembers, due for example to low spatial resolution or mixed materials.)</p><p>The simplest and most popular model used to address this problem is the linear mixing model. It assumes that the spectral signature of a pixel results from the linear combination of the spectral signature of the endmembers it contains. The weights in the linear combination correspond to the abundances of these endmembers in that pixel. For example, if a pixel contains 30% of grass and 70% of road surface, then, under the linear mixing model, its spectral signature will be 0.3 times the spectral signature of the grass plus 0.7 times the spectral signature of the road surface. This is exactly the NMF model: the spectral signatures of the endmembers are the basis elements, that is, the columns of W , while the abundances of the endmembers in each pixel are the weights, that is, the columns of H. Note that the factorization rank r corresponds to the number of endmembers in the hyperspectral image. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates such a decomposition.</p><p>Therefore, given a hyperspectral image, NMF is able to compute the spectral signatures of the endmembers and simultaneously the abundance of each endmember in each pixel. We refer the reader to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b89">90]</ref> for recent surveys on blind HU techniques. ). Each column of the matrix W is the spectral signature of an endmember, while each row of the matrix H is the abundance map of the corresponding endmember, that is, it contains the abundance of all pixels for that endmember. (Note that to obtain this decomposition, we used a sparse prior on the matrix H; see Section 3.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The How -Some Algorithms</head><p>We have seen in the previous section that NMF is a useful LDR technique for nonnegative data. The question is now: can we compute such factorizations? In this paper, we focus on the following optimization problem</p><formula xml:id="formula_3">min W ∈R p×r ,H∈R r×n ||X − W H|| 2 F such that W ≥ 0 and H ≥ 0.<label>(2)</label></formula><p>Hence we implicitly assume Gaussian noise on the data; see Introduction. Although this NMF model is arguably the most popular, it is not always reasonable to assume Gaussian noise for nonnegative data, especially for sparse matrices such as document data sets; see the discussion in <ref type="bibr" target="#b22">[23]</ref>. In fact, many other objective functions are used in practice, e.g., the (generalized) Kullback-Leibler divergence for text mining <ref type="bibr" target="#b22">[23]</ref>, the Itakura-Saito distance for music analysis <ref type="bibr" target="#b41">[42]</ref>, the ℓ 1 norm to improve robustness against outliers <ref type="bibr" target="#b70">[71]</ref>, and the earth mover's distance for computer vision tasks <ref type="bibr" target="#b99">[100]</ref>. Other NMF models are motivated by statistical considerations; we refer the reader to the recent survey <ref type="bibr" target="#b101">[102]</ref>. There are many issues when using NMF in practice. In particular,</p><p>• NMF is NP-hard. Unfortunately, as opposed to the unconstrained problem which can be solved efficiently using the SVD, NMF is NP-hard in general <ref type="bibr" target="#b104">[105]</ref>. Hence, in practice, most algorithms are applications of standard nonlinear optimization methods and may only be guaranteed to converge to stationary points; see Section 3.1. However, these heuristics have been proved to be successful in many applications. More recently, Arora et al. <ref type="bibr" target="#b3">[4]</ref> described a subclass of nonnegative matrices for which NMF can be solved efficiently. These are the near-separable matrices which will be addressed in Section 3.2. Note that Arora et al. <ref type="bibr" target="#b3">[4]</ref> also described an algorithmic approach for exact NMF requiring O (pn) 2 r r 2 operations -later improved to O (pn) r 2 by Moitra <ref type="bibr" target="#b93">[94]</ref>-hence polynomial in the dimensions p and n for r fixed. Although r is usually small in practice, this approach cannot be used to solving real-world problems because of its high computational cost (in contrast, most heuristic NMF algorithms run in O(pnr) operations; see Section 3.1).</p><p>• NMF is ill-posed. Given an NMF (W, H) of X, there usually exist equivalent NMF's (W ′ , H ′ ) with W ′ H ′ = W H. In particular, any matrix Q satisfying W Q ≥ 0 and Q −1 H ≥ 0 generates such an equivalent factorization. The matrix Q can always be chosen as the permutation of a diagonal matrix with positive diagonal elements (that is, as a monomial matrix) and this amounts to the scaling and permutation of the rank-one factors W (:, k)H(k, :) for 1 ≤ k ≤ r; this is not an issue in practice. The issue is when there exist non-monomial matrices Q satisfying the above conditions. In that case, such equivalent factorizations generate different interpretations: for example, in text mining, they would lead to different topics and classifications; see the discussion in <ref type="bibr" target="#b47">[48]</ref>. Here is a simple example</p><formula xml:id="formula_4">  0 1 1 1 1 0 1 1 1 1 0 1   =   0 1 1 1 0 1 1 1 0     1 0 0 0.5 0 1 0 0.5 0 0 1 0.5   =   1 0 0 0 1 0 0 0 1     0 1 1 1 1 0 1 1 1 1 0 1   .</formula><p>We refer the reader to <ref type="bibr" target="#b65">[66]</ref> and the references therein for recent results on non-uniqueness of NMF.</p><p>In practice, this issue is tackled using other priors on the factors W and H and adding proper regularization terms in the objective function. The most popular prior is sparsity which can be tackled with projections <ref type="bibr" target="#b63">[64]</ref> or with ℓ -norm penalty <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b47">48]</ref>. For example, in blind HU (Section 2.3), the abundance maps (that is, the rows of matrix H) are usually very sparse (most pixels contain only a few endmembers) and applying plain NMF (2) usually gives poor results for these data sets. Other priors for blind HU include piece-wise smoothness of the spectral signatures or spatial coherence (neighboring pixels are more likely to contain the same materials) which are usually tackled with TV-like regularizations (that is, ℓ 1 norm of the difference between neighboring values to preserve the edges in the image); see, e.g., <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b66">67]</ref>, and the references therein. Note that the design and algorithmic implementation of refined NMF models for various applications is a very active area of research, e.g., graph regularized NMF <ref type="bibr" target="#b15">[16]</ref>, orthogonal NMF <ref type="bibr" target="#b23">[24]</ref>, tri-NMF <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b84">85]</ref>, semi and convex NMF <ref type="bibr" target="#b35">[36]</ref>, projective NMF <ref type="bibr" target="#b109">[110]</ref>, minimum volume NMF <ref type="bibr" target="#b92">[93]</ref>, and hierarchical NMF <ref type="bibr" target="#b85">[86]</ref>, to cite only a few.</p><p>• Choice of r. The choice of the factorization rank r, that is, the problem of order model selection, is usually rather tricky. Several popular approaches are: trial and error (that is, try different values of r and pick the one performing best for the application at hand), estimation using the SVD (that is, look at the decay of the singular values of the input data matrix), and the use of experts insights (e.g., in blind HU, experts might have a good guess for the number of endmembers present in a scene); see also <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b69">70]</ref> and the references therein.</p><p>In this section, we focus on the first issue. In Section 3.1, we present several standard algorithms for the general problem (2). In Section 3.2, we describe the near-separable NMF problem and several recent algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Standard NMF Algorithms</head><p>Almost all NMF algorithms designed for (2) use a two-block coordinate descent scheme (exact or inexact; see below), that is, they optimize alternatively over one of the two factors, W or H, while keeping the other fixed. The reason is that the subproblem in one factor is convex. More precisely, it is a nonnegative least squares problem (NNLS): for example, for H fixed, we have to solve min W ≥0 ||X − W H|| <ref type="bibr" target="#b1">2</ref> F . Note that this problem has a particular structure as it can be decomposed into p independent NNLS in r variables since</p><formula xml:id="formula_5">||X − W H|| 2 F = p i=1 ||X i: − W i: H|| 2 2 = p i=1 W i: HH T W T i: − 2W i: HX T i: + ||X i: || 2 2 .<label>(3)</label></formula><p>Many algorithms exist to solve the NNLS problem, and NMF algorithms based on two-block coordinate descent differ by which NNLS algorithm is used; see also, e.g., the discussion in <ref type="bibr" target="#b73">[74]</ref>. It is interesting to notice that the problem is symmetric in W and H since</p><formula xml:id="formula_6">||X − W H|| 2 F = ||X T − H T W T || 2 F .</formula><p>Therefore, we can focus on the update of only one factor and, in fact, most NMF algorithms use the same update for W and H, and therefore adhere to the framework described in Algorithm CD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm CD Two-Block Coordinate Descent -Framework of Most NMF Algorithms</head><p>Input: Input nonnegative matrix X ∈ R p×n + and factorization rank r. Output: (W, H) ≥ 0: A rank-r NMF of X ≈ W H.</p><p>1: Generate some initial matrices W (0) ≥ 0 and H (0) ≥ 0; see Section 3.1.8.</p><formula xml:id="formula_7">2: for t = 1, 2, . . . † do 3:</formula><formula xml:id="formula_8">W (t) = update X, H (t−1) , W (t−1) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><formula xml:id="formula_9">H (t) T = update X T , W (t) T , H (t−1) T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5: end for</head><p>† See Section 3.1.7 for stopping criteria.</p><p>The update in steps 3 and 4 of Algorithm CD usually guarantees the objective function to decrease. In this section, we describe the most widely used updates, that is, we describe several standard and widely used NMF algorithms, and compare them in Section 3.1.6. But first we address an important tool to designing NMF algorithms: the optimality conditions. To simplify notations, we will drop the iteration index t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">First-Order Optimality Conditions</head><p>Given X, let us denote F (W, H) = 1 2 ||X − W H|| 2 F . The first-order optimality conditions for (2) are</p><formula xml:id="formula_10">W ≥ 0, ∇ W F = W HH T − XH T ≥ 0, W • ∇ W F = 0,<label>(4)</label></formula><formula xml:id="formula_11">H ≥ 0, ∇ H F = W T W H − W T X ≥ 0, H • ∇ H F = 0,</formula><p>where • is the component-wise product of two matrices. Any (W, H) satisfying these conditions is a stationary point of (2). It is interesting to observe that these conditions give a more formal explanation of why NMF naturally generates sparse solutions <ref type="bibr" target="#b50">[51]</ref>: in fact, any stationary point of (2) is expected to have zero entries because of the conditions W • ∇ W F = 0 and H • ∇ H F = 0, that is, the conditions that for all i, k either W ik is equal to zero or the partial derivative of F with respect to W ik is, and similarly for H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Multiplicative Updates</head><p>Given X, W and H, the multiplicative updates (MU) modify W as follows</p><formula xml:id="formula_12">W ← W • XH T [W HH T ]<label>(5)</label></formula><p>where <ref type="bibr">[ ]</ref> [ ] denotes the component-wise division between two matrices. The MU were first developed in <ref type="bibr" target="#b32">[33]</ref> for solving NNLS problems, and later rediscovered and used for NMF in <ref type="bibr" target="#b79">[80]</ref>. The MU are based on the majorization-minimization framework. In fact, (5) is the global minimizer of a quadratic function majorizing F , that is, a function that is larger than F everywhere and is equal to F at the current iterate <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b79">80]</ref>. Hence minimizing that function guarantees F to decrease and therefore leads to an algorithm for which F monotonically decreases. The MU can also be interpreted as a rescaled gradient method: in fact,</p><formula xml:id="formula_13">W • XH T [W HH T ] = W − [W ] [W HH T ] • ∇ W F.<label>(6)</label></formula><p>Another more intuitive interpretation is as follows: we have that</p><formula xml:id="formula_14">XH T ik [W HH T ] ik ≥ 1 ⇐⇒ (∇ W F ) ik ≤ 0.</formula><p>Therefore, in order to satisfy (4), for each entry of W , the MU either (i) increase it if its partial derivative is negative, (ii) decrease it if its partial derivative is positive, or (iii) leave it unchanged if its partial derivative is equal to zero.</p><p>If an entry of W is equal to zero, the MU cannot modify it hence it may occur that an entry of W is equal to zero while its partial derivative is negative which would not satisfy <ref type="bibr" target="#b3">(4)</ref>. Therefore, the MU are not guaranteed to converge to a stationary point 2 . There are several ways to fix this issue, e.g., rewriting the MU as a rescaled gradient descent method -see Equation <ref type="formula" target="#formula_13">6</ref>: only entries in the same row interact-and modifying the step length <ref type="bibr" target="#b86">[87]</ref>, or using a small positive lower bound for the entries of W and H <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b102">103]</ref>; see also <ref type="bibr" target="#b4">[5]</ref>. A simpler and nice way to guarantee convergence of the MU to a stationary point is proposed in <ref type="bibr" target="#b22">[23]</ref>: use the original updates (5) while reinitializing zero entries of W to a small positive constant when their partial derivatives become negative.</p><p>The MU became extremely popular mainly because (i) they are simple to implement 3 , (ii) they scale well and are applicable to sparse matrices , and (iii) they were proposed in the paper of Lee</p><p>If the initial matrices are chosen positive, some entries can first converge to zero while their partial derivative eventually becomes negative or zero (when strict complementarity is not met) which is numerically unstable; see <ref type="bibr" target="#b51">[52]</ref> for some numerical experiments.</p><p>For example, in Matlab: W = W.*(X*H')./(W*(H*H')).</p><p>When computing the denominator W HH T in the MU, it is crucial to compute HH T first in order to have the lowest computational cost, and make the MU scalable for sparse matrices; see, e.g., footnote 3.</p><p>and Seung <ref type="bibr" target="#b78">[79]</ref> which launched the research on NMF. However, the MU converge relatively slowly; see, e.g., <ref type="bibr" target="#b61">[62]</ref> for a theoretical analysis, and Section 3.1.6 for some numerical experiments. Note that the original MU only update W once before updating H. They can be significantly accelerated using a more effective alternation strategy <ref type="bibr" target="#b51">[52]</ref>: the idea is to update W several times before updating H because the products HH T and XH T do not need to be recomputed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Alternating Least Squares</head><p>The alternating least squares method (ALS) first computes the optimal solution of the unconstrained least squares problem min W ||X − W H|| F and then project the solution onto the nonnegative orthant:</p><formula xml:id="formula_15">W ← max argmin Z∈R p×r ||X − ZH|| F , 0 ,</formula><p>where the max is taken component-wise. The method has the advantage to be relatively cheap, and easy to implement . ALS usually does not converge: the objective function of (2) might oscillate under the ALS updates (especially for dense input matrices X; see Section 3.1.6). It is interesting to notice that, because of the projection, the solution generated by ALS is not scaled properly. In fact, the error can be reduced (sometimes drastically) by multiplying the current solution W H by the constant</p><formula xml:id="formula_16">α * = argmin α≥0 ||X − αW H|| F = X, W H W H, W H = XH T , W W T W, HH T .<label>(7)</label></formula><p>Although it is in general not recommended to use ALS because of the convergence issues, ALS can be rather powerful for initialization purposes (that is, perform a few steps of ALS and then switch to another NMF algorithm), especially for sparse matrices <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Alternating Nonnegative Least Squares</head><p>Alternating nonnegative least squares (ANLS) is a class of methods where the subproblems in W and H are solved exactly, that is, the update for W is given by</p><formula xml:id="formula_17">W ← argmin W ≥0 ||X − W H|| F .</formula><p>Many methods can be used to solve the NNLS argmin W ≥0 ||X − W H|| F , and dedicated active-set methods have shown to perform very well in practice <ref type="bibr" target="#b5">6</ref> ; see <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b74">75]</ref>. Other methods are based for example on projected gradients <ref type="bibr" target="#b87">[88]</ref>, Quasi-Newton <ref type="bibr" target="#b25">[26]</ref>, or fast gradient methods <ref type="bibr" target="#b59">[60]</ref>. ANLS is guaranteed to converge to a stationary point <ref type="bibr" target="#b58">[59]</ref>. Since each iteration of ANLS computes an optimal solution of the NNLS subproblem, each iteration of ANLS decreases the error the most among NMF algorithms following the framework described in Algorithm CD. However, each iteration is computationally more expensive, and more difficult to implement. Note that, because usually the initial guess W H is a poor approximation of X, it does not make much sense to solve the NNLS subproblems exactly at the first steps of Algorithm CD, and therefore it might be profitable to use ANLS rather in a refinement step of a cheaper NMF algorithm (such as the MU or ALS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Hierarchical Alternating Least Squares</head><p>Hierarchical alternating least squares (HALS) solves the NNLS subproblem using an exact coordinate descent method, updating one column of W at a time. The optimal solutions of the corresponding subproblems can be written in closed form. In fact, the entries of a column of W do not interact -see Equation <ref type="formula" target="#formula_5">3</ref>-hence the corresponding problem can be decoupled into p quadratic problems with a single nonnegative variable. HALS updates W as follows. For ℓ = 1, 2, . . . , r:</p><formula xml:id="formula_18">W (:, ℓ) ← argmin W (:,ℓ)≥0 X − k =ℓ W (:, k)H(k, :) − W (:, ℓ)H(ℓ, :) F ← max 0, XH(ℓ, :) T − k =ℓ W (:, k) H(k, :)H(ℓ, :) T ||H(ℓ, :)|| 2 2 .</formula><p>HALS has been rediscovered several times, originally in <ref type="bibr" target="#b26">[27]</ref> (see also <ref type="bibr" target="#b24">[25]</ref>), then as the rank-one residue iteration (RRI) in <ref type="bibr" target="#b62">[63]</ref>, as FastNMF in <ref type="bibr" target="#b83">[84]</ref>, and also in <ref type="bibr" target="#b88">[89]</ref>. Actually, HALS was first described in Rasmus Bro's thesis [14, pp.161-170] (although it was not investigated thoroughly):</p><p>. . . to solve for a column vector w of W it is only necessary to solve the unconstrained problem and subsequently set negative values to zero. Though the algorithm for imposing non-negativity is thus simple and may be advantageous in some situations, it is not pursued here. Since it optimizes a smaller subset of parameters than the other approaches it may be unstable in difficult situations.</p><p>HALS was observed to converge much faster than the MU (see <ref type="bibr">[47, p.131</ref>] for a theoretical explanation, and Section 3.1.6 for a comparison) while having almost the same computational cost; see <ref type="bibr" target="#b51">[52]</ref> for a detailed account of the flops needed per iteration. Moreover, HALS is, under some mild assumptions, guaranteed to converge to a stationary point; see the discussion in <ref type="bibr" target="#b51">[52]</ref>. Note that one should be particularly careful when initializing HALS otherwise the algorithm could set some columns of W to zero initially (e.g., if W H is badly scaled with W H ≫ X) hence it is recommended to initially scale (W, H) according to <ref type="bibr" target="#b6">(7)</ref>; see the discussion in <ref type="bibr">[47, p.72]</ref>.</p><p>In the original HALS, each column of W is updated only once before updating H. However, as for the MU, it can be sped up by updating W several times before updating H <ref type="bibr" target="#b51">[52]</ref>, or selecting the entries of W to update following a Gauss-Southwell-type rule <ref type="bibr" target="#b64">[65]</ref>. HALS can also be generalized to other cost functions using Taylor expansion <ref type="bibr" target="#b82">[83]</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> displays the evolution of the objective function of (2) for the algorithms described in the previous section: on the left, the dense CBCL data set (see also <ref type="figure" target="#fig_0">Figure 1)</ref>, and, on the right, the sparse Classic document data set. As anticipated in the description of the different algorithms in the previous sections, we observe that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.6">Comparison</head><p>• The MU converge rather slowly.</p><p>• ALS oscillates for the dense matrix (CBCL data set) and performs quite poorly while, for the sparse matrix (Classic data set), it converges initially very fast but then stabilizes and cannot compute a solution with small objective function value.  <ref type="figure" target="#fig_0">Figure 1</ref>. On the right: Classic document data set with m = 7094, n = 41681 and r = 20; see, e.g., <ref type="bibr" target="#b112">[113]</ref>. The figure displays the average error using the same ten initial matrices W and H for all algorithms, randomly generated with the rand function of Matlab. All tests were performed using Matlab on a laptop Intel CORE i5-3210M CPU @2.5GHz 2.5GHz 6Go RAM. Note that, for ALS, we display the error after scaling; see Equation <ref type="formula" target="#formula_16">7</ref>. For MU and HALS, we used the implementation from https://sites.google.com/site/nicolasgillis/, for ANLS from http://www.cc.gatech.edu/~hpark/nmfsoftware.php, and ALS was implemented following footnote 5.</p><p>• ANLS performs rather well for the dense matrix and is the second best after HALS. However, it performs rather poorly for the sparse matrix.</p><p>• HALS performs the best as it generates the best solutions within the allotted time.</p><p>For other comparisons of NMF algorithms and more numerical experiments, we refer the reader to the book <ref type="bibr" target="#b27">[28]</ref>, the theses <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b46">47]</ref>, the survey <ref type="bibr" target="#b5">[6]</ref>, and the references therein.</p><p>Further research on NMF includes the design of more efficient algorithms, in particular for regularized problems; see, e.g., <ref type="bibr" target="#b97">[98]</ref> for a recent example of imposing sparsity in a more robust and stable way. We conclude this section with some comments about stopping criteria and initializations of NMF algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.7">Stopping Criterion</head><p>There are several approaches for the stopping criterion of NMF algorithms, as in usual non-linear optimization schemes, e.g., based on the evolution of the objective function, on the optimality conditions (4), or on the difference between consecutive iterates. These criteria are typically combined with either a maximum number of iterations or a time limit to ensure termination; see, e.g., the discussion in <ref type="bibr" target="#b46">[47]</ref>. In this section, we would like to point out an issue which is sometimes overlooked in the literature when using the optimality conditions to assess the convergence of NMF algorithms. A criterion based on the optimality conditions is for example C(W, H) = C W (W ) + C H (H) where</p><formula xml:id="formula_19">C W (W ) = || min(W, 0)|| F W ≥0 + || min(∇ W F, 0)|| F ∇ W F ≥0 + ||W • ∇ W F || F W •∇ W F =0 ,<label>(8)</label></formula><p>and similarly C H (H) for H, so that C(W, H) = 0 ⇐⇒ (W, H) is a stationary point of (2). There are several problems to using C(W, H) (and other similar variants) as a stopping criterion and for comparing the convergence of different algorithms:</p><p>• It is sensitive to scaling. For α &gt; 0 and α = 1, we will have in general that C W (W ) + C H (H) = C(W, H) = C(αW, α −1 H).</p><p>since the first two terms in (8) are sensitive to scaling. For example, if one solves the subproblem in W exactly and obtains C W (W ) = 0 (this will be the case for ANLS; see Section 3.1.4), then ∇ H F can be made arbitrarily small by multiplying W by a small constant and dividing H by the same constant (while, if H ≥ 0, it will not influence the first term which is equal to zero). This issue can be handled with proper normalization, e.g., imposing ||W (:, k)|| 2 = ||H(k, :)|| 2 for all k; see <ref type="bibr" target="#b62">[63]</ref>.</p><p>• The value of C(W, H) after the update of W can be very different from the value after an update of H (in particular, if the scaling is bad or if |m − n| ≫ 0). Therefore, one should be very careful when using this type of criterion to compare ANLS-type methods with other algorithms such as the MU or HALS as the evolution of C(W, H) can be misleading (in fact, an algorithm that monotonically decreases the objective function, such as the MU or HALS, is not guaranteed to monotonically decrease C(W, H).) A potential fix would be to scale the columns of W and the rows of H so that C W (W ) after the update of H and C H (H) after the update of W have the same order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.8">Initialization</head><p>A simple way to initialize W and H is to generate them randomly (e.g., generating all entries uniformly at random in the interval [0,1]). Several more sophisticated initialization strategies have been developed in order to have better initial estimates in the hope to (i) obtain a good factorization with fewer iterations, and (ii) converge to a better stationary point. However, most initialization strategies come with no theoretical guarantee (e.g., a bound on the distance of the initial point to optimality) which can be explained in part by the complexity of the problem (in fact, NMF is NP-hard in general -see the introduction of this section). This could be an interesting direction for further research. We list some initialization strategies here, they are based on</p><p>• Clustering techniques. Use the centroids computed with some clustering method, e.g., with kmeans or spherical k-means, to initialize the columns of W , and initialize H as a proper scaling of the cluster indicator matrix (that is, H kj = 0 ⇐⇒ X(:, j) belongs to the kth cluster) <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b108">109]</ref>; see also <ref type="bibr" target="#b17">[18]</ref> and the references therein for some recent results.</p><p>• The SVD. Let r k=1 u k v T k be the best rank-r approximation of X (which can be computed, e.g., using the SVD; see Introduction). Each rank-one factor u k v T k might contain positive and negative entries (except for the first one, by the Perron-Frobenius theorem 7 ). However, denoting [x] + = max(x, 0), we have</p><formula xml:id="formula_20">u k v T k = [u k ] + [v T k ] + + [−u k ] + [−v T k ] + − [−u k ] + [v T k ] + − [u k ] + [−v T k ] + ,</formula><p>and the first two rank-one factors in this decomposition are nonnegative. Boutsidis et al. <ref type="bibr" target="#b10">[11]</ref> proposed to replace each rank-one factor in r k=1</p><formula xml:id="formula_21">u k v T k with either [u k ] + [v T k ] + or [−u k ] + [−v T k ]</formula><p>+ , selecting the one with larger norm and scaling it properly.</p><p>• Column subset selection. It is possible to initialize the columns of W using data points, that is, initialize W = X(:, K) for some set K with cardinality r; see <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b111">112]</ref> and Section 3.2.</p><p>In practice, one may use several initializations, and keep the best solution obtained; see, e.g., the discussion in <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Near-Separable NMF</head><p>A matrix X is r-separable if there exists an index set K of cardinality r such that X = X(:, K)H</p><p>for some H ≥ 0.</p><p>In other words, there exists a subset of r columns of X which generates a convex cone containing all columns of X. Hence, given a separable matrix, the goal of separable NMF is to identify the subset of columns K that allows to reconstruct all columns of X (in fact, given X(:, K), H can be computed by solving a convex optimization program; see Section 3.1). The separability assumption makes sense in several applications: for example,</p><p>• In text mining (see Section 2.2), separability of the word-by-document matrix requires that for each topic, there exists a document only on that topic. Note that we can also assume separability of the transpose of X (that is, of the document-by-word matrix), i.e., for each topic there exists one word used only by that topic (referred to as an 'anchor' word). In fact, the latter is considered a more reasonable assumption in practice; see <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b38">39]</ref> and also the thesis <ref type="bibr" target="#b45">[46]</ref> for more details.</p><p>• In hyperspectral unmixing (see Section 2.3), separability of the wavelength-by-pixel matrix requires that for each endmember there exists a pixel containing only that endmember. This is the so-called pure-pixel assumption, and makes sense for relatively high spatial resolution hyperspectral images; see <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b89">90]</ref> and the references therein.</p><p>Separability has also been used successfully in blind source separation <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b21">22]</ref>, video summarization and image classification <ref type="bibr" target="#b39">[40]</ref>, and foreground-background separation in computer vision <ref type="bibr" target="#b75">[76]</ref>. Note that for facial feature extraction described in Section 2.1, separability does not make sense since we cannot expect features to be present in the data set.</p><p>It is important to points out that separable NMF is closely related to several problems, including</p><p>• Column subset selection which is a long-standing problem in numerical linear algebra (see <ref type="bibr" target="#b11">[12]</ref> and the references therein).</p><p>• Pure-pixel search in hyperspectral unmixing which has been addressed long before NMF was introduced; see <ref type="bibr" target="#b89">[90]</ref> for a historical note.</p><p>• The problem of identifying a few important data points in a data set (see <ref type="bibr" target="#b39">[40]</ref> and the references therein).</p><p>• Convex NMF <ref type="bibr" target="#b35">[36]</ref>, and the CUR decomposition <ref type="bibr" target="#b90">[91]</ref>.</p><p>Therefore, it is difficult to pinpoint the roots of separable NMF and a comprehensive overview of all methods related to separable NMF is out of the scope of this paper. However, to the best of our knowledge, it is only very recently that provably efficient algorithms for separable NMF have been proposed. This new direction of research was launched by a paper by Arora et al. <ref type="bibr" target="#b3">[4]</ref> which shows that NMF of separable matrices can be computed efficiently (that is, in polynomial time), even in the presence of noise (the error can be bounded in terms of the noise level; see below). We focus in this section on these provably efficient algorithms for separable NMF.</p><p>In the noiseless case, separable NMF reduces to identifying the extreme rays of the cone spanned by the columns of X. If the columns of the input matrix X are normalized so that their entries sum to one, that is, X(:, j) ← ||X(:, j)|| −1 1 X(:, j) for all j (and discarding the zero columns of X), then the problem reduces to identifying the vertices of the convex hull of the columns of X. In fact, since the entries of each column of X sum to one and X = X(:, K)H , the entries of each column of H must also sum to one: as X and H are nonnegative, we have for all j Therefore, the columns of X are convex combinations (that is, linear combinations with nonnegative weights summing to one) of the columns of X(:, K).</p><p>In the presence of noise, the problem is referred to as near-separable NMF, and can be formulated as follows (Near-Separable NMF) Given a noisy r-separable matrixX = X +N with X = W [I r , H ′ ]Π where W and H ′ are nonnegative matrices, Π is a permutation matrix and N is the noise, find a set K of r indices such thatX(:, K) ≈ W .</p><p>In the following, we describe some algorithms for near-separable NMF; they are classified in two categories: algorithms based on self-dictionary and sparse regression (Section 3.2.1) and geometric algorithms (Section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Self-Dictionary and Sparse Regression Framework</head><p>In the noiseless case, separable NMF can be formulated as follows min Y ∈R n×n ||Y || row,0 such that X = XY and Y ≥ 0,</p><p>where ||Y || row,0 is the number of non-zero rows of Y . In fact, if all the entries of a row of Y are equal to zero, then the corresponding column of X is not needed to reconstruct the other columns of X. Therefore, minimizing the number of rows of Y different from zero is equivalent to minimizing the number of columns of X used to reconstruct all the other columns of X, which solves the separable NMF problem. In particular, given an optimal solution Y * of (9) and denoting K = {i|Y * (i, :) = 0}, we have X = W Y * (K, :) where W = X(:, K).</p><p>In the presence of noise, the constraints X = XY are usually reformulated as ||X − XY || ≤ δ for some δ &gt; 0 or put as a penalty λ||X − XY || in the objective function for some penalty parameter λ &gt; 0. In <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, ||Y || row,0 is replaced using ℓ 1 -norm type relaxation:</p><formula xml:id="formula_23">||Y || q,1 = j ||Y (i, :)|| q ,</formula><p>where q &gt; 1 so that ||Y || q,1 is convex and (9) becomes a convex optimization problem. Note that this idea is closely related to compressive sensing where ℓ 1 -norm relaxation is used to find the sparsest solution to an underdetermined linear system. This relaxation is exact given that the matrix involved in the linear system satisfies some incoherence properties. In separable NMF, the columns and rows of matrix X are usually highly correlated hence it is not clear how to extend the results from the compressive sensing literature to this separable NMF model; see, e.g., the discussion in <ref type="bibr" target="#b89">[90]</ref>.</p><p>A potential problem in using convex relaxations of (9) is that it cannot distinguish duplicates of the columns of W . In fact, if a column of W is present twice in the data matrix X, the corresponding rows of Y can both be non-zero hence both columns of W can potentially be extracted (this is because of the convexity and the symmetry of the objective function) -in <ref type="bibr" target="#b39">[40]</ref>, k-means is used as a pre-processing in order to remove duplicates. Moreover, although this model was successfully used to solve real-world problems, no robustness results were developed so far so it is not clear how this model behaves in the presence of noise (only asymptotic results were proved, that is, when the noise level goes to zero and when no duplicates are present <ref type="bibr" target="#b39">[40]</ref>).</p><p>A rather different approach to enforce row sparsity was suggested in <ref type="bibr" target="#b8">[9]</ref>, and later improved in <ref type="bibr" target="#b53">[54]</ref>. Row sparsity of Y is enforced by (i) minimizing a weighted sum of the diagonal entries of Y hence enforcing diag(Y ) to be sparse (in fact, this is nothing but a weighted ℓ 1 norm since Y is nonnegative), and (ii) imposing all entries in a row of Y to be smaller than the corresponding diagonal entry of Y (we assume here that the columns of X are normalized). The second condition implies that if diag(Y ) is sparse then Y is row sparse. The corresponding near-separable NMF model is:</p><formula xml:id="formula_24">min Y ∈R n×n p T diag(Y ) such that ||X − XY || 1 ≤ δ and 0 ≤ Y ij ≤ Y ii ≤ 1,<label>(10)</label></formula><p>for some positive vector p ∈ R n with distinct entries (this breaks the symmetry so that the model can distinguish duplicates). This model has been shown to be robust: defining the parameter <ref type="bibr" target="#b7">8</ref>  Finally, a drawback of the approaches based on self-dictionary and sparse regression is that they are computationally expensive as they require to tackle optimization problems with n 2 variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Geometric Algorithms</head><p>Another class of near-separable algorithms are based on geometric insights and in particular on the fact that the columns of W are the vertices of the convex hull of the normalized columns of X. The first geometric algorithms can be found in the remote sensing literature (they are referred to as endmember extraction algorithms or pure-pixel identification algorithms), see <ref type="bibr" target="#b89">[90]</ref> for a historical note; and <ref type="bibr" target="#b7">[8]</ref> for a comprehensive survey. Because of the large body of literature, we do not aim at surveying all algorithms but rather focus on a single algorithm which is particularly simple while being rather effective in practice: the successive projection algorithm (SPA). Moreover, the ideas behind SPA are at the heart of many geometric-based near-separable NMF algorithms (see below).</p><p>SPA looks for the vertices of the convex hull of the columns of the input data matrix X and works as follows: at each step, it selects the column of X with maximum ℓ 2 norm and then updates X by projecting each column onto the orthogonal complement of the extracted column; see Algorithm SPA. SPA is extremely fast as it can be implemented in 2pnr + O(pr 2 ) operations, using the formula ||(I − uu T )v|| 2 2 = ||v|| 2 2 − (u T v) 2 , for any u, v ∈ R m with ||u|| 2 = 1 <ref type="bibr" target="#b54">[55]</ref>. Moreover, if r is unknown, it can be estimated using the norm of the residual R.</p><p>Algorithm SPA Successive Projection Algorithm <ref type="bibr" target="#b1">[2]</ref> Input: Near-separable matrixX = W [I r , H ′ ]Π + N where W is full rank, H ′ ≥ 0, the entries of each column of H ′ sum to at most one, Π is a permutation and N is the noise, and the number r of columns of W . Output: Set of r indices K such thatX(:, K) ≈ W (up to permutation). Let us prove the correctness of SPA in the noiseless case using induction, and assuming W is full rank (this is a necessary and sufficient condition) and assuming the entries of each column of H ′ sum to at most one (this can be achieved through normalization; see above). At the first step, SPA identifies a column of W because the ℓ 2 norm can only be maximized at a vertex of the convex hull of a set of points; see <ref type="figure" target="#fig_6">Figure 4</ref> for an illustration. In fact, for all 1 ≤ j ≤ n,</p><formula xml:id="formula_25">||X(:, j)|| 2 = ||W H(:, j)|| 2 ≤ r k=1 H(k, j)||W (:, k)|| 2 ≤ max 1≤k≤r ||W (:, k)|| 2 .</formula><p>The first inequality follows from the triangle inequality, and the second since H(k, j) ≥ 0 and k H(k, j) ≤ 1. Moreover, by strong convexity of the ℓ 2 norm and the full rank assumption on W , the first inequality is strict unless H(:, k) is a column of the identity matrix, that is, unless X(:, j) = W (:, k) for some k. For the induction step, assume without loss of generality that SPA has extracted the first ℓ columns of W , and let W ℓ = W (:, 1:ℓ) and P ⊥ W ℓ be the projection onto the orthogonal complement of the columns of W ℓ so that P ⊥ W ℓ W ℓ = 0. We have, for all 1 ≤ j ≤ n,</p><formula xml:id="formula_26">||P ⊥ W ℓ X(:, j)|| 2 = ||P ⊥ W ℓ W H(:, j)|| 2 ≤ r k=1 H(k, j)||P ⊥ W ℓ W (:, k)|| 2 ≤ max ℓ+1≤k≤r ||P ⊥ W ℓ W (:, k)|| 2 ,</formula><p>where P ⊥ W ℓ W (:, k) = 0 for ℓ + 1 ≤ k ≤ r since W is full rank. Hence, using the same reasoning as above, SPA will identify a column of W not extracted yet, which concludes the proof.</p><p>Moreover, SPA is robust to noise: given a near-separable matrixX = W [I r , H ′ ]Π + N with W full rank, H ′ nonnegative with ||H ′ (:, j)|| 1 ≤ 1 ∀j, and ǫ = max identifies the columns of W up to ℓ 2 error proportional to O ǫ κ 2 (W ) , where κ(W ) = σmax(W ) σ min (W ) <ref type="bibr" target="#b54">[55,</ref><ref type="bibr">Th.3]</ref>. These bounds can be improved using post-processing (see below) which reduces the error to O (ǫ κ(W )) <ref type="bibr" target="#b2">[3]</ref>, or preconditioning which significantly increases the upper bound on the noise level, to</p><formula xml:id="formula_27">j ||N (:, j)|| 2 ≤ O σ min (W ) √ rκ 2 (W ) , SPA</formula><formula xml:id="formula_28">ǫ ≤ O σ min (W ) r √ r</formula><p>, and reduces the error to O (ǫ κ(W )) <ref type="bibr" target="#b55">[56]</ref>.</p><p>It is interesting to note that SPA has been developed and used for rather different purposes in various fields:</p><p>• Numerical linear algebra. SPA is closely related to the modified Gram-Schmidt algorithm with column pivoting, used for example to solve linear least squares problems <ref type="bibr" target="#b14">[15]</ref>.</p><p>• Chemistry (and in particular spectroscopy). SPA is used for variable selection in spectroscopic multicomponent analysis; in fact, the name SPA comes from <ref type="bibr" target="#b1">[2]</ref>.</p><p>• Hyperspectral imaging. SPA is closely related to several endmember extraction algorithms; in particular N-FINDR <ref type="bibr" target="#b107">[108]</ref> and its variants, the automatic target generation process (ATGP) <ref type="bibr" target="#b98">[99]</ref>, and the successive volume maximization algorithm (SVMAX) <ref type="bibr" target="#b20">[21]</ref>; see the discussion in <ref type="bibr" target="#b89">[90]</ref> for more details. The motivation behind all these approaches is to identify an index set K that maximizes the volume of the convex hull of the columns of X(:, K). Note that most endmember extraction algorithms use an LDR (such as the SVD) as a pre-processing step for noise filtering, and SPA can be combined with an LDR to improve performance.</p><p>• Text mining. Arora et al. <ref type="bibr" target="#b2">[3]</ref> proposed FastAnchorWords whose differences with SPA are that (i) the projection is made onto the affine hull of the columns extracted so far (instead of the linear span), and (ii) the index set extracted is refined using the following post-processing step: let K be the extracted index set by SPA, for each k ∈ K:</p><p>-Compute the projection R of X into the orthogonal complement of X(:, K\{k}).</p><p>-Replace k with the index corresponding to the column of R with maximum ℓ 2 norm.</p><p>• Theoretical computer science. SPA was proved to be a good heuristic to identify a subset of columns of a given matrix whose convex hull has maximum volume <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> (in the sense that no polynomial-time algorithm can achieve better performance up to some logarithmic factors).</p><p>• Sparse regression with self-dictionary. SPA is closely related to orthogonal matching pursuit and can be interpreted as a greedy method to solve the sparse regression problem with selfdictionary (9); see <ref type="bibr" target="#b43">[44]</ref> and the references therein.</p><p>Moreover, there exist many geometric algorithms which are variants of SPA, e.g., vertex component analysis (VCA) using linear functions instead of the ℓ 2 -norm <ref type="bibr" target="#b95">[96]</ref>, ℓ p -norm based pure pixel algorithm (TRI-P) using p-norms <ref type="bibr" target="#b0">[1]</ref>, FastSepNMF using strongly convex functions <ref type="bibr" target="#b54">[55]</ref>, the successive nonnegative projection algorithm (SNPA) <ref type="bibr" target="#b49">[50]</ref> and the fast conical hull algorithm (XRAY) <ref type="bibr" target="#b76">[77]</ref> using nonnegativity constraints for the projection step.</p><p>Further research on near-separable NMF includes the design of faster and/or provably more robust algorithms. In particular, there does not seem to exist an algorithm guaranteed to be robust for any matrix W such that α(W ) &gt; 0 and running in O(n) operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Connections with Problems in Mathematics and Computer Science</head><p>In this section, we briefly mention several connections of NMF with problems outside data mining and machine learning. The minimum r such that an exact NMF of a nonnegative matrix X exists is the nonnegative rank of X, denoted rank + (X). More precisely, given X ∈ R p×n + , rank + (X) is the minimum r such that there exist W ∈ R p×r + and H ∈ R r×n + with X = W H. The nonnegative rank has tight connections with several problems in mathematics and computer science:</p><p>• Graph Theory. Let G(X) = (V 1 ∪ V 2 , E) be the bipartite graph induced by X (that is, (i, j) ∈ E ⇐⇒ X ij = 0). The minimum biclique cover bc(G(X)) of G(X) is the minimum number of complete bipartite subgraphs needed to cover G(X). It can be checked easily that for any (W, H) ≥ 0 such that X = W H = r k=1 W :k H k: , we have G(X) = ∪ r k=1 G(W :k H k: ),</p><p>where G(W :k H k: ) are complete bipartite subgraphs hence bc(G(W :k H k: )) = 1 ∀k. Therefore, bc(G(X)) ≤ rank + (X).</p><p>This lower bound on the nonnegative rank is referred to as the rectangle covering bound <ref type="bibr" target="#b42">[43]</ref>.</p><p>• Extended Formulations. Given a polytope P , an extended formulation (or lift, or extension) is a higher dimensional polyhedron Q and a linear projection π such that π(Q) = P . When the polytope P has exponentially many facets, finding extended formulations of polynomial size is of great importance since it allows to solve linear programs (LP) over P in polynomial time. It turns out that the minimum number of facets of an extended formulation Q of a polytope P is equal to the nonnegative rank of its slack matrix <ref type="bibr" target="#b110">[111]</ref>, defined as X(i, j) = a T i v j − b i where v j is the jth vertex of P and {x ∈ R n | a T i x − b i ≥ 0} its ith facet with a i ∈ R n and b i ∈ R, that is, X is a facet-by-vertex matrix and X(i, j) is the slack of the jth vertex with respect to ith facet; see the surveys <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b68">69]</ref> and the references therein. These ideas can be generalized to approximate extended formulations, directly related to approximate factorizations (hence NMF) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>• Probability. Let X (k) ∈ {1, . . . , p} and Y (k) ∈ {1, . . . , n} be two independent variables for each ≤ k ≤ r, and P (k) be the joint distribution with P (k) ij = P X (k) = i, Y (k) = j = P X (k) = i P Y (k) = j .</p><p>Each distribution P (k) corresponds to a nonnegative rank-one matrix. Let us define the mixture P of these k independent distributions as follows:</p><p>-Choose the distribution P (k) with probability α k , where r k=1 α k = 1. -Draw X and Y from the distribution P (k) .</p><p>We have that P = r k=1 α k P (k) is the sum of r rank-one nonnegative matrices. In practice, only P is observed and computing its nonnegative rank and a corresponding factorization amounts to explaining the distribution P with as few independent variables as possible; see <ref type="bibr" target="#b16">[17]</ref> and the references therein. Alice only knows x and Bob y, and the aim is to minimize the number of bits exchanged between Alice and Bob to compute f exactly. Nondeterministic communication complexity (NCC) is a variant where Bob and Alice first receive a message before starting their communication; see <ref type="bibr" target="#b80">[81,</ref><ref type="bibr">Ch.3]</ref> and the references therein for more details. The communication matrix X ∈ {0, 1} 2 n ×2 m is equal to the function f for all possible combinations of inputs. Yannakakis <ref type="bibr" target="#b110">[111]</ref> showed that the NCC for computing f is upper bounded by the logarithm of the nonnegative rank of the communication matrix (this result is closely related to the rectangle covering bound described above: in fact, ⌈log(bc(G(X))⌉ equals to the NCC of f ).</p><p>• Computational Geometry. Computing the nonnegative rank is closely related to the problem of finding a polytope with minimum number of vertices nested between two given polytopes <ref type="bibr" target="#b52">[53]</ref>. This is a well-known problem is computational geometry, referred to as the nested polytopes problem; see <ref type="bibr" target="#b30">[31]</ref> and the references therein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>NMF is an easily interpretable linear dimensionality reduction technique for nonnegative data. It is a rather versatile technique with many applications, and brings together a broad range of researchers.</p><p>In the context of 'Big Data' science, which becomes an increasingly important topic, we believe NMF has a bright future; see <ref type="figure" target="#fig_8">Figure 5</ref> for an illustration of the number of publications related to NMF since the publication of the Lee and Seung paper <ref type="bibr" target="#b78">[79]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Decomposition of the CBCL face database, MIT Center For Biological and Computation Learning (2429 gray-level 19-by-19 pixels images) using r = 49 as in [79].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Decomposition of the Urban hyperspectral image from http://www.agc.army.mil/, constituted mainly of six endmembers (r = 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of MU, ALS, ANLS and HALS. On the left: CBCL facial images with r = 49; same data set as in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 =</head><label>1</label><figDesc>||X(:, j)|| 1 = ||X(:, K)H(:, j)|| 1 = k ||X(:, K(k))|| 1 H(k, j) = k H(k, j) = ||H(:, j)|| 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 +</head><label>1</label><figDesc>||W (:, j) − W (:, J j )x|| 1 ,whereJ j = {1,2, . . . , r}\{j}, and for a near-separable matrixX = W [I r , H ′ ]Π + N (see above) with ǫ = max j ||N (:, j)|| 1 ≤ O α 2 (W ) r , the model (10) can be used to identify the columns of W with ℓ 1 error proportional to O rǫ α(W ) , that is, the identified index set K satisfies max j min k∈K ||X(:, k) − W (:, j)|| 1 ≤ O rǫ α(W ) ; see [54, Th.7] for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 :</head><label>1</label><figDesc>Let R =X, K = {}. 2: for k = 1 : r do 3:p = argmax j ||R :j || 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of SPA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>•</head><label></label><figDesc>Communication Complexity. In its simplest variant, communication complexity addresses the following problem: Alice and Bob have to compute the following function f : {0, 1} m × {0, 1} n → {0, 1} : (x, y) → f (x, y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Number of search results for papers containing either 'nonnegative matrix factorization' or 'nonnegative matrix factorization' on Google Scholar and Scopus (as of December 12, 2013).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Exact NMF refers to the NMF problem where an exact factorization is sought: X = W H with W ≥ 0 and H ≥ 0.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For example, in Matlab: W = max(0,(X*H')/(H*H')).In particular, the Matlab function lsqnonneg implements an active-set method from<ref type="bibr" target="#b77">[78]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Actually, the first factor could contain negative entries if the input matrix is reducible and its first two singular values are equal to one another; see, e.g.,[47, p.16].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The larger the parameter α is, the less sensitive the data to noise. For example, it can be easily checked that ǫ = maxj ||N (:, j)||1 &lt; α 2 is a necessary condition to being able to distinguish the columns of W<ref type="bibr" target="#b48">[49]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The author would like to thank Rafal Zdunek, Wing-Kin Ma, Marc Pirlot and the Editors of the book 'Regularization, Optimization, Kernels, and Support Vector Machines' for insightful comments which helped improve the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hyperspectral data geometry based estimation of number of endmembers using p-norm based pure pixel identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ambikapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2753" to="2769" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The successive projections algorithm for variable selection in spectroscopic multicomponent analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saldanha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Galvão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoneyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Visani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemometrics and Intelligent Laboratory Systems</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="73" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML &apos;13)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computing a nonnegative matrix factorization -provably</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 44th Symp. on Theory of Computing (STOC &apos;12)</title>
		<meeting>of the 44th Symp. on Theory of Computing (STOC &apos;12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="145" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stability analysis of multiplicative update algorithms and application to nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Badeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1869" to="1881" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithms and Applications for Approximate Nonnegative Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Langville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plemmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="155" to="173" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Estimation of signal subspace on hyperspectral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nascimento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Remote Sensing, p. 59820L. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyperspectral unmixing overview: Geometrical, statistical, and sparse regression-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dobigeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Parente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="354" to="379" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factoring nonnegative matrices with linear programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bittorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS &apos;12)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SVD based initialization: A head start for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1350" to="1362" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An improved approximation algorithm for the column subset selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th Annual ACM-SIAM Symp. on Discrete Algorithms (SODA &apos;09)</title>
		<meeting>of the 20th Annual ACM-SIAM Symp. on Discrete Algorithms (SODA &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximation limits of linear programs (beyond hierarchies)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fiorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pokutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 53rd Annual IEEE Symp. on Foundations of Computer Science (FOCS&apos; 12)</title>
		<meeting>of the 53rd Annual IEEE Symp. on Foundations of Computer Science (FOCS&apos; 12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="480" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-way analysis in the food industry: Models, algorithms, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bro</surname></persName>
		</author>
		<ptr target="http://curis.ku.dk/ws/files/13035961/Rasmus_Bro.pdf" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>University of Copenhagen</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linear least squares solutions by householder transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Businger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="269" to="276" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probability matrices, non-negative rank, and parameterization of mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rapallo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">433</biblScope>
			<biblScope unit="page" from="424" to="432" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Subtractive clustering for seeding non-negative matrix factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Casalino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Del Buono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mencar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="issue">257</biblScope>
			<biblScope unit="page" from="369" to="387" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On selecting a maximum volume sub-matrix of a matrix and related problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ç Ivril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magdon-Ismail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">410</biblScope>
			<biblScope unit="page" from="4801" to="4811" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exponential inapproximability of selecting a maximum volume sub-matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ç Ivril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magdon-Ismail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="176" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A simplex volume maximization framework for hyperspectral endmember extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ambikapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4177" to="4193" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A convex analysis framework for blind separation of nonnegative sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5120" to="5134" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On tensors, sparsity, and nonnegative factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kolda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1272" to="1299" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Algorithms for orthogonal nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Joint Conf. on Neural Networks</title>
		<meeting>of the Int. Joint Conf. on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1828" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast local algorithms for large scale Nonnegative Matrix and Tensor Factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Phan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. on Fundamentals of Electronics</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="708" to="721" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-negative Matrix Factorization with Quasi-Newton Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4029</biblScope>
			<biblScope unit="page" from="870" to="879" />
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical ALS Algorithms for Nonnegative Matrix and 3D Tensor Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4666</biblScope>
			<biblScope unit="page" from="169" to="176" />
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Amari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Wiley-Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Independent component analysis, A new concept?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="287" to="314" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Extended formulations in combinatorial optimization. 4OR: A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Conforti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cornuéjols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zambelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Operations Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Complexity of Minimum Convex Nested Polyhedra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Canadian Conf. on Computational Geometry</title>
		<meeting>of the 2nd Canadian Conf. on Computational Geometry</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Direct Formulation for Sparse PCA Using Semidefinite Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>El Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="434" to="448" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An iterative image space reconstruction algorithm suitable for volume ECT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daube-Witherspoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Muehllehner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="61" to="66" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Nonnegative Matrix Factorization: An Analytical and Interpretive Tool in Computational Biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Devarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1000029</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the Equivalence of Nonnegative Matrix Factorization and Spectral Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM Int. Conf. Data Mining (SDM&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="606" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convex and semi-nonnegative matrix factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3913" to="3927" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Orthogonal nonnegative matrix t-factorizations for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>of the 12th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="126" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Topic discovery through data dependent and random projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML &apos;13)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="471" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">See all by looking at a few: Sparse modeling for finding representative objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR &apos;12</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A convex model for nonnegative matrix factorization and dimensionality reduction on physical space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3239" to="3252" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with the Itakura-Saito divergence: With application to music analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Durrieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="793" to="830" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Combinatorial bounds on nonnegative rank and extended formulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fiorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kaibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pashkovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Mathematics</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="83" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Greedy algorithms for pure pixels identification in hyperspectral unmixing: A multiple-measurement vector viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Iordache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 21st European Signal Processing Conf. (EUSIPCO &apos;13</title>
		<meeting>of 21st European Signal essing Conf. (EUSIPCO &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Relation between PLSA and NMF and implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval</title>
		<meeting>of the 28th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="601" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Provable algorithms for machine learning problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<ptr target="http://dataspace.princeton.edu/jspui/bitstream/88435/dsp019k41zd62n/1/Ge_princeton_0181D_10819.pdf" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Nonnegative matrix factorization: Complexity, algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/nicolasgillis/" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Université catholique de Louvain</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sparse and unique nonnegative matrix factorization through data preprocessing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3349" to="3386" />
			<date type="published" when="2012-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robustness analysis of Hottopixx, a linear programming model for factoring nonnegative matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1189" to="1212" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Successive nonnegative projection algorithm for robust nonnegative blind source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.7529</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Using underapproximations for sparse nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Glineur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1676" to="1687" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Accelerated multiplicative updates and hierarchical ALS algorithms for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Glineur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1085" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On the geometric interpretation of the nonnegative rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Glineur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">437</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2685" to="2712" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust near-separable nonnegative matrix factorization using linear optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fast and robust recursive algorithms for separable nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vavasis</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2013.226</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Semidefinite programming based preconditioning for more robust near-separable nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vavasis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.2273</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computation</title>
		<meeting><address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<publisher>The Johns Hopkins University Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note>3rd Edition</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Approximate cone factorizations and lifts of polytopes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gouveia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thomas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.2162</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On the convergence of the block nonlinear Gauss-Seidel method under convex constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grippo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sciandrone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="127" to="136" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">NeNMF: an optimal gradient method for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2882" to="2898" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guillamet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vitrià</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="336" to="344" />
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">On the rate of convergence of the image space reconstruction algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operators and Matrices</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="58" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Nonnegative matrix factorization -algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Ho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Université catholique de Louvain</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with sparseness constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fast coordinate descent methods with variable selection for non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th ACM SIGKDD int. conf. on Knowledge discovery and data mining</title>
		<meeting>of the 17th ACM SIGKDD int. conf. on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1064" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization revisited: Uniqueness and algorithm for symmetric decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="211" to="224" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Total variation spatial regularization for sparse hyperspectral unmixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Iordache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4484" to="4502" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Constrained nonnegative matrix factorization for hyperspectral unmixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="173" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Extended Formulations in Combinatorial Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kaibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optima</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="2" to="7" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Rank selection in low-rank matrix approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS &apos;10)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Robust L 1 norm factorization in the presence of outliers and missing data by alternative convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR &apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1495" to="1502" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Non-negative Matrix Factorization Based on Alternating Non-negativity Constrained Least Squares and Active Set Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="713" to="730" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Algorithms for nonnegative matrix and tensor factorizations: A unified view based on block coordinate descent framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10898-013-0035-4</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Fast nonnegative matrix factorization: An active-set-like method and comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3261" to="3281" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Near-separable non-negative matrix factorization with ℓ 1 -and Bregman loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.7167</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Fast conical hull algorithms for near-separable non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kambadur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning (ICML &apos;13)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="231" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Solving Least Squares Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hanson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning the Parts of Objects by Nonnegative Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Algorithms for Non-negative Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing (NIPS &apos;01)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Lower bounds in communication complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shraibman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Dictionary learning methods for single-channel source separations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lefèvre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Ecole Normale Supérieure de Cachan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Fast Bregman divergence NMF using Taylor expansion and coordinate descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 18th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>of the 18th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">FastNMF: highly efficient monotonic fixed-point nonnegative matrix factorization algorithm with good applicability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imaging</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">033004</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A non-negative matrix tri-factorization approach to sentiment classification with lexical prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association of Computational Lingustics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Hierarchical non-negative matrix factorization (hNMF): a tissue pattern differentiation method for glioblastoma multiforme diagnosis using MRSI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Cauter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Croitor Sava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Himmelreich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Huffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NMR in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="319" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">On the convergence of multiplicative update algorithms for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1589" to="1596" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Projected Gradient Methods for Nonnegative Matrix Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2756" to="2779" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Sparse non-negative tensor factorization using columnwise coordinate descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="649" to="656" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">A Signal Processing Perspective on Hyperspectral Unmixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ambikapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="81" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">CUR matrix decompositions for improved data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="697" to="702" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Recommender systems. Encyclopedia of machine learning 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="829" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Endmember extraction from highly mixed data using minimum volume constrained nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="765" to="777" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">An almost optimal algorithm for computing nonnegative rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th Annual ACM-SIAM Symp. on Discrete Algorithms (SODA &apos;13)</title>
		<meeting>of the 24th Annual ACM-SIAM Symp. on Discrete Algorithms (SODA &apos;13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1454" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Blind source separation of positive and partially correlated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Naanaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Nuzillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1711" to="1722" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Vertex component analysis: a fast algorithm to unmix hyperspectral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="898" to="910" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Positive matrix factorization: a non-negative factor model with optimal utilization of error estimates of data values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paatero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Tapper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmetrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Sparse and non-negative BSS for noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rapin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Larue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Starck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="5620" to="5632" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Automatic spectral target recognition in hyperspectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Aerospace and Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1232" to="1249" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with earth mover&apos;s distance metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR &apos;09)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1873" to="1880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Document clustering using nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahnaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pauca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Plemmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="373" to="386" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">A Unied View of Static and Dynamic Source Separation Using Non-Negative Factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mohammadiha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Global convergence of modified multiplicative updates for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hibi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10589-013-9593-0</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Automatic relevance determination in nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing with Adaptive Sparse Structured Representations (SPARS &apos;09</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">On the complexity of nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vavasis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1364" to="1377" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Community discovery using nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Disc</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="493" to="521" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Improving non-negative matrix factorizations through structured initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Curry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2217" to="2232" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">N-FINDR: an algorithm for fast autonomous spectral end-member determination in hyperspectral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Conf. on Imaging Spectrometry V</title>
		<meeting>SPIE Conf. on Imaging Spectrometry V</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="266" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Clustering-based initialization for non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Linear and nonlinear projective nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="734" to="749" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Expressing Combinatorial Optimization Problems by Linear Programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yannakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="466" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Initialization of nonnegative matrix factorization with vertices of convex polytope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zdunek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Soft Computing</title>
		<imprint>
			<biblScope unit="volume">7267</biblScope>
			<biblScope unit="page" from="448" to="455" />
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Generative model-based document clustering: a comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="374" to="384" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
