<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Lifecycle Management Complexity of Datacenter Topologies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><forename type="middle">Niranjan</forename><surname>Mysore</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sucha</forename><surname>Supittayapornpong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Govindan</surname></persName>
						</author>
						<title level="a" type="main">Understanding Lifecycle Management Complexity of Datacenter Topologies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Most recent datacenter topology designs have focused on performance properties such as latency and throughput. In this paper, we explore a new dimension, life cycle management complexity, which attempts to understand the complexity of deploying a topology and expanding it. By analyzing current practice in lifecycle management, we devise complexity metrics for lifecycle management, and show that existing topology classes have low lifecycle management complexity by some measures, but not by others. Motivated by this, we design a new class of topologies, FatClique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past decade, there has been a long line of work on designing datacenter topologies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1]</ref>. While most have focused on performance properties such as latency and throughput, and on resilience to link and switch failures, datacenter lifecycle management <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref> has largely been overlooked. Lifecycle management is the process of building a network, physically deploying it on a data-center floor, and expanding it over several years so that it is available for use by a constantly increasing set of services.</p><p>With datacenters living on for years, sometimes up to a decade <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b11">12]</ref>, their lifecycle costs can be high. A data center design that is hard to deploy can stall the rollout of services for months; this can be expensive considering the rate at which network demands have historically increased <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23]</ref>. A design that is hard to expand can leave the network functioning with degraded capacity impacting the large array of services that depend on it.</p><p>It is therefore desirable to commit to a data-center network design only after getting a sense of its lifecycle management cost and complexity over time. Unfortunately, the costs of the large array of components needed for deployment such as switches, transceivers, cables, racks, patch panels <ref type="bibr" target="#b0">1</ref> , and cable trays, are proprietary and change over time, and so are hard to quantify. An alternative approach is to develop complexity measures (as opposed to dollar costs) for lifecycle management, but as far as we know, no prior work has addressed this. In part, this is due to the fact that intuitions about lifecycle management are developed over time and with operations experience, and these lessons are not made available universally.</p><p>Unfortunately, in our experience, this lack of a clear understanding of lifecycle management complexity often results in costly mistakes in the design of datacenters that are discovered during deployment and therefore cannot be rectified. Our paper is a first step towards useful characterizations of lifecycle management complexity. Contributions. To this end, our paper makes three contributions. First, we design several complexity metrics ( §3 and §4) that can be indicative of lifecycle management costs (i.e., capital expenditure, time and manpower required). These metrics include the number of: switches, patch panels, bundle-types, expansion steps, and links to be re-wired at a patch panel rack during an expansion step.</p><p>We design these metrics by identifying structural elements of network deployments that make their deployment and expansion challenging. For instance, the number of switches in the topology determines how complex the network is in terms of packaging -laying out switches into homogeneous racks in a space efficient manner. Wiring complexity can be assessed by the number of cable bundles and the patch panels a design requires. As these increase, the complexity of manufacturing and packaging all the different cable bundles efficiently into cable trays, and then routing them from one patch panel to the next can be expected to increase. Finally, because expansion is carried out in steps <ref type="bibr" target="#b37">[38]</ref>, where the network operates at degraded capacity at each step, the number of expansion steps is a measure of the reduced availability in the network induced by lifecycle management. Wiring patterns also determine the number of links that need to be rewired at a patch panel during each step of expansion, a measure of step complexity <ref type="bibr" target="#b37">[38]</ref>.</p><p>Our second contribution is to use these metrics to compare the lifecycle management costs of two main classes of datacenter topologies recently explored in the research literature ( §2), Clos <ref type="bibr" target="#b1">[2]</ref> and expander graphs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>. We find that neither class dominates the other: Clos has relatively lower wiring complexity; its symmetric design leads to more uniform bundling (and fewer cable bundle types); but expander graphs at certain scales can have simpler packaging requirements due to their edge expansion property <ref type="bibr" target="#b31">[32]</ref>; they end up using much fewer switches than Clos to achieve the same network capacity. Expander graphs also demonstrate better expansion properties because they have fat edges ( §4) which permit more links to be rewired in each step.</p><p>Finally we design and synthesize a novel and practical class of topologies called FatClique ( §5), that has lower overall lifecycle management complexity compared to Clos and expander graphs. We do this by combining favorable design elements from these two topology classes. By design, Fat-Clique incorporates 3 levels of hierarchy and uses a clique as a building block while ensuring edge expansion. At every level of its hierarchy, FatClique is designed to have fat edges, for easier expansion, while utilizing much fewer patch panels and therefore inter-rack cabling.</p><p>Evaluations of these topology classes at three different scales, the largest of which is 16× the size of Jupiter, shows that FatClique is the best at most scales by all our complexity metrics. It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we are able to derive from published cable prices). Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take 5 × longer to expand the topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Data center topology families. Data centers are often designed for high throughput, low latency and resilience. Existing data center designs can be broadly classified into the following families: (a) Clos-like tree topologies, e.g., Google's Jupiter <ref type="bibr" target="#b30">[31]</ref>, Facebook's fbfabric <ref type="bibr" target="#b2">[3]</ref>, Microsoft's VL2 <ref type="bibr" target="#b12">[13]</ref>, F10 <ref type="bibr" target="#b21">[22]</ref>; (b) Expander graph based topologies, e.g., Jellyfish <ref type="bibr" target="#b31">[32]</ref>, Xpander <ref type="bibr" target="#b34">[35]</ref>; (c) 'Direct' topologies built from multi-port servers, e.g., BCube <ref type="bibr" target="#b13">[14]</ref>, DCell <ref type="bibr" target="#b14">[15]</ref>. (d) Low diameter, strongly-connected topologies that rely on highradix switches, e.g., Slimfly <ref type="bibr" target="#b3">[4]</ref>, Dragonfly <ref type="bibr" target="#b19">[20]</ref>; (e) Reconfigurable optical topologies like Rotornet and Project-ToR <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Of these, Clos and Expander based topologies have been shown to scale using widely deployed merchant silicon. The ecosystem around the hardware used by these two classes, e.g., cabling, cable trays used, rack sizes, is mature and wellunderstood, allowing us to quantify some of the operational complexity of these topologies.</p><p>Direct multi-port server topologies and some reconfigurable optical topologies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39]</ref> rely on newer hardware technologies that are not mainstream yet. It is hard to quantify the operational costs of these classes without making significant assumptions about such hardware. Low diameter topologies like Slimfly <ref type="bibr" target="#b3">[4]</ref> and Dragonfly <ref type="bibr" target="#b19">[20]</ref>, can be built with hardware that is available today, but they require strongly connected groups of switches. Their incremental expansion comes at high cost and complexity; high-radix switches either need to be deployed well in advance, or every switch in the topology needs to be upgraded during expansion, to preserve low diameter.</p><p>To avoid estimating operational complexity of topologies that rely on new hardware, or on topologies that unacceptably constrain expansion, we focus on the Clos and Expander families. Clos. A logical Clos topology with N servers can be constructed using switches with radix k connected in n = log k 2 ( N ) layers based on a canonical recursive algorithm in <ref type="bibr" target="#b35">[36]</ref> 2 . Fattree <ref type="bibr" target="#b1">[2]</ref> and Jupiter <ref type="bibr" target="#b30">[31]</ref> are special cases of Clos topology with 3 and 5 layers respectively. Clos construction naturally allows switches to be packaged together to form a chassis <ref type="bibr" target="#b30">[31]</ref>. Since there are no known generic Clos packaging algorithm that can help design such a chassis, for a Clos of any scale, we designed one to help our study of its operational complexity. We present this algorithm in §A.1. Expander graphs. Jellyfish and Xpander benefit from the high edge expansion property of expander graph to use a near optimal number of switches, while achieving the same bisection bandwidth as Clos based topologies <ref type="bibr" target="#b34">[35]</ref>. Xpander splits N servers among switches by attaching s servers to each switch. With a k port switch, the remaining ports p = k − s are connected to other switches that are organized in p blocks called metanodes. Metanodes are a group of switches, containing l = N/(s • (p + 1)) switches, which increase as topology scale N increases. There are no connections between the switches of a metanode. Jellyfish is a degree bounded random graph (see <ref type="bibr" target="#b31">[32]</ref> for more details). Takeaway. A topology with high edge expansion <ref type="bibr" target="#b34">[35]</ref> can achieve a target capacity with fewer switches, leading to lower overall cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deployment Complexity</head><p>Deployment is the process of realizing a physical topology in a data center space (e.g., a building), from a given logical topology. Deployment complexity can be reduced by careful packaging, placement and bundling strategies <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Packaging, Placement, and Bundling</head><p>Packaging of a topology involves careful arrangement of switches into racks, while placement involves arranging these racks into rows on the data center floor. The spatial arrangement of the topology determines the type of cables needed between switches. For instance, if two connected switches are within the same rack, they can use short-range cheaper copper cables, while connections between racks require more expensive optical cables. Optical cable costs are determined by two factors: the cost of transceivers and the length of cables ( §3.2). Placement of switches on the datacenter floor can also determine costs: connecting two switches placed at two ends of the data center building might require long range cables and high-end transceivers. Chassis, racks, and blocks. Packaging connected switches into a single chassis using a backplane completely removes the need for physical connecting cables. At scale, the cost and complexity savings from using a chassis-backplane can be significant. One or more chassis that are interconnected can be packed into racks such that: (a) racks are as homogeneous as possible, i.e., a topology makes use of only a few types of racks to simplify manufacturing and (b) racks are packed as <ref type="bibr" target="#b1">2</ref> This equation for n can be used to build a Clos with 1:1 oversubscription. densely as possible to reduce space wastage. Some topologies define larger units of co-placement and packaging called blocks, which consist of groups of racks. Examples of blocks include pods in Fattree. External cabling from racks within a block are routed to wiring aggregators (i.e., patch panels <ref type="bibr" target="#b24">[25]</ref>) to be routed to other blocks. For blocks to result in lower deployment complexity, three properties must be met: (a) the ports on the patch panel that it connects to are not wasted, when the topology is built out to full scale, (b) wiring out of the block should be as uniform as possible, and (c) racks in a block must be placed close to each other to reduce the length and complexity of wiring.</p><p>Bundling and cable trays. When multiple fibers from the same set of physically adjoint (or neighboring) racks are destined to another set of neighboring racks, these fibers can be bundled together. A fiber bundle is a fixed number of identical-length fibers between two clusters of switches or racks. Manufacturing bundles is simpler than manufacturing individual fibers, and handling such bundles significantly simplifies operation complexity. Cable bundling reduces capex and opex by around 40% in Jupiter <ref type="bibr" target="#b30">[31]</ref>. Patch panels facilitate bundling since the patch panel represents a convenient aggregation point to create and route bundles from the set of fibers destined to the same patch panel (or the same set of physically proximate patch panels). <ref type="figure">Figure 1</ref> shows a Clos topology instance (left) and its physical realization using patch panels (right). Each aggregation block in the Clos network connects with one link to each spine block. The figure on the right shows how these links are routed physically. Bundles with two fibers each from two aggregations are routed to two (lower) patch panels. At each patch panel, these fibers are rebundled, by grouping fibers that go to the same spine in new bundles, and routed to two other (upper) patch panels that connect to spines. The bundles from the upper patch panels are then routed to the spines. <ref type="figure">Figure 1</ref> assumes that patch panels are used as follows: bundles are connected to both the front and back ports on patch panels. For example, bundles from the aggregation layer connect to front ports on patch panels and bundles from spines connect to the back ports of patch panels. This enables bundle aggregation and rebundling and simplifies topology expansion. <ref type="bibr" target="#b2">3</ref> Bundles and fibers are routed through the datacenter on cable trays. The cables that aggregate at a patch panel rack must be routed overhead by using over-row and cross-row trays <ref type="bibr" target="#b25">[26]</ref>. Trays have capacity constraints <ref type="bibr" target="#b33">[34]</ref>, which can constrain rack placement, block sizes, and patch panel placement. Today, trays can support at most a few thousand fibers <ref type="bibr" target="#b33">[34]</ref>. <ref type="bibr" target="#b37">[38]</ref>'s usage of patch panels is slightly different. All bundles are connected to front ports of patch panels and links are established using jumper cables between the back ports of patch panels. For patch panels of a given port count, both approaches require the same number of patch panels. Our approach enables bundling closer to the aggregation and spine layers; <ref type="bibr" target="#b37">[38]</ref> does not describe how bundling is accomplished in their design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logical Clos Physical Clos Aggr</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spine</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggr</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spine</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Switch</head><p>Patch Panel <ref type="figure">Figure 1</ref>: Fiber Re-bundling for Clos at Patch Panels With current rack and cable tray sizes, a single rack of patch panels can be accommodated by four overhead cable trays, arranged in four directions. In order to avoid aggregating too many links into a single location, it is desirable to space such patch panels apart to accommodate more cable trays. This consideration in turn constrains block sizes; if cables from blocks must be all routed locally, it is desirable that a block only connect to a single rack of patch panel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deployment Complexity Metrics</head><p>Based on the previous discussion, we identify several metrics that quantify the complexity of the two aspects of datacenter topology deployment: packaging and placement. In the next subsection, we use these metrics to identify differences between Clos and Expander graph topology classes. Number of Switches. The total number of switches in the topology determines the capital expenditure for the topology, but it also determines the packaging complexity (switches need to be packed to chassis and racks) and the placement complexity (racks need to be placed on the datacenter floor).</p><p>Number of Patch panels. By acting as bundle waypoints, the number of patch panels captures one measure of wiring complexity. The more the number of patch panels, the shorter the cable lengths from switches to the nearest patch panel, but the fewer the bundling opportunities, and vice versa. The number of patch panels needed is a function of topological structure. For instance, in a Clos topology, if an aggregation layer fits into one rack or a neighboring set of racks, a patch panel is not needed between the ToR and the aggregation layer. However, for larger Clos topologies where an aggregation block can span multiple racks, ToR to aggregation links may need to be rebundled through a patch panel. We discuss this in detail in §6.2.</p><p>Number of Bundle Types. The number of patch panels alone does not capture wiring complexity. The other measure is the number of distinct bundle types. A bundle type is represented by a tuple of (a) the capacity of the number of fibers in the bundle, and (b) the length of the bundle. If a topology requires only a small number of bundle types, its bundling is more homogeneous; manufacturing and procuring such bundles is significantly simpler, and deploying the topology is also simplified since fewer bundling errors are likely with fewer types. These complexity measures are complete. The number of cable trays, the design of the chassis, and the number of racks can be derived from the number of switches (and the number of servers and the datacenter floor dimensions, which  are inputs to the topology design). The number of cables and transceivers can be derived from the number of patch panels. In some cases, a metric is related to another metric, but not completely subsumed by it. For example, the number of switches determines rack packaging, which only partially determines the number of transceivers per switch. The other determinant of this quantity is the connectivity in the logical topology (which switch is connected to which other switch). Similarly, the number of patch panels can influence the number of bundle types, but these are also determined by logical connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparing Topology Classes</head><p>To understand how the two main classes of topologies compare by these metrics, we apply these to a Clos topology and to a Jellyfish topology that support the same number of servers (131,072) and the same bisection bandwidth. This topology corresponds to twice the size of Jupiter. In §6, we perform a more thorough comparison at larger and smaller scales, and we describe the methodology by which these numbers were generated. <ref type="table" target="#tab_1">Table 1</ref> shows that the two topology classes are qualitatively different by these metrics. Consistent with the finding in <ref type="bibr" target="#b31">[32]</ref>, Jellyfish only needs a little over half the switches compared to Clos to achieve comparable capacity due to its high edge expansion property. But, by other measures, Clos performs better. It exposes far fewer ports outside the rack (a little over half that of Jellyfish); we say Clos has better port-hiding. A pod in this Clos contains 16 aggregation and 16 edge switches <ref type="bibr" target="#b3">4</ref> . The aggregation switches can be can be packed into a single rack, so bundles from edge switches to aggregation switches do not need to be rebundled though patch panels, and we only need two layers of patch panels between aggregation and spine layer. However, in Jellyfish, almost all links are inter-rack links, so it requires more patch panels.</p><p>Moreover, for Clos, since each pod has the same number of links to each spine, all bundles in Clos have the same capacity (number of fibers). However, the length of bundles can be different, depending on the relative placement of the patch panels between aggregation and spine layers, so Clos has 74 bundle types. However, since Jellyfish is a purely random graph without structure, to enable bundling, we group a fixed amount of neighbor racks as blocks to enable bundling. Since connectivity is random, the number of links between blocks are not uniform, Jellyfish needs almost 20× the number of bundle types. In §6, we show that Xpander also has we follow the definition of pod in <ref type="bibr" target="#b1">[2]</ref>. qualitatively similar behavior in large scale. Takeaway. Relative to a structured hierarchical class of topologies like Clos, the expander graph topology has inherently higher deployment complexity in terms of the number of bundle types and cannot support port-hiding well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Topology Expansion</head><p>The second important component of topology lifecycle management is expansion. Datacenters are rarely deployed to maximal capacity in one shot; rather, they are gradually expanded as network capacity demands increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Practice of Expansion</head><p>In-place Expansion. At a high-level, expanding a topology involves two conceptual phases: (a) procuring new switches, servers, and cables and laying them on the datacenter floor, and (b) re-wiring (or adding) links between switches in the existing topology and the new switches. Phase (b), the re-wiring phase, can potentially disrupt traffic; as links are re-wired, network capacity can drop, leading to traffic loss. To avoid traffic loss, providers can either take the existing topology offline (migrate services away, for example, to another datacenter), or can carefully schedule link re-wiring while carrying live traffic, but schedule the re-wiring to maintain a desired target capacity. The first choice can impact service availability significantly.</p><p>So, today, datacenters are expanded while carrying live traffic <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b37">38]</ref>. To do this, expansion is carried out in steps, where at each step, the capacity of the topology is guaranteed to be at least a percentage p of the capacity of the existing topology. This fraction is sometimes called the expansion SLO. Today, many providers operate at expansion SLOs of 75% <ref type="bibr" target="#b37">[38]</ref>; higher SLOs of 85-90% can impact availability budgets less while allowing providers to carry more traffic during expansion. The unit of expansion. Since expansion involves procurement, topologies are usually expanded in discrete units called blocks to simplify the procurement and layout logistics. In a structured topology, there are natural candidates for blocks. For example, in a Clos, a pod can be block, while in an Xpander, the metanode can be a block. During expansion, a block is first fully assembled and placed, and links between switches within a block are connected (as an aside, an Xpander metanode has no such links). During the re-wiring phase, only links between existing blocks and new blocks are re-wired. (This phase does not re-wire links between switches within an existing block). Aside from simplifying logistics, expanding at the granularity of a block preserves structure in structured topologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">An Expansion Step</head><p>What happens during a step.   Suppose we want to maintain 87.5% of the capacity of the topology (i.e., the expansion SLO is 0.875), this expansion will require 4 steps in total, where each patch panel is involved in 2 of these steps. In <ref type="figure" target="#fig_1">Figure 2</ref>, we only show the rewiring process on the second existing patch panels. To maintain 87.5% capacity at each pod, only one link is allowed to be drained. In the first step, the red link from the first existing aggregation block and the green link from the second existing aggregation block are rewired to the first new spine block. In the second step, the orange links from the first existing aggregation block and the purple link from the second existing aggregation block are rewired to the first new spine block. A similar process happens in the first patch panel.</p><p>In practice, each step of expansion involves four sub-steps. In the first sub-step, the existing links that are to be re-wired are drained. Draining a link involves programming switches at each end of the link to disable the corresponding ports, and may also require reprogramming other switches or ports to route traffic around the disabled link. Second, one or more human operators physically rewire the links at a patch panel (explained in more detail below). Third, the newly wired links are tested for bit errors by sending test traffic through them. Finally, the new links are undrained.</p><p>By far the most time consuming part of each step is the second sub-step, which requires human involvement. This sub-step is also the most important from an availability perspective; the longer this sub-step takes, the longer the datacenter operates at reduced capacity, which can impact availability targets <ref type="bibr" target="#b11">[12]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Expansion Complexity Metrics</head><p>We identify two metrics that quantify expansion complexity and use these metrics to identify differences between Clos and Jellyfish in the next subsection. Number of Expansion Steps. As mentioned each expansion step requires a series of substeps which cannot be parallelized. Therefore the number of expansion steps determines the total time for expansion. Average number of rewired links in a patch panel rack per step. With patch panels, manual rewiring dominates the time taken within each expansion step. Within steps, it is possible to parallelize rewiring across racks of patch panels. With such parallelization, the time taken to rewire a single patch panel rack will dominate the time taken for each expansion step.  SLO is 90%. ( §6 has more extensive comparisons for these metrics, and also describes the methodology more carefully).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparing Topology Classes</head><p>In this setting, the number of links rewired per patch panel can be a factor of two less than Clos. Moreover, Jellyfish requires 3 steps, while Clos twice the number of steps.</p><p>To understand why Jellyfish requires fewer steps, we define a metric called the north-to-south capacity ratio for a block. This is the ratio of the aggregate capacity of all "northbound" links exiting a block to the aggregate capacity of all "southbound" links to/from the servers within the block. <ref type="figure">Figure 4</ref> illustrates this ratio: a thin edge (left), has an equal number of southbound and northbound links while a fat edge (right), has more northbound links than southbound links. A Clos topology has a thin edge, i.e., this ratio is 1, since the block is a pod. Now, consider an expansion SLO of 75%. This means that the southbound aggregate capacity must be at least 75%. That implies that, for Clos, at most 25% of the links can be rewired in a single step. However, Jellyfish has a much higher ratio of 3, i.e., it has a fat edge. This means that many more links can be rewired in a single step in Jellyfish than in Clos. This property of Jellyfish is required for reducing the number of expansion steps. Takeaway. Clos topologies re-wire more links in each patch panel during an expansion step and require many steps because they have a low north-south capacity ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Towards Lower Lifecycle Complexity</head><p>Our discussions in §3 and §4, together with preliminary results presented in those sections ( §6 has more extensive results) suggest the following qualitative comparison between Clos and the expander graph families with respect to lifecycle management costs <ref type="table" target="#tab_5">(Table 3)</ref>: • Clos uses fewer bundle types and patch panels.</p><p>• Jellyfish has significantly lower switch counts, uses fewer expansion steps, and touches fewer links per patch panel during an expansion step. In all of these comparisons, we compare topologies with the same number of servers and the same bisection bandwidth.</p><p>The question we ask in this paper is: Is there a family of topologies which are comparable to, or dominate, both Clos and expander graphs by all our lifecycle management metrics? In this section, we present the design of the FatClique class of topologies and validate in §6 that FatClique answers this question affirmatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FatClique Construction</head><p>FatClique ( <ref type="figure" target="#fig_4">Figure 5</ref>) combines the hierarchical structure in Clos with the edge expansion in expander graphs to achieve lower lifecycle management complexity. FatClique has three 4-layer Clos (Medium) Jellyfish switches bundle types patch panels re-wired links per patch panel expansion steps </p><formula xml:id="formula_0">= Sc • (pc + p b )</formula><p>radix of a sub-block</p><formula xml:id="formula_1">R b = S b • Sc • p b radix of a block N b = N/(S b • Sc • s) #blocks Lcc = Sc • pc/(S b − 1)</formula><p>#links between two sub-blocks inside a block</p><formula xml:id="formula_2">L bb = R b /(N b − 1)</formula><p>#links between two blocks  <ref type="bibr" target="#b19">[20]</ref>. Additionally, each level in the hierarchy is designed to have a fat edge (a north-south capacity ratio greater than 1). The cliques enable high edge expansion, while hierarchy enables lower wiring complexity than random-graph based expanders <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>FatClique is a class of topologies. To obtain an instance of this class, a topology designer specifies two input parameters: N , the number of servers, and k the chip radix. A synthesis algorithm takes these as inputs, and attempts to instantiate four design variables that completely determine the FatClique instance <ref type="table" target="#tab_6">Table 4</ref>. These four design variables are:</p><p>• s, the number of ports in a switch that connect to servers • p c , the number of ports in each switch that connect to other sub-blocks inside a block • S c , the number of switches in a sub-block • S b , the number of sub-blocks in a block The synthesis algorithm searches for the best combination of values for design variables, guided by six constraints, C 1 through C 6 , described below. The algorithm also defines auxiliary variables for convenience; these can be derived from the design variables <ref type="table" target="#tab_6">(Table 4)</ref>. We define these variables in the narrative below.</p><p>Sub-block connectivity. In FatClique, the sub-block forms the lowest level of the hierarchy, and contains switches and servers. All sub-blocks have the same structure. Servers are distributed uniformly among all switches of the topology, such that each sub-block has the same number of servers attached. However, because this number of servers may not be an exact multiple of the number of switches, we distribute the remainder across the switches, so that some switches may be connected to one more server than others. The alternative would have been to truncate or round up the number of servers per sub-block to be divisible by the number of switches in  <ref type="figure">Figure 6</ref>: FatClique Block the sub-block, which could lead to overprovisioning or underprovisioning. Within a sub-block, every switch has a link to every other switch within its sub-block, to form a clique (or complete graph). To ensure a fat edge at the sub-block level, each switch must connect to more switches than servers, captured by the constraint C 1 : s &lt; r − s, where r is the switch radix and s is the number of ports on a switch connected to servers.</p><p>Block-level connectivity. The next level in the hierarchy is the block. Each sub-block is connected to other sub-blocks within a block using a clique ( <ref type="figure" target="#fig_4">Figure 5</ref>, top-left). In this clique, each sub-block may have multiple links to another sub-block; these inter-sub-block links are evenly distributed among all switches in the sub-block such that every pair of switches from different sub-block has at most one link. Ensuring a fat edge at this level requires that a sub-block has more intersub-block and inter-block links egressing from the sub-block than the number of servers it connects to. Because sub-blocks contain switches which are homogeneous <ref type="bibr" target="#b4">5</ref> , this constraint is ensured if the sum of (a) the number ports on each switch connected to other sub-block (p c ) and (b) those connected to other blocks (p b , an auxiliary variable in <ref type="table" target="#tab_6">Table 4</ref>, see also <ref type="figure">Figure 6</ref>) exceeds the number of servers connected to the switch (captured by</p><formula xml:id="formula_3">C 2 : p c + p b &gt; s).</formula><p>Inter-block connectivity. The top of the hierarchy is the overall network, in which each block is connected to every <ref type="bibr" target="#b4">5</ref> They are nearly homogeneous, since a switch may differ from another by one in the number of servers connected other block, resulting in a clique. The inter-block links are evenly distributed among all sub-blocks, and, within a subblock, evenly among all switches. To ensure a fat edge at this level, the number of inter-block links at each switch should be larger than the number of servers it connects to, captured by C 3 : p b &gt; s. Note that C 3 subsumes (is a stronger constraint than) C 2 . Moreover, the constraint that blocks are connected in a clique imposes a constraint on the block radix (R b , a derived variable). The block radix is the total number of links in a block destined to other blocks. R b should be large enough to reach all other blocks (captured by C 4 : R b ≥ N b − 1) such that the whole topology is a clique.</p><p>Incorporating rack space constraints. Beyond connectivity constraints, we need to consider packaging constraints in sub-block design. Ideally, we need to ensure that a sub-block fits completely into one or more racks with no wasted rack space. For example, if we use 58RU racks, and each switch is to be connected to 8 1RU servers, we can accommodate 6 switches per sub-block, leaving 58 − (6 × 8 + 6) = 4U in the rack for power supply and other equipment. In contrast, choosing 8 switches per sub-block would be a bad choice because it would need 8 × 8 + 8 = 72U rack space, overflowing into a second rack that would have 44RU un-utilized. We model this packaging fragmentation as a soft constraint: our synthesis algorithm generates multiple candidate assignments to the design variables that satisfy our constraints, and of these, we pick the alternative that has the lowest wasted rack space.</p><p>Ensuring edge expansion. At each level of the hierarchy, edge expansion is ensured by using a clique. This is necessary for high edge expansion, but not sufficient, since it does not guarantee that every switch connects to as many other switches across the network as possible. One way to ensure this diversity is to make sure that each pair of switches is connected by at most one link. The constraints discussed so far do not ensure this. For instance, consider <ref type="figure">Figure 6</ref>, in which L cc (another auxiliary variable in <ref type="table" target="#tab_6">Table 4</ref>) is the number of links from one sub-block to another. If this number is greater than the number of switches S c in the sub-block, then, some pair of switches might have more than one link to each other. Thus, C 5 : L cc ≤ S c is a condition to ensure that each pair of switches must be connected by a single link. Our topology synthesis algorithm generates assignments to design variables, and a topology generator then assigns links to ensure this property ( §5.2).</p><p>Incorporating patch panel constraints. The size of the block is also limited by the number of ports in a single patch panel rack (denoted by P P − Rack ports ). It is desirable to ensure that the inter-block links egressing each block connect to at most <ref type="bibr" target="#b0">1</ref> the ports in a patch panel rack, so that the rest of the patch panel ports are available for external connections into the block (captured by C 6 : R b ≤ <ref type="bibr" target="#b0">1</ref> • P P − Rack ports ).</p><p>USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 241  </p><formula xml:id="formula_4">Topology Scalability 3-layer Clos (Fattree) 2 • (k/2) 3 4-layer Clos 2 • (k/2) 4 5-layer Clos (Jupiter) 2 • (k/2) 5 FatClique O(k 5 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">FatClique Synthesis Algorithm</head><p>Generating candidate assignments. The FatClique synthesis algorithm attempts to assign values to the design variables, subject to constraints C 1 to C 6 . The algorithm enumerates all possible combinations of value assignments for these variables, and filters out each assignment that fails to satisfy all the constraints. For each remaining assignment, it generates the topology specified by the design variable, and determines if the topology satisfies a required capacity Cap * , which is an input to the algorithm. Each assignment that fails the capacity test is also filtered out, leaving a candidate set of assignments. These steps are described in §A.6. FatClique placement. For each assignment in this candidate set, the synthesis algorithm generates a topology placement. Because FatClique's design is regular, its topology placement algorithm is conceptually simple. A sub-block may span one or more racks, and these racks are placed adjacent to each other. All sub-blocks within a block are arranged in a rectangular fashion on the datacenter floor. For example, if a block has 25 racks, it is arranged in a 5×5 pattern of racks. Blocks are then arranged in a similar grid-like fashion.</p><p>Selecting best candidate. For each placement, the synthesizer computes the cabling cost of the resulting placement (using <ref type="bibr" target="#b6">[7]</ref>), and picks the candidate with the lowest cost. This step is not shown in Algorithm 3. This approach implicitly filters out candidates whose sub-block cannot be efficiently packed into racks ( §5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">FatClique Expansion</head><p>Re-wiring during expansion. Consider a small FatClique topology, shown top left in <ref type="figure" target="#fig_5">Figure 7</ref>, that has 3 blocks and L bb = 5, i.e., five inter-block links. To expand it to a clique with six blocks, we would need to rewire the topology to have L bb = 2 (top right in <ref type="figure" target="#fig_5">Figure 7</ref>). This means we need to redistribute more than half (6 out of 10) of existing links (red) at each block to new blocks without violating wiring and capacity constraints.</p><p>The expansion process with patch panels is shown in the bottom of <ref type="figure" target="#fig_5">Figure 7</ref>. Similar to the procedure for Clos described in §4.1, all new blocks (shown in orange) are first deployed and interconnected and links from the new blocks are routed to reserved ports on patch panels associated with existing blocks (shown in blue), before re-wiring begins.</p><p>For FatClique, rewiring one existing link requires releasing one patch panel port so that a new link can be added. Since links are already parts of existing bundles and routed through cable trays, we can not rewire them directly, e.g., by rerouting it from one patch panel to another. For example, link 1 (lower half of <ref type="figure" target="#fig_5">Figure 7)</ref> is originally connected blocks 1 and 3 by connecting ports a and b on the patch panel. Suppose we want to remove that link, and add two links, one from block 1 to block 5 (labeled 3), and another from block 3 to block 5 (labeled 4). The part of the original link (labeled 1) between the two patch panels is already bundled, so we cannot physically reroute it from block 3 to block 5. Instead, we effect re-wiring by releasing port a, connecting link 3 to port a, connecting link 1 to port c. Logically, this is equivalent to connecting ports a and d and b and c on the patch panel shown in lower half of <ref type="figure" target="#fig_5">Figure 7</ref>. This preserves bundling, while permitting expansion.</p><p>If the original topology has N b blocks, by comparing the old and target topology, the total number of rewired links is computed by N b (N b − 1)(L bb − L bb )/2. For this example, the total number of links to be rewired is 9. Iterative Expansion Plan Generation. By design, Fat-Clique has fat edges, which allows draining more and more links at each step of the expansion, as network capacity increases. At each step, we drain links across all blocks uniformly, so that each block loses the same aggregate capacity. However the relationship between overall network capacity, and the number of links drained at every block in FatClique is unclear, because traffic needs to be sent over non-shortest paths to fully utilize the fabric.</p><p>Therefore, we use an iterative approach to expansion planning, where, at each step, we search for the maximal ratio of links to be drained that still preserves expansion SLO. ( §A.4 discusses the algorithm in more detail). Our evaluation §6 shows that the number of expansion steps computed by this algorithm is much smaller than that for expanding symmetric Clos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>Achieving low complexity. By construction, FatClique achieves low lifecycle management complexity <ref type="table" target="#tab_5">(Table 3)</ref>, while ensuring full-bisection bandwidth. It ensures high edge expansion, resulting in fewer switches. By packaging clique connections into a sub-block, it exports fewer external ports, an idea we call port hiding. By employing hierarchy and a regular (non-random) structure, it permits bundling and re-quires fewer patch panels. By ensuring fat edges at each level of the hierarchy, it enables fewer re-wired links per patch panel, and fewer expansion steps. We quantify these in §6.</p><p>Scalability. Since Xpander and Jellyfish do not incorporate hierarchy, they can be scaled to arbitrarily large sizes. However, because Clos and FatClique are hierarchical, they can only scale to a fixed size for a given chip radix. <ref type="table" target="#tab_7">Table 5</ref> shows the maximum scale of each topology as a function of switch radix k. FatClique scales to the same order of magnitude as a 5-layer Clos. As shown in §6, both of them can scale to 64 times bisection bandwidth of Jupiter.</p><p>FatClique and Dragonfly. FatClique is inspired by Dragonfly <ref type="bibr" target="#b19">[20]</ref> and they are both hierarchical topologies that use cliques as building blocks, but differ in several respects. First, for a given switch radix, FatClique can scale to larger topologies than Dragonfly because it incorporates one additional layer of hierarchy. Second, the Dragonfly class of topologies is defined by many more degrees of freedom than FatClique, so instantiating an instance of Dragonfly can require an expensive search <ref type="bibr" target="#b32">[33]</ref>. In contrast, FatClique's constraints enable more efficient search for candidate topologies. Finally, since Dragonfly does not explicitly incorporate constraints for expansion, a given instance of Dragonfly may not end up with fat edges.</p><p>Routing and Load Balancing on FatClique. Unlike for Clos, ECMP-based forwarding cannot be used achieve high utilization in more recently proposed topologies <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b18">19]</ref>. FatClique belongs to this latter class, for which a combination of ECMP and Valiant Load Balancing <ref type="bibr" target="#b36">[37]</ref> has been shown to achieve performance comparable to Clos <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluating Lifecycle Complexity</head><p>In this section, we compare three classes of topologies, Clos, expander graphs and FatClique by our complexity metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methodology</head><p>Topology scales. Because the lifecycle complexity of topology classes can be a function of topology scale, we evaluate complexity across three different topology sizes based on the number of servers they support: small, medium, and large. Small topologies support as many servers as a 3-layer clos topology. Medium topologies support as many servers as 4-layer Clos. Large topologies support as many servers as 5-layer Clos topologies <ref type="bibr" target="#b5">6</ref> . All our experiments in this section are based on comparing topologies at the same scale.</p><p>At each scale, we generate one topology for each of Clos, Xpander, Jellyfish, and FatClique. The characteristics of these topologies are listed in <ref type="table">Table 6</ref>. All these topologies use 32-port switching chips, the most common switch radix available today for all port capacities <ref type="bibr" target="#b4">[5]</ref>. To compare topologies fairly, we need to equalize them first. Specifically, at a given scale, each topology has approximately the same bisection bandwidth, computed (following prior work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35]</ref>) using METIS <ref type="bibr" target="#b17">[18]</ref>. All topologies at the same scale support roughly the same number of servers; small, medium and large scale topologies achieve, respectively, <ref type="bibr" target="#b0">1</ref> , 4, and 16 times capacity of Jupiter. (In A.8, we also compare these topologies using two other metrics). <ref type="table">Table 6</ref> also shows the scale of individual building blocks of these topologies in terms of number of switches. For Clos, we use the algorithm in §A.1 to design building blocks (chassis) and then use them to compose Clos. One interesting aspect of this table is that, at the 3 scales we consider, a FatClique's sub-block and block designs are identical, suggesting lower manufacturing and assembly complexity. We plan to explore this dimension in future work.</p><p>For each topology we compute the metrics listed in <ref type="table" target="#tab_5">Table 3</ref>: the number of switches, the number of bundle types, the number of patch panels, the average number of re-wired links at a patch panel during each expansion step, and the number of expansion steps. To compute these, we need component parameters, and placement and expansion algorithms for each topology class. Component Parameters. In keeping with <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40]</ref>, we use optical links for all inter-rack links. We use 96 port 1RU patch panels <ref type="bibr" target="#b9">[10]</ref> in our analysis. A 58RU <ref type="bibr" target="#b27">[28]</ref> rack with patch panels can aggregate 2 * 96 * 58 = 11, 136 fibers. We call this rack a patch-panel rack. Most datacenter settings, such as rack dimensions, aisle dimensions, cable routing and distance between cable trays follow practices in <ref type="bibr" target="#b25">[26]</ref>. We list all parameters used in our paper in §A.7. Placement Algorithms. For Clos, following Facebook's fbfabric <ref type="bibr" target="#b2">[3]</ref>, spine blocks are placed at the center of the datacenter, which might take multiple rows of racks, and pods are placed at two sides of spine blocks. Each pod is organized into a rectangular area with aggregation blocks placed in the middle to reduce the cable length from ToR to aggregation. FatClique's placement algorithm is discussed in §5.2. For Xpander, we use the placement algorithm proposed in <ref type="bibr" target="#b18">[19]</ref>. We follow the practice that all switches in a metanode are placed closed to each other. However, instead of placing a metanode into a row of racks, we place a metanode into a rectangular area of racks, which reduces cable lengths when metanodes are large. For Jellyfish, we design a random search algorithm to aggressively reduce the cable length ( §A.2). Expansion Algorithms. For Clos, as shown in <ref type="bibr" target="#b37">[38]</ref>, it is fairly complex to compute the optimal number of rewired links for asymmetric Clos during expansion. However, when the original and target topologies are both symmetric, this number is easy to compute. For this case, we design an optimal algorithm ( §A.5) which rewires the maximum number of links at each step and therefore uses the smallest number of steps to finish expansion. For FatClique, we use the algorithm discussed in §5. <ref type="bibr" target="#b2">3</ref>  <ref type="table">Table 6</ref>: Capacities of topologies built with 32 port 40G switches. Small, medium and large scale topologies achieve 1 4 , 4, 16 times capacity of Jupiter. The table also shows sizes of individual building blocks of these topologies in terms of number of switches. Abbreviations: e:edge, a:aggregation, sp:spine, cap:capacity, svr:server. expansion algorithm based on the intuition from <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b31">32]</ref> that, to expand a topology by n ports requires breaking n 2 existing links. Finally, we have found that for all topologies, the number of expansion steps at a given SLO is scale invariant: it does not depend on the size of the original topology as long as the expansion ratio (target-topology-size-to-originaltopology-size ratio) is fixed ( §A.3). Presenting results. In order to bring out the relative merits of topologies, and trends of how cost and complexity increase with scale, we present values for metrics we measure for all topologies and scales in the same graph. In most cases, we present the absolute values of these metrics; in some cases though, because our three topologies span a large size range, for some metrics the results across topologies are so far apart that we are unable to do so without loss of information. In these cases, we normalize our results by the most expensive, or complex topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Patch Panel Placement</head><p>The placement of patch panels is determined both by the structure of the topology and its scale. Between edge and aggregation layers in Clos. For small and medium scale Clos, no patch panels are needed between edge and aggregation layers. Each pod at these scales contains 16 aggregation switches, which can be packed into a single rack (we call this an aggregation-rack). Given that a pod at this scale is small, all links from the edge can connect to this rack. Since all links connect to one physical location, bundles form naturally. In this case, each bundle from edge racks contains 3 × 16 fibers 7 . Therefore, no patch panels are needed between edge and aggregation layers.</p><p>However, a large Clos needs one layer of patch panels between edge and aggregation layers since a pod at this scale is large. An aggregation block consists of 16 middle blocks <ref type="bibr" target="#b7">8</ref> , each with 32 switches. The aggregation block by itself occupies a single rack. Based on the logical connectivity, links from any edge need to connect to all middle blocks. Without using patch panels, each bundle could at most contain 3 × 16/16 = 3 fibers. In our design, we use patch panels to aggregate local bundles from edges first and then rebundle them on patch panels to form new high capacity bundles from patch panels to aggregation racks. Based on the patch panel In our setting, each rack with 58RU can accommodate at most 3 switches and 48 associated servers. The total number of links out of this rack is 3 * 16. <ref type="bibr" target="#b7">8</ref> We follow the terminology in <ref type="bibr" target="#b30">[31]</ref>. A middle block is a sub-block in an aggregation block. rack capacity constraint, two patch panel racks are enough to form high capacity bundles from edge to aggregation layers. Specifically, in our design 128 edge switches and 8 aggregation racks connect to a single patch panel. In this design, each edge-side bundle contains 48 fibers and each aggregation-side bundle contains 128 fibers. Between aggregation and spine layers. The topology between aggregation and spine layer in Clos is much larger than that inside a pod. For this reason, to form high capacity bundles, two layers of patch panels are needed. As shown in <ref type="figure">Figure 1</ref>, one layer of patch panels is placed near spine blocks at the center of the data center floor. Each patch panel rack aggregates local bundles from four spine racks in medium and large scale topologies. Similarly, another layer of patch panels are placed near aggregation rack, permitting long bundles between those patch panels. In expanders and FatClique. For Jellyfish, Xpander and FatClique, patch panels are deployed at the server block side and long bundles form between those patch panels. In Fat-Clique, each block requires one patch panel rack ( §5.3). In a large Xpander, since a metanode is too big <ref type="table">(Table 6)</ref>, it is not possible to use one patch panel rack to aggregate all links from a metanode. Therefore, we divide a metanode into homogeneous sections, called sub-metanodes, such that links from a sub-metanode can be aggregated at one patch panel rack. For Jellyfish, we partition the topology into groups, each of which contains the same number of switches as in a block in FatClique, so each group needs one patch panel rack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Deployment Complexity</head><p>In this section, we evaluate our different topologies by our three measures of deployment complexity ( §3.2).  <ref type="figure" target="#fig_6">Figure 8</ref> shows how the different topologies compare in terms of number of switches used at various topology scales. <ref type="figure" target="#fig_6">Figure 8(a)</ref> shows the total number of switches for the small topologies, <ref type="figure" target="#fig_6">Figure 8</ref>(b) for the medium, and <ref type="figure" target="#fig_6">Figure 8</ref>(c) for the large. The y-axes increase in scale by about an order of magnitude from left to right. FatClique has 20% fewer switches than Clos for a small topology, and 50% fewer for the large. The results for Jellyfish and Xpander are similar, consistent with findings in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b31">32]</ref>. This benefit comes from the edge expansion property of the non-Clos topologies we consider. This implies that Clos topologies, at large scale, may require nearly twice the capital expenditures for switches, racks, and space as the other topologies. Number of Patch panels. <ref type="figure" target="#fig_7">Figure 9</ref> shows the number of patch panels at different scales. As before, across these graphs, the y-axis scale increases approximately by one order of magnitude from left to right. At small and medium scales, Clos relies on patch panels mainly for connections between aggregation and spine blocks. Of all topologies at these scales, Clos uses the fewest number of patch panels: FatClique uses about 11% more patch panels, and Jellyfish and Xpander use almost 44-50% more. Xpander and Jellyfish rely on patch panels for all northbound links, and therefore in general, as scale increases, the number of patch panels in these networks grows (as seen by the increase in the y-axis scale from left to right).</p><p>At large scale, however, Clos needs many more patch panels, comparable to Xpander and Jellyfish. At this scale, Clos aggregation blocks span multiple racks, and patch panels are also needed for connections between ToRs and aggregation blocks. Here, FatClique's careful packaging strategy becomes more evident, as it needs nearly 25% fewer patch panels than Clos. The majority of patch panels used in FatClique at all scales comes from inter-block links (which increase with scale).</p><p>For this metric, Clos and FatClique are comparable at small and medium scales, but FatClique dominates at large scale.  Number of Bundle Types. <ref type="table" target="#tab_10">Table 7</ref> shows the number of bundle types used by different topologies at different scales. A bundle type ( §3.1) is characterized by (a) the number of (c) Large <ref type="figure" target="#fig_9">Figure 10</ref>: Cabling cost. C is Clos, J is Jellyfish, X is Xpander and F is FatClique. fibers in the bundle, and (b) the length of the bundle. The number of bundle types is a measure of wiring complexity. In this table, if bundles differ by more than 1m in length, they are designated as separate bundle types. <ref type="table" target="#tab_10">Table 7</ref> shows that Clos and FatClique use the fewest number of bundle types; this is due to the hierarchical structure of the topology, where links between different elements in the hierarchy can be bundled. As the topology size increases, the number of bundle types also increases in these topologies, by a factor of about 40 for Clos to 20 for FatClique when going from small to large topologies.</p><p>On the other hand, Xpander and Jellyfish use an order of magnitude more bundle types compared to Clos and FatClique at medium and large scales, but use a comparable number for small scale topologies. Even at the small scale, Jellyfish uses many more bundle types because it uses a random connectivity pattern. At small scales Xpander metanodes use a single patch panel rack and bundles from all metanodes are uniform. With larger scales, Xpander metanodes become too big to connect to a single patch panel rack. We have to divide a metanode into several homogeneous sub-metanodes such that all links from sub-metanodes connect to a patch panel rack. However, because of the randomness in connectivity, this subdivision cannot ensure uniformity of bundles egressing sub-metanode patch panel racks, so we find that Xpander has a large number of bundle types in medium and large topologies.</p><p>Thus, by this metric, Clos and FatClique have the lowest complexity across all three scales, while Xpander and Jellyfish have an order of magnitude more complexity. Moreover, across all metrics FatClique has lowest deployment complexity, especially at large scales.</p><p>Case Study: Quantifying cabling costs. While not all aspects of lifecycle management complexity can be translated to actual dollar costs, it is possible to estimate one aspect, namely the cost of cables. Cabling cost includes the cost of transceivers and cables, and is reported to be the dominant component of overall datacenter network cost <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20]</ref>. We can estimate costs because our placement algorithms generate cable or bundle lengths, the topology packaging determines the number of transceivers, and estimates of cable and transceiver costs as a function of cable length are publicly available <ref type="bibr" target="#b6">[7]</ref>.  <ref type="figure">Figure 11</ref>: Expansion steps across different scales. Clos has higher cabling costs at small and medium scales compared to expander graphs, although the relative difference decreases at medium scale. At large scales, the reverse is true. Clos is around 12% cheaper than Xpander in terms of cabling cost since Xpander does not support port-hiding at all and uses more long inter-rack cables. Thus, given that cabling cost is the dominant component of overall cost, it is unclear whether the tradeoff Xpander and Jellyfish makes in terms of number of switches and cabling design pays off in terms of capital expenditure, especially at large scale. We find that FatClique has the lowest cabling cost of the topologies we study with a cabling cost 23-36% less than Clos. This result came as a surprise to us, because intuitively topologies that require all-to-all clique like connections might use longer length cables (and therefore more expensive transceivers). However on deeper examination, we found that Clos uses a larger number of cables (especially inter-rack cables) compared to other topologies since it has a relatively higher number of switches <ref type="figure" target="#fig_6">(Figure 8</ref>) to achieve the same bisection bandwidth. Thus, more switches leads to more racks and datacenter floor area, which stretches the cable length. All those factors together explain why Clos cabling costs are higher than FatClique's.</p><p>Thus, from an equipment capital expenditure perspective, at large scale a FatClique can be at least 23% cheaper than a Clos, because it has at least 23% fewer switches, 33% fewer patch panel racks, and 23% lower cabling costs than Clos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Expansion Complexity</head><p>In this section, we evaluate topologies by our two measures of expansion complexity ( §4.3): number of expansion steps required, and number of rewired-links per patch panel rack per step. Since the number of steps is scale-invariant ( §6.1), we only present the results from expanding medium size topologies for both metrics <ref type="bibr" target="#b8">9</ref> . When evaluating Clos, we study the expansion of symmetric Clos topologies; generic Clos expansion is studied in <ref type="bibr" target="#b37">[38]</ref>. As discussed in §6.1, for symmetric Clos, we have developed an algorithm with optimal number of rewiring steps. Number of expansion steps. <ref type="figure">Figure 11</ref> shows the number of steps (y-axis) required to expand topologies to twice their existing size (expansion ratio = 2) at different expansion SLOs (x-axis). We find that at 75% SLO, all topologies require the same number of expansion steps. But the number</p><p>We have verified that the relative trend in the number of re-wired links per patch panel holds for small and large topologies of steps required to expand Clos with tighter SLOs steeply increases. This is because the number of links that can be rewired per aggregation block in Clos per step, is limited (due to north-to-south capacity ratio §4.3) by the SLO. The tighter the SLO, fewer the number of links rewired per aggregation block per step, and larger the number of steps required to complete expansion. FatClique, Xpander and Jellyfish require fewer and comparable number of expansion steps due to their fat edge property, allowing many more links to be rewired per block per step. Their curves largely overlap (with FatClique taking one more step as SLO increases beyond 95%) .</p><p>Number of rewired links per patch panel rack per step. This metric is an indication of the time it takes to finish an expansion step because, today, rewiring each patch panel requires a human operator <ref type="bibr" target="#b37">[38]</ref>. A datacenter operator can reduce re-wiring time by employing staff to rewire each patch panel rack in parallel, in which case, the number of links per patch panel rack per step is a good indicator of the complexity of an expansion step. <ref type="figure" target="#fig_1">Figure 12</ref> shows the average of the maximum rewired links per patch panel rack, per step (y-axis), when expanding to twice the topology size size at different SLOs (y-axis). Even though the north-to-south capacity ratio restricts the number of links that can be rewired in Clos per step, the number of rewired links per patch panel rack per step in Clos remains consistently higher than other topologies, until we hit 97.5% SLO. The reason is that the links that need to be rewired in Clos are usually concentrated in few patch panel racks by design. As such, it is harder to parallelize rewiring in Clos, than it is in the other topologies. FatClique has the lowest rewiring step complexity across all topologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">FatClique Result Summary</head><p>We find that FatClique is the best at most scales by all our complexity metrics. (The one exception is that at small and medium scales, Clos has slightly fewer patch panels). It uses 50% fewer switches and 33% fewer patch panels than Clos at large scale, and has a 23% lower cabling cost (an estimate we are able to derive from published cable prices). Finally, FatClique can permit fast expansion while degrading network capacity by small amounts (2.5-10%): at these levels, Clos can take × longer to expand the topology, and each step of Clos expansion can take longer than FatClique because the number of links to be rewired at each step per patch panel can be 30-50% higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Topology Design. Previous topology designs have focused on cost effective, high capacity and low diameter datacenter topologies like <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b19">20]</ref>. Although they achieve good performance and cost properties, the lifecycle management complexity of these topologies have not been investigated either in the original papers or in subsequent work that has compared topologies <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. In contrast to these, we explore topology designs that have low lifecycle complexity. Recent work has explored datacenter topologies based on free space optics <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39</ref>] but because we lack operational experience with them at scale, it is harder to design and evaluate lifecycle complexity metrics for them. Topology Expansion. Prior work has discussed several aspects of topology expansion <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b37">38]</ref>. Condor <ref type="bibr" target="#b29">[30]</ref> permits synthesis of Clos-based datacenter topologies with declarative constraints some of which can be used to specify expansion properties. A more recent paper <ref type="bibr" target="#b37">[38]</ref> attempts to develop a target topology for expansion, given an existing Clos topology, that would require the least number of link rewiring. REWIRE <ref type="bibr" target="#b7">[8]</ref> finds target expansion topologies with highest capacity and smallest latency without preserving topological structure. Jellyfish <ref type="bibr" target="#b31">[32]</ref> and Xpander <ref type="bibr" target="#b34">[35]</ref> study expansion properties of their topology, but do not consider practical details in re-wiring. Unlike these, our work is examines lifecycle management as a whole, across different topology classes, and develops new performance-equivalent topologies with better lifecycle management properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>In this paper, we have attempted to characterize the complexity of lifecycle management of datacenter topologies, an unexplored but critically important area of research. Lifecycle management consists of network deployment and expansion, and we devise metrics that capture the complexity of each. We use these to compare topology classes explored in the research literature: Clos and expander graphs. We find that each class has low complexity by some metrics, but high by others. However, our evaluation suggests topological features important for low lifecycle complexity: hierarchy, edge expansion and fat edges. We design a family of topologies called FatClique that incorporates these features, and this class has low complexity by all our metrics at large scale.</p><p>As the management complexity of networks increases, the importance of designing for manageability will increase in the coming years. Our paper is only a first step in this direction; several future directions remain. Topology oversubscription. In our comparisons, we have only considered topologies with an over-subscription ratio of 1:1. Jupiter <ref type="bibr" target="#b30">[31]</ref> permits over-subscription at the edge of the network, but there is anecdotal evidence that providers also over-subscribe at higher levels in Clos topologies. To explore the manageability of over-subscribed topologies it will be necessary to design over-subscription techniques in FatClique, Xpander and Jellyfish in a way in which all topologies can be compared on a equal footing. Topology heterogeneity. In practice, topologies have a long lifetime over which they accrue heterogeneity: new blocks with higher radix switches, patch panels with different port counts etc. These complicate lifecycle management. To evaluate these, we need to develop data-driven models for how heterogeneity accrues in topologies over time and adapt our metrics for lifecycle complexity to accommodate heterogeneity. Other management problems. Our paper focuses on topology lifecycle management, and explicitly does not consider other network management problems like fault isolation or control plane complexity. Designs for manageability must take these into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Clos Generation Algorithm</head><p>For Clos topologies, the canonical recursive algorithm in <ref type="bibr" target="#b35">[36]</ref> can only generate non-modular topologies as shown in <ref type="figure">Figure</ref> 13. In practice, as shown in Jupiter <ref type="bibr" target="#b30">[31]</ref>, the topology is composed of heterogenous building blocks (chassis), which are packed into a single rack and therefore enforce port hiding (the idea that as few ports from a rack are exposed outside the rack). Although Jupiter is modular and supports port hiding, it is single instance of a Clos-like topology with a specific set of parameters. We seek an algorithm that can take any valid set of Clos parameters and produce chassis-based topologies automatically. Besides, it would be desirable for this algorithm to generate all possible feasible topologies satisfying the parameters, so we can select the one that is most compactly packed.</p><p>Our logical Clos generation algorithm achieves these goals. Specifically, the algorithm uses the following steps:</p><p>1. Compute the total number of layers of homogeneous switching chips needed. Namely, given N servers and radix k switches, we use n = log k ( N ) to compute the number of layers of chips n needed.</p><p>2. Determine the total number of layers of chips for edge, aggregation and core layers, which are represented by e, a and s respectively, such that e + a + s = n.</p><p>3. Identify blocks for edge, aggregation and core layer.</p><p>Clos networks rely on every edge being able to reach every spine through exactly one path, by fanning out via as many different aggregation blocks as possible (and vice versa). We find that the resulting interconnection is a derivative of the classical perfect shuffle Omega network ( <ref type="bibr" target="#b20">[21]</ref>, e.g., aggregation blocks in <ref type="figure">Figure 14</ref> and <ref type="figure" target="#fig_4">Figure 15</ref>). Therefore, we use Omega networks to build both the edge and aggregation blocks, and to define the connections between edge-aggregation and aggregationspines. The spine block on the other hand needs to be rearrangeably-nonblocking, so it can relay flows from any edge to any other edge with full capacity. Therefore it is built as a smaller Clos topology <ref type="bibr" target="#b5">[6]</ref> (e.g., spine blocks in <ref type="figure">Figure 14</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Compose the whole network using edge, aggregation and core blocks. The process to compose the whole topology is to link all these blocks and uses the same procedure as Jupiter <ref type="bibr" target="#b30">[31]</ref>.</p><p>We have verified that topologies generated by our construction algorithm, such as the ones in <ref type="figure">Figure 14</ref> and <ref type="figure" target="#fig_4">Figure 15</ref>, are isomorphic to a topology generated using the canonical algorithm in <ref type="figure">Figure 13</ref>. By changing different combinations of e, a and s, we can obtain multiple candidate topologies, as shown in <ref type="figure">Figure 14</ref> and <ref type="figure" target="#fig_4">Figure 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Jellyfish Placement Algorithm</head><p>For Jellyfish, we use a heuristic random search algorithm to place switches and servers. The algorithm works as follows. At each stage of the algorithm, a node can be in one of two states: placed, or un-placed. A placed node is one which has been positioned in a rack. Each step of the algorithm randomly selects an un-placed node. If the selected node has logical neighbor nodes that have already been placed, we place this node at the centroid of the area formed by its placed logical neighbors. If no placed neighbor exists, the algorithm randomly selects a rack to place the node. We have also tried other heuristics like neighbor-first, which tries to place a switch's logical neighbors as close as possible around it. However, this performs worse than our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Scale-invariance of Expansion</head><p>Scale-invariance of Expandability for Symmetric Clos. For a symmetric Clos network, the number of expansion steps is scale-invariant and independent of the degree to which the original topology is partially deployed. Consider a simplified Clos where the original topology has g aggregation blocks.</p><p>Each aggregation block has p ports for spine-aggregation links, each of which has the unit capacity. Assume the worstcase traffic in which all sources are located in the left half of aggregation blocks and all destinations are in the right half. This network contains g • p/2 crossing links between left and right halves. If, during expansion, the network is expected to support a demand of d units capacity per aggregation block, the total demand traversing the cut between the left and right halves in one direction is d • g/2. Then, the maximum number of links that can be redistributed in an expansion step is k = g • p/2 − d • g/2 = g(p − d)/2, which is linear in the number of aggregation blocks (network size). This linearity between k and g implies scale-invariant expandability, e.g., when an aggregation block is doubled to 2g, the maximum number of redistributed links per expansion step becomes 2k. Scale-invariance of Expandability for Jellyfish, Xpander, and FatClique. A random graph consists of s nodes, which is a first-order approximation for Jellyfish's switch, Xpander's metanode and FatClique's block. Each node has p internode ports, so there are s • p/2 inter-node links. We can treat the network as a bipartite graph. We assume the worstcase traffic matrix, where all traffic is sent through one part of the bipartite graph to the other. Suppose an expansion SLO requires each source-destination node pair to support nodes is doubled to 2s, we can redistribute 2k links in the first step. It is easy to see that, after each expansion step, the number of links added to the bottleneck is also linear with the number of nodes, so the expandability is scale-invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 FatClique Expansion Algorithm</head><p>Algorithm 1 shows the expansion algorithm for FatClique. The input to the algorithm includes original and target topologies T o and T n , the link break ratio during an expansion step α, multipliers β &lt; 1 and γ &gt; 1, which are used to adjust α based on network capacity. α specifies the fraction of existing links that must be broken for re-wiring. The output of the algorithm is the expansion plan P lan.</p><p>Our expansion algorithm is an iterative trial-and-error approach (Line 4). Each iteration tries to find the right amount of links to break while satisfying the aggregate capacity constraint (Line 11) and the edge capacity constraint (Line 6), which guarantees that the north-to-south capacity ratio is always not smaller than 1 during any expansion step. If all constraints are satisfied, we accept this plan and tentatively increase the link break ratio α (Line 16, by multiplying by γ) due to capacity increase. Otherwise, the link break ratio α (Line 12) is decreased (by multiplying by β conservatively.) </p><formula xml:id="formula_5">input : T o , T n , SLO output: P lan 1 Initialize α ∈ (0, ∞), β ∈ (0, 1), γ ∈ (1, ∞)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Expansion for Clos</head><p>Since the motivation of this work is to compare topologies, we only focus on developping optimal expansion solutions for symmetrical Clos. More general algorithms for Clos' expansion can be found in <ref type="bibr" target="#b37">[38]</ref>. Also, similar to <ref type="bibr" target="#b37">[38]</ref>, we assume the worst case traffic matrices for Clos, i.e., servers under a pod will send traffic using full capacity to servers in other pods. Target Topology Generation. As mentioned in §4.1, a pod is the unit of expansion in Clos. When we add new pods and associated spines to a Clos topology for expansion, the wiring pattern inside a pod remains unchanged. To make the target topology non-blocking and to ease expansion (i.e., number of to-be-redistributed links on each pod is the same), links from a pod should be distributed across all spines as evenly as possible. Expansion plan generation. Once a target Clos topology is generated, the next step is to redistribute links to convert the original topology into the target topology. By comparing the original and target topology, it is easy to figure out which new links should be routed to which patch panels to satisfy the wiring constraint. In this section, we mainly focus on how to drain links such that the capacity constraint is satisfied and the number of expansion steps is minimized. Insight 1: Maximum rewired links at each pod is bounded. At each expansion step, when links are drained, network capacity drops. At the same time, as expansion proceeds, new devices are added incrementally, the overall network capacity increases gradually during the whole expansion process. In general, during expansion, the incrementally added capacity should be leveraged to speed up the expansion process. Due to the thin edges in Clos, no matter what the overall network capacity is, the maximum number links to be drained at each pod is bounded by the number of links on each pod multiplied by (1 − SLO). <ref type="figure">Figure 16</ref> shows an example. The leftmost figure is a folded Clos, where each pod has 16 links (4 trunks). If the SLO is 75%, the maximum number of links to be drained at a single step is 16 × (1 − 0.75) = 4. For our expansion plan generation algorithm, we try to achieve this bound at each pod at every single step.</p><p>Insight 2: Drain links at spines uniformly across edges (pods). Given the number of links allowed to be drained at each pod, we need to carefully select which links are to be drained. <ref type="figure">Figure 16</ref> shows two draining plans. Drain plan 1 will drain links from two spines uniformly across all pods. The residual capacity is 48, satisfying the requirement  <ref type="figure">Figure 16</ref>: Original topology is a Folded Clos with capacity=64. The required SLO during expansion is 75%, which means capacity should be no smaller than 48. There are 16 links on each pod. Due to the SLO constraint, for all plans, 4 links are allowed to be drained at each pod. SLO=75%. By uniformly, we mean the number of drained links between the spine and all pods are the same. Drain plan 2 also drains 4 links from each pods but not uniformly (for example, more links are drained at the third spine compared to the fourth spine), which violates the SLO requirement since the residual capacity is only 40, smaller than the 48 in Drain plan 1. Insight 3: Create physical loops by selecting the right target spines. Ideally, drained links with the same index on a pod on the same original spine should be redistributed to the same spine because the traffic sent from the pod to the target spine has a return path to the pod. Otherwise, the traffic will be dropped. <ref type="figure" target="#fig_5">Figure 17</ref> illustrates this insight. The right side of the figure shows the performance of two redistribution plans. The y axis shows the normalized capacity of the network at each expansion step. In the first plan, link 1 is first moved to spine s1 (1-s1),followed by link 3 to the same spine s1 (3-s1) which results in 75% capacity loss, since the two pods are connected by three paths instead of four. Once links 1 and 3 are undrained, s1 connects the two pods by a fourth path, and the normalized capacity is restored to 1. This redistribution step now provides leeway for supporting 25% capacity loss in the next step. In this next step, links 2 and 4 are rewired to connect to s2. During the rewiring, capacity again drops to 75%, with three paths between the pods. On undraining links 2 and 4, the capacity is once again restored to 1. In contrast, redistribution plan 2 violates SLO because it does not focus on restoring capacity by establishing paths via the new spine, as suggested by the insight (links 1 and 3 are moved to different spines).</p><p>Inspired by these insights, we designed Algorithm 2, which can achieve all our insights simultaneously when both original and target topologies are symmetric. The algorithm is optimal since at every expansion step, it achieves the upper bound of the links that could be drained. Therefore, our algorithm uses smallest steps to expand Clos.</p><p>The input to the algorithm is the original and new symmetric topology T o and T n . We use T o sp and T n sp to represent the number of links between spine s and pod p in the old and new topology respectively. Initially, T o s p = 0, where s is a new spine. The output of the algorithm is the draining plan, Subplan i , for expansion step i. The final expansion plan P lan = {Subplan i } and the number of Subplan, |P lan|, is the total expansion step.</p><p>The algorithm starts by indexing old spines, new spines and links on each pod from left to right respectively (Line 1-2), which are critical for the correctness of the algorithm since the algorithm relies on these indexes to break ties when selecting spines and links to redistribute. Then, based on our Insight 1, Line 3 computes the upper bound on the number of links to be redistributed on each pod, n p . We show experimentally that our algorithm can always achieve this upper bound in each individual step as long as T o and T n are symmetric. Next, the algorithm iterates over all indexed old spines (Line 4) and tries to drain n p links uniformly across all pods (Line 5) such that Insight 2 is satisfied. Line 6 compares the number of remaining to-be-redistributed links δ sp and n p and is useful only at the last expansion step. For each pod, the algorithm needs to find spines to redistribute links to (Line 7-14) while satisfying the constraint in Insight 3, i.e., drained links with the same index on a pod on the same original spine are redistributed to the same spine. Due to indexing and symmetric structure of Clos, our algorithm can always satisfy Insight 3. Specifically, when selecting spines, the spine satisfying δ s p = T n s p − T o s p &gt; 0 with the smallest index will be considered first (Line 8-Line 10). When selecting links from pod to redistribute, we always select the first n a links to redistribute (Line 14).</p><p>Theorem 1 Algorithm 2 produces the optimal expansion plan for Clos topology.</p><p>The proof is simple. Since at every expansion step, our algorithm achieves the upper bound of the links that could be drained, our algorithm uses smallest steps to finish the expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 FatClique Topology Synthesis Algorithm</head><p>The topology synthesis algorithm for FatClique is shown in Algorithm 3. Essentially, the algorithm is a search algorithm, and leverages the constraints C 1 to C 6 in §5.1 to prune the search space. It works as follows. The outermost loop (Line 2) enumerates the number of racks used for a sub-block. Based on the rack space constraints, sub-block size S c is determined Line 4. Next, the algorithm iterates over the number of subblocks in a block S b Line 5, whose size is constrained by M axBlockSize. Inside this loop, we leverage constraints C 1 to C 6 and derivations in §5.1 to find the feasible set of p c , which is represented by P c (Line 6). Then we construct FatClique based all design variables Line 8 and compute its capacity Line 9. If the capacity matches the target capacity, we add this topology into candidate set (Line 15). If the capacity is larger than required, the algorithm will increase s by 1 which will decrease the number of switches used n = N/s (N is fixed) and therefore reduce the network capacity in next search step (Line 13). If the capacity is smaller than required, the algorithm will decrease s by 1 (Line 11) to increase the number of switches and capacity in next search step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Parameter Setting</head><p>The cable price with transceivers used in our evaluation is listed in <ref type="table" target="#tab_13">Table 9</ref>. We found that a simple linear model does not fit the data. The data is better approximated by a piecewise linear function: cables shorter than 100 meters are fit using one linear model and cables beyond 100 meters are fit using another linear model. The latter has a larger slope because beyond 100 meters, more advanced and expensive transceivers are necessary. In our experiment, since we only know the discrete price for cables and associated transceivers, we do the following: if the length of the cable is X, we use the exact price; if the length if larger than X, we use the first cable price larger than X.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Other Metrics</head><p>In our evaluations, we have tried to topologies with qualitatively similar properties 6. In this section, we quantify other properties of these topologies. Edge Expansion and Spectral Gap. Since computing edge expansion is computationally hard, we follow the method in <ref type="bibr" target="#b34">[35]</ref> using spectral gap <ref type="bibr" target="#b16">[17]</ref> to approximate edge expansion. A larger spectral gap implies larger edge expansion. To fairly compare topologies, we equalize their bisection bandwidth first. As shown before, to achieve the same bisection USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 253 bandwidth, Clos uses many more switches. Also, Clos is not a d-regular graph and do not know of a way to compute the spectral graph for Clos-like topologies. Therefore, we compare the spectral gap only for d-regular graphs, Jellyfish, Xpander and FatClique at different scales (1k-4k nodes). The spectral gap is defined as follows <ref type="bibr" target="#b16">[17]</ref>. Let G with node degree d and A(G) denote the d-regular topology and its adjacent matrix. The matrix A(G) has n real eigenvalues which we denote by</p><formula xml:id="formula_6">λ 1 ≥ λ 2 ≥ • • • ≥ λ n . Spectral gap SG = d − λ 2 .</formula><p>In our experiments, chip radix is 32 and each node in those topologies connects to 8 servers, d = 24. The result is shown in <ref type="figure" target="#fig_6">Figure 18</ref>. First, we observe that spectral gap stays roughly the same under different scales. Also, the spectral gap of Fat-Clique is slightly lower than that of other topologies, which implies that FatClique has slightly smaller edge expansion compared to Jellyfish and Xpander. This is to be expected, since FatClique adds some hierarchical structure to cliques. Path Diversity. We compute the path diversity for different topologies. For Clos, we only calculate the number of shortest paths between two ToR switches from different pods. For other topologies, we compute the number of paths which are no longer than the shortest paths in the same-scale Clos. For example, for small-scale Clos, the shortest path length is 5. We will only calculate paths whose length is no larger than 5 in other topologies. This is a rough metric for path diversity. The results are shown in <ref type="figure" target="#fig_7">Figure 19</ref> and <ref type="figure" target="#fig_1">Figure 20</ref>. We found that Jellyfish, Xpander and FatClique have the same level of path diversity, which is higher than that of Clos. Also, those topologies have shorter paths than Clos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For a</head><label></label><figDesc>Clos with an over-subscription x:y we would need n = log k ( y•N/x ) layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>shows an example of Clos expansion. The upper left figure shows a partiallydeployed logical Clos, in which each spine and aggregation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Clos Expansion with Patch Panels block are connected by two links. The upper right is the target fully-deployed Clos, where each spine and aggregation block are connected by a single link. During expansion, we need to redistribute half of existing links (dashed) to the newly added spines without violating wiring and capacity constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>The role of patch panels in re-wiring. The lower figure in Figure 2 depicts the physical realization of the (logical) rewiring shown in the upper figure. (For simplicity, the figure only shows the re-wiring of links on one patch panel to a new pod). Fibers and bundles originate and terminate at patch panels, so re-wiring requires reconnecting input and output Basic Rewiring Operations at a patch panel Thin and Fat Edge Comparison ports at each patch panel. One important constraint in this process is that re-wiring cannot remove fibers that are already part of an existing bundle. Patch panels help localize rewiring and reuse existing cable bundling during expansions. Figure 3 shows, in more detail the rewiring process at a single patch panel. The leftmost figure shows the original wiring with connections (a, A), (b, B), (c, C), (d, D). To enable expansion, a topology is always deployed such that some ports at the patch panel are reserved for expansion steps. In the figure, we use these reserved ports to connect new fibers e, f , E and F (Phase 1). To get to a target wiring in the expanded network with connections (a, A), (b, B), (e, C), (f , D), (c, E), (d, F ), the following steps are taken: (1) Traffic is drained from (c, C), (d, D), (2) Connections (c, C), (d, D) are rewired, with c being connected to E, d being connected to F and so on, and (3) The new links are undrained, allowing traffic to use new capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>(Clique of Switches) Block (Clique of Sub-blocks) Local Bundles The Whole Network (Clique of Blocks) Global Bundles FatClique Topology sub-block Block switch server</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>FatClique Expansion example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Number of switches. C is Clos, J is Jellyfish, X is Xpander and F is FatClique. Number of Switches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Number of patch panels. C is Clos, J is Jellyfish, X is Xpander and F is FatClique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10</head><label>10</label><figDesc>quantifies the cabling cost of all topologies,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Average Number of Rewired Links at a Single Patch Panel across Steps</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>d unit demand. Then the total demands from all sources are d • s/2. The probability of a link being a cross link is 1/2, and the expected number of cross links is s • p/4. These cross links are expected to be the bottleneck between the sourcedestinations pairs. Therefore, in the first expansion step, we can redistribute at most k = s • p/4 − d • s/2 = s(p/4 − d/2) links, and the maximum number of redistributed links is linear in the number of nodes (network size), e.g., if the number of Block-Based Construction 1 Block-Based Construction 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2 6 if L b does not satisfy edge capacity constraint then α = α • β 8 end 9 Delete L b from T o 10 c 13 add L b back to T o 14 elseT 16 α = α • γ 17 P 1 :</head><label>268910131416171</label><figDesc>Find the total set of links to break, L, based on T o and T n 3 Compute original capacity c 0 4 while |L| &gt; 0 do Select a subset of links L b , from L uniformly across all blocks, where |L b | = α|L|. = ComputeCapacity(T o ) 11 if c &lt; c 0 • SLO thenα = α • β o = AddNewLinks(L b , T o , T n ) lan.add(L b ) FatClique Expansion Plan Generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 :</head><label>17</label><figDesc>Clos Draining Link Redistribution Scheduling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>7 while np &gt; 0 do foreach New Spine s do // Insight 3 δ s p = T n s p − T o s p 10 if δ s p &gt; 0 then break 11 12 end 13 na 14 Find 16 Subplan 2 :</head><label>7310121314162</label><figDesc>input : T o , T n , SLO output: Subplan 1 Index original and new spines from left to right starting from 1 respectively 2 Index links at each pod from left to right starting from 1 3 ∀ pod p, np = num_links_per_pod • (1-SLO) // Insight 1 4 foreach Original Spine s do foreach pod p do // Insight 2 δsp = T o sp − T n sp , np = min(np, δsp) // Insight 2 = min(δ s p , np) the first na to-be-distributed links, Lsp 15 np = np − na, update(T o ) Single Step Clos Expansion Plan Generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>4 S 9 Cap = ComputeCapacity(T ) 10 if</head><label>4910</label><figDesc>input : N ,r,Cap * ,s 0 output: candidate 1 candidate = [] 2 for i = 1; i &lt; M axRackP erSubblock; i + + do s = s 0 c = i • RackCapacity/(1 + s) 5 for S b = 1; S b &lt;= M axBlockSize; S b + + do P c = CheckConstraints(S c , S b ) 7 foreach p c in P c do T = ConstructTopology(S c , S b , s, p c ) Cap &lt; Cap * then s = s − 1 12 else if Cap &gt; Cap * then s = s + 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 19 :Figure 20 :</head><label>1920</label><figDesc>Path Diversity for Small-scale Topologies Path Diversity for Mediumscale Topologies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Deployment Complexity Comparison</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table><row><cell>Topology</cell><cell>4-layer Clos (Medium)</cell><cell>Jellyfish</cell></row><row><cell>Average # links rewired</cell><cell></cell><cell></cell></row><row><cell>per patch panel rack</cell><cell></cell><cell></cell></row><row><cell>Expansion steps</cell><cell>6</cell><cell></cell></row><row><cell>North-to-south capacity ratio</cell><cell></cell><cell>3</cell></row></table><note>the value of these measures for a medium-sized Clos and a comparable Jellyfish topology, when the expansion USENIX Association 16th USENIX Symposium on Networked Systems Design and Implementation 239</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Expansion Comparison (SLO = 90%)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Qualitative comparison of lifecycle management complexity</figDesc><table><row><cell>Auxiliary Variable</cell><cell>Description</cell></row><row><cell>ps = Sc − 1</cell><cell># ports per switch to</cell></row><row><cell></cell><cell>connect other switches inside a sub-block</cell></row><row><cell>p</cell><cell></cell></row></table><note>b = k − s − ps − pc # ports per switch to connect other blocks Rc</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>FatClique Variables</figDesc><table /><note>levels of hierarchy: individual sub-block (top left), intercon- nected into a block (top right), which are in turn intercon- nected to form FatClique (bottom). The interconnection used at every level in the hierarchy is a clique, similar to Dragon- fly</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Scalability of Topologies</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Bundle Types (Switch Radix = 32)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Datacenter settings mostly<ref type="bibr" target="#b25">[26]</ref> </figDesc><table><row><cell>Length</cell><cell>5</cell><cell>10</cell><cell>15</cell></row><row><cell>Price</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Length</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Price</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>40G QSFP Mellanox cable length in meter (Length) and price with transceivers (Price)<ref type="bibr" target="#b6">[7]</ref> </figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A patch panel or a wiring aggregator is a device that simplifies cable re-wiring.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">To achieve low wiring complexity, a full 5-layer Clos topology would require patch panel racks with four times as many ports as available today, so we restrict ourselves to the largest Clos that can be constructed with today's patch panel capacities</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="254">16th USENIX Symposium on Networked Systems Design and Implementation USENIX Association</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hyperx: Topology, routing, and packaging of efficient large-scale networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mclaren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SC9</title>
		<meeting>SC9</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A scalable, commodity data center network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Fares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukissas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Introducing data center fabric, the next-generation Facebook data center network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andreyev</surname></persName>
		</author>
		<ptr target="https://code.fb.com/production-engineering/introducing-data-center-fabric-the-next-generation-facebook-data-center-network/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Slim fly: A cost effective low-diameter network topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SC14</title>
		<meeting>SC14</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Broadcom Tomahawk Swiching chips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Broadcom</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="https://www.broadcom.com/products/ethernet-connectivity/switching/strataxgs/bcm56960-series" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A study of non-blocking switching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="406" to="424" />
			<date type="published" when="1953-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<ptr target="http://www.colfaxdirect.com" />
	</analytic>
	<monogr>
		<title level="j">Colfax International. Colfax direct</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rewire: An optimization-based framework for unstructured data center network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommy</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Elsheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>López-Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Keshav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOMM</title>
		<meeting>IEEE INFOCOMM</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Helios: A hybrid electrical/optical switch architecture for modular data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Farrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bazzaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fainman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Com</surname></persName>
		</author>
		<ptr target="https://www.fs.com/products/43552.html" />
		<title level="m">96 Fibers 12x MTP/MPO-8 to LC/UPC Single Mode 1U 40GB QSFP+ Breakout Patch Panel Flat</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Projector: Agile reconfigurable data center interconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghobadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ranade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Blanche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rastegarfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kilper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evolve or die: High-availability design principles drawn from googles network infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Govindan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Minei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kallahalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Koley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vl2: a scalable and flexible data center network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BCube: A High Performance, Server-centric Network Architecture for Modular Data Centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DCell: A Scalable and Fault-tolerant Network Structure for Data Centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Firefly: A reconfigurable wireless data center fabric using freespace optics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hamedazimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Longtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tanwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Expander graphs and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Hoory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="439" to="562" />
			<date type="published" when="2006-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond fat-trees without antennae, mirrors, and disco-balls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kassing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valadarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shahaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schapira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Technologydriven, highly-scalable dragonfly topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Access and alignment of data in an array processor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lawrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers, C-24</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1145" to="1155" />
			<date type="published" when="1975-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">F10: A fault-tolerant engineered network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX NSDI</title>
		<meeting>USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lessons learned from b4, google&apos;s sdn wan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<ptr target="https://www.usenix.org/sites/default/files/conference/protected-files/atc15_slides_mandal.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rotornet: A scalable, low-complexity, optical datacenter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Mellette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forencich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">What are Patch Panels &amp; When to Use Them?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitchell</surname></persName>
		</author>
		<ptr target="https://www.lonestarracks.com/news/2016/10/28/patch-panels/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Taming the flying cable monster: A topology design and optimization framework for data-center networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mudigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yalagandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX ATC</title>
		<meeting>USENIX ATC</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A cost comparison of datacenter network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Iannaccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International COnference</title>
		<meeting>the 6th International COnference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">RackSolutions. Open Frame Server Racks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Rackspace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>The Rackspace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cloud</surname></persName>
		</author>
		<ptr target="www.rackspacecloud.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Condor: Better topologies through declarative design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schlinker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Katz-Bassett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX NSDI</title>
		<meeting>USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Jupiter rising: A decade of clos topologies and centralized control in google&apos;s datacenter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Armistead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Felderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanagala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wanderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hölzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Jellyfish: Networking data centers randomly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Godfrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX NSDI</title>
		<meeting>USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Design space exploration of the dragonfly topology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Wilke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rumley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISC Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="https://www.siemon.com/us/white_papers/07-09-24-trunk-cable-planning-installation.asp" />
		<title level="m">The Siemon Company. Trunk Cable Planning &amp; Installation Guide</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Xpander: Towards optimal-performance datacenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Valadarsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shahaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dinitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schapira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM CoNEXT</title>
		<meeting>ACM CoNEXT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Computer Architecture: Single and Parallel Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Zargham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Designing a predictable internet backbone with valiant load-balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang-Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE IWQoS</title>
		<meeting>IEEE IWQoS</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Minimal rewiring: Efficient live expansion for clos data center networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX NSDI</title>
		<meeting>USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mirror mirror on the ceiling: Flexible wireless links for data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding and mitigating packet corruption in data center networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghobadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Förster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
