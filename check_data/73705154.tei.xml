<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-layer Representation Learning for Medical Concepts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-02-17">17 Feb 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology † Children Healthcare of Atlanta</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taha</forename><surname>Bahadori</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology † Children Healthcare of Atlanta</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Searles</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology † Children Healthcare of Atlanta</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Coffey</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology † Children Healthcare of Atlanta</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology † Children Healthcare of Atlanta</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-layer Representation Learning for Medical Concepts</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-02-17">17 Feb 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1602.05568v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Learning efficient representations for concepts has been proven to be an important basis for many applications such as machine translation or document classification. Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits will have broad applications in healthcare analytics. However, in Electronic Health Records (EHR) the visit sequences of patients include multiple concepts (diagnosis, procedure, and medication codes) per visit. This structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within each visit. In this work, we propose Med2Vec, which not only learns distributed representations for both medical codes and visits from a large EHR dataset with over 3 million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec displays significant improvement in key medical applications compared to popular baselines such as Skipgram, GloVe and stacked autoencoder, while providing clinically meaningful interpretation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discovering efficient representations of discrete high dimensional concepts has been a key challenge in a variety of applications recently <ref type="bibr" target="#b2">[3]</ref>. Using various types of neural networks, high-dimensional raw data can be transformed to continuous real-valued concept vectors that efficiently capture their latent relationship from data. Such succinct representations have been shown to improve the performance of various complex tasks across domains spanning from image processing <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref>, language modeling <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>, word embedding <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>, music information retrieval <ref type="bibr" target="#b29">[30]</ref>, sentiment analysis <ref type="bibr" target="#b30">[31]</ref>, and more recently multi-modal learning of images and text <ref type="bibr" target="#b17">[18]</ref>.</p><p>Efficient representations for concepts is an important, if not essential, element in healthcare as well. Healthcare concepts contain rich latent relationships that cannot be represented by simple one-hot coding <ref type="bibr" target="#b27">[28,</ref><ref type="bibr">Chapter 2.3.2]</ref>. For example, pneumonia and bronchitis are clearly more related than pneumonia and obesity. In one-hot coding, such relationship between different codes are not represented. Despite its limitation, many healthcare applications <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1]</ref> still use the simple sum over one-hot vectors to derive patient feature vectors. To overcome this limitation, it is common in healthcare applications, to rely on carefully designed feature representations <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36]</ref>. However, this process often involves supervision information and ad-hoc feature engineering that requires considerable expert medical knowledge and is not scalable in general.</p><p>Recently, studies have shown that it is possible to learn efficient representations of healthcare concepts without medical expertise and still significantly improve the performance of various healthcare applications. Choi et al. <ref type="bibr" target="#b8">[9]</ref> learned distributed representations of medical codes (e.g. diagnosis, medication, procedure codes) using Skip-gram <ref type="bibr" target="#b24">[25]</ref> and applied them to heart failure prediction. Choi et al. <ref type="bibr" target="#b9">[10]</ref> also learned the representations for medical concepts from a medical claims dataset and compared the learned representations to existing medical ontologies and code groupers. Despite these progress, learning efficient representations of healthcare concepts, however, is still an open challenge. The difficulty stems from several aspects:</p><p>1. Healthcare data have a unique structure where the visits are temporally ordered but the medical codes within a visit form an unordered set. A sequence of visits possesses sequential relationship among them which cannot be captured by simply aggregating code-level representations. Moreover, given the demographic information for patients, the structure becomes more complex.</p><p>2. Learned representations should be interpretable. While the interpretability of the model in the clinical domain is considered to be an essential requirement, some of the state-of-the art representation learning methods such as recurrent neural networks (RNN) are difficult to interpret.</p><p>3. The algorithm should be scalable enough to handle real-world healthcare datasets with millions of patients and hundred millions of visits.</p><p>To address such challenges in healthcare concept representation learning, we propose Med2Vec and make the following contributions.</p><p>• We propose Med2Vec, a simple and robust algorithm to efficiently learn succinct code-, and visit-level representations by using real-world electronic health record (EHR) datasets, without depending on expert medical knowledge.</p><p>• Med2Vec learns interpretable representations and enables clinical applications to offer more than just improved performances. We conducted detailed user study with clinical experts to validate the interpretability of the resulting representation.</p><p>• We conduct experiments to demonstrate the scalability of Med2Vec, and show that our model can be readily applied to near 30K medical codes over two large datasets with 3 million and 5.5 million visits, respectively.</p><p>• We apply the learned representations to various real-world health problems and demonstrate the improved performance enabled by Med2Vec compared to popular baselines.</p><p>In the following section, we discuss related works, then describe our method in section 3. In section 4, we explain experiment design and interpretation method in detail. We present the results and discussion in section 5. Then we conclude this paper with future work in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Related Work</head><p>In this section, we first describe the preliminary ideas used in learning representation for words. Then, we review the algorithms developed for representing healthcare data. <ref type="figure">Figure 1</ref>: Skip-gram model architecture: v(w t ) is a vector representation for the word w t . The goal of Skip-gram is to learn vector representations of words that are good at predicting neighboring words.</p><formula xml:id="formula_0">w t+1 w t-1 projection w t-2 w t+2 . . . . . . w t v(w t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning representation for words</head><p>Representation learning of words using neural network based methods have been studied since the early 2000's <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33]</ref>. Among these techniques, Skip-gram <ref type="bibr" target="#b24">[25]</ref> is the basis of many concept representation learning methods, including our own. Skip-gram is able to capture the subtle relationships between words, thus outperforming the previous works in a word analogy task <ref type="bibr" target="#b22">[23]</ref>.</p><p>Given a sequence of words w 1 , w 2 , . . . , w T , Skip-gram learns the word representations based on the co-occurrence information of words inside a context window of a predefined size. The key principle of Skip-gram is that a word's representation should be able to predict the neighboring words. The objective of Skip-gram is to maximize the following average log probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">T</head><formula xml:id="formula_1">T t=1 −c≤j≤c,j =0 log p(w t+j |w t )</formula><p>where c is the size of the context window. The conditional probability is defined by the softmax function:</p><formula xml:id="formula_2">p(w O |w I ) = exp v w O v w I W w=1 exp v w v w I</formula><p>where v w and v w are the input and output vector representations of word w. W is the number of words in the vocabulary. Basically, Skip-gram tries to maximize the softmax probability of the inner product of the center word's vector and its context word's vectors. <ref type="bibr" target="#b0">1</ref> Pennington et al. proposed GloVe, <ref type="bibr" target="#b28">[29]</ref> which learns another word representations by using a similar principle as Skip-gram. GloVe uses the global word co-occurrence matrix to learn the word representations. Since the global co-occurrence matrix is often sparse, GloVe can be computationally less demanding than Skip-gram, which is a neural network model using the sliding context window. On the other hand, GloVe employs a weighting function that could require a considerable amount tuning effort.</p><p>Beyond one level representation like Skip-gram and GloVe, researchers also proposed hierarchical learning representations for the text corpus, which has some analogy to our healthcare setting with two level concepts namely: codes and visits. Le and Mikolov <ref type="bibr" target="#b19">[20]</ref> proposes to learn representations for paragraphs and words simultaneously by treating paragraphs as one of the words. However, their algorithm assigns a fixed set of vectors for both words and paragraphs in the training data. Moreover, their approach does not capture the sequential order among paragraphs. Skip-thought <ref type="bibr" target="#b18">[19]</ref> proposes an encoder-decoder structure: an encoder (Gated Recurrent Units (GRU) in their case) learns a representation for a sentence that is able to regenerate its surrounding sentences (via GRU again). Skip-thought cannot be applied directly to EHR data because unlike words in sentences, the codes in a visit are unordered. Also, the interpretation of Skip-thought model is difficult, as they rely on complex RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Representation learning in healthcare</head><p>Recently researchers start to explore the possibility of efficient representation learning in the medical domain.</p><p>Medical text analysis Minarro et al. <ref type="bibr" target="#b25">[26]</ref> learns the representations of medical terms by applying Skip-gram to various medical text collected from PubMed, Merck Manuals, Medscape and Wikipedia. De Vine et al. <ref type="bibr" target="#b12">[13]</ref> learns the representations of UMLS concepts from free-text patient records and medical journal abstracts. They first replaced the words in documents to UMLS concepts, then applied Skip-gram to learn the distributed representations of the concepts. However, none of them studied longitudinal EHR data with a large number of medical codes.</p><p>Structured visit records analysis Choi et al. <ref type="bibr" target="#b8">[9]</ref>, and Choi et al. <ref type="bibr" target="#b9">[10]</ref> both learned the distributed representation of medical codes (e.g. diagnosis, medication, procedure codes) from structured longitudinal visit records of patients using Skip-gram. In addition, the authors demonstrated that simply aggregating the learned representation of medical codes to create a visit representation leads to improved predictive performance. However, simply aggregating the code representations is not the optimal method to generate a visit representation as it completely ignores the temporal relations across adjacent visits. We believe that taking advantage of the two-level information (the co-occurrence of codes within a visit and the sequential nature of visits) and the demographic information of patients will give us better representation for both medical codes and patient visits.</p><p>Choi et al. <ref type="bibr" target="#b7">[8]</ref> trained a recurrent neural networks (RNN) model to analyze the longitudinal patient records in a temporal order, and predict the diagnosis and medication codes the patient will receive in the future. In <ref type="bibr" target="#b7">[8]</ref>, the hidden layer of the RNN can be seen as the representation of the patient status over time. However, despite its outstanding performance, RNNs are difficult to interpret.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe the proposed algorithm Med2Vec. We start by mathematically formulating the EHR data structure and our goal. Then we describe our approach in a top-down fashion. We also explain how to interpret the learned representations. We conclude this section with complexity analysis. </p><formula xml:id="formula_3">+ . . . 0 1 0 0 0 0 1 0 v t x t+1 u t d t x t-1 x t Softmax x t-2 x t+2 . . . . . . ReLU(W v [u t , d t ] + b v ) ReLU(W c x t + b c ) {0, 1} |C|</formula><formula xml:id="formula_4">. . , x t−2 , x t−1 , x t+1 , x t+2 , . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EHR structure and our notation</head><p>We denote the set of all medical codes c 1 , c 2 , . . . , c |C| in our EHR dataset by C with size |C|. EHR data for each patient is in the form of a sequence of visits V 1 , . . . , V T where each visit contains a subset of medical codes V t ⊆ C. Without loss of generality, all algorithms will be presented for a single patient to avoid cluttered notations. The goal of Med2Vec is to learn two types of representations:</p><p>Code representations We aim to learn an embedding function f C : C → R m + that maps every code in the set of all medical codes C to non-negative real-valued vectors of dimension m. The non-negativity constraint is introduced to improve interpretability, as discussed in details in Section 3.5. Visit representations Our second task is to learn another embedding function f V : V → R n that maps every visit (a set of medical codes) to a real-valued vector of dimension n. The set V is the power set of the set of codes C. <ref type="figure" target="#fig_0">Figure 2</ref> depicts the architecture of Med2Vec. Given a visit V t , we use a multi-layer perceptron (MLP) to to generate the corresponding visit</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Med2Vec architecture</head><formula xml:id="formula_5">representation v t . First, visit V t is represented by a binary vector x t ∈ {0, 1} |C| where the i-th entry is 1 only if c i ∈ V t . Then x t is converted to an intermediate visit representation u t ∈ R m as follows, u t = ReLU (W c x t + b c )<label>(1)</label></formula><p>using the code weight matrix W c ∈ R m×|C| and the bias vector b c ∈ R m . The rectified linear unit is defines as ReLU (v) = max(v, 0). Note that max() applies element-wise to vectors. We use the rectified linear unit (ReLU) as the activation function to enable interpretability, which will be discussed in section 3.3. We concatenate the demographic information </p><formula xml:id="formula_6">d t ∈ R d ,</formula><formula xml:id="formula_7">v t ∈ R n as follows, v t = ReLU (W v [u t , d t ] + b v )</formula><p>using the visit weight matrix W v ∈ R n×(m+d) and the bias vector b v ∈ R n , where n is the predefined size of the visit representation. We use ReLU once again as the activation function. We discuss our efficient training procedure of the parameters W c , b c , W v and b v in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning from the visit-level information</head><p>As mentioned in the introduction, the sequential information of visits can be exploited for learning efficient representations of visits and potentially codes. We train the MLP using a very straightforward intuition as follows: a visit describes a state in a continuous process that is a patient's clinical experience. Therefore, given a visit representation, we should be able to predict what has happened in the past, and what will happen in the future. Specifically, given a visit representation v t , we train a softmax classifier that predicts the medical codes of the visits within a context window. We minimize the cross entropy error as follows,</p><formula xml:id="formula_8">min Ws,bs 1 T T t=1 −w≤i≤w,i =0 −x t+i logŷ t − (1 − x t+i ) log(1 −ŷ t ),<label>(2)</label></formula><p>whereŷ</p><formula xml:id="formula_9">t = exp(W s v t + b s ) |C| j=1 exp(W s [j, :]v t + b s [j])</formula><p>where W s ∈ R |C|×n and b s ∈ R |C| are the weight matrix and bias vector for the softmax classifier, w the predefined context window size, exp the element-wise exponential function, and 1 denotes an all one vector. We have used MATLAB's notation for selecting a row in W s and a coordinate of b s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning from the code-level information</head><p>As we described in the introduction, healthcare datasets contain two-level information: visit-level sequence information and code-level co-occurrence information. Since the loss function in Eq. (2) can efficiently capture the sequence level information, now we need to find a way to use the second source of information, i.e., the intra-visit co-occurrence of the codes. A natural choice to capture the code co-occurrence information is to use Skip-gram. The main idea would be that the representations for the codes that occur in the same visit should predict each other. To embed Skip-gram in Med2Vec, we can train W c ∈ R m×|C| (which also produces intermediate visit level representations) so that the i-th column of W c will be the representation for the i-th medical code among total |C| codes. Note that given the unordered nature of the codes inside a visit, unlike the original Skip-gram, we do not distinguish between the "input" medical code and the "output" medical code. In text, it is sensible to assume that a word can serve a different role as a center word and a context word, whereas in EHR datasets, we cannot classify codes as center or context codes. It is also desirable to learn the representations of different types of codes (e.g. diagnosis, medication, procedure code) in the same latent space so that we can capture the hidden relationships between them.</p><p>However, precise interpretation of Skip-gram codes will be difficult as W c will have positive and negative values. For intuitive interpretation, we should learn code representations with non-negative values. Note that in Eq.(1), if the binary vector x t is a one-hot vector, then the intermediate visit representation u t becomes a code representation. Therefore, using the Skip-gram algorithm, we train the non-negative weight ReLU (W c ) instead of W c . This will not only use the intra-visit co-occurrence information, but also guarantee non-negative code representations. Moreover, ReLU produces sparse code representations, which further facilitates easier interpretation of the codes.</p><p>The code representations to be learned is denoted as a matrix W c = ReLU (W c ) ∈ R m×|C| . From a sequence of visits V 1 , V 2 , . . . , V T , the code-level representations can be learned by maximizing the following log-likelihood,</p><formula xml:id="formula_10">min W c 1 T T t=1 i:c i ∈Vt j:c j ∈Vt,j =i log p(c j |c i ),<label>(3)</label></formula><p>where</p><formula xml:id="formula_11">p(c j |c i ) = exp W c [:, j] W c [:, i] |C| k=1 exp W c [:, k] W c [:, i] .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Unified training</head><p>The single unified framework can be obtained by adding the two objective functions <ref type="formula" target="#formula_10">3</ref>and <ref type="formula" target="#formula_8">2</ref>as follows,</p><formula xml:id="formula_12">argmin W ,b 1 T T t=1 − i:c i ∈Vt j:c j ∈Vt,j =i log p(c j |c i ) + −w≤k≤w,k =0 −x t+k logŷ t − (1 − x t+k ) log(1 −ŷ t )</formula><p>By combining the two objective functions we learn both code representations and visit representations from the same source of patient visit records, exploiting both intra-visit co-occurrence information as well as inter-visit sequential information at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Interpretation of learned representations</head><p>While the original Skip-gram learns code representations that have interesting properties such as additivity, in healthcare we need stronger interpretability. We need to be able to associate clinical meaning to each dimension of both code and visit representations. Interpreting the learned representations is based on analyzing each coordinate in both code and visit embedding spaces.</p><p>Interpreting code representations If information is properly embedded into a lower dimensional non-negative space, each coordinate of the lower dimension can be readily interpreted. Nonnegative matrix factorization (NMF) is a good example. Since we trained ReLU (W c ) ∈ R m×|C| , a non-negative matrix, to represent the medical codes, we can employ a simple method to interpret the meaning of each coordinate of the m-dimensional code embedding space. We can find the top k codes that have the largest values for the i-th coordinate of the code embedding space as follows,</p><formula xml:id="formula_13">argsort(W c [i, :])[1 : k]</formula><p>where argsort returns the indices of a vector that index its values in a descending order. By studying the returned medical codes, we can view each coordinate as a disease group. Detailed examples are given in section 5.1</p><p>Interpreting visit representations To interpret the learned visit vectors, we can use the same principle we used for interpreting the code representation. For the i-th coordinate of the n-dimensional visit embedding space, we can find the top k coordinates of the code embedding space that have the strongest values as follows,</p><formula xml:id="formula_14">argsort(W v [i, :])[1 : k]</formula><p>where we use the same argsort as before. Once we obtain a set of code coordinates, we can use the knowledge learned from interpreting the code representations to understand how each visit coordinate is associated with a group of diseases. This simple interpretation is possible because the intermediate visit representation u t is a non-negative vector, due to the ReLU activation function.</p><p>In the experiments, we also tried to find the input vector x t that most activates the target visit coordinate <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21]</ref>. However, the results were very sensitive to the initial value of x t , and even averaging over multiple samples were producing unreliable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Complexity analysis</head><p>We first analyze the computational complexity of the code-level objective function Eq. (3). Without loss of generality, we assume the visit records of all patients are concatenated into a single sequence of visits. Then the complexity for Eq. (3) is as follows,</p><formula xml:id="formula_15">O(T M 2 |C|m)</formula><p>where T is the number of visits, M 2 is the average of squared number of medical codes within a visit, |C| the number of unique medical codes, m the size of the code representation. The M 2 factor comes from iterating over all possible pairs of codes within a visit. The complexity of the visit-level objective function Eq.(2) is as follows,</p><formula xml:id="formula_16">O(T w(|C|(m + n) + mn))</formula><p>where w is the size of the context window, n the size of the visit representation. The added terms come from generating a visit representation via MLP. Since size of code representation m and size of visit representation n generally have the same order of magnitude, we can replace n with m. Furthermore, m is generally smaller than |C| by at least two orders of magnitude. Therefore the overall complexity of Med2Vec can be simplified as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O(T |C|m(M 2 + w))</head><p>Here we notice that M 2 is generally larger than w. In our work, the average number of codes M per visit for two datasets are 7.88 and 3.19 according to Tables 1, respectively, whereas we select the window size w to be at most 5 in our experiments. Therefore the complexity of Med2Vec is dominated by the code representation learning process, for which we use the Skip-gram algorithm. This means that exploiting visit-level information to learn efficient representations for both visits and codes does not incur much additional cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the performance of Med2Vec in both public and proprietary datasets. First we describe the datasets. Then we describe evaluation strategies for code and visit representations, along with implementation details. Then we present the experiment results of code and visit representations with discussion. We conclude with convergence and scalability study. We make the source code of Med2Vec publicly available at https://github.com/mp2893/med2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset description</head><p>We evaluate performance of Med2Vec on a dataset provided by Children's Healthcare of Atlanta (CHOA) 2 . We extract visit records from the dataset, where each visit contains several medical codes (e.g. diagnosis, medication, procedure codes). The diagnosis codes follow ICD-9 codes, the medication codes are denoted by National Drug Codes (NDC), and the procedure codes follow Category I of Current Procedural Terminology (CPT). We exclude patients who had less that two visits to showcase Med2Vec's ability to use sequential information of visits. The basic statistics of the dataset are summarized in <ref type="table" target="#tab_1">Table 1</ref>. The data are fully de-identified and do not include any personal health information (PHI). We divide the dataset into two groups in a 4:1 ratio. The former is used to train Med2Vec. The latter is held off for evaluating the visit-level representations, where we train models to predict visit-related labels. The details of the evaluation will be provided in the following subsections.</p><p>We also use CMS dataset, a publicly available 3 synthetic EHR dataset. The basic information of CMS is also given in <ref type="table" target="#tab_1">Table 1</ref>. Compared to CHOA dataset, the CMS dataset has more patients but fewer unique medical codes. The average number of codes per visit is also smaller than that of CHOA dataset. Since CMS dataset is synthetic, we use it only for testing the scalability of Med2Vec and baseline models in section 4.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Strategy of code representations</head><p>Qualitative evaluation by medical experts For a comprehensive qualitative evaluation, we perform a relatedness test by selecting 100 most frequent diagnosis codes and their 5 closest diagnoses, medications and procedures in terms of cosine similarity. This allow us to know if the learned representations effectively capture the latent relationships among them. Two medical experts from CHOA check each item and assign related, possible and unrelated labels.</p><p>Quantitative evaluation with baselines We use medical code groupers to quantitatively evaluate the code representations. Code groupers are used to collapse individual medical codes into clinically meaningful categories. For example, Clinical Classifications Software (CCS) groups ICD9 diagnosis codes into 283 categories such as tuberculosis, bacterial infection, and viral infection.</p><p>We apply K-means clustering to the learned code representations and calculate the normalized mutual information (NMI) based on the group label of each code. We use the CCS as the ground truth for evaluating the code representation for diagnosis. For medication code evaluation, we use American Hospital Formulary Service (AHFS) pharmacologic-therapeutic classification, which groups NDC codes into 165 categories. For procedure code evaluation, we use the second-level grouping of CPT category I, which groups CPT codes into 115 categories.Thus, we set the number of clusters k to 283, 165, 115 respectively for the diagnosis, medication, procedure code evaluation, which matches the numbers of groups from individual groupers.</p><p>For baselines, we use popular methods that efficiently exploit co-occurrence information. Skipgram (which is used in learning representations of medical concepts by <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref>) is trained using Eq. (3). GloVe will be trained on the co-occurrence matrix of medical codes, for which we counted the codes co-occurring within a visit. Additionally, we also report well-known baselines such as singular value decomposition on the co-occurrence matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation strategy of visit representation</head><p>We evaluate the quality of the visit representations by performing two visit-level prediction tasks: predicting the future visit and predicting the present status. The former will evaluate a visit representation's potential effectiveness in predictive healthcare while the latter will evaluate the how well it captures the information in the given visit. The details of the two tasks are given below. Predicting future medical codes: We predict the medical codes that will occur in the next visit using the visit representations. Specifically, given two consecutive visits V i and V j , the medical codes c ∈ V j will be the target y, the medical codes c ∈ V i will be the input x, and we use softmax to predict y given x. The predictive performance will be measured by Top-k Recall, which mimics the differential diagnosis conducted by doctors. We set k = 30 to cover even the complex cases of CHOA dataset, as over 167,000 visits are assigned with more than 20 medical codes according to <ref type="table" target="#tab_1">Table 1</ref>. We predict the grouped medical codes, obtained by the medical groupers used in Section 4.2.</p><p>Predicting Clinical Risk Groups (CRG) level: A patient's CRG level indicates his severity level. It ranges from 1 to 9, including 5a and 5b. The CRG levels can be divided into two groups: non-severe (CRG 1-5a) and severe (CRG 5b-9). Given a visit, we use logistic regression to predict the binary CRG class associated with the visit. We use Area Under The Curve (AUC) to measure the classification accuracy, as it is more robust to class imbalance in data.</p><p>Baselines For baselines, we use the following methods. Binary vector model (One-hot+): In order to compare with the raw input data, we use the binary vector x t as the visit representation. Stacked autoencoder (SA): Stacked autoencoder is one of the most popular unsupervised representation learning algorithms <ref type="bibr" target="#b34">[35]</ref>. Using the binary vector x t concatenated with patient demographic information as the input, we train a 3-layer stacked autoencoder (SA) <ref type="bibr" target="#b4">[5]</ref> to minimize the reconstruction error. The trained SA will then be used to generate visit representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sum of Skip-gram vectors (Skip-gram+):</head><p>We first learn the code-level representations with Skip-gram only (Eq. (3)). Then for the visit-level representation, we simply add the representations of the codes within the visit. This approach was proven very effective for heart failure prediction in <ref type="bibr" target="#b8">[9]</ref>. We append patient demographic information at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sum of GloVe vectors (GloVe+):</head><p>We perform the same process as Skip-gram+, but use GloVe vectors instead of Skip-gram vectors. We use the recommended hyperparameter setting from <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation details</head><p>We use the held-off dataset, which was not used to learn the code and visit representations, to perform the two prediction tasks. The held-off dataset contains 672,110 visits assigned with CRG levels. In order to train the predictors, we divide the held-off data to training and testing folds with ration 4:1. Both softmax and logistic regression are trained for 10 epochs on the training fold. We perform 5-fold cross validation for each task to tune the regularization parameter. For all baseline models and Med2Vec, we use age, sex and ethnicity as the demographic information in the input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation and training details</head><p>For learning code and visit representations using Med2Vec and all baselines, we use Adadelta <ref type="bibr" target="#b36">[37]</ref> in a mini-batch fashion. For Skip-gram, SA and Med2Vec, we use 1,000 visits 4 per batch. For GloVe, we use 1,000 non-zero entries of the co-occurrence matrix per batch. The optimization terminates after a fixed number of epochs. In section 4.6, we show the relationship between training epochs and the performance. We also show the convergence behavior of Med2Vec and the baselines in section 4.7.</p><p>Med2Vec, Skip-gram, GloVe and SA are implemented with Theano 0.7.0 <ref type="bibr" target="#b5">[6]</ref>. K-means clustering for the code-level evaluation and SVD are performed using Scikit-learn 0.14.1. Softmax and logistic regression models for the visit-level evaluation are implemented with Keras 0.3.1, and trained for 10 epochs. All tasks are executed on a machine equipped with Intel Xeon E5-2697v3, 256GB memory and two Nvidia K80 Tesla cards.</p><p>We train multiple models using various hyperparameter settings. For all models we vary the size of the code representations m (or the size of the hidden layer for SA), and the number of  To alleviate the curse of dimensionality when training the softmax classifier (Eq.(2)) of Med2Vec, we always use the medical code groupers of section 4.2 so that the softmax classifier is trained to predict the grouped medical codes instead of the exact medical codes. To confirm the impact of this strategy, we train an additional Med2Vec without using the medical code groupers. <ref type="table" target="#tab_2">Table 2</ref> shows the average score of the medical codes from the qualitative code evaluation. On average, Med2Vec successfully captures the relationship between medical codes. However, Med2Vec seems to have a hard time capturing proper representation of medications. This is due to the precise nature the medication prescription. For example, Med2Vec calculated that Ofloxacin, an antibiotic sometimes used to treat middle-ear infection, was related to sensorineural hearling loss (SNHL), an inner-ear problem. On the surface level, this is a wrong relationship. But Med2Vec can be seen as capturing the deeper relationship between medical concepts that is not always clear on the surface level. <ref type="table" target="#tab_3">Table 3</ref> shows the clustering NMI of diagnosis, medication and procedure codes, measured for various models. Med2Vec shows more or less similar conformity to the existing groupers as Skipgram. SVD shows the weakest conformity among all models. GloVe exhibits significantly stronger conformity than any other models. Exploiting the global co-occurrence matrix seems to help learn code representations where similar codes are closer to each other in terms of Euclidean distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results of the code-level evaluation</head><p>However, the degree of conformity of the code representations to the groupers does not necessarily indicate how well the code representations capture the hidden relationships. For example, CCS categorizes ICD9 224.4 Benign neoplasm of cornea as CCS 47 Other and unspecified benign neoplasm, and ICD9 370.00 Unspecified corneal ulcer as CCS 91 Other eye disorders. But the two diagnosis codes are both eye related problems, and they could be considered related in that sense. Therefore we recommend the readers use the evaluation results for comparing the performance between Med2Vec and other baselines, rather than for measuring the absolute performance.</p><p>In the following visit-level evaluation, we show that the code representations' strong conformity   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results of the visit-level evaluation</head><p>The first row of <ref type="figure" target="#fig_1">Figure 3</ref> shows the Recall@30 for predicting the future medical codes. First, in all of the experiments, Med2Vec achieves the highest performance, despite the fact that it is constrained to be positive and interpretable. The second observation is that Med2Vec's performance is robust to choice of the hyperparameters in a wide range of values. Comparing to a more volatile performance of Skip-gram, we can see that including the visit information in training not only improves the performance, but also stabilizes it too. Another fascinating aspect of the results is the overfitting pattern in different algorithms. Increasing the code representation size degrades the performance of all of the algorithms, as it leads to overfitting. Similar behavior can be seen as we train GloVe+ for more epochs which suggests early stopping technique should be used in representation learning <ref type="bibr" target="#b1">[2]</ref>. For Med2Vec, increasing the visit representation size n seems to have the strongest influence to its predictive performance.</p><p>The bottom row of figures in <ref type="figure" target="#fig_1">Figure 3</ref> shows the AUC for predicting the CRG class of the given visit. The overfitting patterns are not as prominent as the previous task. This is due to the different nature of the two prediction tasks. While the goal of CRG prediction is to predict a value related to the current visit, predicting the future codes is taking a step away from the current visit. This different nature of the two tasks also contributes to the better performance of One-hot+ on the CRG prediction. One-hot+ contains the entire information of the given visit, although in a very high-dimensional space. Therefore predicting the CRG level, which has a tight relationship with the medical codes within a visit, is an easier task for One-hot+ than predicting the future codes. <ref type="table" target="#tab_4">Table 4</ref> shows the performance comparison between two different Med2Vec models. The top model is trained with the grouped codes as explained in section 4.4, while the bottom models is trained with the exact codes. Considering the marginal difference of the CRG prediction AUC, it is evident that our strategy to alleviate the curse of dimensionality was beneficial. Moreover, using the grouped codes will improve the training speed as the softmax function will require less computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Convergence behavior and scalability</head><p>We first compare the convergence behavior of Med2Vec with Skip-gram (Eq. (3)), GloVe and SA. For SA, we measure the convergence behavior of a single-layer. We train the models for 50 epochs and plot the normalized difference of the loss value Lt−L t−1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lt</head><p>, where L t denotes the loss value at time t. To study the scalability of the models, we use both CHOA dataset and CMS dataset. We vary the size of the training data and plot the time taken for each model to run one epoch. <ref type="figure" target="#fig_2">Fig 4</ref> shows the convergence behavior of all models when trained on the CHOA dataset. SA shows the most stable convergence behavior, which is natural given that we used a single-layer SA, a much less complex model compared to GloVe, Skip-gram and Med2Vec. All models except SA seem to reach convergence after 10 epochs of training. Note that Med2Vec shows similar, if not better convergence behavior compared to Skip-gram even with added complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The left figure of</head><p>The center figure of <ref type="figure" target="#fig_2">Fig 4 shows</ref> the minutes taken to train all models for one epoch using the CHOA dataset. As we have analzyed in section ssec:complexity, Med2Vec takes essentially the same time to train for one epoch. Both Skip-gram and Med2Vec, however, takes longer than SA and GloVe. This is mainly due to having the softmax function for training the code representations. GloVe, which is trained on the very sparse co-occurrence matrix naturally takes the least time to train.</p><p>The right figure of <ref type="figure" target="#fig_2">Fig 4</ref> shows the training time when using the CMS dataset. Note that Med2Vec and Skip-gram takes similar time to train as SA. This is due to the smaller number of codes per visit, which is the computationally dominating factor of both Med2Vec and Skip-gram. GloVe takes less time as the number of unique codes are smaller in the CMS dataset. SA, on the other hand, takes more time because the number of visits have doubled while the the number of unique codes is about 73% of that of the CHOA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Interpretation</head><p>Given the importance of interpretability in healthcare, we demonstrate three stages of interpretability for our model in collaboration with the medical experts from CHOA. First, to analyze the learned code representations we show top five medical codes for each of six coordinates of the code embedding space and explain the characteristic of each coordinate. This way, we show how we can annotate each dimension of the code embedding space with clinical concepts. The six coordinates are specifically chosen so that they can be used in the later stages. Second, we demonstrate the interpretability of Med2Vec's visit representations by analyzing the meaning of two coordinates in the visit embedding space.</p><p>Finally, we extend the interpretability of Med2Vec to a real-world task, the CRG prediction, and analyze the medical codes that have strong influence on the CRG level. Once we learn the logistic regression weight w LR for the CRG prediction, we can extract knowledge from the learned weights by analyzing the visit coordinates to which the weights are strongly connected.</p><p>Instead of analyzing the visit coordinates, however, we propose an approximate way of directly finding out which code coordinate plays an important role in predicting the CRG class. Our goal is to find u t such that maximizes the output activation as follows 5</p><formula xml:id="formula_17">u t = argmax ut, ut 2 =1,ut 0 [ReLU (W v u t + b v )] w LR<label>(5)</label></formula><p>Given the fact that ReLU (•) is an increasing function (not-strictly though), we make an approximation and find the solution without the ReLU (•) term. The approximate solution can be found in closed form u t ∝ (W v w LR ) + . Finally, we calculate the element-wise product of u t and max(W c + b c ). This is to take into account the fact that each code coordinate has different</p><p>As we are interested in influential codes, we assume the demographic information vector is zero vector and omit it for ease of notation. maximum value. Therefore, instead of simply selecting the code coordinate with the strongest connection to the CRG level, we consider each coordinate's maximum ability to activate the positive CRG prediction. The resulting vector will show the maximum influence each code coordinate can have on the CRG prediction. <ref type="table" target="#tab_5">Table 5</ref> shows top ten codes with the largest value in each of the six coordinates of the code embedding space. The coordinate 112 is clearly related to sickle-cell disease and organ transplant. The two are closely related in that sickle cell disease can be treated with bone-marrow transplant. Prograf is a medication used for preventing organ rejection. Coordinate 152 groups medical codes related to sports-related injuries, specifically broken bones. Coordinate 141 is related to brain injuries and hearing loss due to the brain injuries. Neurofibromatosis(NF) is also related to this coordinate because it can cause tumors along the nerves in the brain. Cystic fibrosis(CF) seems to be a weak link in this group as it is only related to NF in the sense that both NF and CF are genetically inherited. Coordinate 184 clearly represents medical codes related to epilepsy. Epilepsy is often accompanied by convulsions, which can cause joint pain. Cerebral artery occlusion is related epilepsy in the sense that epileptic seizures can be a manifestation of cerebral arterial occlusive diseases <ref type="bibr" target="#b10">[11]</ref>. Also, both blood in feces and the joint pain can be attributed to Henoch-Schönlein purpura, a disease primarily found in children. Coordinate 190 groups diseases that are caused by congenital chromosome anomalies, especially the autosome. Acquired hypothyroidism seems to be an outlier of this coordinate. Coordinate 199 is strongly related to congenital paralysis. Baclofen is a medication used as a muscle relaxer. Quadraplegia patients can have weakened respiratory function due to impaired abdominal muscles <ref type="bibr" target="#b14">[15]</ref>, in which case tracheostomy could be required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results</head><p>We now analyze two visit coordinates: coordinate 50 and 41. Both visit coordinates have the strongest connection to the logistic regression learned for the CRG prediction. For visit coordinate 50, the two strongest code coordinates connected to it are code coordinates 112 and 152. Then naturally, from our analysis above, we can easily see that visit coordinate 50 is strongly activated by sickle-cell disease and sports-related injuries. For visit coordinate 41, code coordinates 141 and 184 have the strongest connection. Again from the analysis above, we can directly infer that visit coordinate 41 can be seen as a patient group consisting of brain damage &amp; hearing loss patients and epilepsy patients. By repeating this process, we can find the code coordinates that are likely to strongly influence the CRG level.</p><p>However, finding the influential code coordinates for CRG level can be achieved without analyzing the visit representation if we use Eq.(5). Applying Eq.(5) to the logistic regression weight of the CRG prediction, we learned that code coordinates 190 and 199 are the two strongest influencer of the CRG level. Using the analysis from above, we can naturally conclude that patients suffering from congenital chromosome anomalies or congenital paralysis are most likely to be considered to be in severe states, which is obviously true in any clinical setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed Med2Vec, a scalable two layer neural network for learning lower dimensional representations for medical concepts. Med2Vec incorporates both code co-occurence information and visit sequence information of the EHR data which improves the accuracy of both code and visit representations. Throughout several experiments, we successfully demonstrated the superior performance of Med2Vec in two predictive tasks and provided clinical interpretation of the learned representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Structure of Med2Vec: A visit comprised of several medical codes is converted to a binary vector x t ∈ {0, 1} |C| . The binary vector is then converted to an intermediate visit representation u t . u t is concatenated with a vector of demographic information d t , and converted to the final visit representation v t , which is trained to predict its neighboring visits .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The top row and the bottom row respectively show the Recall@30 for predicting the future medical codes and the AUC for predicting the CRG class when changing different hyperparameters. The basic configuration for Med2Vec is m, n = 200, w = 1, and the training epoch set to 10. The basic configuration for all baseline models is 200 for code representation size (or hidden layer size) and training epoch also set to 10. In each column, we change one hyperparameter while fixing others to the basic configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The first figure shows the convergence behavior of all models on the CHOA dataset. The second and third figures show the relationship between the training time and the dataset size for all models respectively using the CHOA dataset and the CMS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where d is the size of the demographic information vector, to the intermediate visit representation u t and create the final visit representation</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of CHOA and CMS dataset.</figDesc><table><row><cell>Dataset</cell><cell>CHOA</cell><cell>CMS</cell></row><row><cell># of patients</cell><cell>550,339</cell><cell>831,210</cell></row><row><cell># of visits</cell><cell cols="2">3,359,240 5,464,950</cell></row><row><cell>Avg. # of visits per patient</cell><cell>6.1</cell><cell>6.57</cell></row><row><cell># of unique medical codes</cell><cell>28,840</cell><cell>21,033</cell></row><row><cell>-# of unique diagnosis codes</cell><cell>10,414</cell><cell>14,111</cell></row><row><cell>-# of unique medication codes</cell><cell>12,892</cell><cell>N/A</cell></row><row><cell>-# of unique procedure codes</cell><cell>5,534</cell><cell>6,922</cell></row><row><cell>Avg. # of codes per visit</cell><cell>7.88</cell><cell>3.19</cell></row><row><cell>Max # of codes per visit</cell><cell>440</cell><cell>44</cell></row><row><cell>(95%, 99%) percentile # of codes per visit</cell><cell>(22, 53)</cell><cell>(9, 13)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average score of the medical codes from the relatedness test. 2 was assigned for related, 1 for possible and 0 for unrelated</figDesc><table><row><cell cols="4">Average Diagnosis Medication Procedure</cell></row><row><cell>1.34</cell><cell>1.59</cell><cell>0.95</cell><cell>1.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Clustering NMI of the diagnosis, medication and procedure code representations of various models. All models learned 200 dimensional code vectors. All models except SVD were trained for 10 epochs.</figDesc><table><row><cell>Model</cell><cell cols="3">Diagnosis Medication Procedure</cell></row><row><cell>SVD (σV )</cell><cell>0.1824</cell><cell>0.0843</cell><cell>0.1781</cell></row><row><cell>Skip-gram</cell><cell>0.2251</cell><cell>0.1216</cell><cell>0.2432</cell></row><row><cell>GloVe</cell><cell>0.4205</cell><cell>0.2163</cell><cell>0.3499</cell></row><row><cell>Med2Vec</cell><cell>0.2328</cell><cell>0.1089</cell><cell>0.21</cell></row></table><note>training epochs. Additionally for Med2Vec, we vary the size of the visit representations n, and the size of the visit context window w.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of two Med2Vec models. The top row was trained with the grouped code as mentioned in section 4.4. The bottom row was trained without using the groupers. Both models were trained for 10 epochs with m, n = 200, w = 1.</figDesc><table><row><cell>Model</cell><cell cols="2">Future code prediction CRG prediction</cell></row><row><cell>Grouped codes</cell><cell>0.7605</cell><cell>0.9150</cell></row><row><cell>Exact codes</cell><cell>0.7574</cell><cell>0.9155</cell></row><row><cell cols="3">to the existing groupers alone does not directly transfer to good visit representations.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Medical codes with the strongest value in six different coordinates of the 200 dimensional code embedding space. We choose ten medical codes per coordinate. Shortened descriptions of diagnosis codes are compensated by their ICD9 codes. Medications and procedures are appended with (R) and (P) respectively.</figDesc><table><row><cell>Coordinate 112</cell><cell>Coordinate 152</cell><cell>Coordinate 141</cell></row><row><cell>Kidney replaced by transplant (V42.0) Hb-SS disease without crisis (282.61) Heart replaced by transplant (V42.1) RBC antibody screening (P) Complications of transplanted bone marrow (996.85) Sickle-cell disease (282.60) Liver replaced by transplant (V42.7) Hb-SS disease with crisis (282.62) Prograf PO (R) Complications of transplanted heart (996.83)</cell><cell>X-ray, knee (P) X-ray, thoracolumbar (P) Accidents in public building (E849.6) Activities involving gymnastics (E005.2) Struck by objects/persons in sports (E917.0) Encounter for removal of sutures (V58.32) Struck by object in sports (E917.5) Unspecified fracture of ankle (824.8) Accidents occurring in place for recreation and sport (E849.4) Activities involving basketball (E007.6)</cell><cell>Cystic fibrosis (277.02) Intracranial injury (854.00) Persistent mental disorders (294.9) Subdural hemorrhage (432.1) Neurofibromatosis (237.71) Other conditions of brain (348.89) Conductive hearing loss (389.05) Unspecified causes of encephalitis, myelitis, encephalomyelitis (323.9) Sensorineural hearing loss (389.15) Intracerebral hemorrhage (431)</cell></row><row><cell>Coordinate 184</cell><cell>Coordinate 190</cell><cell>Coordinate 199</cell></row><row><cell></cell><cell>Down's syndrome (758.0)</cell><cell></cell></row><row><cell></cell><cell>Congenital anomalies (759.89)</cell><cell></cell></row><row><cell>Pain in joint, shoulder region (719.41)</cell><cell>Tuberous sclerosis (759.5)</cell><cell>Infantile cerebral palsy (343.9)</cell></row><row><cell>Pain in joint, lower leg (719.46)</cell><cell>Anomalies of larynx, trachea,</cell><cell>Congenital quadriplegia (343.2)</cell></row><row><cell>Pain in joint, ankle and foot (719.47)</cell><cell>and bronchus (748.3)</cell><cell>Congenital diplegia (343.0)</cell></row><row><cell>Pain in joint, multiple sites (719.49)</cell><cell>Autosomal deletions (758.39)</cell><cell>Quadriplegia (344.00)</cell></row><row><cell>Generalized convulsive epilepsy (345.10)</cell><cell>Conditions due to anomaly of</cell><cell>Congenital hemiplegia (343.1)</cell></row><row><cell>Pain in joint, upper arm (719.42)</cell><cell>unspecified chromosome (758.9)</cell><cell>Baclofen 10mg tablet (R)</cell></row><row><cell>Cerebral artery occlusion (434.91)</cell><cell>Acquired hypothyroidism (244.9)</cell><cell>Wheelchair management (P)</cell></row><row><cell>MRI, brain (780.59)</cell><cell>Conditions due to chromosome</cell><cell>Tracheostomy status (V44.0)</cell></row><row><cell>Other joint derangement (718.81)</cell><cell>anomalies (758.89)</cell><cell>Paraplegia (344.1)</cell></row><row><cell>Fecal occult blood (790.6)</cell><cell>Anomalies of spleen (759.0)</cell><cell>Baclofen 5mg/ml liquid (R)</cell></row><row><cell></cell><cell>Conditions due to autosomal</cell><cell></cell></row><row><cell></cell><cell>anomalies (758.5)</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1"> Mikolov et al. [25]  also use hierarchical softmax and negative sampling to speed up the learning process. We focus on the original simple formulation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">http://www.choa.org/ 3 https://www.cms.gov/Medicare/Quality-Initiatives-Patient-Assessment-Instruments/OASIS/DataSet. html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">for efficient computation, we preprocessed the EHR dataset so that the visit records of all patients are concatenated into a single sequence of visits.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work was supported by the National Science Foundation, award IIS-#1418511 and CCF-#1533768, Children's Healthcare of Atlanta, CDC I-SMILE project, Google Faculty Award, AWS Research Award, Microsoft Azure Research Award and UCB.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Predicting changes in hypertension control using electronic health records from a chronic disease management program</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning deep architectures for ai. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<title level="m">Representation learning: A review and new perspectives. PAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Janvin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Greedy layer-wise training of deep networks. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Theano: a cpu and gpu math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SciPy</title>
		<meeting>SciPy</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cloud-based predictive modeling system and its application to asthma readmission prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khalilia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tejedor-Sojo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Searles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<editor>AMIA. AMIA</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Doctor ai: Predicting clinical events via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05942</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Medical concept representation learning from electronic health records and its application on heart failure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03686</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning low-dimensional representations of medical concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-I</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>To be submitted to AMIA CRI</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Epileptic seizures in cerebral arterial occlusive disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cocito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Favale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Reni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stroke</title>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Medical semantic similarity with a neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Vine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sitbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bruza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lung volumes and mechanics of breathing in tetraplegics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spinal Cord</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="258" to="266" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unfolding physiological state: Mortality modelling in intensive care units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A multiplicative model for learning distributed text-based attribute representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring the application of deep learning techniques on medical text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Minarro-Giménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Marín-Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Samwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in health technology and informatics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improved musical onset detection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised patient similarity measure of heterogeneous patient records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edabollahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD Explorations</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Early detection of heart failure with varying prediction windows by structured and unstructured data in electronic health records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebadollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Daar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Defilippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Steinhubl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMBC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
