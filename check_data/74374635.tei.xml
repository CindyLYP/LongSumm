<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Regularized Framework for Sparse and Structured Neural Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
							<email>mathieu@mblondel.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University Ithaca</orgName>
								<address>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">NTT Communication Science Laboratories Kyoto</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">NTT Commmunication Science Laboratories, Kyoto, Japan. 31st Conference on Neural Information Processing Systems (NIPS 2017)</orgName>
								<address>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Regularized Framework for Sparse and Structured Neural Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a smoothed max operator. We show that the gradient of this operator defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in more interpretable attention mechanisms, that focus on entire segments or groups of an input. We derive efficient algorithms to compute the forward and backward passes of our attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing ones, we evaluate our attention mechanisms on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the standard attention mechanisms based on softmax and sparsemax.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern neural network architectures are commonly augmented with an attention mechanism, which tells the network where to look within the input in order to make the next prediction. Attentionaugmented architectures have been successfully applied to machine translation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref>, speech recognition <ref type="bibr" target="#b9">[10]</ref>, image caption generation <ref type="bibr" target="#b43">[44]</ref>, textual entailment <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b30">31]</ref>, and sentence summarization <ref type="bibr" target="#b38">[39]</ref>, to name but a few examples. At the heart of attention mechanisms is a mapping function that converts real values to probabilities, encoding the relative importance of elements in the input. For the case of sequence-to-sequence prediction, at each time step of generating the output sequence, attention probabilities are produced, conditioned on the current state of a decoder network. They are then used to aggregate an input representation (a variable-length list of vectors) into a single vector, which is relevant for the current time step. That vector is finally fed into the decoder network to produce the next element in the output sequence. This process is repeated until the end-of-sequence symbol is generated. Importantly, such architectures can be trained end-to-end using backpropagation.</p><p>Alongside empirical successes, neural attention-while not necessarily correlated with human attention-is increasingly crucial in bringing more interpretability to neural networks by helping explain how individual input elements contribute to the model's decisions. However, the most commonly used attention mechanism, softmax, yields dense attention weights: all elements in the input always make at least a small contribution to the decision. To overcome this limitation, sparsemax was recently proposed <ref type="bibr" target="#b30">[31]</ref>, using the Euclidean projection onto the simplex as a sparse alternative to r u s s i a : Attention weights produced by the proposed fusedmax, compared to softmax and sparsemax, on sentence summarization. The input sentence to be summarized (taken from <ref type="bibr" target="#b38">[39]</ref>) is along the x-axis. From top to bottom, each row shows where the attention is distributed when producing each word in the summary. All rows sum to 1, the grey background corresponds to exactly 0 (never achieved by softmax), and adjacent positions with exactly equal weight are not separated by borders. Fusedmax pays attention to contiguous segments of text with equal weight; such segments never occur with softmax and sparsemax. In addition to enhancing interpretability, we show in §4.3 that fusedmax outperforms both softmax and sparsemax on this task in terms of ROUGE scores.</p><p>softmax. Compared to softmax, sparsemax outputs more interpretable attention weights, as illustrated in <ref type="bibr" target="#b30">[31]</ref> on the task of textual entailment. The principle of parsimony, which states that simple explanations should be preferred over complex ones, is not, however, limited to sparsity: it remains open whether new attention mechanisms can be designed to benefit from more structural prior knowledge.</p><p>Our contributions. The success of sparsemax motivates us to explore new attention mechanisms that can both output sparse weights and take advantage of structural properties of the input through the use of modern sparsity-inducing penalties. To do so, we make the following contributions:</p><formula xml:id="formula_0">1)</formula><p>We propose a new general framework that builds upon a max operator, regularized with a strongly convex function. We show that this operator is differentiable, and that its gradient defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes as special cases both softmax and a slight generalization of sparsemax. ( §2)</p><p>2) We show how to incorporate the fused lasso <ref type="bibr" target="#b41">[42]</ref> in this framework, to derive a new attention mechanism, named fusedmax, which encourages the network to pay attention to contiguous segments of text when making a decision. This idea is illustrated in Figure on sentence summarization. For cases when the contiguity assumption is too strict, we show how to incorporate an OSCAR penalty <ref type="bibr" target="#b6">[7]</ref> to derive a new attention mechanism, named oscarmax, that encourages the network to pay equal attention to possibly non-contiguous groups of words. ( §3)</p><p>3) In order to use attention mechanisms defined under our framework in an autodiff toolkit, two problems must be addressed: evaluating the attention itself and computing its Jacobian. However, our attention mechanisms require solving a convex optimization problem and do not generally enjoy a simple analytical expression, unlike softmax. Computing the Jacobian of the solution of an optimization problem is called argmin/argmax differentiation and is currently an area of active research (cf. <ref type="bibr" target="#b0">[1]</ref> and references therein). One of our key algorithmic contributions is to show how to compute this Jacobian under our general framework, as well as for fused lasso and OSCAR. </p><formula xml:id="formula_1">:= sup y ≤1 y T x.</formula><p>We denote the subdifferential of a function f at y by ∂f (y). Elements of the subdifferential are called subgradients and when f is differentiable, ∂f (y) contains a single element, the gradient of f at y, denoted by ∇f (y). We denote the Jacobian of a function g :</p><formula xml:id="formula_2">R d → R d at y by J g (y) ∈ R d×d</formula><p>and the Hessian of a function f :  </p><formula xml:id="formula_3">R d → R at y by H f (y) ∈ R d×d .</formula><formula xml:id="formula_4">x i = sup y∈∆ d y T x.</formula><p>The equality on the r.h.s comes from the fact that the supremum of a linear form over the simplex is always achieved at one of the vertices, i.e., one of the standard basis vectors</p><formula xml:id="formula_5">{e i } d i=1 .</formula><p>Moreover, it is not hard to check that any solution y ⋆ of that supremum is precisely a subgradient of max(x):</p><formula xml:id="formula_6">∂ max(x) = {e i ⋆ : i ⋆ ∈ arg max i∈[d] x i }.</formula><p>We can see these subgradients as a mapping Π :</p><formula xml:id="formula_7">R d → ∆ d</formula><p>that puts all the probability mass onto a single element: Π(x) = e i for any e i ∈ ∂ max(x). However, this behavior is undesirable, as the resulting mapping is a discontinuous function (a Heaviside step function when x = [t, 0]), which is not amenable to optimization by gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A regularized max operator and its gradient mapping</head><p>These shortcomings encourage us to consider a regularization of the maximum operator. Inspired by the seminal work of Nesterov <ref type="bibr" target="#b34">[35]</ref>, we apply a smoothing technique. The conjugate of max(x) is</p><formula xml:id="formula_8">max * (y) = 0, if y ∈ ∆ d ∞, o.w.</formula><p>.</p><p>For a proof, see for instance <ref type="bibr" target="#b32">[33,</ref><ref type="bibr">Appendix B]</ref>. We now add regularization to the conjugate</p><formula xml:id="formula_9">max * Ω (y) := γΩ(y), if y ∈ ∆ d ∞, o.w. ,</formula><p>where we assume that Ω : R d → R is β-strongly convex w.r.t. some norm • and γ &gt; 0 controls the regularization strength. To define a smoothed max operator, we take the conjugate once again</p><formula xml:id="formula_10">max Ω (x) = max * * Ω (x) = sup y∈R d y T x − max * Ω (y) = sup y∈∆ d y T x − γΩ(y).<label>(1)</label></formula><p>Our main proposal is a mapping Π Ω :</p><formula xml:id="formula_11">R d → ∆ d , defined</formula><p>as the argument that achieves this supremum.</p><p>Π Ω (x) := arg max</p><formula xml:id="formula_12">y∈∆ d y T x − γΩ(y) = ∇max Ω (x)</formula><p>The r.h.s. holds by combining that i) max</p><formula xml:id="formula_13">Ω (x) = (y ⋆ ) T x − max * Ω (y ⋆ ) ⇔ y ⋆ ∈ ∂max Ω (x) and ii) ∂max Ω (x) = {∇max Ω (x)}, since (1)</formula><p>has a unique solution. Therefore, Π Ω is a gradient mapping. We illustrate max Ω and Π Ω for various choices of Ω in <ref type="figure">Figure 2</ref> (2-d) and in Appendix C.1 (3-d).</p><p>Importance of strong convexity. Our β-strong convexity assumption on Ω plays a crucial role and should not be underestimated. Recall that a function f : . This is sufficient to ensure that max Ω is 1 γβ -smooth, or, in other words, that it is differentiable everywhere and its gradient, Π Ω , is <ref type="bibr" target="#b0">1</ref> γβ -Lipschitz continuous w.r.t. • * . Training by backpropagation. In order to use Π Ω in a neural network trained by backpropagation, two problems must be addressed for any regularizer Ω. The first is the forward computation: how to evaluate Π Ω (x), i.e., how to solve the optimization problem in <ref type="bibr" target="#b0">(1)</ref>. The second is the backward computation: how to evaluate the Jacobian of Π Ω (x), or, equivalently, the Hessian of max Ω (x). One of our key contributions, presented in §3, is to show how to solve these two problems for general differentiable Ω, as well as for two structured regularizers: fused lasso and OSCAR.</p><formula xml:id="formula_14">R d → R is β-strongly convex w.r.t.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Recovering softmax and sparsemax as special cases</head><p>Before deriving new attention mechanisms using our framework, we now show how we can recover softmax and sparsemax, using a specific regularizer Ω.</p><p>Softmax. We choose Ω(y) = d i=1 y i log y i , the negative entropy. The conjugate of the negative entropy restricted to the simplex is the log sum exp [9, Example 3.25]. Moreover, if f (x) = γg(x) for γ &gt; 0, then f * (y) = γg * (y/γ). We therefore get a closed-form expression: max Ω (x) = γ log sum exp(x/γ) := γ log d i=1 e xi/γ . Since the negative entropy is 1-strongly convex w.r.t.</p><formula xml:id="formula_15">• 1 over ∆ d , we get that max Ω is 1 γ -smooth w.r.t. • ∞ .</formula><p>We obtain the classical softmax, with temperature parameter γ, by taking the gradient of max Ω (x),</p><formula xml:id="formula_16">Π Ω (x) = e x/γ d i=1 e xi/γ ,<label>(softmax)</label></formula><p>where e x/γ is evaluated element-wise. Note that some authors also call max Ω a "soft max." Although Π Ω is really a soft arg max, we opt to follow the more popular terminology. When x = [t, 0], it can be checked that max Ω (x) reduces to the softplus <ref type="bibr" target="#b15">[16]</ref> and Π Ω (x) 1 to a sigmoid.</p><p>Sparsemax. We choose Ω(y) = 1 2 y 2 2 , also known as Moreau-Yosida regularization in proximal operator theory <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. Since <ref type="bibr" target="#b0">1</ref> 2 y 2 is 1-strongly convex w.r.t. • 2 , we get that max Ω is 1 γ -smooth w.r.t. • 2 . In addition, it is easy to verify that</p><formula xml:id="formula_17">Π Ω (x) = P ∆ d (x/γ) = arg min y∈∆ d y − x/γ 2 .<label>(sparsemax)</label></formula><p>This mapping was introduced as is in <ref type="bibr" target="#b30">[31]</ref> with γ = 1 and was named sparsemax, due to the fact that it is a sparse alternative to softmax. Our derivation thus gives us a slight generalization, where γ controls the sparsity (the smaller, the sparser) and could be tuned; in our experiments, however, we follow the literature and set γ = 1. The Euclidean projection onto the simplex, P ∆ d , can be computed exactly <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15]</ref> (we discuss the complexity in Appendix B). Following <ref type="bibr" target="#b30">[31]</ref>, the Jacobian of Π Ω is</p><formula xml:id="formula_18">J ΠΩ (x) = 1 γ J P ∆ d (x/γ) = 1 γ diag(s) − ss T / s 1 ,</formula><p>where s ∈ {0 Before tackling more structured regularizers, we address in this section the case of general differentiable regularizer Ω. Because Π Ω (x) involves maximizing (1), a concave function over the simplex, it can be computed globally using any off-the-shelf projected gradient solver. Therefore, the main challenge is how to compute the Jacobian of Π Ω . This is what we address in the next proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1 Jacobian of Π Ω for any differentiable Ω (backward computation)</head><p>Assume that Ω is differentiable over ∆ d and that Π Ω (x) = arg max y∈∆ d y T x − γΩ(y) = y ⋆ has been computed. Then the Jacobian of Π Ω at x, denoted J ΠΩ , can be obtained by solving the system (I + A(B − I)) J ΠΩ = A, where we defined the shorthands A :</p><formula xml:id="formula_19">= J P ∆ d (y ⋆ − γ∇Ω(y ⋆ ) + x) and B := γH Ω (y ⋆ ).</formula><p>The proof is given in Appendix A.1. Unlike recent work tackling argmin differentiation through matrix differential calculus on the Karush-Kuhn-Tucker (KKT) conditions <ref type="bibr" target="#b0">[1]</ref>, our proof technique relies on differentiating the fixed point iteration y * = P ∆ d (y ⋆ − ∇f (y ⋆ )). To compute J ΠΩ v for an arbitrary v ∈ R d , as required by backpropagation, we may directly solve (I + A(B − I)) (J ΠΩ v) = Av. We show in Appendix B how this system can be solved efficiently thanks to the structure of A.</p><p>Squared p-norms. As a useful example of a differentiable function over the simplex, we consider squared p-norms: </p><formula xml:id="formula_20">Ω(y) = 1 2 y 2 p = d i=1 y p i 2/p ,</formula><formula xml:id="formula_21">Π Ω (x) = arg min y∈∆ d γ y 2 p − y T x. (sq-pnorm-max)</formula><p>The gradient and Hessian needed for Proposition 1 can be computed by</p><formula xml:id="formula_22">∇Ω(y) = y p−1 y p−2 p and H Ω (y) = diag(d) + uu T , where d = (p − 1) y p−2 p y p−2 and u = (2 − p) y 2p−2 p y p−1 ,</formula><p>with the exponentiation performed element-wise. sq-pnorm-max recovers sparsemax with p = 2 and, like sparsemax, encourages sparse outputs. However, as can be seen in the zoomed box in <ref type="figure">Figure 2</ref> (right), the transition between y ⋆ = [0, 1] and y ⋆ = [1, 0] can be smoother when 1 &lt; p &lt; 2.</p><p>Throughout our experiments, we use p = 1.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structured regularizers: fused lasso and OSCAR</head><p>Fusedmax. For cases when the input is sequential and the order is meaningful, as is the case for many natural languages, we propose fusedmax, an attention mechanism based on fused lasso <ref type="bibr" target="#b41">[42]</ref>, also known as 1-d total variation (TV). Fusedmax encourages paying attention to contiguous segments, with equal weights within each one. It is expressed under our framework by choosing Ω(y) = 1 2 y</p><formula xml:id="formula_23">2 2 + λ d−1 i=1</formula><p>|y i+1 − y i |, i.e., the sum of a strongly convex term and of a 1-d TV penalty. It is easy to verify that this choice yields the mapping</p><formula xml:id="formula_24">Π Ω (x) = arg min y∈∆ d 1 y − x/γ 2 + λ d−1 i=1 |y i+1 − y i |. (fusedmax)</formula><p>Oscarmax. For situations where the contiguity assumption may be too strict, we propose oscarmax, based on the OSCAR penalty <ref type="bibr" target="#b6">[7]</ref>, to encourage attention weights to merge into clusters with the same value, regardless of position in the sequence. This is accomplished by replacing the 1-d TV penalty in fusedmax with an ∞-norm penalty on each pair of attention weights, i.e., Ω(y) = 1 y 2 2 + λ i&lt;j max(|y i |, |y j |). This results in the mapping</p><formula xml:id="formula_25">Π Ω (x) = arg min y∈∆ d 1 y − x/γ 2 + λ i&lt;j max(|y i |, |y j |).<label>(oscarmax)</label></formula><p>Forward computation. Due to the y ∈ ∆ d constraint, computing fusedmax/oscarmax does not seem trivial on first sight. The next proposition shows how to do so, without any iterative method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 2 Computing fusedmax and oscarmax (forward computation)</head><p>fusedmax: Π Ω (x) = P ∆ d (P TV (x/γ)) , P TV (x) := arg min</p><formula xml:id="formula_26">y∈R d 1 y − x 2 + λ d−1 i=1 |y i+1 − y i |.</formula><p>oscarmax: Π Ω (x) = P ∆ d (P OSC (x/γ)) , P OSC (x) := arg min</p><formula xml:id="formula_27">y∈R d 1 y − x 2 + λ i&lt;j max(|y i |, |y j |).</formula><p>Here, P TV and P OSC indicate the proximal operators of 1-d TV and OSCAR, and can be computed exactly by <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b46">[47]</ref>, respectively. To remind the reader, P ∆ d denotes the Euclidean projection onto the simplex and can be computed exactly using <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b14">15]</ref>. Proposition 2 shows that we can compute fusedmax and oscarmax using the composition of two functions, for which exact noniterative algorithms exist. This is a surprising result, since the proximal operator of the sum of two functions is not, in general, the composition of the proximal operators of each function. The proof follows by showing that the indicator function of ∆ d satisfies the conditions of <ref type="bibr" target="#b44">[45,</ref><ref type="bibr">Corollaries 4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Groups induced by P TV and P OSC . Let z ⋆ be the optimal solution of P TV (x) or P OSC (x). For P TV , we denote the group of adjacent elements with the same value as</p><formula xml:id="formula_28">z ⋆ i by G ⋆ i , ∀i ∈ [d]. Formally, G ⋆ i = [a, b] ∩ N with a ≤ i ≤ b</formula><p>where a and b are the minimal and maximal indices such that z ⋆ i = z ⋆ j for all j ∈ G ⋆ i . For P OSC , we define G ⋆ i as the indices of elements with the same absolute value as z ⋆ i , more formally</p><formula xml:id="formula_29">G ⋆ i = {j ∈ [d] : |z ⋆ i | = |z ⋆ j |}. Because P ∆ d (z ⋆ ) = max(z ⋆ − θ, 0)</formula><p>for some θ ∈ R, fusedmax/oscarmax either shift a group's common value or set all its elements to zero.</p><p>λ controls the trade-off between no fusion (sparsemax) and all elements fused into a single trivial group. While tuning λ may improve performance, we observe that λ = 0.1 (fusedmax) and λ = 0.01 (oscarmax) are sensible defaults that work well across all tasks and report all our results using them.</p><p>Backward computation. We already know that the Jacobian of P ∆ d is the same as that of sparsemax with γ = 1. Then, by Proposition 2, if we know how to compute the Jacobians of P TV and P OSC , we can obtain the Jacobians of fusedmax and oscarmax by straightforward application of the chain rule. However, although P TV and P OSC can be computed exactly, they lack analytical expressions. We next show that we can nonetheless compute their Jacobians efficiently, without needing to solve a system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 3 Jacobians of P TV and P OSC (backward computation)</head><p>Assume z ⋆ = P TV (x) or P OSC (x) has been computed. Define the groups derived from z ⋆ as above.</p><formula xml:id="formula_30">Then, [J PTV (x)] i,j = 1 |G ⋆ i | if j ∈ G ⋆ i , 0 o.w. and [J POSC (x)] i,j = sign(z ⋆ i z ⋆ j ) |G ⋆ i | if j ∈ G ⋆ i and z ⋆ i = 0, 0 o.w. .</formula><p>The proof is given in Appendix A.2. Clearly, the structure of these Jacobians permits efficient Jacobian-vector products; we discuss the computational complexity and implementation details in Appendix B. Note that P TV and P OSC are differentiable everywhere except at points where groups change. For these points, the same remark as for sparsemax applies, and we can use Clarke's Jacobian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>We showcase the performance of our attention mechanisms on three challenging natural language tasks: textual entailment, machine translation, and sentence summarization. We rely on available, well-established neural architectures, so as to demonstrate simple drop-in replacement of softmax with structured sparse attention; quite likely, newer task-specific models could lead to further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Textual entailment (a.k.a. natural language inference) experiments</head><p>Textual entailment is the task of deciding, given a text T and an hypothesis H, whether a human reading T is likely to infer that H is true <ref type="bibr" target="#b13">[14]</ref>. We use the Stanford Natural Language Inference (SNLI) dataset <ref type="bibr" target="#b7">[8]</ref>, a collection of 570,000 English sentence pairs. Each pair consists of a sentence and an hypothesis, manually labeled with one of the labels ENTAILMENT, CONTRADICTION, or NEUTRAL. We use a variant of the neural attention-based classifier proposed for this dataset by <ref type="bibr" target="#b37">[38]</ref> and follow the same methodology as <ref type="bibr" target="#b30">[31]</ref> in terms of implementation, hyperparameters, and grid search. We employ the CPU implementation provided in <ref type="bibr" target="#b30">[31]</ref> and simply replace sparsemax with fusedmax/oscarmax; we observe that training time per epoch is essentially the same for each of the four attention mechanisms (timings and more experimental details in Appendix C.2). <ref type="table" target="#tab_5">Table 1</ref> shows that, for this task, fusedmax reaches the highest accuracy, and oscarmax slightly outperforms softmax. Furthermore, fusedmax results in the most interpretable feature groupings: <ref type="figure" target="#fig_2">Figure 3</ref> shows the weights of the neural network's attention to the text, when considering the hypothesis "No one is dancing." In this case, all four models correctly predicted that the text "A band is playing on stage at a concert and the attendants are dancing to the music," denoted along the x-axis, contradicts the hypothesis, although the attention weights differ. Notably, fusedmax identifies the meaningful segment "band is playing".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Machine translation experiments</head><p>Sequence-to-sequence neural machine translation (NMT) has recently become a strong contender in machine translation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref>. In NMT, attention weights can be seen as an alignment between source and translated words. To demonstrate the potential of our new attention mechanisms for NMT, we ran experiments on 10 language pairs. We build on OpenNMT-py <ref type="bibr" target="#b23">[24]</ref>, based on PyTorch <ref type="bibr" target="#b36">[37]</ref>, with all default hyperparameters (detailed in Appendix C.3), simply replacing softmax with the proposed Π Ω .</p><p>OpenNMT-py with softmax attention is optimized for the GPU. Since sparsemax, fusedmax, and oscarmax rely on sorting operations, we implement their computations on the CPU for simplicity, keeping the rest of the pipeline on the GPU. However, we observe that, even with this context switching, the number of tokens processed per second was within 3 /4 of the softmax pipeline. For sq-pnorm-max, we observe that the projected gradient solver used in the forward pass, unlike the linear system solver used in the backward pass, could become a computational bottleneck. To mitigate this effect, we set the tolerance of the solver's stopping criterion to 10 −2 .</p><p>Quantitatively, we find that all compared attention mechanisms are always within 1 BLEU score point of the best mechanism (for detailed results, cf. Appendix C.3). This suggests that structured sparsity does not restrict accuracy. However, as illustrated in <ref type="figure" target="#fig_1">Figure 4</ref>, fusedmax and oscarmax often lead to more interpretable attention alignments, as well as to qualitatively different translations.  <ref type="figure" target="#fig_0">Figure 1</ref>. Within a row, weights grouped by oscarmax under the same cluster are denoted by "•". Here, oscarmax finds a slightly more natural English translation. More visulizations are given in Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentence summarization experiments</head><p>Attention mechanisms were recently explored for sentence summarization in <ref type="bibr" target="#b38">[39]</ref>. To generate sentence-summary pairs at low cost, the authors proposed to use the title of a news article as a noisy summary of the article's leading sentence. They collected 4 million such pairs from the Gigaword dataset and showed that this seemingly simplistic approach leads to models that generalize Our evaluation follows <ref type="bibr" target="#b38">[39]</ref>: we use the standard DUC 2004 dataset (500 news articles each paired with 4 different human-generated summaries) and a randomly held-out subset of Gigaword, released by <ref type="bibr" target="#b38">[39]</ref>. We report results on ROUGE-1, ROUGE-2, and ROUGE-L. Our results, in <ref type="table" target="#tab_6">Table 2</ref>, indicate that fusedmax is the best under nearly all metrics, always outperforming softmax. In addition to <ref type="figure" target="#fig_0">Figure 1</ref>, we exemplify our enhanced interpretability and provide more detailed results in Appendix C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Smoothed max operators. Replacing the max operator by a differentiable approximation based on the log sum exp has been exploited in numerous works. Regularizing the max operator with a squared 2-norm is less frequent, but has been used to obtain a smoothed multiclass hinge loss <ref type="bibr" target="#b40">[41]</ref> or smoothed linear programming relaxations for maximum a-posteriori inference <ref type="bibr" target="#b32">[33]</ref>. Our work differs from these in mainly two aspects. First, we are less interested in the max operator itself than in its gradient, which we use as a mapping from R d to ∆ d . Second, since we use this mapping in neural networks trained with backpropagation, we study and compute the mapping's Jacobian (the Hessian of a regularized max operator), in contrast with previous works.</p><p>Interpretability, structure and sparsity in neural networks. Providing interpretations alongside predictions is important for accountability, error analysis and exploratory analysis, among other reasons. Toward this goal, several recent works have been relying on visualizing hidden layer activations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref> and the potential for interpretability provided by attention mechanisms has been noted in multiple works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. Our work aims to fulfill this potential by providing a unified framework upon which new interpretable attention mechanisms can be designed, using well-studied tools from the field of structured sparse regularization.</p><p>Selecting contiguous text segments for model interpretations is explored in <ref type="bibr" target="#b25">[26]</ref>, where an explanation generator network is proposed for justifying predictions using a fused lasso penalty. However, this network is not an attention mechanism and has its own parameters to be learned. Furthemore, <ref type="bibr" target="#b25">[26]</ref> sidesteps the need to backpropagate through the fused lasso, unlike our work, by using a stochastic training approach. In constrast, our attention mechanisms are deterministic and drop-in replacements for existing ones. As a consequence, our mechanisms can be coupled with recent research that builds on top of softmax attention, for example in order to incorporate soft prior knowledge about NMT alignment into attention through penalties on the attention weights <ref type="bibr" target="#b11">[12]</ref>.</p><p>A different approach to incorporating structure into attention uses the posterior marginal probabilities from a conditional random field as attention weights <ref type="bibr" target="#b22">[23]</ref>. While this approach takes into account structural correlations, the marginal probabilities are generally dense and different from each other. Our proposed mechanisms produce sparse and clustered attention weights, a visible benefit in interpretability. The idea of deriving constrained alternatives to softmax has been independently explored for differentiable easy-first decoding <ref type="bibr" target="#b31">[32]</ref>. Finally, sparsity-inducing penalties have been used to obtain convex relaxations of neural networks <ref type="bibr" target="#b4">[5]</ref> or to compress models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b39">40]</ref>. These works differ from ours, in that sparsity is enforced on the network parameters, while our approach can produce sparse and structured outputs from neural attention layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future directions</head><p>We proposed in this paper a unified regularized framework upon which new attention mechanisms can be designed. To enable such mechanisms to be used in a neural network trained by backpropagation, we demonstrated how to carry out forward and backward computations for general differentiable regularizers. We further developed two new structured attention mechanisms, fusedmax and oscarmax, and demonstrated that they enhance interpretability while achieving comparable or better accuracy on three diverse and challenging tasks: textual entailment, machine translation, and summarization.</p><p>The usefulness of a differentiable mapping from real values to the simplex or to [0, 1] with sparse or structured outputs goes beyond attention mechanisms. We expect that our framework will be useful to sample from categorical distributions using the Gumbel trick <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>, as well as for conditional computation <ref type="bibr" target="#b5">[6]</ref> or differentiable neural computers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. We plan to explore these in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>n d e f e n s e m i n i s t e r i v a n o v c a l l e d s u n d a y f o r t h e c r e a t i o n o f a j o i n t f r o n t f o r c o m b a t i n g g l o b a l t e r r o r i ss s i a n d e f e n s e m i n i s t e r i v a n o v c a l l e d s u n d a y f o r t h e c r e a t i o n o f a j o i n t f r o n t f o r c o m b a t i n g g l o b a l t e r r o r i s m . softmax r u s s i a n d e f e n s e m i n i s t e r i v a n o v c a l l e d s u n d a y f o r t h e c r e a t i o n o f a j o i n t f r o n t f o r c o m b a t i n g g l o b a l t e r r o r i s m . sparsemax Figure 1</head><label>1</label><figDesc>Figure 1: Attention weights produced by the proposed fusedmax, compared to softmax and sparsemax, on sentence summarization. The input sentence to be summarized (taken from [39]) is along the x-axis. From top to bottom, each row shows where the attention is distributed when producing each word in the summary. All rows sum to 1, the grey background corresponds to exactly 0 (never achieved by softmax), and adjacent positions with exactly equal weight are not separated by borders. Fusedmax pays attention to contiguous segments of text with equal weight; such segments never occur with softmax and sparsemax. In addition to enhancing interpretability, we show in §4.3 that fusedmax outperforms both softmax and sparsemax on this task in terms of ROUGE scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>AFigure 3 :</head><label>3</label><figDesc>band is playing on stage ata concert and the attendants are dancing to the music. playing on stage ata concert and the attendants are dancing to the music. playing on stage ata concert and the attendants are dancing to the music. playing on stage ata concert and the attendants are dancing to the music. Attention weights when considering the contradicted hypothesis "No one is dancing."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Attention weights for French to English translation, using the conventions of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We denote the set {1, . . . , d} by[d]. We denote the (d − 1)-dimensional probability simplex by ∆ d := {x ∈ R d : x 1 = 1, x ≥ 0} and the Euclidean projection onto it by P ∆ d (x) := arg min y∈∆ d y − x 2 . Given a function f : R d → R ∪ {∞}, its convex conjugate is defined by f</figDesc><table><row><cell>( §3)</cell></row><row><cell>4) To showcase the potential of our new attention mechanisms as a drop-in replacement for existing</cell></row><row><cell>ones, we show empirically that our new attention mechanisms enhance interpretability while achieving</cell></row><row><cell>comparable or better accuracy on three diverse and challenging tasks: textual entailment, machine</cell></row><row><cell>translation, and sentence summarization. ( §4)</cell></row><row><cell>Notation.</cell></row></table><note>* (x) := sup y∈dom f y T x−f (y). Given a norm • , its dual is defined by x *</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>a norm • if and only if its conjugate f * is 1 β -smooth w.r.t. the dual norm • * [46, Corollary 3.5.11] [22, Theorem 3]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, 1} d indicates the nonzero elements of Π Ω (x). Since Π Ω is Lipschitz continuous, Rademacher's theorem implies that Π Ω is differentiable almost everywhere. For points where Π Ω is not differentiable (where max Ω is not twice differentiable), we can take an arbitrary matrix in the set of Clarke's generalized Jacobians<ref type="bibr" target="#b10">[11]</ref>, the convex hull of Jacobians of the form lim</figDesc><table /><note>xt→x J ΠΩ (x t ) [31].3 Deriving new sparse and structured attention mechanisms3.1 Differentiable regularizer Ω</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Textual entailment</cell></row><row><cell cols="2">test accuracy on SNLI [8].</cell></row><row><cell cols="2">attention accuracy</cell></row><row><cell>softmax</cell><cell>81.66</cell></row><row><cell>sparsemax</cell><cell>82.39</cell></row><row><cell>fusedmax</cell><cell>82.41</cell></row><row><cell>oscarmax</cell><cell>81.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Sentence summarization results, following the same experimental setting as in<ref type="bibr" target="#b38">[39]</ref>. We follow their experimental setup and are able to reproduce comparable results to theirs with OpenNMT when using softmax attention. The models we use are the same as in §4.2.</figDesc><table><row><cell></cell><cell></cell><cell>DUC 2004</cell><cell></cell><cell></cell><cell>Gigaword</cell><cell></cell></row><row><cell cols="7">attention ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L</cell></row><row><cell>softmax</cell><cell>27.16</cell><cell>9.48</cell><cell>24.47</cell><cell>35.13</cell><cell>17.15</cell><cell>32.92</cell></row><row><cell>sparsemax</cell><cell>27.69</cell><cell>9.55</cell><cell>24.96</cell><cell>36.04</cell><cell>17.78</cell><cell>33.64</cell></row><row><cell>fusedmax</cell><cell>28.42</cell><cell>9.96</cell><cell>25.55</cell><cell>36.09</cell><cell>17.62</cell><cell>33.69</cell></row><row><cell>oscarmax</cell><cell>27.84</cell><cell>9.46</cell><cell>25.14</cell><cell>35.36</cell><cell>17.23</cell><cell>33.03</cell></row><row><cell>sq-pnorm-max</cell><cell>27.94</cell><cell>9.28</cell><cell>25.08</cell><cell>35.94</cell><cell>17.75</cell><cell>33.66</cell></row><row><cell>surprisingly well.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to André Martins, Takuma Otsuka, Fabian Pedregosa, Antoine Rolet, Jun Suzuki, and Justine Zhang for helpful discussions. We thank the anonymous reviewers for the valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">OptNet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Carlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Lieb</surname></persName>
		</author>
		<idno type="DOI">https://link.springer.com/article/10.1007/BF01231769</idno>
		<title level="m">Sharp uniform convexity and smoothness inequalities for trace norms. Inventiones Mathematicae</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="463" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
		<idno type="DOI">http://epubs.siam.org/doi/abs/10.1137/080716542</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convex neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marcotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Bondell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Reich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="123" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Optimization and nonsmooth analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">http://epubs.siam.org/doi/book/10.1137/1.9781611971309</idno>
		<imprint>
			<date type="published" when="1990" />
			<pubPlace>SIAM</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D V</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A direct algorithm for 1-d total variation denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Condat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1054" to="1057" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recognizing textual entailment: Rational, evaluation and approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<editor>i-xvii</editor>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient projections onto the ℓ 1 -ball for learning in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incorporating second-order functional knowledge for better option pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bélisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pathwise coordinate optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Höfling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="302" to="332" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural Turing Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep sequential and structural neural models of compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Regularization techniques for learning with matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1865" to="1890" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">OpenNMT: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visualizing and understanding neural models in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICCVPR</title>
		<meeting>of ICCVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning what&apos;s easy: Fully differentiable neural easy-first taggers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kreutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Smooth and strong: MAP inference with linear convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Meshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A finite algorithm for finding the projection of a point onto the canonical simplex of R n</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michelot</surname></persName>
		</author>
		<idno type="DOI">https://link.springer.com/article/10.1007/BF00938486</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Smooth minimization of non-smooth functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<idno type="DOI">https://link.springer.com/article/10.1007/s10107-004-0552-5</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Proximal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Optimization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="239" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="http://pytorch.org" />
		<title level="m">PyTorch</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Group sparse regularization for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comminiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="page" from="81" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="145" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sparsity and smoothness via the fused lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="108" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On decomposing the proximal map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Convex analysis in general vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zalinescu</surname></persName>
		</author>
		<idno type="DOI">http://www.worldscientific.com/worldscibooks/10.1142/5021</idno>
		<imprint>
			<date type="published" when="2002" />
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Solving OSCAR regularization problems by fast approximate proximal splitting algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="124" to="135" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The ordered weighted ℓ 1 norm: Atomic formulation, dual norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Mario</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>and projections. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient sparse modeling with automatic feature grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1436" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
