<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HIERARCHICAL RL USING AN ENSEMBLE OF PROPRIOCEPTIVE PERIODIC POLICIES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
							<email>kdmarino@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<email>abhinavg@cs.cmu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
							<email>fergus@cs.nyu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
							<email>aszlam@fb.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HIERARCHICAL RL USING AN ENSEMBLE OF PROPRIOCEPTIVE PERIODIC POLICIES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>In this work we introduce a simple, robust approach to hierarchically training an agent in the setting of sparse reward tasks. The agent is split into a low-level and a high-level policy. The low-level policy only accesses internal, proprioceptive dimensions of the state observation. The low-level policies are trained with a simple reward that encourages changing the values of the non-proprioceptive dimensions. Furthermore, it is induced to be periodic with the use a &quot;phase function.&quot; The high-level policy is trained using a sparse, task-dependent reward, and operates by choosing which of the low-level policies to run at any given time. Using this approach, we solve difficult maze and navigation tasks with sparse rewards using the Mujoco Ant and Humanoid agents and show improvement over recent hierarchical methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The notion of hierarchy is fundamental to AI systems. Effective approaches to perceptual tasks such as vision or audio rely on hierarchical neural network architectures (e.g. convnets and wavenets). Hierarchical methods are also a natural approach to solving reinforcement learning problems where reward is sparse. Consider an environment where a legged robot has to navigate a maze through winding paths to one of several possible goals. The sequence of actions (at the level of joint controls) required for this agent to ever see a reward is too long and complex for na√Øve reinforcement learning methods to find. Hierarchical controllers provide an intuitive solution to this problem: first learn a low-level controller that is able to handle low-level tasks such as locomotion and balance, and then learn a high-level controller that controls the inputs to that low-level controller and handles the high-level planning tasks (e.g. which path to take in the maze).</p><p>While conceptually straightforward, hierarchical approaches have been difficult to realize in practice for several reasons: (i) the lack of constraints on intermediate representations; (ii) the difficulty of learning a high-level controller when the low-level policies shifts, and (iii) the continued problem of sample-inefficiency when training both low-level and high-level controller jointly with sparse rewards. While there has been some progress recently, there is still no general hierarchical reinforcement learning method that is robust across domains, easy to implement, and effective.</p><p>The approach in this work avoids these problems by simplifying the high-level optimization problem to choosing from a set of independently trained low-level controllers. The problem then becomes how to train a variety of controllers which are generally useful for an agent acting in the world and are sufficiently diverse so that the high-level controller will always have access to the building blocks it needs to solve the high-level task.</p><p>The basic method of learning our low-level skills is as follows. First, as with <ref type="bibr" target="#b11">Heess et al. (2016)</ref>, we factorize the state space into components that describe the internal and external properties of the agent. The former carries information needed for proprioception (e.g. internal joint angles) and is exclusively used by the low-level policy and the latter is used exclusively by the high-level policy. Second, we provide a weak supervisory signal to the low-level policies that rewards them for changes in the external part of the space. We do not specify how; the low-level agent is rewarded for any change in the perceptual features. The intuition is that a policy is useful to an agent if following it changes the external world or the agent's place in it, (e.g. by changing position or moving objects). Finally, we build into the agent a notion of repeated cycles of action. Biological systems are known to possess central pattern generators (CPGs) that produce rhythmic outputs, assisting with perception, locomotion, and self-regulation <ref type="bibr" target="#b15">(Marder &amp; Bucher, 2001</ref>). Taking inspiration from this, we give our agent access to a "phase function" and reward it for acting in consistent, cyclic ways.</p><p>Using this method of training, we train a number of low-level controllers on Mujoco locomotion tasks and are able to learn a diverse and effective set of low-level policies. We then show that these policies are easily controllable by our high-level controller and sufficient for solving the final task. We show impressive results on a variety of difficult Mujoco maze and navigation tasks with sparse rewards and demonstrate that our method compares favorably to other recent hierarchical RL approaches. Finally, we demonstrate that our method is robust to choice of RL algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>There is an extensive literature on hierarchical approaches to reinforcement learning <ref type="bibr" target="#b25">(Sutton et al., 1999;</ref><ref type="bibr" target="#b2">Dayan &amp; Hinton, 1992;</ref><ref type="bibr" target="#b3">Dietterich, 1999)</ref>. <ref type="bibr">Options (Sutton et al., 1999)</ref> are perhaps the most standard framing for hierarchical RL, formalizing the notion of a subroutine in an MDP. Our lowlevel networks are options with a deterministic return condition. While earlier works used prespecified subroutines, and focused on learning when to call them, recent works, for example <ref type="bibr" target="#b0">Bacon et al. (2017)</ref>; <ref type="bibr" target="#b27">Vezhnevets et al. (2017)</ref>; <ref type="bibr" target="#b19">Nachum et al. (2018)</ref>, have demonstrated success in learning multiple levels of hierarchy end-to-end from only task reward.</p><p>Still, in more complex settings such as the ones discussed in this work, with high dimensional action spaces, and especially with sparse rewards, performing unsupervised pre-training, or using auxiliary tasks during learning or pre-training have led to better results <ref type="bibr" target="#b11">(Heess et al., 2016;</ref><ref type="bibr" target="#b6">Frans et al., 2017;</ref><ref type="bibr" target="#b5">Florensa et al., 2017;</ref><ref type="bibr" target="#b9">Haarnoja et al., 2018b;</ref><ref type="bibr" target="#b4">Eysenbach et al., 2018;</ref><ref type="bibr" target="#b10">Hausman et al., 2018)</ref>.</p><p>One line of work in this direction starts from a variational inference approach to intrinsic motivation and exploration <ref type="bibr" target="#b18">(Mohamed &amp; Rezende, 2015;</ref><ref type="bibr" target="#b7">Gregor et al., 2016;</ref><ref type="bibr" target="#b5">Florensa et al., 2017;</ref><ref type="bibr" target="#b4">Eysenbach et al., 2018)</ref>. They use an actor parameterized by state and a latent vector in such a way that the latent vector is predictable from a final state or a sequence of states the actor visits, but otherwise, the actions have high entropy. The latter of these works use this approach for designing hierarchical policies.</p><p>In <ref type="bibr" target="#b5">Florensa et al. (2017)</ref>, they use a pre-training task, and the prediction cost is an auxiliary reward. The main differences from our work is that we do not try to use stochastic neural networks, and we do not try to compress the one-hot representation of the low level networks into a dense vector during low level training. Instead we keep them fully separate. We also do not attempt to impose any regularization to encourage the low level networks to be diverse. These can be considered simplifications; what we show is that the simple thing works quite well.</p><p>In <ref type="bibr" target="#b4">Eysenbach et al. (2018)</ref> the low-level policy is trained without using any environmental reward by pre-training using the unsupervised variational objective; they then training a high-level actor to issue commands via the latent vector. This latter work is in some senses more general than ours, in that a user is not required to separate out the proprioceptive variables from the external sensory variables. In addition, the formulation directly encourages diverse and controllable low-level policies. Nevertheless, both these works were validated mostly in the same settings we use here, where the separation between variables is natural, and the reward in equation 3.2 is well motivated. In these settings our method is simpler, and performs well in comparison (see <ref type="figure" target="#fig_5">Figure 7a</ref>).</p><p>Our work is most closely related to <ref type="bibr" target="#b11">Heess et al. (2016)</ref>. We operate under the same basic philosophy, separating the agent into a low-level component that only sees "proprioceptive" information, and works at a fine time scale, and a coarse agent that has access to "sensory" information. Like that work, we evaluate or method in the setting of locomotive agents. However, there are several differences between <ref type="bibr" target="#b11">Heess et al. (2016)</ref> and this work. There, the low-level component is trained along with a provisional high-level component to solve a set of high-level tasks designed for the agent and transfer tasks, whereas in this work, the low-level controller is trained with only the objective "change the sensory responses" in each setting. Furthermore, our underlying architecture is different. Whereas in <ref type="bibr" target="#b11">Heess et al. (2016)</ref> the high-level agent interacts with the low-level component via continuous control vectors, in this work, we sample one of N low-level policies, as in <ref type="bibr" target="#b6">Frans et al. (2017)</ref>. In addition, whereas they train their low-level actor with only the current state, our low-level actor is shown to benefit from using a pattern generator. Finally, because of the structure of our lowlevel task, we need to ensure that the low-level policies are diverse and controllable (which in <ref type="bibr" target="#b11">Heess et al. (2016)</ref> is given by the design of the pre-training task), and we do so by using multiple random instantiations of the "change the sensory response" policies that maximize equation 3.2. Our work can thus be considered in some ways a simplification and generalization of <ref type="bibr" target="#b11">Heess et al. (2016)</ref>.</p><p>The use of phase information in the low-level actor recalls Holden et al. <ref type="formula">2017</ref>; <ref type="bibr" target="#b24">Sharma &amp; Kitani (2018)</ref>. In the former, a neural network is used to control the gait of a video game character. The weights of the neural network are a function of a phase variable whose cycle corresponds to a cycle of the gait, and are trained as regressors on motion-capture data. The latter work takes this approach to RL problems that have an explicit cyclic structure, in particular the walker and hopper environments from MuJoCo. Phase has also been used in similar way in <ref type="bibr" target="#b22">Peng et al. (2018;</ref>). While we share a similar motivation with these, our low-level actor processes the phases in a different way: instead of a phase variable directly modulating the weight of the network, we concatenate an embedded phase index to the input of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>Section 3.1 describes the division of the state space into the proprioceptive or internal state and the external state. In section 3.2 we describe the reward function which we use to train the low-level policies. These policies take in the proprioceptive state and the reward maximizes displacement in the external state. Section 3.3 describes how we augment the low-level policy by adding an additional input that tells the agent where it is in a "phase cycle" and constraining the motion of the agent to match that phase cycle. Finally, in section 3.4 we describe how we learn a high-level controller by learning to choose when and which of the trained low-level policies should be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PROPRIOCEPTIVE AND EXTERNAL STATES</head><p>We divide the agent's sensory output into two sets of variables. The proprioceptive states, s p , are the measurements of the agents own body that can be directly affected by control. The external states, s e are measurements of the environment outside of the agents direct control.</p><p>The proprioceptive part of the state is fed into the low-level policy network. This is the part of the state that gives the agent the information about the configuration of its own body and actuators. The low-level policy is independent of the external state as the low-level policies are intended to be generally usefully in moving in the external state independently of the exact location.</p><p>The external part of the state is fed into the high-level policy network. The high-level policy is responsible for things like moving to particular coordinates in the world, and so the external state is necessary. The high-level policy does not need the low-level states such as joint locations as that is handled by the low-level policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LOW-LEVEL POLICY TRAINING</head><p>We would like a set of low-level policies that are diverse, effective, and controllable. Here "effective" means that they can make significant changes to the environment. "Diverse" means they make different changes, hopefully covering all of the things the agent will need to be able to do in its test task. Finally "controllable" means that the low-level policies are easy for the high-level policy to use, for example because they are predictable and their effect is independent of the external state. Empirically, we have found that simply instantiating multiple randomly initialized neural networks trained to change the external perception suffices.</p><p>Formally, our basic low-level reward at each time step is R disp (s t , a t ) = ||s e t+1 ‚àí s e t || 2 Using this reward scheme, we train a number of agents (16 in the experiments below). We use the same reward for each agent, and across environments (except regularized with the standard environment-specific control and survival rewards). As we show in Section 4.1, changing only the random seed gives us a wide variety of policies that maximize the displacement of the agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PHASE-CONDITIONED POLICIES</head><p>One important aspect of our training procedure for our low-level policies is time index or phase inputs. We expect good movement policies to be at least roughly periodic, and phase inputs allow the model to more easily achieve periodicity. Formally, we denote our phase-conditioned policy as œÄ(s p , œÜ), where s p is our proprioceptive observation and œÜ ‚àà {0, 1, . . . K ‚àí 1} is the phase index, where K is the length of the period (in all our experiments, we choose K = 10). At each time-step t, during training and evaluation, we receive our new observation s p , and phase index œÜ = t (mod K).</p><p>In particular, in our phase-conditioned networks, we take as input s p and a vector b œÜ ‚àà R d , where b œÜ is a learned parameter, like a bias term, for each possible phase indices œÜ. d = 16 in our experiments.</p><p>For our phase-conditioned policies, we also want to encourage the agents actions and internal states to be cyclic during training; at any given time step, the current action and proprioceptive state should match the one from the last cycle. Just as after one phase period, a pendulum is in the same state as it was before, during any given cycle, the behavior of the agent should be identical.</p><p>Formally, the reward maximization with a cyclic constraint can be written as:</p><formula xml:id="formula_0">argmax at R(s t , a t ) subject to ||s p t ‚àí s p t‚àíK || 2 ‚â§ œÉ s , ||a t ‚àí a t‚àíK || 2 ‚â§ œÉ a</formula><p>This can be changed to be a cyclic loss as</p><formula xml:id="formula_1">argmax at R(s t , a t ) ‚àí Œª s ||s p t ‚àí s p t‚àíK || 2 ‚àí Œª a ||a t ‚àí a t‚àíK || 2</formula><p>For the phase conditioned policies, we include these losses in the reward function during training. In contrast to Holden et al. <ref type="formula">2017</ref>; <ref type="bibr" target="#b24">Sharma &amp; Kitani (2018)</ref>, the model need not be exactly periodic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">HIERARCHICAL TRAINING</head><p>Once we have learned a variety of low-level policies, we want to learn how and when to use these low-level policies in a way that lets us solve much more complex task with sparse rewards.</p><p>The way we compose our low-level policies is quite simple. Similar to <ref type="bibr" target="#b4">Eysenbach et al. (2018)</ref> and many other works, at each time step, our high-level policy chooses one of our low-level policies to run. Given N trained low-level policies œÄ ll (s p t , œÜ), our high-level policy is</p><formula xml:id="formula_2">Œ∏ = œÄ hl (s e )</formula><p>This is actually just a choice from among our low-level policies.</p><p>We go through the complete formulation for our high-level policy in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In our experiments, we test on a variety of difficult sparse reward problems simulated through Mujoco <ref type="bibr" target="#b26">(Todorov et al., 2012)</ref>. We use two popular and challenging agents: Ant and Humanoid.</p><p>In section 4.1 we show some results from our low-level training on Ant and Humanoid. In section 4.2 we show the results of our high-level policies on several different mazes and a navigation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LOW-LEVEL CONTROL</head><p>As described in Section 3.2 and Section 3.3, we train a number of low-level policies using our simple reward function. We train Ant and Humanoid in the default flat plane environments during this stage. <ref type="figure">Figure 1</ref> shows the reward curves of our phase-conditioned low-level policies compared to standard networks. All networks are trained using PPO . See Appendix D for more training and network details. <ref type="figure">Figure 1</ref>: Environmental reward for our low-level policies, with and without phase conditioning For Ant, phase-conditioning gives a respectable boost, but both achieve reasonable rewards and easily move. For Humanoid, vanilla PPO (using a widely used implementation (Kostrikov, 2018)) using the same default hyperparameters performs poorly, even though it performs well for all other standard Open AI Gym Mujoco tasks. With the phase conditioning, however, using PPO, we are able to achieve good results with Humanoid surviving and moving forward.</p><p>In <ref type="figure" target="#fig_0">Figure 2</ref> we show the traces of our low-level policies learned on Ant and Humanoid for 100 time steps. Each trajectory represents a different low-level policy. We can see that we learn a great variety of movements, more than sufficient for the high-level policy. We see that Humanoid moves more slowly, both because Humanoid is much more difficult to control, and because we use the standard OpenAI Gym <ref type="bibr" target="#b1">(Brockman et al., 2016)</ref> simulator timesteps, which is shorter for Humanoid. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MAZES AND NAVIGATION</head><p>The high-level policy network is trained according to Section 4.2, also using PPO. See Appendix E for more training and network details.</p><p>The first high-level environment that we evaluate on is the Cross Maze environment with fixed goals from <ref type="bibr" target="#b8">Haarnoja et al. (2018a)</ref>. As described in that work, the Cross Maze has three different goals along three different paths. In their version of this environment, three different environments are used for each of the three different goal locations and thus a different model is learned for each version of the maze. In <ref type="figure" target="#fig_1">Figure 3</ref> we compare our high-level policy to the published numbers from their method (SAC-LSP) and their baselines (see <ref type="bibr" target="#b8">Haarnoja et al. (2018a)</ref>  <ref type="figure">Figure 5</ref>). We used a different value for the length of the timestep for Ant in the simulator, so to make a fair comparison, we multiply our number of frames by 2.5 so that our x-axis compares the same amount of simulator time for high-level training. This comparison favors their method as we receive 2.5x fewer observations.</p><p>We can see that our method with a phase-input low-level policy converges faster than SAC-LSP and converges to a smaller final distance to goal, meaning that our method is both more efficient and more consistent on the task. Our method also has much smaller standard deviation across trials.  <ref type="bibr" target="#b8">Haarnoja et al. (2018a)</ref>. We show the training curves for each method, showing the final distance of the agent to the goal.</p><p>One major limitation of this environment is that from the perspective of each model, there is just one fixed goal. So to solve it, the policy need only find the one fixed goal for that version of the environment and learn a policy to consistently move to it.</p><p>A much more difficult environment would be one where the goal is randomly chosen from a set of available options each episode. To solve this kind of maze, the policy would have to explore enough to find each goal during the fraction of episodes where it appears, and then consistent learn and keep separate paths of actions in its memory depending on the random goal location for the episode. We refer to these kinds of mazes as "random goal" mazes. All of the maze results except for <ref type="figure" target="#fig_1">Figure 3</ref> use random goals. <ref type="figure" target="#fig_2">Figure 4</ref> shows the training curves for two random goal mazes: the cross maze from before and the "skull maze." See <ref type="figure">Figure 5</ref> for the layout of these mazes. The skull maze has four possible goals instead of three and also forces the agent to make a decision immediately about which way to move, either to the top, left, or right corridor.</p><p>We compare our method to baselines similar to those used in <ref type="bibr" target="#b8">Haarnoja et al. (2018a)</ref>, all trained with PPO as is our method. The baseline models are either trained with or without the phase conditioning, and either from scratch, or finetuned (meaning that we initialize the network using a network trained on our low-level objective). We also give some of the baselines more information by also giving them a velocity reward during high-level training (meaning they are rewarded for movement of the agent).</p><p>Even with velocity rewards and pre-training on the low-level objective (both of which could be considered exploration bonuses), all of the baselines fail to get close to the goal locations. The problem of exploration and consistent navigation to different random goals is too difficult to learn from scratch. <ref type="figure">Figure 5</ref> shows traces of Ant navigating the cross and skull mazes. Most traces reach the goal, although some terminate early. All move fairly decisively towards the correct goal. In <ref type="figure" target="#fig_3">Figure 6</ref>, we show the results using Humanoid on a smaller version of the cross maze. Because the PPO without phase fails at the low-level policy, we omit all non-phase results. The low-level Humanoid control problem is much harder, and moves less quickly (as we saw in <ref type="figure" target="#fig_0">Figure 2</ref>), so our method does not do as well as it did on Ant. But it is still able to successfully reach the goal much of the time, and again the baselines again fail to learn the maze. To our knowledge, ours is the only work that shows results on a Humanoid maze task.</p><p>In Appendix F <ref type="table" target="#tab_4">Table 4</ref>, we also show the rates of success of these methods at reaching the random goals. Here it is even more obvious that na√Øve RL methods fail completely in this task.</p><p>In <ref type="figure" target="#fig_5">Figure 7a</ref>, we compare to <ref type="bibr" target="#b4">Eysenbach et al. (2018)</ref> which is similar to our method in the way it composes low-level policies, but generates these using an entropy method. We compare to their results on the Ant Navigation task (see section 5.2 of their paper) in which Ant has to reach four different waypoints and then travel back to the original waypoint. The agent receives reward after reaching each waypoint, so it is another sparse reward task. Because the plot in their paper measures by walltime instead of timesteps, we show their maximum result (DIAYN) as a dotted horizontal line. While their method is only able to achieve an average of about 2.5 waypoints after 15 hours on their hardware, ours can consistently get to all 5 in less time than that, running serially on CPU.</p><p>In <ref type="figure" target="#fig_5">Figure 7b</ref>, we look at the effect of the choice of algorithm on our high-level training. We compare PPO with A2C <ref type="bibr" target="#b17">(Mnih et al., 2016)</ref> using the implementation from <ref type="bibr" target="#b14">Kostrikov (2018)</ref>, and our own implementation of DQN <ref type="bibr" target="#b16">(Mnih et al., 2013;</ref><ref type="bibr" target="#b28">Watkins, 1989)</ref>. See Appendix C for more implimentation and hyperparameter details. We end DQN early as it took much longer in walltime, but we can see that the choice of algorithm is not critical for successfully training. This suggests that our method is fairly robust to the choice of RL algorithm.   <ref type="bibr" target="#b4">Eysenbach et al. (2018)</ref>. We show the average number of waypoints reached by each method.</p><p>(b) RL algorithm ablation study for our method. We compare the results of our method on Random Ant Cross Maze when training the high-level policy with PPO, A2C and DQN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>In this work we present a simple, yet effective way of solving difficult sparse reward RL problems. We first train a set of proprioceptive low-level agents on an intuitive reward and then combine them by training a high-level policy to select the appropriate sequence of low-level policies. With this method we solve difficult Mujoco navigation tasks with sparse rewards that appear impossible to solve using standard RL algorithms.</p><p>We show that our method performs well in difficult sparse reward problems in Mujoco locomotion tasks. However, it can be applied to any environment where we have a strong notion of internal and external observation. Our reward function encourages behaviors of the agent that change the robot's state in the environment (i.e. "keep moving") or changes something else external to the agent. This notion exists in environments beyond our Mujoco locomotion problems. Some possible applications for our method are Atari and video game environments with sparse rewards (e.g. Montezuma's Revenge) where to receive rewards the agent must complete a long sequence of actions such as finding a key in one area and bringing it to a lock in another. It could also have potential applications for robotics such as in navigation and manipulation where there are not dense rewards or there is a high labeling cost to those rewards.</p><p>In this work, we do not show any results where we fine-tune the low-level policies during highlevel training. This is relatively straightforward, and we have implemented this. We omitted these results as they did not substantially improve the results. For other problems, however, there may be cases where this is useful. For example, if the friction of the ground or some other property of the environment changed between the high-level and low-level environment fine-tuning might be useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A HIGH LEVEL RL FORMULATION DETAILS</head><p>Our high-level action space is</p><formula xml:id="formula_3">Œò = {Œ∏ 1 , Œ∏ 2 , . . . Œ∏ N } Œò = {œÄ = œÄ ll 1 (s e t , œÜ), œÄ = œÄ ll 2 (s e t , œÜ), . . . œÄ = œÄ ll N (s e t , œÜ)}</formula><p>As is standard, we add a slowness parameter to the high-level network. That is, the high-level policy makes a choice of which low-level policy to run for the next T time-steps. In our experiments, we choose T = 10. So our optimization for our sparse reward task is </p><formula xml:id="formula_4">argmax Œ∏ T ‚àí1 i=0 R task (s t+i , Œ∏) ‚â° argmax j T ‚àí1 i=0 R task (s t+i , œÄ ll j (s e t , œÜ)) B BASIC ALGORITHM</formula><formula xml:id="formula_5">initialize(œÄ ll i ) 5: train(œÄ ll i , env ll , T ll , algo, objective ll ) 6: Train high-level policy: 7: initialize(œÄ hl ) 8: train(œÄ hl , env hl , T hl , algo, objective hl )</formula><p>In Algorithm 1 above, we give the high-level algorithm we use. We first train each of our N low-level policies œÄ ll i for T ll steps, using our RL algorithm algo, on the low-level objective and environment. We then train our high-level policiy œÄ hl for T hl using RL algorithm algo on the high-level objective and using the high-level environment. Remember that our high-level policy takes our N low-level policies as input and trains to choose which one to run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IMPLEMENTATION DETAILS AND HYPERPARAMETERS</head><p>For all RL algorithms in the paper (except for DQN in <ref type="figure" target="#fig_5">Figure 7b</ref> and the values from other works in <ref type="figure" target="#fig_1">Figure 3</ref> and <ref type="figure" target="#fig_5">Figure 7a</ref>), we used the implementation of <ref type="bibr" target="#b14">Kostrikov (2018)</ref>. For DQN, we wrote our own simple implementation. The hyperparameters for these three algorithms are shown in Tables 1, 2 and 3 We use the ADAM (Kingma &amp; Ba, 2014) optimizer.   During low-level training we train 80 policies using different random seeds. This is to give us 16 low-level policies for 5 variance runs of our high-level policy. The variance plots in <ref type="figure">Figure 1</ref> use all 80 runs.</p><p>For our Ant models, we use a 3-layer MLP with tanh activation functions and a hidden size of 32. For Humanoid we add skip connections between layers and decrease the hidden size to 16.</p><p>For all models, we use a vector of size 16 for our phase learned parameters b œÜ .</p><p>We choose the cyclic constraint multipliers for state (Œª s ) and action (Œª a ) to be 0.05 and 0.01 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E HIGH-LEVEL TRAINING</head><p>For all high-level result plots, we run 5 independent runs with different random seeds for each result. For any model using pre-trained low-level policies, we use different random low-level policies for each run. For our method, we use N = 16 low-level policies for our high-level training.</p><p>We give the methods the goal index as a one-hot vector to the high-level policies and baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F SUCCESS RATES ON MAZE TASKS</head><p>To give another way of looking at the results on the maze tasks, we show below the average success rate on our three random goal maze environments averaged across runs across the last 100 episodes of training. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Traces of low-level policies after 100 timesteps of movement using different random seeds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Results on Fixed Ant Cross Maze and comparison to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Training curves on the Random Ant Cross and Skull Maze environments. We again show the average final distance to the goalFigure 5: Traces of our method navigating in the ant Ant Cross and Skull Mazes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Training curves of our method and baseline on the Random Cross Maze Humanoid environment. This maze is smaller than the Ant version, so all curves start closer to the goal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Training curves for our method on the Ant Navigation task and comparison to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Additional experiments and comparisons on Ant</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>PPO Hyperparameter values</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Timesteps per batch</cell><cell>2048</cell></row><row><cell>Clip param</cell><cell>0.2</cell></row><row><cell>Entropy coeff</cell><cell>0.0</cell></row><row><cell>Number of parallel processes</cell><cell>1</cell></row><row><cell cols="2">Optimizer epochs per iteration 10</cell></row><row><cell>Optimizer step size</cell><cell>3e ‚àí4</cell></row><row><cell>Optimizer batch size</cell><cell>32</cell></row><row><cell>Discount Œ≥</cell><cell>0.99</cell></row><row><cell>GAE Œª</cell><cell>0.95</cell></row><row><cell>learning rate schedule</cell><cell>constant</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>A2C Hyperparameter values</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Timesteps per batch</cell><cell>5</cell></row><row><cell>entropy coeff</cell><cell>0.01</cell></row><row><cell cols="2">number of parallel processes 16</cell></row><row><cell>Optimizer step size</cell><cell>7e ‚àí4</cell></row><row><cell>Optimizer batch size</cell><cell>32</cell></row><row><cell>Discount Œ≥</cell><cell>0.99</cell></row><row><cell>GAE</cell><cell>No</cell></row><row><cell>learning rate schedule</cell><cell>constant</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>DQN Hyperparameter values    </figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Timesteps per batch</cell><cell>2048</cell></row><row><cell cols="2">Target update frequency 10000</cell></row><row><cell>Replay memory size</cell><cell>10 7</cell></row><row><cell>Optimizer step size</cell><cell>3e ‚àí4</cell></row><row><cell>Optimizer batch size</cell><cell>128</cell></row><row><cell>Discount Œ≥</cell><cell>0.99</cell></row><row><cell>learning rate schedule</cell><cell>constant</cell></row></table><note>D LOW-LEVEL TRAINING AND NETWORK DETAILS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Success Rates on Mazes Parameter Ant Cross Maze Ant Skull Maze Humanoid Cross Maze</figDesc><table><row><cell>Ours (no phase)</cell><cell>88.6%</cell><cell>86.8%</cell><cell>N/A</cell></row><row><cell>Ours (phase)</cell><cell>92.8%</cell><cell>89.4%</cell><cell>38.8%</cell></row><row><cell>PPO goal reward (no phase, scratch)</cell><cell>0%</cell><cell>0%</cell><cell>N/A</cell></row><row><cell>PPO goal reward (phase, scratch)</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>PPO goal+vel Reward (no phase, scratch)</cell><cell>0%</cell><cell>0%</cell><cell>N/A</cell></row><row><cell>PPO goal+vel Reward (phase, scratch)</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>PPO goal reward (no phase, finetuned)</cell><cell>0%</cell><cell>0%</cell><cell>N/A</cell></row><row><cell>PPO goal reward (phase, finetuned)</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell cols="2">PPO goal+vel Reward (no phase, finetuned) 0%</cell><cell>0%</cell><cell>N/A</cell></row><row><cell>PPO goal+vel Reward (phase, finetuned)</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The option-critic architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1726" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Openai gym. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feudal reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-12-03" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
	<note>NIPS Conference</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">State abstraction in MAXQ hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-12-04" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="994" to="1000" />
		</imprint>
	</monogr>
	<note>NIPS Conference</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06070</idno>
		<title level="m">Diversity is all you need: Learning skills without a reward function</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Florensa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03012</idno>
		<title level="m">Stochastic neural networks for hierarchical reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09767</idno>
		<title level="m">Meta learning shared hierarchies</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07507</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational intrinsic control. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent space policies for hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm√§ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
			<biblScope unit="page" from="1846" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Latent space policies for hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02808</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning an embedding space for transferable robot skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning and transfer of modulated locomotor controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05182</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Phase-functioned neural networks for character control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pytorch implementations of reinforcement learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<ptr target="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Central pattern generators and the control of rhythmic movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eve</forename><surname>Marder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Bucher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current biology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="986" to="996" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variational information maximisation for intrinsically motivated reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2125" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Data-efficient hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofir</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08296</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Terrain-adaptive locomotion skills using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Berseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangkang</forename><surname>Berseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deepmimic: Example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Xue Bin Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Panne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02717</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Phase-parametric policies for reinforcement learning in cyclic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mujoco: A physics engine for model-based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feudal networks for hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="3540" to="3549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher John Cornish Hellaby</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>King&apos;s College, Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
