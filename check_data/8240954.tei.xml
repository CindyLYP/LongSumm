<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Normalizing Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-06-08">8 Jun 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
							<email>klambauer@bioinf.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
							<email>unterthiner@bioinf.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
							<email>mayr@bioinf.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
							<email>hochreit@bioinf.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Normalizing Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-06-08">8 Jun 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1706.02515v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>A4</term>
					<term>1 121 UCI Machine Learning Repository data sets: Hyperparameters</term>
					<term>85 A4</term>
					<term>2 121 UCI Machine Learning Repository data sets: detailed results</term>
					<term>87</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are &quot;scaled exponential linear units&quot; (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance-even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep Learning has set new records at different benchmarks and led to various commercial applications <ref type="bibr" target="#b29">[25,</ref><ref type="bibr" target="#b37">33]</ref>. Recurrent neural networks (RNNs) <ref type="bibr" target="#b22">[18]</ref> achieved new levels at speech and natural language processing, for example at the TIMIT benchmark <ref type="bibr" target="#b16">[12]</ref> or at language translation <ref type="bibr" target="#b40">[36]</ref>, and are already employed in mobile devices <ref type="bibr" target="#b35">[31]</ref>. RNNs have won handwriting recognition challenges (Chinese and Arabic handwriting) <ref type="bibr" target="#b37">[33,</ref><ref type="bibr" target="#b17">13,</ref><ref type="bibr" target="#b10">6]</ref> and Kaggle challenges, such as the "Grasp-and Lift EEG" competition. Their counterparts, convolutional neural networks (CNNs) <ref type="bibr" target="#b28">[24]</ref> excel at vision and video tasks. CNNs are on par with human dermatologists at the visual detection of skin cancer <ref type="bibr" target="#b13">[9]</ref>. The visual processing for self-driving cars is based on CNNs <ref type="bibr" target="#b23">[19]</ref>, as is the visual input to AlphaGo which has beaten one of the best human GO players <ref type="bibr" target="#b38">[34]</ref>. At vision challenges, CNNs are constantly winning, for example at the large ImageNet competition <ref type="bibr" target="#b27">[23,</ref><ref type="bibr" target="#b20">16]</ref>, but also almost all Kaggle vision challenges, such as the "Diabetic Retinopathy" and the "Right Whale" challenges <ref type="bibr">[8,</ref><ref type="bibr" target="#b18">14]</ref>.</p><p>However, looking at Kaggle challenges that are not related to vision or sequential tasks, gradient boosting, random forests, or support vector machines (SVMs) are winning most of the competitions. Deep Learning is notably absent, and for the few cases where FNNs won, they are shallow. For example, the HIGGS challenge, the Merck Molecular Activity challenge, and the Tox21 Data challenge were all won by FNNs with at most four hidden layers. Surprisingly, it is hard to find success stories with FNNs that have many hidden layers, though they would allow for different levels of abstract representations of the input <ref type="bibr" target="#b7">[3]</ref>.</p><p>To robustly train very deep CNNs, batch normalization evolved into a standard to normalize neuron activations to zero mean and unit variance <ref type="bibr" target="#b24">[20]</ref>. Layer normalization <ref type="bibr" target="#b6">[2]</ref> also ensures zero mean and unit variance, while weight normalization <ref type="bibr" target="#b36">[32]</ref> ensures zero mean and unit variance if in the previous layer the activations have zero mean and unit variance. However, training with normalization techniques is perturbed by stochastic gradient descent (SGD), stochastic regularization (like dropout), and the estimation of the normalization parameters. Both RNNs and CNNs can stabilize learning via weight sharing, therefore they are less prone to these perturbations. In contrast, FNNs trained with normalization techniques suffer from these perturbations and have high variance in the training error (see <ref type="figure">Figure 1</ref>). This high variance hinders learning and slows it down. Furthermore, strong regularization, such as dropout, is not possible as it would further increase the variance which in turn would lead to divergence of the learning process. We believe that this sensitivity to perturbations is the reason that FNNs are less successful than RNNs and CNNs.</p><p>Self-normalizing neural networks (SNNs) are robust to perturbations and do not have high variance in their training errors (see <ref type="figure">Figure 1</ref>). SNNs push neuron activations to zero mean and unit variance thereby leading to the same effect as batch normalization, which enables to robustly learn many layers. SNNs are based on scaled exponential linear units "SELUs" which induce self-normalizing properties like variance stabilization which in turn avoids exploding and vanishing gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-normalizing Neural Networks (SNNs)</head><p>Normalization and SNNs. For a neural network with activation function f , we consider two consecutive layers that are connected by a weight matrix W . Since the input to a neural network is a random variable, the activations x in the lower layer, the network inputs z = W x, and the activations y = f (z) in the higher layer are random variables as well. We assume that all activations x i of the lower layer have mean µ := E(x i ) and variance ν := Var(x i ). An activation y in the higher layer has meanμ := E(y) and varianceν := Var(y). Here E(.) denotes the expectation and Var <ref type="bibr">(.)</ref> the variance of a random variable. A single activation y = f (z) has net input z = w T x. For n units with activation x i , 1 i n in the lower layer, we define n times the mean of the weight vector w ∈ R n as ω := n i=1 w i and n times the second moment as τ := n i=1 w 2 i . We consider the mapping g that maps mean and variance of the activations from one layer to mean and variance of the activations in the next layer</p><formula xml:id="formula_0">µ ν → μ ν : μ ν = g µ ν .<label>(1)</label></formula><p>Normalization techniques like batch, layer, or weight normalization ensure a mapping g that keeps (µ, ν) and (μ,ν) close to predefined values, typically (0, 1). Definition 1 (Self-normalizing neural net). A neural network is self-normalizing if it possesses a mapping g : Ω → Ω for each activation y that maps mean and variance from one layer to the next and has a stable and attracting fixed point depending on (ω, τ ) in Ω. Furthermore, the mean and the variance remain in the domain Ω, that is g(Ω) ⊆ Ω, where Ω = {(µ, ν) | µ ∈ [µ min , µ max ], ν ∈ [ν min , ν max ]}. When iteratively applying the mapping g, each point within Ω converges to this fixed point.</p><p>Therefore, we consider activations of a neural network to be normalized, if both their mean and their variance across samples are within predefined <ref type="bibr">intervals</ref>  <ref type="figure">Figure 1</ref>: The left panel and the right panel show the training error (y-axis) for feed-forward neural networks (FNNs) with batch normalization (BatchNorm) and self-normalizing networks (SNN) across update steps (x-axis) on the MNIST dataset the CIFAR10 dataset, respectively. We tested networks with 8, 16, and 32 layers and learning rate 1e-5. FNNs with batch normalization exhibit high variance due to perturbations. In contrast, SNNs do not suffer from high variance as they are more robust to perturbations and learn faster.</p><p>these intervals, then also mean and variance of y remain in these intervals, i.e., the normalization is transitive across layers. Within these intervals, the mean and variance both converge to a fixed point if the mapping g is applied iteratively.</p><p>Therefore, SNNs keep normalization of activations when propagating them through layers of the network. The normalization effect is observed across layers of a network: in each layer the activations are getting closer to the fixed point. The normalization effect can also observed be for two fixed layers across learning steps: perturbations of lower layer activations or weights are damped in the higher layer by drawing the activations towards the fixed point. If for all y in the higher layer, ω and τ of the corresponding weight vector are the same, then the fixed points are also the same. In this case we have a unique fixed point for all activations y. Otherwise, in the more general case, ω and τ differ for different y but the mean activations are drawn into [µ min , µ max ] and the variances are drawn into [ν min , ν max ].</p><p>Constructing Self-Normalizing Neural Networks. We aim at constructing self-normalizing neural networks by adjusting the properties of the function g. Only two design choices are available for the function g: (1) the activation function and (2) the initialization of the weights.</p><p>For the activation function, we propose "scaled exponential linear units" (SELUs) to render a FNN as self-normalizing. The SELU activation function is given by</p><formula xml:id="formula_1">selu(x) = λ x if x &gt; 0 αe x − α if x 0 .<label>(2)</label></formula><p>SELUs allow to construct a mapping g with properties that lead to SNNs. SNNs cannot be derived with (scaled) rectified linear units (ReLUs), sigmoid units, tanh units, and leaky ReLUs. The activation function is required to have (1) negative and positive values for controlling the mean, <ref type="bibr" target="#b6">(2)</ref> saturation regions (derivatives approaching zero) to dampen the variance if it is too large in the lower layer, (3) a slope larger than one to increase the variance if it is too small in the lower layer, (4) a continuous curve. The latter ensures a fixed point, where variance damping is equalized by variance increasing. We met these properties of the activation function by multiplying the exponential linear unit (ELU) <ref type="bibr" target="#b11">[7]</ref> with λ &gt; 1 to ensure a slope larger than one for positive net inputs.</p><p>For the weight initialization, we propose ω = 0 and τ = 1 for all units in the higher layer. The next paragraphs will show the advantages of this initialization. Of course, during learning these assumptions on the weight vector will be violated. However, we can prove the self-normalizing property even for weight vectors that are not normalized, therefore, the self-normalizing property can be kept during learning and weight changes.</p><p>Deriving the Mean and Variance Mapping Function g. We assume that the x i are independent from each other but share the same mean µ and variance ν. Of course, the independence assumptions is not fulfilled in general. We will elaborate on the independence assumption below. The network input z in the higher layer is z = w T x for which we can infer the following moments E(z) = n i=1 w i E(x i ) = µ ω and Var(z) = Var( n i=1 w i x i ) = ν τ , where we used the independence of the x i . The net input z is a weighted sum of independent, but not necessarily identically distributed variables x i , for which the central limit theorem (CLT) states that z approaches a normal distribution: z ∼ N (µω, √ ντ ) with density p N (z; µω, √ ντ ). According to the CLT, the larger n, the closer is z to a normal distribution. For Deep Learning, broad layers with hundreds of neurons x i are common. Therefore the assumption that z is normally distributed is met well for most currently used neural networks (see <ref type="figure" target="#fig_59">Figure A8</ref>). The function g maps the mean and variance of activations in the lower layer to the meanμ = E(y) and varianceν = Var(y) of the activations y in the next layer:</p><formula xml:id="formula_2">g : µ ν → μ ν :μ(µ, ω, ν, τ ) = ∞ −∞ selu(z) p N (z; µω, √ ντ ) dz (3) ν(µ, ω, ν, τ ) = ∞ −∞ selu(z) 2 p N (z; µω, √ ντ ) dz − (μ) 2 .</formula><p>These integrals can be analytically computed and lead to following mappings of the moments: Stable and Attracting Fixed Point (0, 1) for Normalized Weights. We assume a normalized weight vector w with ω = 0 and τ = 1. Given a fixed point (µ, ν), we can solve equations Eq. <ref type="bibr" target="#b8">(4)</ref> and <ref type="bibr">Eq. (5)</ref> for α and λ. We chose the fixed point (µ, ν) = (0, 1), which is typical for activation normalization. We obtain the fixed point equationsμ = µ = 0 andν = ν = 1 that we solve for α and λ and obtain the solutions α 01 ≈ 1.6733 and λ 01 ≈ 1.0507, where the subscript 01 indicates that these are the parameters for fixed point (0, 1). The analytical expressions for α 01 and λ 01 are given in Eq. <ref type="bibr" target="#b18">(14)</ref>. We are interested whether the fixed point (µ, ν) = (0, 1) is stable and attracting. If the Jacobian of g has a norm smaller than 1 at the fixed point, then g is a contraction mapping and the fixed point is stable. The (2x2)-Jacobian J (µ, ν) of g : (µ, ν) → (μ,ν) evaluated at the fixed point (0, 1) with α 01 and λ 01 is </p><formula xml:id="formula_3">µ = 1 2 λ (µω) erf µω √ 2 √ ντ +<label>(4</label></formula><formula xml:id="formula_4">J (µ, ν) =    ∂ µ new (µ,ν) ∂µ ∂ µ new (µ,ν) ∂ν ∂ ν new (µ,ν)</formula><p>The spectral norm of J (0, 1) (its largest singular value) is 0.7877 &lt; 1. That means g is a contraction mapping around the fixed point (0, 1) (the mapping is depicted in <ref type="figure" target="#fig_7">Figure 2</ref>). Therefore, (0, 1) is a stable fixed point of the mapping g.</p><p>Stable and Attracting Fixed Points for Unnormalized Weights. A normalized weight vector w cannot be ensured during learning. For SELU parameters α = α 01 and λ = λ 01 , we show in the next theorem that if (ω, τ ) is close to (0, 1), then g still has an attracting and stable fixed point that is close to (0, 1). Thus, in the general case there still exists a stable fixed point which, however, depends on (ω, τ ). If we restrict (µ, ν, ω, τ ) to certain intervals, then we can show that (µ, ν) is mapped to <ref type="figure" target="#fig_7">Figure 2</ref>: For ω = 0 and τ = 1, the mapping g of mean µ (x-axis) and variance ν (y-axis) to the next layer's meanμ and varianceν is depicted. Arrows show in which direction (µ, ν) is mapped by g : (µ, ν) → (μ,ν). The fixed point of the mapping g is (0, 1).</p><p>the respective intervals. Next we present the central theorem of this paper, from which follows that SELU networks are self-normalizing under mild conditions on the weights.</p><p>Theorem 1 (Stable and Attracting Fixed Points). We assume α = α 01 and λ = λ 01 . We restrict the range of the variables to the following intervals µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, <ref type="bibr">1.5]</ref>, and τ ∈ [0.95, <ref type="bibr">1.1]</ref>, that define the functions' domain Ω. For ω = 0 and τ = 1, the mapping Eq.</p><p>(3) has the stable fixed point (µ, ν) = (0, 1), whereas for other ω and τ the mapping Eq. Proof. We provide a proof sketch (see detailed proof in Appendix Section A3). With the Banach fixed point theorem we show that there exists a unique attracting and stable fixed point. To this end, we have to prove that a) g is a contraction mapping and b) that the mapping stays in the domain, that is, g(Ω) ⊆ Ω. The spectral norm of the Jacobian of g can be obtained via an explicit formula for the largest singular value for a 2 × matrix. g is a contraction mapping if its spectral norm is smaller than 1. We perform a computer-assisted proof to evaluate the largest singular value on a fine grid and ensure the precision of the computer evaluation by an error propagation analysis of the implemented algorithms on the according hardware. Singular values between grid points are upper bounded by the mean value theorem. To this end, we bound the derivatives of the formula for the largest singular value with respect to ω, τ, µ, ν. Then we apply the mean value theorem to pairs of points, where one is on the grid and the other is off the grid. This shows that for all values of ω, τ, µ, ν in the domain Ω, the spectral norm of g is smaller than one. Therefore, g is a contraction mapping on the domain Ω.</p><p>Finally, we show that the mapping g stays in the domain Ω by deriving bounds onμ andν. Hence, the Banach fixed-point theorem holds and there exists a unique fixed point in Ω that is attained.</p><p>Consequently, feed-forward neural networks with many units in each layer and with the SELU activation function are self-normalizing (see definition 1), which readily follows from Theorem 1. To give an intuition, the main property of SELUs is that they damp the variance for negative net inputs and increase the variance for positive net inputs. The variance damping is stronger if net inputs are further away from zero while the variance increase is stronger if net inputs are close to zero. Thus, for large variance of the activations in the lower layer the damping effect is dominant and the variance decreases in the higher layer. Vice versa, for small variance the variance increase is dominant and the variance increases in the higher layer.</p><p>However, we cannot guarantee that mean and variance remain in the domain Ω. Therefore, we next treat the case where (µ, ν) are outside Ω. It is especially crucial to consider ν because this variable has much stronger influence than µ. Mapping ν across layers to a high value corresponds to an exploding gradient, since the Jacobian of the activation of high layers with respect to activations in lower layers has large singular values. Analogously, mapping ν across layers to a low value corresponds to an vanishing gradient. Bounding the mapping of ν from above and below would avoid both exploding and vanishing gradients. Theorem 2 states that the variance of neuron activations of SNNs is bounded from above, and therefore ensures that SNNs learn robustly and do not suffer from exploding gradients. Theorem 2 (Decreasing ν). For λ = λ 01 , α = α 01 and the domain Ω + : −1 µ 1, −0.1 ω 0.1, 3 ν 16, and 0.8 τ 1.25, we have for the mapping of the varianceν(µ, ω, ν, τ, λ, α) given in Eq. (5):ν(µ, ω, ν, τ, λ 01 , α 01 ) &lt; ν.</p><p>The proof can be found in the Appendix Section A3. Thus, when mapped across many layers, the variance in the interval <ref type="bibr" target="#b7">[3,</ref><ref type="bibr" target="#b20">16]</ref> is mapped to a value below 3. Consequently, all fixed points (µ, ν) of the mapping g (Eq. (3)) have ν &lt; 3. Analogously, Theorem 3 states that the variance of neuron activations of SNNs is bounded from below, and therefore ensures that SNNs do not suffer from vanishing gradients. The proof can be found in the Appendix Section A3. All fixed points (µ, ν) of the mapping g (Eq. (3)) ensure for 0.8 τ thatν &gt; 0.16 and for 0.9 τ thatν &gt; 0. <ref type="bibr" target="#b28">24</ref>. Consequently, the variance mapping Eq. (5) ensures a lower bound on the variance ν. Therefore SELU networks control the variance of the activations and push it into an interval, whereafter the mean and variance move toward the fixed point. Thus, SELU networks are steadily normalizing the variance and subsequently normalizing the mean, too. In all experiments, we observed that self-normalizing neural networks push the mean and variance of activations into the domain Ω .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization. Since SNNs have a fixed point at zero mean and unit variance for normalized weights</head><formula xml:id="formula_6">ω = n i=1 w i = 0 and τ = n i=1 w 2 i = 1 (see above)</formula><p>, we initialize SNNs such that these constraints are fulfilled in expectation. We draw the weights from a Gaussian distribution with E(w i ) = 0 and variance Var(w i ) = 1/n. Uniform and truncated Gaussian distributions with these moments led to networks with similar behavior. The "MSRA initialization" is similar since it uses zero mean and variance 2/n to initialize the weights <ref type="bibr" target="#b21">[17]</ref>. The additional factor 2 counters the effect of rectified linear units.</p><p>New Dropout Technique. Standard dropout randomly sets an activation x to zero with probability 1 − q for 0 &lt; q 1. In order to preserve the mean, the activations are scaled by 1/q during training. If x has mean E(x) = µ and variance Var(x) = ν, and the dropout variable d follows a binomial distribution B(1, q), then the mean E(1/qdx) = µ is kept. Dropout fits well to rectified linear units, since zero is in the low variance region and corresponds to the default value. For scaled exponential linear units, the default and low variance value is lim x→−∞ selu(x) = −λα = α . Therefore, we propose "alpha dropout", that randomly sets inputs to α . The new mean and new variance is</p><formula xml:id="formula_7">E(xd + α (1 − d)) = qµ + (1 − q)α , and Var(xd + α (1 − d)) = q((1 − q)(α − µ) 2 + ν).</formula><p>We aim at keeping mean and variance to their original values after "alpha dropout", in order to ensure the self-normalizing property even for "alpha dropout". </p><formula xml:id="formula_8">b = − q + α 2 q(1 − q) −1/2 ((1 − q)α ).</formula><p>The parameters a and b only depend on the dropout rate 1 − q and the most negative activation α . Empirically, we found that dropout rates 1 − q = 0.05 or 0.10 lead to models with good performance. "Alpha-dropout" fits well to scaled exponential linear units by randomly setting activations to the negative saturation value.</p><p>Applicability of the central limit theorem and independence assumption. In the derivative of the mapping (Eq. (3)), we used the central limit theorem (CLT) to approximate the network inputs z = n i=1 w i x i with a normal distribution. We justified normality because network inputs represent a weighted sum of the inputs x i , where for Deep Learning n is typically large. The Berry-Esseen theorem states that the convergence rate to normality is n −1/2 <ref type="bibr" target="#b26">[22]</ref>. In the classical version of the CLT, the random variables have to be independent and identically distributed, which typically does not hold for neural networks. However, the Lyapunov CLT does not require the variable to be identically distributed anymore. Furthermore, even under weak dependence, sums of random variables converge in distribution to a Gaussian distribution <ref type="bibr">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We compare SNNs to other deep networks at different benchmarks. Hyperparameters such as number of layers (blocks), neurons per layer, learning rate, and dropout rate, are adjusted by grid-search for each dataset on a separate validation set (see Section A4). We compare the following FNN methods:</p><p>• "MSRAinit": FNNs without normalization and with ReLU activations and "Microsoft weight initialization" <ref type="bibr" target="#b21">[17]</ref>.</p><p>• "MSRAinit": FNNs without normalization and with ReLU activations and "Microsoft weight initialization" <ref type="bibr" target="#b21">[17]</ref>.</p><p>• "BatchNorm": FNNs with batch normalization <ref type="bibr" target="#b24">[20]</ref>.</p><p>• "LayerNorm": FNNs with layer normalization <ref type="bibr" target="#b6">[2]</ref>.</p><p>• "WeightNorm": FNNs with weight normalization <ref type="bibr" target="#b36">[32]</ref>.</p><p>• "Highway": Highway networks <ref type="bibr" target="#b39">[35]</ref>.</p><p>• "ResNet": Residual networks <ref type="bibr" target="#b20">[16]</ref> adapted to FNNs using residual blocks with 2 or 3 layers with rectangular or diavolo shape.</p><p>• "SNNs": Self normalizing networks with SELUs with α = α 01 and λ = λ 01 and the proposed dropout technique and initialization strategy.</p><p>121 UCI Machine Learning Repository datasets. The benchmark comprises 121 classification datasets from the UCI Machine Learning repository <ref type="bibr">[10]</ref> from diverse application areas, such as physics, geology, or biology. The size of the datasets ranges between and 130, 000 data points and the number of features from 4 to 250. In abovementioned work <ref type="bibr">[10]</ref>, there were methodological mistakes <ref type="bibr" target="#b41">[37]</ref> which we avoided here. Each compared FNN method was optimized with respect to its architecture and hyperparameters on a validation set that was then removed from the subsequent analysis. The selected hyperparameters served to evaluate the methods in terms of accuracy on the pre-defined test sets (details on the hyperparameter selection are given in Section A4). The accuracies are reported in the <ref type="table">Table A11</ref>. We ranked the methods by their accuracy for each prediction task and compared their average ranks. SNNs significantly outperform all competing networks in pairwise comparisons (paired Wilcoxon test across datasets) as reported in <ref type="table">Table 1 (left panel)</ref>.</p><p>We further included 17 machine learning methods representing diverse method groups <ref type="bibr">[10]</ref> in the comparison and the grouped the data sets into "small" and "large" data sets (for details see Section A4).</p><p>On 75 small datasets with less than 1000 data points, random forests and SVMs outperform SNNs and other FNNs. On 46 larger datasets with at least 1000 data points, SNNs show the highest performance followed by SVMs and random forests (see right panel of <ref type="table" target="#tab_3">Table 1, for complete results  see Tables A12 and A12)</ref>. Overall, SNNs have outperformed state of the art machine learning methods on UCI datasets with more than 1,000 data points.</p><p>Typically, hyperparameter selection chose SNN architectures that were much deeper than the selected architectures of other FNNs, with an average depth of 10.8 layers, compared to average depths of 6.0 for BatchNorm, <ref type="bibr">3.8</ref> WeightNorm, 7.0 LayerNorm, 5.9 Highway, and 7.1 for MSRAinit networks. For ResNet, the average number of blocks was 6. <ref type="bibr" target="#b39">35</ref>. SNNs with many more than 4 layers often provide the best predictive accuracies across all neural networks.</p><p>Drug discovery: The Tox21 challenge dataset. The Tox21 challenge dataset comprises about 12,000 chemical compounds whose twelve toxic effects have to be predicted based on their chemical structure. We used the validation sets of the challenge winners for hyperparameter selection (see <ref type="table">Table 1</ref>: Left: Comparison of seven FNNs on 121 UCI tasks. We consider the average rank difference to rank 4, which is the average rank of seven methods with random predictions. The first column gives the method, the second the average rank difference, and the last the p-value of a paired Wilcoxon test whether the difference to the best performing method is significant. SNNs significantly outperform all other methods. Right: Comparison of 24 machine learning methods (ML) on the UCI datasets with more than 1000 data points. The first column gives the method, the second the average rank difference to rank <ref type="bibr">12.5</ref>, and the last the p-value of a paired Wilcoxon test whether the difference to the best performing method is significant. Methods that were significantly worse than the best method are marked with "*". The full tables can be found in <ref type="table" target="#tab_3">Table A11, Table A12 and Table A13</ref>. </p><formula xml:id="formula_9">SNNs</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Section A4</head><p>) and the challenge test set for performance comparison. We repeated the whole evaluation procedure 5 times to obtain error bars. The results in terms of average AUC are given in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>In 2015, the challenge organized by the US NIH was won by an ensemble of shallow ReLU FNNs which achieved an AUC of 0.846 <ref type="bibr" target="#b32">[28]</ref>. Besides FNNs, this ensemble also contained random forests and SVMs. Single SNNs came close with an AUC of 0.845±0.003. The best performing SNNs have layers, compared to the runner-ups ReLU networks with layer normalization with 2 and 3 layers. Also batchnorm and weightnorm networks, typically perform best with shallow networks of 2 to 4 layers ( <ref type="table" target="#tab_3">Table 2)</ref>. The deeper the networks, the larger the difference in performance between SNNs and other methods (see columns 5-8 of <ref type="table" target="#tab_3">Table 2</ref>). The best performing method is an SNN with 8 layers.  <ref type="bibr" target="#b31">[27]</ref>. Recently, the High Time Resolution Universe Survey (HTRU2) dataset has been released with 1,639 real pulsars and 16,259 spurious signals. Currently, the highest AUC value of a 10-fold cross-validation is 0.976 which has been achieved by Naive Bayes classifiers followed by decision tree C4.5 with 0.949 and SVMs with 0.929. We used eight features constructed by the PulsarFeatureLab as used previously <ref type="bibr" target="#b31">[27]</ref>. We assessed the performance of FNNs using 10-fold nested cross-validation, where the hyperparameters were selected in the inner loop on a validation set (for details on the hyperparameter selection see Section A4). <ref type="table" target="#tab_4">Table 3</ref> reports the results in terms of AUC. SNNs outperform all other methods and have pushed the state-of-the-art to an AUC of 0.98. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have introduced self-normalizing neural networks for which we have proved that neuron activations are pushed towards zero mean and unit variance when propagated through the network. Additionally, for activations not close to unit variance, we have proved an upper and lower bound on the variance mapping. Consequently, SNNs do not face vanishing and exploding gradient problems. Therefore, SNNs work well for architectures with many layers, allowed us to introduce a novel regularization scheme, and learn very robustly. On 121 UCI benchmark datasets, SNNs have outperformed other FNNs with and without normalization techniques, such as batch, layer, and weight normalization, or specialized architectures, such as Highway or Residual networks. SNNs also yielded the best results on drug discovery and astronomy tasks. The best performing SNN architectures are typically very deep in contrast to other FNNs.</p><p>This appendix is organized as follows: the first section sets the background, definitions, and formulations. The main theorems are presented in the next section. The following section is devoted to the proofs of these theorems. The next section reports additional results and details on the performed computational experiments, such as hyperparameter selection. The last section shows that our theoretical bounds can be confirmed by numerical methods as a sanity check.</p><p>The proof of theorem 1 is based on the Banach's fixed point theorem for which we require (1) a contraction mapping, which is proved in Subsection A3. <ref type="bibr" target="#b8">4</ref>.1 and (2) that the mapping stays within its domain, which is proved in Subsection A3. <ref type="bibr">4.2</ref> For part (1), the proof relies on the main Lemma 12, which is a computer-assisted proof, and can be found in Subsection A3. <ref type="bibr" target="#b8">4</ref>. <ref type="bibr" target="#b5">1</ref>. The validity of the computer-assisted proof is shown in Subsection A3. <ref type="bibr" target="#b8">4</ref>.5 by error analysis and the precision of the functions' implementation. The last Subsection A3. <ref type="bibr" target="#b8">4</ref>.6 compiles various lemmata with intermediate results that support the proofs of the main lemmata and theorems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1 Background</head><p>We consider a neural network with activation function f and two consecutive layers that are connected by weight matrix W . Since samples that serve as input to the neural network are chosen according to a distribution, the activations x in the lower layer, the network inputs z = W x, and activations y = f (z) in the higher layer are all random variables. We assume that all units x i in the lower layer have mean activation µ := E(x i ) and variance of the activation ν := Var(x i ) and a unit y in the higher layer has mean activationμ := E(y) and varianceν := Var(y). Here E <ref type="bibr">(.)</ref> denotes the expectation and Var(.) the variance of a random variable. For activation of unit y, we have net input z = w T x and the scaled exponential linear unit (SELU) activation y = selu(z), with</p><formula xml:id="formula_10">selu(x) = λ x if x &gt; 0 αe x − α if x 0 .<label>(7)</label></formula><p>For n units x i , 1 i n in the lower layer and the weight vector w ∈ R n , we define n times the mean by ω := n i=1 w i and n times the second moment by τ := n i=1 w 2 i . We define a mapping g from mean µ and variance ν of one layer to the meanμ and varianceν in the next layer:</p><formula xml:id="formula_11">g : (µ, ν) → (μ,ν) .<label>(8)</label></formula><p>For neural networks with scaled exponential linear units, the mean is of the activations in the next layer computed according tõ</p><formula xml:id="formula_12">µ = 0 −∞ λα(exp(z) − 1)p Gauss (z; µω, √ ντ )dz + ∞ 0 λzp Gauss (z; µω, √ ντ )dz ,<label>(9)</label></formula><p>and the second moment of the activations in the next layer is computed according tõ</p><formula xml:id="formula_13">ξ = 0 −∞ λ 2 α 2 (exp(z) − 1) 2 p Gauss (z; µω, √ ντ )dz + ∞ 0 λ 2 z 2 p Gauss (z; µω, √ ντ )dz . (10)</formula><p>Therefore, the expressionsμ andν have the following form:</p><formula xml:id="formula_14">µ(µ, ω, ν, τ, λ, α) = 1 2 λ −(α + µω) erfc µω √ 2 √ ντ +<label>(11)</label></formula><p>αe</p><formula xml:id="formula_15">µω+ ντ 2 erfc µω + ντ √ 2 √ ντ + 2 π √ ντ e − µ 2 ω 2 2ντ + 2µω ν(µ, ω, ν, τ, λ, α) =ξ(µ, ω, ν, τ, λ, α) − (μ(µ, ω, ν, τ, λ, α)) 2 (12) ξ(µ, ω, ν, τ, λ, α) = 1 2 λ 2 (µω) 2 + ντ erf µω √ 2 √ ντ + 1 + (13) α 2 −2e µω+ ντ erfc µω + ντ √ 2 √ ντ + e 2(µω+ντ ) erfc µω + 2ντ √ 2 √ ντ + erfc µω √ 2 √ ντ + 2 π (µω) √ ντ e − (µω) 2 2(ντ )</formula><p>We solve equations Eq. 4 and Eq. 5 for fixed pointsμ = µ andν = ν. For a normalized weight vector with ω = 0 and τ = 1 and the fixed point (µ, ν) = (0, 1), we can solve equations Eq. 4 and Eq. 5 for α and λ. We denote the solutions to fixed point (µ, ν) = (0, 1) by α 01 and λ 01 .</p><formula xml:id="formula_16">α 01 = − 2 π erfc 1 √ 2 exp 1 2 − 1 ≈ 1.67326<label>(14)</label></formula><formula xml:id="formula_17">λ 01 = 1 − erfc 1 √ 2 √ e 2π 2 erfc √ 2 e 2 + π erfc 1 √ 2 2 e − 2(2 + π) erfc 1 √ 2 √ e + π + 2 −1/2 λ 01 ≈ 1.0507 .</formula><p>The parameters α 01 and λ 01 ensureμ</p><formula xml:id="formula_18">(0, 0, 1, 1, λ 01 , α 01 ) = 0 ν(0, 0, 1, 1, λ 01 , α 01 ) = 1</formula><p>Since we focus on the fixed point (µ, ν) = (0, 1), we assume throughout the analysis that α = α and λ = λ 01 . We consider the functionsμ(µ, ω, ν, τ, λ , α 01 ),ν(µ, ω, ν, τ, λ 01 , α 01 ), and ξ(µ, ω, ν, τ, λ 01 , α 01 ) on the domain <ref type="figure" target="#fig_7">Figure 2</ref> visualizes the mapping g for ω = 0 and τ = 1 and α 01 and λ 01 at few pre-selected points. It can be seen that (0, 1) is an attracting fixed point of the mapping g. </p><formula xml:id="formula_19">Ω = {(µ, ω, ν, τ ) | µ ∈ [µ min , µ max ] = [−0.1, 0.1], ω ∈ [ω min , ω max ] = [−0.1, 0.1], ν ∈ [ν min , ν max ] = [0.8, 1.5], τ ∈ [τ min , τ max ] = [0.95, 1.1]}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2 Theorems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2 Theorem 2: Decreasing Variance from Above</head><p>The next Theorem 2 states that the variance of unit activations does not explode through consecutive layers of self-normalizing networks. Even more, a large variance of unit activations decreases when propagated through the network. In particular this ensures that exploding gradients will never be observed. In contrast to the domain in previous subsection, in which ν ∈ [0.8, 1.5], we now consider a domain in which the variance of the inputs is higher ν ∈ <ref type="bibr" target="#b7">[3,</ref><ref type="bibr" target="#b20">16]</ref> and even the range of the mean is increased µ ∈ [−1, 1]. We denote this new domain with the symbol Ω ++ to indicate that the variance lies above the variance of the original domain Ω.</p><p>In Ω ++ , we can show that the varianceν in the next layer is always smaller then the original variance ν. Concretely, this theorem states that: Theorem 2 (Decreasing ν). For λ = λ 01 , α = α 01 and the domain Ω ++ : −1 µ 1, −0.1 ω 0.1, 3 ν 16, and 0.8 τ 1.25 we have for the mapping of the varianceν(µ, ω, ν, τ, λ, α) given in Eq.</p><formula xml:id="formula_20">(5)ν (µ, ω, ν, τ, λ 01 , α 01 ) &lt; ν .<label>(15)</label></formula><p>The variance decreases in <ref type="bibr" target="#b7">[3,</ref><ref type="bibr" target="#b20">16]</ref> and all fixed points (µ, ν) of mapping Eq. (5) and Eq. (4) have ν &lt; 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.3 Theorem 3: Increasing Variance from Below</head><p>The next Theorem 3 states that the variance of unit activations does not vanish through consecutive layers of self-normalizing networks. Even more, a small variance of unit activations increases when propagated through the network. In particular this ensures that vanishing gradients will never be observed. In contrast to the first domain, in which ν ∈ [0.8, 1.5], we now consider two domains Ω − 1 and Ω − 2 in which the variance of the inputs is lower 0.05 ν 0.16 and 0.05 ν 0.24, and even the parameter τ is different 0.9 τ 1.25 to the original Ω. We denote this new domain with the symbol Ω − i to indicate that the variance lies below the variance of the original domain Ω.</p><p>In Ω −</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>and Ω − 2 , we can show that the varianceν in the next layer is always larger then the original variance ν, which means that the variance does not vanish through consecutive layers of self-normalizing networks. Concretely, this theorem states that: Theorem 3 (Increasing ν). We consider λ = λ 01 , α = α 01 and the two domains</p><formula xml:id="formula_21">Ω − 1 = {(µ, ω, ν, τ ) | − 0.1 µ 0.1, −0.1 ω 0.1, 0.05 ν 0.16, 0.8 τ 1.25} and Ω − 2 = {(µ, ω, ν, τ ) | − 0.1 µ 0.1, −0.1 ω 0.1, 0</formula><p>.05 ν 0.24, 0.9 τ 1.25}. The mapping of the varianceν(µ, ω, ν, τ, λ, α) given in Eq. (5) increases ν(µ, ω, ν, τ, λ 01 , α 01 ) &gt; ν <ref type="bibr" target="#b20">(16)</ref> in both Ω − 1 and Ω − 2 . All fixed points (µ, ν) of mapping Eq. (5) and Eq. (4) ensure for 0.8 τ that ν &gt; 0.16 and for 0.9 τ thatν &gt; 0. <ref type="bibr" target="#b28">24</ref> We have to show that the mapping g defined by Eq. (4) and Eq. (5) has a stable and attracting fixed point close to (0, 1). To proof this statement and Theorem 1, we apply the Banach fixed point theorem which requires (1) that g is a contraction mapping and (2) that g does not map outside the function's domain, concretely:</p><p>Theorem 4 (Banach Fixed Point Theorem). Let (X, d) be a non-empty complete metric space with a contraction mapping f : X → X. Then f has a unique fixed-point</p><formula xml:id="formula_22">x f ∈ X with f (x f ) = x f . Every sequence x n = f (x n−1 ) with starting element x 0 ∈ X converges to the fixed point: x n − −−− → n→∞ x f .</formula><p>Contraction mappings are functions that map two points such that their distance is decreasing:</p><p>Definition (Contraction mapping). A function f : X → X on a metric space X with distance d is a contraction mapping, if there is a δ &lt; 1, such that for all points u and v in X:</p><formula xml:id="formula_23">d(f (u), f (v)) δd(u, v).</formula><p>To show that g is a contraction mapping in Ω with distance . 2 , we use the Mean Value Theorem for</p><formula xml:id="formula_24">u, v ∈ Ω g(u) − g(v) 2 M u − v 2 ,<label>(17)</label></formula><p>in which M is an upper bound on the spectral norm the Jacobian H of g. The spectral norm is given by the largest singular value of the Jacobian of g. If the largest singular value of the Jacobian is smaller than 1, the mapping g of the mean and variance to the mean and variance in the next layer is contracting. We show that the largest singular value is smaller than 1 by evaluating the function for the singular value S(µ, ω, ν, τ, λ, α) on a grid. Then we use the Mean Value Theorem to bound the deviation of the function S between grid points. To this end, we have to bound the gradient of S with respect to (µ, ω, ν, τ ). If all function values plus gradient times the deltas (differences between grid points and evaluated points) is still smaller than 1, then we have proofed that the function is below 1 (Lemma 12). To show that the mapping does not map outside the function's domain, we derive bounds on the expressions for the mean and the variance (Lemma 13). Section A3. <ref type="bibr" target="#b8">4</ref>.1 and Section A3. <ref type="bibr" target="#b8">4</ref>.2 are concerned with the contraction mapping and the image of the function domain of g, respectively.</p><p>With the results that the largest singular value of the Jacobian is smaller than one (Lemma 12) and that the mapping stays in the domain Ω (Lemma 13), we can prove Theorem 1. We first recall Theorem 1:</p><p>Theorem (Stable and Attracting Fixed Points). We assume α = α 01 and λ = λ 01 . We restrict the range of the variables to the domain µ ∈ [−0. Proof. According to Lemma 12 the mapping g (Eq. (4) and Eq. (5)) is a contraction mapping in the given domain, that is, it has a Lipschitz constant smaller than one. We showed that (µ, ν) = (0, 1) is a fixed point of the mapping for (ω, τ ) = (0, 1).</p><p>The domain is compact (bounded and closed), therefore it is a complete metric space. We further have to make sure the mapping g does not map outside its domain Ω. According to <ref type="bibr">Lemma</ref>  </p><formula xml:id="formula_25">µ, ω, ν, τ, λ, α) given in Eq. (5)ν (µ, ω, ν, τ, λ 01 , α 01 ) &lt; ν .<label>(18)</label></formula><p>The variance decreases in <ref type="bibr" target="#b7">[3,</ref><ref type="bibr" target="#b20">16]</ref> and all fixed points (µ, ν) of mapping Eq. (5) and Eq. (4) have ν &lt; 3.</p><p>Proof. We start to consider an even larger domain −1 µ 1, −0.1 ω 0.1, 1.5 ν 16, and 0.8 τ 1. <ref type="bibr" target="#b29">25</ref>. We prove facts for this domain and later restrict to 3 ν 16, i.e. Ω ++ . We consider the function g of the difference between the second momentξ in the next layer and the variance ν in the lower layer:</p><formula xml:id="formula_26">g(µ, ω, ν, τ, λ 01 , α ) =ξ(µ, ω, ν, τ, λ , α ) − ν .<label>(19)</label></formula><p>If we can show that g(µ, ω, ν, τ, λ 01 , α 01 ) &lt; 0 for all (µ, ω, ν, τ ) ∈ Ω ++ , then we would obtain our desired resultν ξ &lt; ν. The derivative with respect to ν is according to Theorem 16:</p><formula xml:id="formula_27">∂ ∂ν g(µ, ω, ν, τ, λ 01 , α 01 ) = ∂ ∂νξ (µ, ω, ν, τ, λ 01 , α01) − 1 &lt; 0 .<label>(20)</label></formula><p>Therefore g is strictly monotonically decreasing in ν. Sinceξ is a function in ντ (these variables only appear as this product), we have for x = ντ</p><formula xml:id="formula_28">∂ ∂νξ = ∂ ∂xξ ∂x ∂ν = ∂ ∂xξ τ<label>(21)</label></formula><p>and</p><formula xml:id="formula_29">∂ ∂τξ = ∂ ∂xξ ∂x ∂τ = ∂ ∂xξ ν .<label>(22)</label></formula><p>Therefore we have according to Theorem 16:</p><formula xml:id="formula_30">∂ ∂τξ (µ, ω, ν, τ, λ 01 , α01) = ν τ ∂ ∂νξ (µ, ω, ν, τ, λ 01 , α01) &gt; 0 .<label>(23)</label></formula><p>Therefore</p><formula xml:id="formula_31">∂ ∂τ g(µ, ω, ν, τ, λ 01 , α01) = ∂ ∂τξ (µ, ω, ν, τ, λ 01 , α01) &gt; 0 .<label>(24)</label></formula><p>Consequently, g is strictly monotonically increasing in τ . Now we consider the derivative with respect to µ and ω. We start with ∂ ∂µξ (µ, ω, ν, τ, λ, α), which is ∂ ∂µξ (µ, ω, ν, τ, λ, α) = (25)</p><formula xml:id="formula_32">λ 2 ω α 2 −e µω+ ντ 2 erfc µω + ντ √ 2 √ ντ + α 2 e 2µω+2ντ erfc µω + 2ντ √ 2 √ ντ + µω 2 − erfc µω √ 2 √ ντ + 2 π √ ντ e − µ 2 ω 2 2ντ</formula><p>.</p><p>We consider the sub-function</p><formula xml:id="formula_33">2 π √ ντ − α 2 e µω+ντ √ 2 √ ντ 2 erfc µω + ντ √ 2 √ ντ − e µω+2ντ √ 2 √ ντ 2 erfc µω + 2ντ √ 2 √ ντ .<label>(26)</label></formula><p>We set x = ντ and y = µω and obtain π</p><formula xml:id="formula_34">√ x − α 2 e x+y √ 2 √ x 2 erfc x + y √ 2 √ x − e 2x+y √ 2 √ x 2 erfc 2x + y √ 2 √ x .<label>(27)</label></formula><p>The derivative to this sub-function with respect to y is</p><formula xml:id="formula_35">α 2 e (2x+y) 2 2x (2x + y) erfc 2x+y √ 2 √</formula><p>x − e</p><formula xml:id="formula_36">(x+y) 2 2x (x + y) erfc x+y √ 2 √ x x = (28) √ 2α 2 √ x   e (2x+y) 2 2x (x+y) erfc x+y √ 2 √ x √ 2 √ x − e (x+y) 2 2x (x+y) erfc x+y √ 2 √ x √ 2 √ x   x &gt; 0 .</formula><p>The inequality follows from Lemma 24, which states that ze z 2 erfc(z) is monotonically increasing in z. Therefore the sub-function is increasing in y. The derivative to this sub-function with respect to x is</p><formula xml:id="formula_37">1 √ πx 2 √ πα 2 e (2x+y) 2 2x 4x 2 − y 2 erfc 2x + y √ 2 √ x (29) −e (x+y) 2 2x (x − y)(x + y) erfc x + y √ 2 √ x − √ α 2 − 1 x 3/2 .</formula><p>The sub-function is increasing in x, since the derivative is larger than zero:</p><formula xml:id="formula_38">√ πα 2 e (2x+y) 2 2x 4x 2 − y 2 erfc 2x+y √ 2 √ x − e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x − √ 2x 3/2 α 2 − 1 2 √ πx 2 (30) √ πα 2    (2x−y)(2x+y)2 √ π 2x+y √ 2 √ x + 2x+y √ √ x 2 +2 − (x−y)(x+y)2 √ π x+y √ 2 √ x + x+y √ 2 √ x 2 + 4 π    − √ 2x 3/2 α − 1 2 √ πx 2 = √ πα 2 (2x−y)(2x+y)2( √ 2 √ x) √ π 2x+y+ √ (2x+y) 2 +4x − (x−y)(x+y)2( √ 2 √ x) √ π x+y+ √ (x+y) 2 + 8x π − √ 2x 3/2 α 2 − 1 2 √ πx 2 = √ πα 2 (2x−y)(2x+y)2 √ π 2x+y+ √ (2x+y) 2 +4x − (x−y)(x+y)2 √ π x+y+ √ (x+y) 2 + 8x π − x α 2 − 1 √ 2 √ πx 3/2 &gt; √ πα 2 (2x−y)(2x+y)2 √ π 2x+y+ √ (2x+y) 2 +2(2x+y)+1 − (x−y)(x+y)2 √ π x+y+ √ (x+y) 2 +0.878•2(x+y)+0.878 2 − x α 2 − 1 √ 2 √ πx 3/2 = √ πα 2 (2x−y)(2x+y)2 √ π 2x+y+ √ (2x+y+1) 2 − (x−y)(x+y)2 √ π x+y+ √ (x+y+0.878) 2 − x α 2 − 1 √ 2 √ πx 3/2 = √ πα 2 (2x−y)(2x+y)2 √ π(2(2x+y)+1) − (x−y)(x+y)2 √ π(2(x+y)+0.878) − x α 2 − 1 √ 2 √ πx 3/2 = √ πα 2 (2(x+y)+0.878)(2x−y)(2x+y)2 √ π − (x−y)(x+y)(2(2x+y)+1)2 √ π (2(2x + y) + 1)(2(x + y) + 0.878) √ 2 √ πx 3/2 + √ πα 2 −x α 2 − 1 (2(2x + y) + 1)(2(x + y) + 0.878) (2(2x + y) + 1)(2(x + y) + 0.878) √ 2 √ πx 3/2 = 8x 3 + 12x 2 y + 4.14569x 2 + 4xy 2 − 6.76009xy − 1.58023x + 0.683154y 2 (2(2x + y) + 1)(2(x + y) + 0.878) √ √ πx 3/2 &gt; 8x 3 − 0.1 • 12x + 4.14569x 2 + 4 • (0.0) 2 x − 6.76009 • 0.1x − 1.58023x + 0.683154 • (0.0) 2 (2(2x + y) + 1)(2(x + y) + 0.878) √ 2 √ πx 3/2 = 8x 2 + 2.94569x − 2.25624 (2(2x + y) + 1)(2(x + y) + 0.878) √ 2 √ π √ x = 8(x − 0.377966)(x + 0.746178) (2(2x + y) + 1)(2(x + y) + 0.878) √ 2 √ π √ x &gt; 0 .</formula><p>We explain this chain of inequalities:</p><p>• First inequality: We applied Lemma 22 two times.</p><p>• Equalities factor out √ 2 √ x and reformulate.</p><p>• Second inequality part 1: we applied</p><formula xml:id="formula_39">0 &lt; 2y =⇒ (2x + y) 2 + 4x + 1 &lt; (2x + y) 2 + 2(2x + y) + 1 = (2x + y + 1) . (31)</formula><p>• Second inequality part 2: we show that for a = • Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x + y) + 0.878).</p><p>• We set α = α 01 and multiplied out. Thereafter we also factored out x in the numerator. </p><formula xml:id="formula_40">2 − erfc µω √ 2 √ ντ &gt; −0.1e 0.1 2 2•1.2 2 − erfc 0.1 √ 2 √ 1.2 .<label>(34)</label></formula><p>We compute the minimum of the term in brackets of ∂ ∂µξ (µ, ω, ν, τ, λ, α) in Eq. <ref type="formula">25</ref>: </p><formula xml:id="formula_41">µωe µ 2 ω 2 2ντ 2 − erfc µω √ 2 √ ντ + (35) α 2 01 − e µω+ντ √ √ ντ erfc µω + ντ √ 2 √ ντ − e µω+2ντ √ 2 √ ντ 2 erfc µω + 2ντ √ 2 √ ντ + 2 π √ ντ &gt; α 2 01 − e 1.2−0.1 √ 2 √ 1.2 2 erfc 1.2 − 0.1 √ 2 √ 1.2 − e 2•1.2−0.1 √ 2 √ 1.2 2 erfc 2 • 1.2 − 0.1 √ 2 √ 1.2 − 0.</formula><formula xml:id="formula_42">∂ ∂ωξ (µ, ω, ν, τ, λ 01 , α01) = µ ω ∂ ∂µξ (µ, ω, ν, τ, λ 01 , α01) .<label>(38)</label></formula><p>Since ∂ ∂µξ has the sign of ω, ∂ ∂µξ has the sign of µ. Therefore</p><formula xml:id="formula_43">∂ ∂ω g(µ, ω, ν, τ, λ 01 , α01) = ∂ ∂ωξ (µ, ω, ν, τ, λ 01 , α01)<label>(39)</label></formula><p>has the sign of µ.</p><p>We now divide the µ-domain into −1 µ 0 and 0 µ 1. Analogously we divide the ω-domain into −0.1 ω 0 and 0 ω 0.1. In this domains g is strictly monotonically.</p><p>For all domains g is strictly monotonically decreasing in ν and strictly monotonically increasing in τ . Note that we now consider the range 3 ν 16. For the maximal value of g we set ν = 3 (we set it to 3!) and τ = 1. <ref type="bibr" target="#b29">25</ref>.</p><p>We consider now all combination of these domains:</p><p>• −1 µ 0 and −0.1 ω 0:</p><p>g is decreasing in µ and decreasing in ω. We set µ = −1 and ω = −0. <ref type="bibr" target="#b5">1</ref></p><formula xml:id="formula_44">. g(−1, −0.1, 3, 1.25, λ 01 , α 01 ) = −0.0180173 .<label>(40)</label></formula><p>• −1 µ 0 and 0 ω 0.1:</p><p>g is increasing in µ and decreasing in ω. We set µ = 0 and ω = 0.</p><formula xml:id="formula_45">g(0, 0, 3, 1.25, λ 01 , α 01 ) = −0.148532 .<label>(41)</label></formula><p>• 0 µ 1 and −0.1 ω 0:</p><p>g is decreasing in µ and increasing in ω. We set µ = 0 and ω = 0. g(0, 0, 3, 1.25, λ 01 , α 01 ) = −0.148532 .</p><p>• 0 µ and ω 0.1:</p><p>g is increasing in µ and increasing in ω. We set µ = and ω = 0.1.</p><formula xml:id="formula_47">g(1, 0.1, 3, 1.25, λ 01 , α 01 ) = −0.0180173 .<label>(43)</label></formula><p>Therefore the maximal value of g is −0.0180173.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.3 Proof of Theorem 3</head><p>First we recall Theorem 3: Theorem (Increasing ν). We consider λ = λ 01 , α = α 01 and the two domains</p><formula xml:id="formula_48">Ω − 1 = {(µ, ω, ν, τ ) | − 0.1 µ 0.1, −0.1 ω 0.1, 0.05 ν 0.16, 0.8 τ 1.25} and Ω − 2 = {(µ, ω, ν, τ ) | − 0.1 µ 0.1, −0.1 ω 0.1, 0.05 ν 0.24, 0.9 τ 1.25} . The mapping of the varianceν(µ, ω, ν, τ, λ, α) given in Eq. (5) increases ν(µ, ω, ν, τ, λ 01 , α 01 ) &gt; ν (44) in both Ω − 1 and Ω − 2 .</formula><p>All fixed points (µ, ν) of mapping Eq. <ref type="formula">5</ref>and Eq. (4) ensure for 0.8 τ that ν &gt; 0.16 and for 0.9 τ thatν &gt; 0. <ref type="bibr" target="#b28">24</ref>. Consequently, the variance mapping Eq. <ref type="formula">5</ref>and Eq. <ref type="formula" target="#formula_3">4</ref>ensures a lower bound on the variance ν.</p><p>Proof. The mean value theorem states that there exists a t ∈ [0, 1] for which</p><formula xml:id="formula_49">ξ(µ, ω, ν, τ, λ 01 , α 01 ) −ξ(µ, ω, ν min , τ, λ 01 , α 01 ) = (45) ∂ ∂νξ (µ, ω, ν + t(ν min − ν), τ, λ 01 , α 01 ) (ν − ν min ) . Thereforeξ (µ, ω, ν, τ, λ 01 , α 01 ) =ξ(µ, ω, ν min , τ, λ 01 , α 01 ) + (46) ∂ ∂νξ (µ, ω, ν + t(ν min − ν), τ, λ 01 , α 01 ) (ν − ν min ) .</formula><p>Therefore we are interested to bound the derivative of the ξ-mapping Eq. <ref type="formula">13</ref>with respect to ν:</p><formula xml:id="formula_50">∂ ∂νξ (µ, ω, ν, τ, λ 01 , α 01 ) = (47) 1 2 λ 2 τ e − µ 2 ω 2ντ α 2 − e µω+ντ √ 2 √ ντ 2 erfc µω + ντ √ 2 √ ντ − 2e µω+2ντ √ 2 √ ντ 2 erfc µω + 2ντ √ √ ντ − erfc µω √ 2 √ ντ + 2 .</formula><p>The sub-term Eq. (308) enters the derivative Eq. (47) with a negative sign! According to Lemma 18, the minimal value of sub-term Eq. (308) is obtained by the largest largest ν, by the smallest τ , and the largest y = µω = 0.01. Also the positive term erfc</p><formula xml:id="formula_51">µω √ 2 √</formula><p>ντ + 2 is multiplied by τ , which is minimized by using the smallest τ . Therefore we can use the smallest τ in whole formula Eq. (47) to lower bound it.</p><p>First we consider the domain 0.05 ν 0.16 and 0.8 τ 1. <ref type="bibr" target="#b29">25</ref>. The factor consisting of the exponential in front of the brackets has its smallest value for e − 0.01•0. </p><formula xml:id="formula_52">1 2 λ 2 τ e − µ 2 ω 2 2ντ α 2 − e µω+ντ √ 2 √ ντ 2 erfc µω + ντ √ 2 √ ντ − 2e µω+2ντ √ 2 √ ντ 2 erfc µω + 2ντ √ 2 √ ντ − (48) erfc µω √ 2 √ ντ + 2 &gt; 1 2 0.8e − 0.01•0.01 2•0.05•0.8 λ 2 01 α 2 01 − e 0.16•0.8+0.01 √ 2 √ 0.16•0.8 2 erfc 0.16 • 0.8 + 0.01 √ 2 √ 0.16 • 0.8 − 2e 2•0.16•0.8+0.01 √ 2 √ 0.16•0.8 2 erfc 2 • 0.16 • 0.8 + 0.01 √ 2 √ 0.16 • 0.8 − erfc − 0.01 √ 2 √ 0.05 • 0.8 + 2 ) &gt; 0.969231 .</formula><p>For applying the mean value theorem, we require the smallestν(ν). We follow the proof of Lemma 8, which shows that at the minimum y = µω must be maximal and x = ντ must be minimal. Thus, the smallestξ(µ, ω, ν, τ, λ 01 , α 01 ) isξ(0.01, 0.01, 0.05, 0.8, λ 01 , α 01 ) = 0.0662727 for 0.05 ν and 0.8 τ .</p><p>Therefore the mean value theorem and the bound on (μ) (Lemma 43) providẽ</p><formula xml:id="formula_53">ν =ξ(µ, ω, ν, τ, λ 01 , α 01 ) − (μ(µ, ω, ν, τ, λ 01 , α 01 )) 2 &gt; (49) 0.0662727 + 0.969231(ν − 0.05) − 0.005 = 0.01281115 + 0.969231ν &gt; 0.08006969 • 0.16 + 0.969231ν 1.049301ν &gt; ν .</formula><p>Next we consider the domain 0.05 ν 0.24 and 0.9 τ 1. <ref type="bibr" target="#b29">25</ref>. The factor consisting of the exponential in front of the brackets has its smallest value for e − 0. Thus, applying Lemma 18, we obtain the lower bound on the derivative:</p><formula xml:id="formula_54">1 λ 2 τ e − µ 2 ω 2 2ντ α 2 − e µω+ντ √ 2 √ ντ 2 erfc µω + ντ √ 2 √ ντ − 2e µω+2ντ √ 2 √ ντ 2 erfc µω + 2ντ √ 2 √ ντ − (50) erfc µω √ 2 √ ντ + 2 &gt; 1 2 0.9e − 0.01•0.01 2•0.05•0.9 λ 2 01 α 2 01 − e 0.24•0.9+0.01 √ 2 √ 0.24•0.9 2 erfc 0.24 • 0.9 + 0.01 √ √ 0.24 • 0.9 − 2e 2•0.24•0.9+0.01 √ 2 √ 0.24•0.9 2 erfc 2 • 0.24 • 0.9 + 0.01 √ √ 0.24 • 0.9 − erfc − 0.01 √ 2 √ 0.05 • 0.9 + 2 ) &gt; 0.976952 .</formula><p>For applying the mean value theorem, we require the smallestν(ν). We follow the proof of Lemma 8, which shows that at the minimum y = µω must be maximal and x = ντ must be minimal. Thus, the smallestξ(µ, ω, ν, τ, λ 01 , α 01 ) isξ(0.01, 0.01, 0.05, 0.9, λ 01 , α 01 ) = 0.0738404 for 0.05 ν and 0.9 τ . Therefore the mean value theorem and the bound on (μ) 2 (Lemma 43) gives</p><formula xml:id="formula_55">ν =ξ(µ, ω, ν, τ, λ 01 , α 01 ) − (μ(µ, ω, ν, τ, λ 01 , α 01 )) 2 &gt; (51) 0.0738404 + 0.976952(ν − 0.05) − 0.005 = 0.0199928 + 0.976952ν &gt; 0.08330333 • 0.24 + 0.976952ν 1.060255ν &gt; ν .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.4 Lemmata and Other Tools Required for the Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.4.1 Lemmata for proofing Theorem 1 (part 1): Jacobian norm smaller than one</head><p>In this section, we show that the largest singular value of the Jacobian of the mapping g is smaller than one. Therefore, g is a contraction mapping. This is even true in a larger domain than the original Ω. We do not need to restrict τ ∈ [0.95, 1.1], but we can extend to τ ∈ [0.8, <ref type="bibr">1.25]</ref>. The range of the other variables is unchanged such that we consider the following domain throughout this section:</p><formula xml:id="formula_56">µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].</formula><p>Jacobian of the mapping. In the following, we denote two Jacobians: (1) the Jacobian J of the mapping h : (µ, ν) → (μ,ξ), and (2) the Jacobian H of the mapping g : (µ, ν) → (μ,ν) because the influence ofμ onν is small, and many properties of the system can already be seen on J .</p><formula xml:id="formula_57">J = J 11 J 12 J 21 J 22 = ∂ ∂µμ ∂ ∂νμ ∂ ∂µξ ∂ ∂νξ (52) H = H H 12 H 21 H 22 = J 11 J 12 J 21 − 2μJ 11 J 22 − 2μJ 12 (53)</formula><p>The definition of the entries of the Jacobian J is:</p><formula xml:id="formula_58">J 11 (µ, ω, ν, τ, λ, α) = ∂ ∂µμ (µ, ω, ν, τ, λ, α) = (54) 1 2 λω αe µω+ ντ 2 erfc µω + ντ √ 2 √ ντ − erfc µω √ √ ντ + J 12 (µ, ω, ν, τ, λ, α) = ∂ ∂νμ (µ, ω, ν, τ, λ, α) = (55) 1 4 λτ αe µω+ ντ 2 erfc µω + ντ √ 2 √ ντ − (α − 1) 2 πντ e − µ 2 ω 2 2ντ J 21 (µ, ω, ν, τ, λ, α) = ∂ ∂µξ (µ, ω, ν, τ, λ, α) = (56) λ 2 ω α 2 −e µω+ ντ 2 erfc µω + ντ √ 2 √ ντ + α 2 e 2µω+2ντ erfc µω + 2ντ √ 2 √ ντ + µω 2 − erfc µω √ 2 √ ντ + 2 π √ ντ e − µ 2 ω 2 2ντ J 22 (µ, ω, ν, τ, λ, α) = ∂ ∂νξ (µ, ω, ν, τ, λ, α) = (57) 1 λ 2 τ α 2 −e µω+ ντ 2 erfc µω + ντ √ √ ντ + 2α 2 e 2µω+2ντ erfc µω + 2ντ √ 2 √ ντ − erfc µω √ √ ντ + 2</formula><p>Proof sketch: Bounding the largest singular value of the Jacobian. If the largest singular value of the Jacobian is smaller than 1, then the spectral norm of the Jacobian is smaller than 1. Then the mapping Eq. (4) and Eq. (5) of the mean and variance to the mean and variance in the next layer is contracting.</p><p>We show that the largest singular value is smaller than 1 by evaluating the function S(µ, ω, ν, τ, λ, α) on a grid. Then we use the Mean Value Theorem to bound the deviation of the function S between grid points. Toward this end we have to bound the gradient of S with respect to (µ, ω, ν, τ ). If all function values plus gradient times the deltas (differences between grid points and evaluated points) is still smaller than 1, then we have proofed that the function is below 1.</p><p>The singular values of the 2 × 2 matrix</p><formula xml:id="formula_59">A = a 11 a 12 a 21 a 22<label>(58)</label></formula><p>are</p><formula xml:id="formula_60">s 1 = 1 (a 11 + a 22 ) 2 + (a 21 − a 12 ) 2 + (a 11 − a 22 ) 2 + (a 12 + a 21 ) 2 (59) s 2 = 1 (a 11 + a 22 ) 2 + (a 21 − a 12 ) 2 − (a 11 − a 22 ) 2 + (a 12 + a 21 ) 2 .<label>(60)</label></formula><p>We used an explicit formula for the singular values <ref type="bibr" target="#b8">[4]</ref>. We now set H 11 = a 11 , H 12 = a 12 , H 21 = a 21 , H 22 = a 22 to obtain a formula for the largest singular value of the Jacobian depending on (µ, ω, ν, τ, λ, α). The formula for the largest singular value for the Jacobian is:</p><formula xml:id="formula_61">S(µ, ω, ν, τ, λ, α) = (H 11 + H 22 ) 2 + (H 21 − H 12 ) 2 + (H 11 − H 22 ) 2 + (H 12 + H 21 ) 2 = (61) = 1 (J 11 + J − 2μJ 12 ) 2 + (J 21 − 2μJ − J 12 ) 2 + (J 11 − J 22 + 2μJ 12 ) 2 + (J 12 + J 21 − 2μJ 11 ) 2 ,</formula><p>where J are defined in Eq. (54) and we left out the dependencies on (µ, ω, ν, τ, λ, α) in order to keep the notation uncluttered, e.g. we wrote J 11 instead of J 11 (µ, ω, ν, τ, λ, α).</p><p>Bounds on the derivatives of the Jacobian entries. In order to bound the gradient of the singular value, we have to bound the derivatives of the Jacobian entries</p><formula xml:id="formula_62">J 11 (µ, ω, ν, τ, λ, α), J 12 (µ, ω, ν, τ, λ, α), J 21 (µ, ω, ν, τ, λ, α)</formula><p>, and J 22 (µ, ω, ν, τ, λ, α) with respect to µ, ω, ν, and τ . The values λ and α are fixed to λ 01 and α 01 . The 16 derivatives of the 4 Jacobian entries with respect to the 4 variables are:</p><formula xml:id="formula_63">∂J 11 ∂µ = 1 2 λω 2 e − µ 2 ω 2 2ντ   αe (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ − 2 π (α − 1) √ ντ   (62) ∂J 11 ∂ω = 1 2 λ   −e − µ 2 ω 2 2ντ   2 π (α − 1)µω √ ντ − α(µω + 1)e (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ   − erfc µω √ 2 √ ντ + 2 ∂J 11 ∂ν = 1 4 λτ ωe − µ 2 ω 2 2ντ αe (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 2 π (α − 1)µω (ντ ) 3/2 − α √ ντ ∂J 11 ∂τ = 1 4 λνωe − µ 2 ω 2 2ντ αe (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 2 π (α − 1)µω (ντ ) 3/2 − α √ ντ ∂J 12 ∂µ = ∂J 11 ∂ν ∂J 12 ∂ω = 1 4 λµτ e − µ 2 ω 2 2ντ αe (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 2 π (α − 1)µω (ντ ) 3/2 − α √ ντ ∂J 12 ∂ν = 1 8 λe − µ 2 ω 2 2ντ ατ 2 e (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 2 π (−1)(α − 1)µ 2 ω 2 ν 5/2 √ τ + √ τ (α + αµω − 1) ν 3/2 − ατ 3/2 √ ν ∂J 12 ∂τ = 1 8 λe − µ 2 ω 2 2ντ 2αe (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + αντ e (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 2 π (−1)(α − 1)µ 2 ω 2 (ντ ) 3/2 + −α + αµω + 1 √ ντ − α √ ντ ∂J 21 ∂µ = λ 2 ω 2 α 2 −e − µ 2 ω 2 2ντ e (µω+ντ ) 2 2ντ erfc µω + ντ √ √ ντ + 2α 2 e (µω+2ντ ) 2 2ντ e − µ 2 ω 2 2ντ erfc µω + 2ντ √ 2 √ ντ − erfc µω √ 2 √ ντ + 2 ∂J ∂ω = λ 2 α 2 (µω + 1) −e − µ 2 ω 2 2ντ e (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + α 2 (2µω + 1)e (µω+2ντ ) 2 2ντ e − µ 2 ω 2 2ντ erfc µω + 2ντ √ √ ντ + 2µω 2 − erfc µω √ 2 √ ντ + 2 π √ ντ e − µ 2 ω 2 2ντ ∂J ∂ν = 1 2 λ 2 τ ωe − µ 2 ω 2 2ντ α 2 −e (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 4α 2 e (µω+2ντ ) 2 2ντ erfc µω + 2ντ √ 2 √ ντ + 2 π (−1) α 2 − 1 √ ντ   ∂J ∂τ = 1 λ 2 νωe − µ 2 ω 2 2ντ α 2 −e (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 4α 2 e (µω+2ντ ) 2 2ντ erfc µω + 2ντ √ 2 √ ντ + 2 π (−1) α 2 − 1 √ ντ   ∂J ∂µ = ∂J 21 ∂ν ∂J ∂ω = 1 λ 2 µτ e − µ 2 ω 2 2ντ α 2 −e (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 4α 2 e (µω+2ντ ) 2 2ντ erfc µω + 2ντ √ 2 √ ντ + 2 π (−1) α 2 − 1 √ ντ   ∂J ∂ν = 1 4 λ 2 τ 2 e − µ 2 ω 2 2ντ α 2 −e (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 8α 2 e (µω+2ντ ) 2 2ντ erfc µω + 2ντ √ 2 √ ντ + 2 π α 2 − 1 µω (ντ ) 3/2 − 3α 2 √ ντ ∂J ∂τ = 1 4 λ −2α 2 e − µ 2 ω 2 2ντ e (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ − α ντ e − µ 2 ω 2 2ντ e (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 4α 2 e (µω+2ντ ) 2 2ντ e − µ 2 ω 2 2ντ erfc µω + 2ντ √ 2 √ ντ + 8α 2 ντ e (µω+2ντ ) 2 2ντ e − µ 2 ω 2 2ντ erfc µω + 2ντ √ 2 √ ντ + 2 2 − erfc µω √ 2 √ ντ + 2 π e − µ 2 ω 2 2ντ α 2 − 1 µω √ ντ − 3α 2 √ ντ</formula><p>Lemma 5 (Bounds on the Derivatives). The following bounds on the absolute values of the derivatives of the Jacobian entries</p><formula xml:id="formula_64">J 11 (µ, ω, ν, τ, λ, α), J 12 (µ, ω, ν, τ, λ, α), J 21 (µ, ω, ν, τ, λ, α)</formula><p>, and J 22 (µ, ω, ν, τ, λ, α) with respect to µ, ω, ν, and τ hold: Proof. See proof 39.</p><formula xml:id="formula_65">∂J 11 ∂µ &lt; 0.0031049101995398316<label>(63)</label></formula><p>Bounds on the entries of the Jacobian. Lemma 6 (Bound on J11). The absolute value of the function Proof.</p><formula xml:id="formula_66">J 11 = 1 2 λω αe µω+ ντ 2 erfc µω+ντ √ 2 √ ντ − erfc µω √ 2 √ ντ + 2 is bounded by |J 11 | 0.104497 in the domain −0.1 µ 0.1, −0.1 ω 0.1, 0.</formula><formula xml:id="formula_67">|J 11 | = 1 2 λω αe µω+ ντ 2 erfc µω + ντ √ 2 √ ντ + 2 − erfc µω √ 2 √ ντ | 1 2 ||λ||ω| (|α|0.587622 + 1.00584) 0.104497,<label>(64)</label></formula><p>where we used that (a) J 11 is strictly monotonically increasing in µω and |2 − erfc  Proof. Bounds on mean, variance and second moment. For deriving bounds onμ,ξ, andν, we need the following lemma. Lemma 8 (Derivatives of the Mapping). We assume α = α 01 and λ = λ 01 . We restrict the range of the variables to the domain µ ∈</p><formula xml:id="formula_68">J 12 = 1 4 λτ αe µω+ ντ 2 erfc µω+ντ √ 2 √ ντ − (α − 1)</formula><formula xml:id="formula_69">|J 12 | 1 4 |λ||τ | αe µω+ ντ erfc µω + ντ √ 2 √ ντ − (α − 1) 2 πντ e − µ 2</formula><formula xml:id="formula_70">[−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25]. The derivative ∂ ∂µμ (µ, ω, ν, τ, λ, α) has the sign of ω. The derivative ∂ ∂νμ (µ, ω, ν, τ, λ, α) is positive. The derivative ∂ ∂µξ (µ, ω, ν, τ, λ, α) has the sign of ω.</formula><p>The derivative ∂ ∂νξ (µ, ω, ν, τ, λ, α) is positive.</p><p>Proof. See 40.</p><p>Lemma 9 (Bounds on mean, variance and second moment). The expressionsμ,ξ, andν for α = α 01 and λ = λ 01 are bounded by −0.041160 &lt;μ &lt; 0.087653, 0.703257 &lt;ξ &lt; 1.643705 and 0.695574</p><formula xml:id="formula_71">&lt;ν &lt; 1.636023 in the domain µ ∈ [−0.1, 0.1], ν ∈ [0.8, 15], ω ∈ [−0.1, 0.1], τ ∈ [0.8, 1.25].</formula><p>Proof. We use Lemma 8 which states that with given sign the derivatives of the mapping Eq. (4) and Eq. (5) with respect to ν and µ are either positive or have the sign of ω. Therefore with given sign of ω the mappings are strict monotonic and the their maxima and minima are found at the borders. The minimum ofμ is obtained at µω = −0.01 and its maximum at µω = 0.01 and σ and τ at minimal or maximal values, respectively. It follows that</p><formula xml:id="formula_72">−0.041160 &lt;μ(−0.1, 0.1, 0.8, 0.8, λ 01 , α 01 ) μ μ(0.1, 0.1, 1.5, 1.25, λ 01 , α 01 ) &lt; 0.087653.<label>(66)</label></formula><p>Similarly, the maximum and minimum ofξ is obtained at the values mentioned above:</p><formula xml:id="formula_73">0.703257 &lt;ξ(−0.1, 0.1, 0.8, 0.8, λ 01 , α 01 ) ξ ξ (0.1, 0.1, 1.5, 1.25, λ 01 , α 01 ) &lt; 1.643705.<label>(67)</label></formula><p>Hence we obtain the following bounds onν:</p><formula xml:id="formula_74">0.703257 −μ 2 &lt;ξ −μ 2 &lt; 1.643705 −μ 2 (68) 0.703257 − 0.007683 &lt;ν &lt; 1.643705 − 0.007682 0.695574 &lt;ν &lt; 1.636023.</formula><p>Upper Bounds on the Largest Singular Value of the Jacobian.</p><p>Lemma 10 (Upper Bounds on Absolute Derivatives of Largest Singular Value). We set α = α 01 and λ = λ 01 and restrict the range of the variables to µ</p><formula xml:id="formula_75">∈ [µ min , µ max ] = [−0.1, 0.1], ω ∈ [ω min , ω max ] = [−0.1, 0.1], ν ∈ [ν min , ν max ] = [0.8, 1.5], and τ ∈ [τ min , τ max ] = [0.8, 1.25].</formula><p>The absolute values of derivatives of the largest singular value S(µ, ω, ν, τ, λ, α) given in Eq. (61) with respect to (µ, ω, ν, τ ) are bounded as follows:</p><formula xml:id="formula_76">∂S ∂µ &lt; 0.32112 ,<label>(69)</label></formula><formula xml:id="formula_77">∂S ∂ω &lt; 2.63690 ,<label>(70)</label></formula><formula xml:id="formula_78">∂S ∂ν &lt; 2.28242 ,<label>(71)</label></formula><formula xml:id="formula_79">∂S ∂τ &lt; 2.98610 .<label>(72)</label></formula><p>Proof. The Jacobian of our mapping Eq. <ref type="formula" target="#formula_3">4</ref>and Eq. <ref type="formula">5</ref>is defined as</p><formula xml:id="formula_80">H = H 11 H 12 H 21 H 22 = J 11 J 12 J 21 − 2μJ 11 J 22 − 2μJ 12<label>(73)</label></formula><p>and has the largest singular value</p><formula xml:id="formula_81">S(µ, ω, ν, τ, λ, α) = 1 (H 11 − H 22 ) 2 + (H 12 + H 21 ) 2 + (H 11 + H 22 ) 2 + (H 12 − H 21 ) 2 ,<label>(74)</label></formula><p>according to the formula of Blinn <ref type="bibr" target="#b8">[4]</ref>. The distance of the singular value at S(µ, ω, ν, τ, λ 01 , α 01 ) and that at S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ 01 , α 01 ) is bounded as follows: We now apply Lemma 10 which gives bounds on the derivatives, which immediately gives the statement of the lemma. The the largest singular value of the Jacobian is smaller than 1:</p><formula xml:id="formula_82">|S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ 01 , α 01 ) − S(µ, ω, ν, τ, λ 01 , α 01 )| &lt;<label>(</label></formula><formula xml:id="formula_83">S(µ, ω, ν, τ, λ 01 , α 01 ) &lt; 1 .<label>(93)</label></formula><p>Therefore the mapping Eq. (4) and Eq. (5) is a contraction mapping.</p><p>Proof. We set ∆µ = 0.0068097371, ∆ω = 0.0008292885, ∆ν = 0.0009580840, and ∆τ = 0.0007323095.</p><p>According to Lemma 11 we have  Proof. Directly follows from Lemma 13.</p><formula xml:id="formula_84">|S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ 01 , α 01 ) − S(µ, ω, ν, τ, λ 01 , α 01 )| &lt;<label>(</label></formula><formula xml:id="formula_85">g(Ω ) ⊆ Ω ,<label>(98)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.4.3 Lemmata for proofing Theorem 2: The variance is contracting</head><p>Main Sub-Function. We consider the main sub-function of the derivate of second moment, J22 (Eq. (54)):</p><formula xml:id="formula_86">∂ ∂νξ = 1 λ 2 τ −α 2 e µω+ ντ erfc µω + ντ √ 2 √ ντ + 2α 2 e 2µω+2ντ erfc µω + 2ντ √ 2 √ ντ − erfc µω √ 2 √ ντ + 2<label>(99)</label></formula><p>that depends on µω and ντ , therefore we set x = ντ and y = µω. Algebraic reformulations provide the formula in the following form:</p><formula xml:id="formula_87">∂ ∂νξ = 1 2 λ 2 τ α 2 −e − y 2 2x e (x+y) 2 2x erfc y + x √ 2 √ x − 2e (2x+y) 2 2x erfc y + 2x √ 2 √ x − erfc y √ 2 √ x + 2<label>(100)</label></formula><p>For λ = λ 01 and α = α 01 , we consider the domain −1 µ 1, −0.1 ω 0.1, 1.5 ν 16, and, 0.8 τ 1. <ref type="bibr" target="#b29">25</ref>. </p><formula xml:id="formula_88">erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x (101)</formula><p>is smaller than zero, is strictly monotonically increasing in x, and strictly monotonically decreasing in y for the minimal x = 12/10 = 1.2.</p><p>Proof. See proof 44.</p><p>The graph of the subfunction in the specified domain is displayed in <ref type="figure" target="#fig_1">Figure A3</ref>. </p><formula xml:id="formula_89">∂ ∂νν (µ, ω, ν, τ, λ 01 , α 01 ) &lt; 1 .<label>(102)</label></formula><p>Proof. In this domain Ω + we have the following three properties (see further below): ∂ ∂νξ &lt; 1, µ &gt; 0, and ∂ ∂νμ &gt; 0. Therefore, we have</p><formula xml:id="formula_90">∂ ∂νν = ∂ ∂νξ − 2μ ∂ ∂νμ &lt; ∂ ∂νξ &lt; 1<label>(103)</label></formula><p>• We first proof that ∂ ∂νξ &lt; 1 in an even larger domain that fully contains Ω + . According to Eq. (54), the derivative of the mapping Eq. (5) with respect to the variance ν is</p><formula xml:id="formula_91">∂ ∂νξ (µ, ω, ν, τ, λ 01 , α 01 ) = (104) 1 2 λ 2 τ α 2 −e µω+ ντ 2 erfc µω + ντ √ 2 √ ντ + 2α 2 e 2µω+2ντ erfc µω + 2ντ √ 2 √ ντ − erfc µω √ 2 √ ντ + 2 .</formula><p>For </p><formula xml:id="formula_92">λ = λ 01 , α = α 01 , −1 µ 1, −0.1 ω 0.1</formula><formula xml:id="formula_93">erfc µω + ντ √ 2 √ ντ − 2e (µω+2ντ ) 2 2ντ erfc µω + 2ντ √ 2 √ ντ − erfc µω √ √ ντ + 2 1 1.25λ 2 α 01 e 1.2+0.1 √ 2 √ 1.2 2 erfc 1.2 + 0.1 √ 2 √ 1.2 − 2e 2•1.2+0.1 √ 2 √ 1.2 2 erfc 2 • 1.2 + 0.1 √ 2 √ 1.2 −e − µ 2 ω 2 2ντ − erfc µω √ 2 √ ντ + 2 1 2 1.25λ 2 01 −e 0.0 α 2 01 e 1.2+0.1 √ 2 √ 1.2 2 erfc 1.2 + 0.1 √ 2 √ 1.2 − 2e 2•1.2+0.1 √ 2 √ 1.2 2 erfc 2 • 1.2 + 0.1 √ 2 √ 1.2 − erfc µω √ 2 √ ντ + 2 1 2 1.25λ 2 01 −e 0.0 α 2 01 e 1.2+0.1 √ 2 √ 1.2 2 erfc 1.2 + 0.1 √ 2 √ 1.2 − 2e 2•1.2+0.1 √ 2 √ 1.2 2 erfc 2 • 1.2 + 0.1 √ 2 √ 1.2 − erfc 0.1 √ 2 √ 1.2 + 2 0.995063 &lt; 1 .</formula><p>We explain the chain of inequalities:</p><p>-First equality brings the expression into a shape where we can apply Lemma 15 for the the function Eq. (101). -First inequality: The overall factor τ is bounded by 1. <ref type="bibr" target="#b29">25</ref>. We insert these values into the expression.</p><p>-Third inequality: We use for the whole expression the maximal factor e − µ 2 ω 2 2ντ &lt; 1 by setting this factor to 1.</p><p>-Fourth inequality: erfc is strictly monotonically decreasing. Therefore we maximize its argument to obtain the least value which is subtracted. We use the minimal x = ντ • We now show thatμ &gt; 0. The expressionμ(µ, ω, ν, τ ) (Eq. (4)) is strictly monotonically increasing im µω and ντ . Therefore, the minimal value in Ω + is obtained at µ(0.01, 0.01, 1.5, 0.8) = 0.008293 &gt; 0.</p><p>• Last we show that ∂ ∂νμ &gt; 0. The expression ∂ ∂νμ (µ, ω, ν, τ ) = J 12 (µ, ω, ν, τ ) (Eq. (54)) can we reformulated as follows:</p><formula xml:id="formula_94">J 12 (µ, ω, ν, τ, λ, α) = λτ e − µ 2 ω 2 2ντ √ παe (µω+ντ ) 2 2ντ erfc µω+ντ √ 2 √ ντ − √ 2(α−1) √ ντ 4 √ π<label>(108)</label></formula><p>is larger than zero when the term √ παe</p><formula xml:id="formula_95">(µω+ντ ) 2 2ντ erfc µω+ντ √ 2 √ ντ − √ 2(α−1) √ ντ</formula><p>is larger than zero. This term obtains its minimal value at µω = 0.01 and ντ = 16 • 1.25, which can easily be shown using the Abramowitz bounds (Lemma 22) and evaluates to 0.16, therefore</p><formula xml:id="formula_96">J 12 &gt; 0 in Ω + .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.4.4 Lemmata for proofing Theorem 3: The variance is expanding</head><p>Main Sub-Function From Below. We consider functions in µω and ντ , therefore we set x = µω and y = ντ . In this domain, we consider the main sub-function of the derivate of second moment in the next layer, J22 (Eq. (54)):</p><formula xml:id="formula_97">∂ ∂νξ = 1 λ 2 τ −α 2 e µω+ ντ 2 erfc µω + ντ √ 2 √ ντ + 2α 2 e 2µω+2ντ erfc µω + 2ντ √ 2 √ ντ − erfc µω √ 2 √ ντ + 2<label>(109)</label></formula><p>that depends on µω and ντ , therefore we set x = ντ and y = µω. Algebraic reformulations provide the formula in the following form:  </p><formula xml:id="formula_98">∂ ∂νξ = (110) 1 λ 2 τ α 2 −e − y 2 2x e (x+y) 2 2x erfc y + x √ 2 √ x − 2e (2x+y) 2 2x erfc y + 2x √ 2 √ x − erfc y √ 2 √ x + 2<label>Lemma</label></formula><formula xml:id="formula_99">erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x<label>(</label></formula><formula xml:id="formula_100">2 erfc µω + ντ √ 2 √ ντ − 2e µω+2•ντ √ 2 √ ντ 2 erfc µω + 2ντ √ 2 √ ντ .<label>(112)</label></formula><p>The derivative of the equation above with respect to</p><p>• ν is larger than zero;</p><p>• τ is smaller than zero for maximal ν = 0.7, ν = 0.16, and ν = 0.24 (with 0.9 τ );</p><p>• y = µω is larger than zero for ντ = 0.008750. <ref type="bibr">8</ref>  Error Analysis. We investigate the error propagation for the singular value (Eq. (61)) if the function arguments µ, ω, ν, τ suffer from numerical imprecisions up to . To this end, we first derive error propagation rules based on the mean value theorem and then we apply these rules to the formula for the singular value. </p><p>It follows that for computation with error ∆x, there exists a t ∈ [0, 1] with</p><formula xml:id="formula_102">|f (x + ∆x) − f (x)| ∇f (x + t∆x) ∆x .<label>(114)</label></formula><p>Therefore the increase of the norm of the error after applying function f is bounded by the norm of the gradient ∇f (x + t∆x) .</p><p>We now compute for the functions, that we consider their gradient and its 2-norm:</p><p>• addition:</p><formula xml:id="formula_103">f (x) = x 1 + x 2 and ∇f (x) = (1, 1), which gives ∇f (x) = √ 2.</formula><p>We further know that</p><formula xml:id="formula_104">|f (x + ∆x) − f (x)| = |x 1 + x 2 + ∆x 1 + ∆x 2 − x 1 − x 2 | |∆x 1 | + |∆x 2 | .<label>(115)</label></formula><p>Adding n terms gives:</p><formula xml:id="formula_105">n i=1 x i + ∆x i − n i=1 x i n i=1 |∆x i | n |∆x i | max .<label>(116)</label></formula><p>• subtraction:</p><formula xml:id="formula_106">f (x) = x 1 − x 2 and ∇f (x) = (1, −1), which gives ∇f (x) = √ 2.</formula><p>We further know that</p><formula xml:id="formula_107">|f (x + ∆x) − f (x)| = |x 1 − x 2 + ∆x 1 − ∆x 2 − x 1 + x 2 | |∆x 1 | + |∆x 2 | .<label>(117)</label></formula><p>Subtracting n terms gives:</p><formula xml:id="formula_108">n i=1 −(x i + ∆x i ) + n i=1 x i n i=1 |∆x i | n |∆x i | max .<label>(118)</label></formula><p>• multiplication:</p><formula xml:id="formula_109">f (x) = x 1 x 2 and ∇f (x) = (x 2 , x 1 ), which gives ∇f (x) = x .</formula><p>We further know that</p><formula xml:id="formula_110">|f (x + ∆x) − f (x)| = |x 1 • x 2 + ∆x 1 • x 2 + ∆x 2 • x 1 + ∆x 1 • ∆x s − x 1 • x 2 | (119) |∆x 1 | |x 2 | + |∆x 2 | |x 1 | + O(∆ 2 ) .</formula><p>Multiplying n terms gives:</p><formula xml:id="formula_111">n i=1 (x i + ∆x i ) − n i=1 x i = n i=1 x i n i=1 ∆x i x i + O(∆ ) (120) n i=1 |x i | n i=1 ∆x i x i + O(∆ 2 ) n n i=1 |x i | ∆x i x i max + O(∆ ) .</formula><p>• division:</p><formula xml:id="formula_112">f (x) = x1 x2 and ∇f (x) = 1 x2 , − x1 x 2 2 , which gives ∇f (x) = x x 2 2 .</formula><p>We further know that</p><formula xml:id="formula_113">|f (x + ∆x) − f (x)| = x 1 + ∆x 1 x 2 + ∆x 2 − x 1 x 2 = (x 1 + ∆x 1 )x 2 − x 1 (x 2 + ∆x 2 ) (x 2 + ∆x 2 )x 2 = (121) ∆x 1 • x 2 − ∆x 2 • x 1 x 2 2 + ∆x 2 • x 2 = ∆x 1 x 2 − ∆x 2 • x 1 x 2 2 + O(∆ 2 ) .</formula><p>• square root: Proof. We have the numerical precision of the parameters µ, ω, ν, τ , that we denote by ∆µ, ∆ω, ∆ν, ∆τ together with our domain Ω.</p><formula xml:id="formula_114">f (x) = √ x and f (x) = 1 2 √ x , which gives |f (x)| = 1 2 √ x . • exponential function: f (x) = exp(x) and f (x) = exp(x), which gives |f (x)| = exp(x). • error function: f (x) = erf(x) and f (x) = 2 √ π exp(−x 2 ), which gives |f (x)| = 2 √ π exp(−x 2 ). • complementary error function: f (x) = erfc(x) and f (x) = − 2 √ π exp(−x 2 ), which gives |f (x)| = 2 √ π exp(−x 2 ).</formula><p>With the error propagation rules that we derived in Subsection A3. <ref type="bibr" target="#b8">4</ref>.5, we can obtain bounds for the numerical errors on the following simple expressions:</p><formula xml:id="formula_115">∆ (µω) ∆µ |ω| + ∆ω |µ| 0.2 (122) ∆ (ντ ) ∆ν |τ | + ∆τ |ν| 1.5 + 1.5 = 3 ∆ ντ (∆(ντ )2 + ∆2 |ντ |) 1 2 2 (6 + 1.25 • 1.5 )/4 &lt; 2 ∆ (µω + ντ ) ∆ (µω) + ∆ (ντ ) = 3.2 ∆ µω + ντ 2 ∆ (µω) + ∆ ντ 2 &lt; 2.2 ∆ √ ντ ∆ (ντ ) 2 √ ντ 3 √ 0.64 = 1.875 ∆ √ 2 ∆2 2 √ 2 1 2 √ 2 ∆ √ 2 √ ντ √ 2∆ √ ντ + ντ ∆ √ 2 √ 2 • 1.875 + 1.5 • 1.25 • 1 2 √ 2 &lt; 3.5 ∆ µω √ 2 √ ντ ∆ (µω) √ 2 √ ντ + |µω| ∆ √ 2 √ ντ 1 √ 2 √ ντ 2 0.2 √ 2 √ 0.64 + 0.01 • 3.5 1 2 • 0.64 &lt; 0.25 ∆ µω + ντ √ 2 √ ντ ∆ (µω + ντ ) √ 2 √ ντ + |µω + ντ | ∆ √ 2 √ ντ 1 √ 2 √ ντ 2 3.2 √ 2 √ 0.64 + 1.885 • 3.5 1 2 • 0.64 &lt; 8 .</formula><p>Using these bounds on the simple expressions, we can now calculate bounds on the numerical errors of compound expressions:</p><formula xml:id="formula_116">∆ erfc µω √ 2 √ ντ √ π e − µω √ 2 √ ντ 2 ∆ µω √ √ ντ &lt; (123) 2 √ π 0.25 &lt; 0.3 ∆ erfc µω + ντ √ 2 √ ντ 2 √ π e − µω+ντ √ 2 √ ντ 2 ∆ µω + ντ √ 2 √ ντ &lt; (124) 2 √ π 8 &lt; 10 ∆ e µω+ ντ 2 e µω+ ντ 2 ∆ e µω+ ντ 2 &lt; (125) e 0.9475 2.2 &lt; 5.7<label>(126)</label></formula><p>Subsequently, we can use the above results to get bounds for the numerical errors on the Jacobian entries (Eq. (54)), applying the rules from Subsection A3.4.5 again:</p><formula xml:id="formula_117">∆ (J 11 ) = ∆ 1 2 λω αe µω+ ντ 2 erfc µω + ντ √ 2 √ ντ − erfc µω √ 2 √ ντ + 2 &lt; 6 ,<label>(127)</label></formula><p>and we obtain ∆ (J 12 ) &lt; 78 , ∆ (J 21 ) &lt; 189 , ∆ (J 22 ) &lt; 405 and ∆ (μ) &lt; . We also have bounds on the absolute values on J ij andμ (see Lemma 6, Lemma 7, and Lemma 9), therefore we can propagate the error also through the function that calculates the singular value (Eq. (61)).</p><formula xml:id="formula_118">∆ (S(µ, ω, ν, τ, λ, α)) = (128) ∆ 1 2 (J 11 + J 22 − 2μJ 12 ) 2 + (J 21 − 2μJ 11 − J 12 ) 2 + (J 11 − J 22 + 2μJ 12 ) 2 + (J 12 + J 21 − 2μJ 11 ) 2 &lt; 292 .</formula><p>Precision of Implementations. We will show that our computations are correct up to 3 ulps. For our implementation in GNU C library and the hardware architectures that we used, the precision of all mathematical functions that we used is at least one ulp. The term "ulp" (acronym for "unit in the last place") was coined by W. Kahan in 1960. It is the highest precision (up to some factor smaller 1), which can be achieved for the given hardware and floating point representation.</p><p>Kahan defined ulp as <ref type="bibr" target="#b25">[21]</ref>:</p><p>"Ulp(x) is the gap between the two finite floating-point numbers nearest x, even if x is one of them. (But ulp(NaN) is NaN.)"</p><p>Harrison defined ulp as <ref type="bibr" target="#b19">[15]</ref>:</p><p>"an ulp in x is the distance between the two closest straddling floating point numbers a and b, i.e. those with a x b and a = b assuming an unbounded exponent range."</p><p>In the literature we find also slightly different definitions <ref type="bibr" target="#b33">[29]</ref>.</p><p>According to <ref type="bibr" target="#b33">[29]</ref> who refers to <ref type="bibr" target="#b15">[11]</ref>:</p><p>"IEEE-754 mandates four standard rounding modes:" "Round-to-nearest: r(x) is the floating-point value closest to x with the usual distance; if two floating-point value are equally close to x, then r(x) is the one whose least significant bit is equal to zero." "IEEE-754 standardises 5 operations: addition (which we shall note ⊕ in order to distinguish it from the operation over the reals), subtraction ( ), multiplication (⊗), division ( ), and also square root." "IEEE-754 specifies em exact rounding <ref type="bibr">[Goldberg, 1991, §1.5]</ref>: the result of a floating-point operation is the same as if the operation were performed on the real numbers with the given inputs, then rounded according to the rules in the preceding section. Thus, x ⊕ y is defined as r(x + y), with x and y taken as elements of R ∪ {−∞, +∞}; the same applies for the other operators."</p><p>Consequently, the IEEE-754 standard guarantees that addition, subtraction, multiplication, division, and squared root is precise up to one ulp.</p><p>We have to consider transcendental functions. First the is the exponential function, and then the complementary error function erfc(x), which can be computed via the error function erf(x).</p><p>Intel states <ref type="bibr" target="#b33">[29]</ref>:</p><p>"With the Intel486 processor and Intel 387 math coprocessor, the worst-case, transcendental function error is typically 3 or 3.5 ulps, but is some-times as large as 4. which is the same for freebsd https://www.freebsd.org/cgi/man.cgi?query=exp&amp;sektion= 3&amp;apropos=0&amp;manpath=freebsd:</p><p>"The values of exp(0), expm1(0), exp2(integer), and pow(integer, integer) are exact provided that they are representable. Otherwise the error in these functions is generally below one ulp."</p><p>The same holds for "FDLIBM" http://www.netlib.org/fdlibm/readme: "FDLIBM is intended to provide a reasonably portable (see assumptions below), reference quality (below one ulp for major functions like sin,cos,exp,log) math library (libm.a)."</p><p>In http://www.gnu.org/software/libc/manual/html_node/ Errors-in-Math-Functions.html we find that both exp and erf have an error of 1 ulp while erfc has an error up to 3 ulps depending on the architecture. For the most common architectures as used by us, however, the error of erfc is 1 ulp.</p><p>We implemented the function in the programming language C. We rely on the GNU C Library <ref type="bibr" target="#b30">[26]</ref>. According to the GNU C Library manual which can be obtained from http://www.gnu.org/ software/libc/manual/pdf/libc.pdf, the errors of the math functions exp, erf, and erfc are not larger than 3 ulps for all architectures <ref type="bibr">[26, pp. 528</ref>]. For the architectures ix86, i386/i686/fpu, and m68k/fpmu68k/m680x0/fpu that we used the error are at least one ulp <ref type="bibr">[26, pp. 528</ref>]. x y <ref type="figure" target="#fig_51">Figure A4</ref>: Graphs of the upper and lower bounds on erfc. The lower bound</p><formula xml:id="formula_119">2e −x 2 √ π( √ x 2 +2+x) (red), the upper bound 2e −x 2 √ π √ x 2 + 4 π +x</formula><p>(green) and the function erfc(x) (blue) as treated in Lemma 22.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.4.6 Intermediate Lemmata and Proofs</head><p>Since we focus on the fixed point (µ, ν) = (0, 1), we assume for our whole analysis that α = α 01 and λ = λ 01 . Furthermore, we restrict the range of the variables µ ∈ According to its definition erfc(x) is strictly monotonically decreasing from 2 at −∞ to 0 at ∞.</p><formula xml:id="formula_120">[µ min , µ max ] = [−0.1, 0.1], ω ∈ [ω min , ω max ] = [−0.1, 0.1], ν ∈ [ν</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Next we introduce a bound on erfc:</head><p>Lemma 22 (Erfc bound from Abramowitz).</p><formula xml:id="formula_121">2e −x 2 √ π √ x 2 + 2 + x &lt; erfc(x) 2e −x 2 √ π x 2 + 4 π + x ,<label>(129)</label></formula><p>for x &gt; 0.</p><p>Proof. The statement follows immediately from <ref type="bibr" target="#b5">[1]</ref> (page 298, formula 7.1.13).</p><p>These bounds are displayed in <ref type="figure" target="#fig_51">figure A4</ref>.</p><p>Lemma 23 (Function e x 2 erfc(x)). e x 2 erfc(x) is strictly monotonically decreasing for x &gt; 0 and has positive curvature (positive 2nd order derivative), that is, the decreasing slowes down.</p><p>A graph of the function is displayed in <ref type="figure" target="#fig_24">Figure A5</ref>. Proof. The derivative of e x 2 erfc(x) is</p><formula xml:id="formula_122">∂e x 2 erfc(x) ∂x = 2e x 2 x erfc(x) − 2 √ π .<label>(130)</label></formula><p>Using Lemma 22, we get</p><formula xml:id="formula_123">∂e x 2 erfc(x) ∂x = 2e x 2 x erfc(x) − 2 √ π &lt; 4x √ π x 2 + 4 π + x − 2 √ π = 2 2 πx 2 +1+1 − 1 √ π &lt; 0<label>(131)</label></formula><p>Thus e x 2 erfc(x) is strictly monotonically decreasing for x &gt; 0.</p><p>The second order derivative of e x 2 erfc(x) is</p><formula xml:id="formula_124">∂ 2 e x 2 erfc(x) ∂x 2 = 4e x 2 x 2 erfc(x) + 2e x 2 erfc(x) − 4x √ π .<label>(132)</label></formula><p>Again using Lemma 22 (first inequality), we get</p><formula xml:id="formula_125">2x 2 + 1 e x 2 erfc(x) − 2x √ π &gt; (133) 4 2x 2 + 1 √ π √ x 2 + 2 + x − 4x √ π = 4 x 2 − √ x 2 + 2x + 1 √ π √ x 2 + 2 + x = 4 x 2 − √ x 4 + 2x 2 + 1 √ π √ x 2 + 2 + x &gt; 4 x 2 − √ x 4 + 2x 2 + 1 + 1 √ π √ x 2 + 2 + x = 0</formula><p>For the last inequality we added 1 in the numerator in the square root which is subtracted, that is, making a larger negative term in the numerator. Proof. The derivative of xe</p><formula xml:id="formula_126">x 2 erfc(x) is 2e x 2 x 2 erfc(x) + e x 2 erfc(x) − 2x √ π .<label>(134)</label></formula><p>This derivative is positive since</p><formula xml:id="formula_127">2e x x 2 erfc(x) + e x 2 erfc(x) − 2x √ π = (135) e x 2 2x 2 + 1 erfc(x) − 2x √ π &gt; 2 2x 2 + √ π √ x 2 + 2 + x − 2x √ π = 2 2x 2 + 1 − x √ x 2 + 2 + x √ π √ x 2 + 2 + x = 2 x 2 − x √ x 2 + 2 + 1 √ π √ x 2 + 2 + x = 2 x 2 − x √ x 2 + 2 + 1 √ π √ x 2 + 2 + x &gt; 2 x 2 − x x 2 + 1 x 2 + 2 + 1 √ π √ x 2 + 2 + x = 2 x 2 − √ x 4 + 2x 2 + 1 + 1 √ π √ x 2 + 2 + x = 2 x 2 − (x 2 + 1) 2 + 1 √ π √ x 2 + 2 + x = 0 .</formula><p>We apply Lemma 22 to x erfc(x)e x 2 and divide the terms of the lemma by x, which gives</p><formula xml:id="formula_128">2 √ π 2 x 2 + 1 + 1 &lt; x erfc(x)e x 2 2 √ π 4 πx 2 + 1 + 1 .<label>(136)</label></formula><p>For lim x→∞ both the upper and the lower bound go to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 27 (Function µω+ντ</head><formula xml:id="formula_129">√ 2 √ ντ ). h 1 (µ, ω, ν, τ ) = µω+ντ √ 2 √</formula><p>ντ is larger than zero and increasing in both ντ and µω. It has minimal value t 1 = 0.5568 and maximal value T 1 = 0.9734.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. The derivative of the function µω+x</head><formula xml:id="formula_130">√ 2 √ x with respect to x is 1 √ 2 √ x − µω + x 2 √ 2x 3/2 = 2x − (µω + x) 2 √ 2x 3/2 = x − µω 2 √ 2x 3/2 &gt; 0 ,<label>(137)</label></formula><formula xml:id="formula_131">since x &gt; 0.8 • 0.8 and µω &lt; 0.1 • 0.1. Lemma 28 (Function µω+2ντ √ 2 √ ντ ). h 2 (µ, ω, ν, τ ) = µω+2ντ √ 2 √</formula><p>ντ is larger than zero and increasing in both ντ and µω. It has minimal value t 2 = 1.1225 and maximal value T 2 = 1.9417.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. The derivative of the function µω+2x</head><formula xml:id="formula_132">√ 2 √ x with respect to x is √ 2 √ x − µω + 2x 2 √ 2x 3/2 = 4x − (µω + 2x) 2 √ 2x 3/2 = 2x − µω 2 √ 2x 3/2 &gt; 0 . (138) Lemma 29 (Function µω √ 2 √ ντ ). h 3 (µ, ω, ν, τ ) = µω √ 2 √</formula><p>ντ monotonically decreasing in ντ and monotonically increasing in µω. It has minimal value t 3 = −0.0088388 and maximal value T 3 = 0.0088388.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. Obvious.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 30 (Function</head><formula xml:id="formula_133">µω √ 2 √ ντ 2 ). h 4 (µ, ω, ν, τ ) = µω √ 2 √ ντ 2</formula><p>has a minimum at for µ = 0 or ω = 0 and has a maximum for the smallest ντ and largest |µω| and is larger or equal to zero. It has minimal value t 4 = 0 and maximal value T 4 = 0.000078126. </p><formula xml:id="formula_134">Proof. Obvious. Lemma 31 (Function √ π (α−1) √ ντ</formula><formula xml:id="formula_135">(α−1)µω (ντ ) 3/2 − α √ ντ ). For λ = λ 01 and α = α 01 , 2 π (α−1)µω (ντ ) 3/2 − α √ ντ</formula><p>&lt; 0 and increasing in both ντ and µω.</p><p>Proof. We consider the function 2</p><formula xml:id="formula_136">π (α−1)µω x 3/2 − α √</formula><p>x , which has the derivative with respect to x:</p><formula xml:id="formula_137">2 π α 2x 3/2 − 3(α − 1)µω 2x 5/2 .<label>(139)</label></formula><p>This derivative is larger than zero, since</p><formula xml:id="formula_138">2 π α 2(ντ ) 3/2 − 3(α − 1)µω 2(ντ ) 5/2 &gt; 2 π α − 3(α−1)µω ντ 2(ντ ) 3/2 &gt; 0 .<label>(140)</label></formula><p>The last inequality follows from α − 3</p><formula xml:id="formula_139">•0.1•0.1(α−1) 0.8•0.8 &gt; 0 for α = α 01 .</formula><p>We next consider the function 2</p><formula xml:id="formula_140">π (α−1)x (ντ ) 3/2 − α √</formula><p>ντ , which has the derivative with respect to x:</p><formula xml:id="formula_141">2 π (α − 1) (ντ ) 3/2 &gt; 0 . (141) Lemma 34 (Function 2 π (−1)(α−1)µ 2 ω 2 (ντ ) 3/2 + −α+αµω+1 √ ντ − α √ ντ ). The function π (−1)(α−1)µ 2 ω 2 (ντ ) 3/2 + −α+αµω+1 √ ντ − α √ ντ &lt; 0</formula><p>is decreasing in ντ and increasing in µω.</p><p>Proof. We define the function</p><formula xml:id="formula_142">2 π (−1)(α − 1)µ 2 ω 2 x 3/2 + −α + αµω + 1 √ x − α √ x<label>(142)</label></formula><p>which has as derivative with respect to x: π</p><formula xml:id="formula_143">3(α − 1)µ 2 ω 2 2x 5/2 − −α + αµω + 1 2x 3/2 − α √ x = (143) 1 √ 2πx 5/2 3(α − 1)µ 2 ω 2 − x(−α + αµω + 1) − αx 2 .</formula><p>The derivative of the term 3(α − 1)µ 2 ω 2 − x(−α + αµω + 1) − αx 2 with respect to x is −1 + α − µωα − 2αx &lt; 0, since 2αx &gt; 1.6α. Therefore the term is maximized with the smallest value for x, which is x = ντ = 0.8 • 0.8. For µω we use for each term the value which gives maximal contribution. We obtain an upper bound for the term:</p><formula xml:id="formula_144">3(−0.1 • 0.1) 2 (α 01 − 1) − (0.8 • 0.8) 2 α 01 − 0.8 • 0.8((−0.1 • 0.1)α − α + 1) = −0.243569 .<label>(144)</label></formula><p>Therefore the derivative with respect to x = ντ is smaller than zero and the original function is decreasing in ντ</p><p>We now consider the derivative with respect to x = µω. The derivative with respect to x of the function</p><formula xml:id="formula_145">2 π −α √ ντ − (α − 1)x 2 (ντ ) 3/2 + −α + αx + 1 √ ντ (145) is 2 π (αντ − 2(α − 1)x) (ντ ) 3/2 .<label>(146)</label></formula><p>Since </p><formula xml:id="formula_146">0.1 • 0.1α 01 − α 01 + 1 √ 0.8 • 0.8 + 0.1 2 0.1 2 (−1)(α 01 − 1) (0.8 • 0.8) 3/2 − √ 0.8 • 0.8α 01 = −1.72296 .<label>(147)</label></formula><p>Therefore the original function is smaller than zero.</p><formula xml:id="formula_147">Lemma 35 (Function 2 π (α 2 −1)µω (ντ ) 3/2 − 3α 2 √ ντ ). For λ = λ 01 and α = α 01 , 2 π (α 2 −1)µω (ντ ) 3/2 − 3α 2</formula><p>√ ντ &lt; 0 and increasing in both ντ and µω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. The derivative of the function</head><formula xml:id="formula_148">2 π α 2 − 1 µω x 3/2 − 3α 2 √ x (148) with respect to x is 2 π 3α 2 2x 3/2 − 3 α 2 − 1 µω 2x 5/2 = 3 α 2 x − α 2 − 1 µω √ 2πx 5/2 &gt; 0 ,<label>(149)</label></formula><formula xml:id="formula_149">since α 2 x − µω(−1 + α 2 ) &gt; α 2 01 0.8 • 0.8 − 0.1 • 0.1 • (−1 + α 2 01 ) &gt; 1.77387</formula><p>The derivative of the function π</p><formula xml:id="formula_150">α 2 − 1 x (ντ ) 3/2 − 3α 2 √ ντ (150) with respect to x is 2 π α 2 − 1 (ντ ) 3/2 &gt; 0 .<label>(151)</label></formula><p>The maximal function value is obtained by maximal ντ = 1. </p><formula xml:id="formula_151">2 π α 2 − 1 µω √ x − 3α 2 √ x<label>(152)</label></formula><p>with respect to x is </p><formula xml:id="formula_152">2 π − α 2 − 1 µω 2x 3/2 − 3α 2 2 √ x = − α 2 − 1 µω − 3α 2 x √ 2πx 3/2 &lt; 0 ,<label>(153)</label></formula><formula xml:id="formula_153">2 π α 2 − 1 x √ ντ − 3α 2 √ ντ (154) with respect to x is 2 π α 2 − 1 √ ντ &gt; 0 .<label>(155)</label></formula><p>The maximal function value is obtained for minimal ντ = 0. x</p><formula xml:id="formula_154">(x + 2) − µ 2 ω 2 erfc µω+x √ 2 √ x 2x + µω − x √ 2π √ x .<label>(157)</label></formula><p>This derivative is larger than zero, since e (µω+ντ ) 2 2ντ</p><formula xml:id="formula_155">ντ (ντ + 2) − µ 2 ω 2 erfc µω+ντ √ 2 √ ντ 2ντ + µω − ντ √ 2π √ ντ &gt; (158) 0.4349 ντ (ντ + 2) − µ 2 ω 2 −0.5µ 2 ω 2 + µω √ ντ + 0.5(ντ ) 2 − ντ √ ντ + ντ √ 2πντ = −0.5µ 2 ω 2 + µω √ ντ + (0.5ντ − √ ντ ) 2 + 0.25(ντ ) 2 √ 2πντ &gt; 0 .</formula><p>We explain this chain of inequalities:</p><p>• The first inequality follows by applying Lemma 23 which says that e • The equalities are just algebraic reformulations.</p><p>• The last inequality follows from −0.5µ</p><formula xml:id="formula_156">2 ω 2 + µω √ ντ + 0.25(ντ ) 2 &gt; 0.25(0.8 • 0.8) 2 − 0.5 • (0.1) 2 (0.1) 2 − 0.1 • 0.1 • √ 0.8 • 0.8 = 0.09435 &gt; 0.</formula><p>Therefore the function is increasing in ντ . </p><formula xml:id="formula_157">2x(2x + 1) − µ 2 ω 2 erfc µω+2x √ x + √ x(µω − 2x) 2 √ πx .<label>(160)</label></formula><p>We only have to determine the sign of √ πe</p><formula xml:id="formula_158">(µω+2x) 2 4x 2x(2x + 1) − µ 2 ω 2 erfc µω+2x √ x + √ x(µω−</formula><p>2x) since all other factors are obviously larger than zero.</p><p>This derivative is larger than zero, since</p><formula xml:id="formula_159">√ πe (µω+2ντ ) 2 4ντ 2ντ (2ντ + 1) − µ 2 ω 2 erfc µω + 2ντ √ ντ + √ ντ (µω − 2ντ ) &gt; (161) 0.463979 2ντ (2ντ + 1) − µ 2 ω 2 + √ ντ (µω − 2ντ ) = − 0.463979µ 2 ω 2 + µω √ ντ + 1.85592(ντ ) 2 + 0.927958ντ − 2ντ √ ντ = µω √ ντ − 0.463979µω + 0.85592(ντ ) 2 + ντ − √ ντ 2 − 0.0720421ντ &gt; 0 .</formula><p>We explain this chain of inequalities:</p><p>• The first inequality follows by applying Lemma 23 which says that e (µω+2ντ ) 2 2ντ</p><formula xml:id="formula_160">erfc µω+2ντ √ 2 √ ντ</formula><p>is strictly monotonically decreasing. The minimal value that is larger than 0.261772 is taken on at the maximal values ντ = 1. Lemma 39 (Bounds on the Derivatives). The following bounds on the absolute values of the derivatives of the Jacobian entries J 11 (µ, ω, ν, τ, λ, α), J 12 (µ, ω, ν, τ, λ, α), J 21 (µ, ω, ν, τ, λ, α), and J 22 (µ, ω, ν, τ, λ, α) with respect to µ, ω, ν, and τ hold: Proof. For each derivative we compute a lower and an upper bound and take the maximum of the absolute value. A lower bound is determined by minimizing the single terms of the functions that represents the derivative. An upper bound is determined by maximizing the single terms of the functions that represent the derivative. Terms can be combined to larger terms for which the maximum and the minimum must be known. We apply many previous lemmata which state properties of functions representing single or combined terms. The more terms are combined, the tighter the bounds can be made. </p><formula xml:id="formula_161">∂J</formula><formula xml:id="formula_162">α 01 e t 2 1 erfc(t 1 ) − 2 π (α 01 − 1) √ T 22 = 0.591017 .<label>(163)</label></formula><p>A lower bound on the minimum is</p><formula xml:id="formula_163">α 01 e T 2 1 erfc(T 1 ) − 2 π (α 01 − 1) √ t 22 = 0.056318 .<label>(164)</label></formula><p>Thus, an upper bound on the maximal absolute value is 2</p><formula xml:id="formula_164">λ 01 ω 2 max e t4   α 01 e t 2 1 erfc(t 1 ) − 2 π (α 01 − 1) √ T 22   = 0.0031049101995398316 .<label>(165)</label></formula><p>• ∂J11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∂ω</head><p>We use Lemma 31 and consider the expression  </p><formula xml:id="formula_165">π (α 01 − 1)T 11 √ t 22 − α 01 (t 11 + 1)e T 2 1 erfc(T 1 ) = −0.713808 .<label>(166)</label></formula><p>A lower bound on the minimum is</p><formula xml:id="formula_166">π (α 01 − 1)t 11 √ t 22 − α 01 (T 11 + 1)e t 2 1 erfc(t 1 ) = −0.99987 .<label>(167)</label></formula><p>This term is subtracted, and 2 − erfc(x) &gt; 0, therefore we have to use the minimum and the maximum for the argument of erfc.</p><p>Thus, an upper bound on the maximal absolute value is</p><formula xml:id="formula_167">1 2 λ 01   −e t4   2 π (α 01 − 1)t 11 √ t 22 − α 01 (T 11 + 1)e t 2 1 erfc(t 1 )   − erfc(T 3 ) + 2   = (168) 1.055872374194189 . • ∂J11 ∂ν</formula><p>We consider the term in brackets</p><formula xml:id="formula_168">αe (µω+ντ ) 2 2ντ erfc µω + ντ √ 2 √ ντ + 2 π (α − 1)µω (ντ ) 3/2 − α √ ντ .<label>(169)</label></formula><p>We apply Lemma 33 for the first sub-term. An upper bound on the maximum is</p><formula xml:id="formula_169">α 01 e t 2 1 erfc(t 1 ) + 2 π (α − 1)T 11 T 3/2 22 − α 01 √ T 22 = 0.0104167 .<label>(170)</label></formula><p>A lower bound on the minimum is For the second term in brackets, we see that α 01 τ 2 min e T 2 1 erfc(T ) = 0.465793 and α 01 τ 2 max e t 2 1 erfc(t 1 ) = 1.53644. We now check different values for π</p><formula xml:id="formula_170">α 01 e T 2 1 erfc(T 1 ) + 2 π (α 01 − 1)t 11 t 3/2 22 − α 01 √ t 22 = −0.95153 .<label>(171)</label></formula><formula xml:id="formula_171">(−1)(α − 1)µ 2 ω 2 ν 5/2 √ τ + √ τ (α + αµω − 1) ν 3/2 − ατ 3/2 √ ν ,<label>(176)</label></formula><p>where we maximize or minimize all single terms.</p><p>A lower bound on the minimum of this expression is</p><formula xml:id="formula_172">2 π (−1)(α 01 − 1)µ 2 max ω 2 max ν 5/2 min √ τ min + √ τ min (α 01 + α 01 t 11 − 1) ν 3/2 max − α 01 τ 3/2 max √ ν min = (177) − 1.83112 .</formula><p>An upper bound on the maximum of this expression is π </p><formula xml:id="formula_173">(−1)(α 01 − 1)µ 2 min ω 2 min ν 5/2 max √ τ max + √ τ max (α 01 + α 01 T 11 − 1) ν 3/2 min − α 01 τ 3/2 min √ ν max =<label>(178</label></formula><formula xml:id="formula_174">erfc(T 1 ) + (180) 2 π (−1)(α 01 − 1)µ 2 max ω 2 max ν 5/2 min √ τ min + √ τ min (α 01 + α 01 t 11 − 1) ν 3/2 max − α 01 τ 3/2 max √ ν min = − 0.179318 .</formula><p>Thus, an upper bound on the maximal absolute value is 8 λ 01 e t4 2 π</p><formula xml:id="formula_175">(−1)(α 01 − 1)µ 2 min ω 2 min ν 5/2 max √ τ max − α 01 τ 3/2 min √ ν max + (181) √ τ max (α 01 + α 01 T 11 − 1) ν 3/2 min + α 01 τ 2 max e t 2 1</formula><p>erfc(t 1 ) = 0.21232788238624354 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• ∂J12 ∂τ</head><p>We use Lemma 34 to obtain an upper bound on the maximum of the expression of the lemma:</p><formula xml:id="formula_176">2 π 0.1 2 • 0.1 2 (−1)(α 01 − 1) (0.8 • 0.8) 3/2 − √ 0.8 • 0.8α 01 + (0.1 • 0.1)α 01 − α 01 + 1 √ 0.8 • 0.8 = −1.72296 .<label>(182)</label></formula><p>We use Lemma 34 to obtain an lower bound on the minimum of the expression of the lemma:  The derivative ∂ ∂µμ (µ, ω, ν, τ, λ, α) has the sign of ω. The derivative ∂ ∂νμ (µ, ω, ν, τ, λ, α) is positive. The derivative ∂ ∂µξ (µ, ω, ν, τ, λ, α) has the sign of ω. </p><formula xml:id="formula_177">2 π 0.1 2 • 0.1 2 (−1)(α 01 − 1)<label>(1.</label></formula><p>We set x = ντ and y = µω and obtain</p><formula xml:id="formula_179">2 π √ x − α 2 e x+y √ 2 √ x 2 erfc x + y √ 2 √ x − e 2x+y √ 2 √ x 2 erfc 2x + y √ 2 √ x .<label>(212)</label></formula><p>The derivative of this sub-function with respect to y is</p><formula xml:id="formula_180">α 2 e (2x+y) 2 2x (2x + y) erfc 2x+y √ 2 √ x − e (x+y) 2 2x (x + y) erfc x+y √ √ x x = (213) √ 2α 2 √ x   e (2x+y) 2 2x (x+y) erfc x+y √ 2 √ x √ 2 √ x − e (x+y) 2 2x (x+y) erfc x+y √ 2 √ x √ 2 √ x   x &gt; 0 .</formula><p>The inequality follows from Lemma 24, which states that ze z 2 erfc(z) is monotonically increasing in z. Therefore the sub-function is increasing in y.</p><p>The derivative of this sub-function with respect to x is √ πα 2 e (2x+y) 2 2x</p><formula xml:id="formula_181">4x 2 − y 2 erfc 2x+y √ 2 √ x − e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x − √ 2 α 2 − 1 x 3/2 2 √ πx 2 .<label>(214)</label></formula><p>The sub-function is increasing in x, since the derivative is larger than zero: </p><formula xml:id="formula_182">√ πα 2 e (2x+y) 2 2x 4x 2 − y 2 erfc 2x+y √ 2 √ x − e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x − √ 2x 3/2 α 2 − 1 2 √ πx 2 (215) √ πα 2    (2x−y)(2x+y)2 √ π 2x+y √ 2 √ x + 2x+y √ 2 √ x 2 +2 − (x−y)(x+y)2 √ π x+y √ 2 √ x + x+y √ 2 √ x 2 + 4 π    − √ 2x 3/2 α 2 − 1 2 √ πx 2 = √ πα 2 (2x−y)(2x+y)2( √ 2 √ x) √ π 2x+y+ √ (2x+y) 2 +4x − (x−y)(x+y)2( √ 2 √ x) √ π x+y+ √ (x+y) 2 + 8x π − √ 2x 3/2 α 2 − 1 2 √ πx 2 = √ πα 2 (2x−y)(2x+y)2 √ π 2x+y+ √ (2x+y) 2 +4x − (x−y)(x+y)2 √ π x+y+ √ (x+y) 2 + 8x π − x α 2 − 1 √ 2 √ πx 3/2 &gt; √ πα 2 (2x−y)(2x+y)2 √ π 2x+y+ √ (2x+y) 2 +2(2x+y)+1 − (x−y)(x+y)2 √ π x+y+ √ (x+y) 2 +0.782•2(x+y)+0.782 2 − x α 2 − 1 √ 2 √ πx 3/2 = √ πα 2 (2x−y)(2x+y)2 √ π 2x+y+ √ (2x+y+1) 2 − (x−y)(x+y)2 √ π x+y+ √ (x+y+0.782) 2 − x α 2 − 1 √ 2 √ πx 3/2 = √ πα 2 (2x−y)(2x+y)2 √ π(2(2x+y)+1) − (x−y)(x+y)2 √ π(2(x+y)+0.782) − x α 2 − 1 √ 2 √ πx 3/2 = √ πα 2 (2(x+y)+0.782)(2x−y)(2x+y)2 √ π − (x−y)(</formula><formula xml:id="formula_183">+ y) + 1)(2(x + y) + 0.782) √ 2 √ π √ x = 8x 2 + 2.56657x − 1.472 (2(2x + y) + 1)(2(x + y) + 0.782) √ 2 √ π √ x = 8(x + 0.618374)(x − 0.297553) (2(2x + y) + 1)(2(x + y) + 0.782) √ 2 √ π √ x &gt; 0 .</formula><p>We explain this chain of inequalities:</p><p>-First inequality: We applied Lemma 22 two times.</p><p>-Equalities factor out √ 2 √</p><p>x and reformulate.  Therefore the term in brackets is larger than zero.</p><p>Thus, ∂ ∂µξ (µ, ω, ν, τ, λ, α) has the sign of ω.</p><p>• ∂ ∂νξ (µ, ω, ν, τ, λ, α)</p><p>We look at the sub-term</p><formula xml:id="formula_184">2e 2x+y √ 2 √ x 2 erfc 2x + y √ 2 √ x − e x+y √ 2 √ x 2 erfc x + y √ 2 √ x .<label>(221)</label></formula><p>We obtain a chain of inequalities: We explain this chain of inequalities:</p><formula xml:id="formula_185">2e 2x+y √ 2 √ x erfc 2x + y √ 2 √ x − e x+y √ 2 √ x 2 erfc x + y √ 2 √ x &gt; (222) 2 • 2 √ π 2x+y √ 2 √ x + 2x+y √ 2 √ x 2 + 2 − 2 √ π x+y √ 2 √ x + x+y √ 2 √ x 2 + 4 π = 2 √ 2 √ x 2 √ (2x+y) 2 +4x+2x+y − 1 √ (x+y) 2 + 8x π +x+y √ π &gt; 2 √ 2 √ x 2 √ (2x+y) 2 +2(2x+y)+1+2x+y − 1 √ (x+y) 2 +0.782•2(x+y)+0.782 2 +x+y √ π = 2 √ 2 √ x 2 2(2x+y)+1 − 1 2(x+y)+0.782 √ π = 2 √ 2 √ x<label>(</label></formula><p>-First inequality: We applied Lemma 22 two times.</p><p>-Equalities factor out √ 2 √</p><p>x and reformulate. We know that (2 − erfc(x) &gt; 0 according to Lemma 21. For the sub-term we derived</p><formula xml:id="formula_186">2e 2x+y √ 2 √ x 2 erfc 2x + y √ 2 √ x − e x+y √ 2 √ x 2 erfc x + y √ 2 √ x &gt; 0 .<label>(226)</label></formula><p>Consequently, both terms in the brackets of ∂ ∂νξ (µ, ω, ν, τ, λ, α) are larger than zero. Therefore ∂ ∂νξ (µ, ω, ν, τ, λ, α) is larger than zero. </p><p>We can considerμ with given µω as a function in x = ντ . We show the graph of this function at the maximal µω = 0.01 in the interval x ∈ [0, 1] in <ref type="figure" target="#fig_41">Figure A6</ref>.  Similarly, we can use the monotonicity of the terms in ντ to show that</p><formula xml:id="formula_188">µ(µ, ω, ν, τ, λ, α) μ(0.1, −0.1, ν, τ, λ, α) &gt; −0.289324,<label>(231)</label></formula><p>such that |μ| &lt; 0.289324 at low variances.</p><p>Furthermore, when (ντ ) → 0, the terms with the arguments of the complementary error functions erfc and the exponential function go to infinity, therefore these three terms converge to zero. Hence, the remaining terms are only 2µω 1 2 λ. Lemma 42 (Bounds on derivatives ofμ in Ω − ). The derivatives of the functionμ(µ, ω, ν, τ, λ 01 , α 01 (Eq. (4)) with respect to µ, ω, ν, τ in the domain </p><formula xml:id="formula_189">Ω − = {µ, ω, ν, τ | − 0.1 µ 0.1, −0.1 ω 0.1,</formula><formula xml:id="formula_190">erfc µω+ντ √ 2 √</formula><p>ντ which are monotonically decreasing in their arguments (Lemma 23). We can therefore obtain their minima and maximal at the minimal and maximal arguments. Since the first term has a negative sign in the expression, both terms reach their maximal value at µω = −0.01, ν = 0.05, and τ = 0.8. </p><p>Since,μ is symmetric in µ and ω, these bounds also hold for the derivate to ω. We visualize the functionμ 2 at its maximal µν = −0.01 and for x = ντ in the form <ref type="figure" target="#fig_43">Figure A7</ref>.</p><formula xml:id="formula_192">h(x) = µ 2 (0.1, −0.1, x, 1, λ 01 , α 01 ) in</formula><p>Proof. We use a similar strategy to the one we have used to show the bound on the singular value (Lemmata 10, 11, and 12), where we evaluted the function on a grid and used bounds on the derivatives together with the mean value theorem. Here we have We evaluated the functionμ 2 in a grid G of Ω − with ∆µ = 0.001498041, ∆ω = 0.001498041, ∆ν = 0.0004033190, and ∆τ = 0.0019065994 using a computer and obtained the maximal value max G (μ) 2 = 0.00451457, therefore the maximal value ofμ 2 is bounded by </p><formula xml:id="formula_193">μ 2 (µ, ω,</formula><formula xml:id="formula_194">max (µ,ω,ν,τ )∈Ω − (μ) 2</formula><p>Furthermore we used error propagation to estimate the numerical error on the function evaluation.</p><p>Using the error propagation rules derived in Subsection A3. <ref type="bibr" target="#b8">4</ref>.5, we found that the numerical error is smaller than 10 −13 in the worst case.</p><p>Lemma 44 (Main subfunction). </p><formula xml:id="formula_196">erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x (242)</formula><p>is smaller than zero, is strictly monotonically increasing in x, and strictly monotonically decreasing in y for the minimal x = 12/10 = 1.2.</p><p>Proof. We first consider the derivative of sub-function Eq. </p><formula xml:id="formula_197">erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x (243) with respect to x is √ π e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √</formula><p>x − 2e</p><formula xml:id="formula_198">(2x+y) 2 2x 4x 2 − y 2 erfc 2x+y √ 2 √ x + √ √ x(3x − y) √ πx 2 = (244) √ π e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x − 2e (2x+y) 2 2x (2x + y)(2x − y) erfc 2x+y √ 2 √ x + √ √ x(3x − y) √ πx 2 = √ π   e (x+y) 2 2x (x−y)(x+y) erfc x+y √ 2 √ x √ 2 √ x − 2e (2x+y) 2 2x (2x+y)(2x−y) erfc 2x+y √ 2 √ x √ 2 √ x   + (3x − y) √ √ πx 2 √ x .</formula><p>We consider the numerator</p><formula xml:id="formula_199">√ π    e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x √ 2 √ x − 2e (2x+y) 2 2x (2x + y)(2x − y) erfc 2x+y √ 2 √ x √ 2 √ x    + (3x − y) .<label>(245)</label></formula><p>For bounding this value, we use the approximation</p><formula xml:id="formula_200">e z 2 erfc(z) ≈ 2.911 √ π(2.911 − 1)z + √ πz 2 + 2.911 2 .<label>(246)</label></formula><p>from Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>. We start with an error analysis of this approximation. According to Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>  <ref type="figure">(Figure 1</ref>), the approximation error is positive in the range [0.7, <ref type="bibr">3.2]</ref>. This range contains all possible arguments of erfc that we consider. Numerically we maximized and minimized the approximation error of the whole expression</p><formula xml:id="formula_201">E(x, y) =    e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x √ 2 √ x − 2e (2x+y) 2 2x (2x − y)(2x + y) erfc 2x+y √ 2 √ x √ 2 √ x    − (247)       2.911(x − y)(x + y) √ 2 √ x √ π(2.911−1)(x+y) √ 2 √ x + π x+y √ 2 √ x 2 + 2.911 2 − 2 • 2.911(2x − y)(2x + y) √ 2 √ x √ π(2.911−1)(2x+y) √ 2 √ x + π 2x+y √ 2 √ x 2 + 2.911 2       .</formula><p>We numerically determined 0.0113556 E(x, y) 0.0169551 for 1.2 x 20 and −0.1 y 0.1. We used different numerical optimization techniques like gradient based constraint BFGS algorithms and non-gradient-based Nelder-Mead methods with different start points. Therefore our approximation is smaller than the function that we approximate. We subtract an additional safety gap of 0.0131259 from our approximation to ensure that the inequality via the approximation holds true. With this safety gap the inequality would hold true even for negative x, where the approximation error becomes negative and the safety gap would compensate. Of course, the safety gap of 0.0131259 is not necessary for our analysis but may help or future investigations.</p><p>We have the sequences of inequalities using the approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>: We explain this sequence of inequalities:</p><formula xml:id="formula_202">(3x − y) +    e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x √ 2 √ x − 2e (2x+y) 2 2x (2x − y)(2x + y) erfc 2x+y √ 2 √ x √ 2 √ x    √ π (248) (3x − y) +       2.911(x − y)(x + y) π x+y √ 2 √ x 2 + 2.911 2 + (2.911−1) √ π(x+y) √ 2 √ x √ 2 √ x − 2(2x − y)(2x + y)2.911 √ 2 √ x π 2x+y √ 2 √ x 2 + 2.911 2 + (2.911−1) √ π(2x+y) √ 2 √ x       √ π − 0.0131259 = (3x − y) +   √ √ x2.911 (x − y)(x + y) π(x + y) 2 + 2 • 2.911 2 x + (2.911 − 1)(x + y) √ π √ 2 √ x − 2(2x − y)(2x + y) √ 2 √ x2.911 √ 2 √ x π(2x + y) 2 + 2 • 2.</formula><p>• First inequality: The approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref> and then subtracting a safety gap (which would not be necessary for the current analysis).</p><p>• Equalities: The factor √ 2 √</p><p>x is factored out and canceled.</p><p>• Second inequality: adds a positive term in the first root to obtain a binomial form. The term containing the root is positive and the root is in the denominator, therefore the whole term becomes smaller.</p><p>• Equalities: solve for the term and factor out.</p><p>• Bringing all terms to the denominator (x + y)</p><formula xml:id="formula_203">+ 2.911 π (2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.911 2 x π .</formula><p>• Equalities: Multiplying out and expanding terms.</p><p>• Last inequality &gt; 0 is proofed in the following sequence of inequalities.</p><p>We look at the numerator of the last expression of Eq. <ref type="formula">248</ref> The factor in front of the root is positive. If the term, that does not contain the root, was positive, then the whole expression would be positive and we would have proofed that the numerator is positive. Therefore we consider the case that the term, that does not contain the root, is negative. The term that contains the root must be larger than the other term in absolute values. We obtain the inequalities: </p><formula xml:id="formula_204">− 1.2227x 5 +</formula><formula xml:id="formula_205">erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x<label>(255)</label></formula><p>is strictly monotonically increasing in x.</p><p>The main subfunction is smaller than zero. Next we show that the sub-function Eq. (101) is smaller than zero. We consider the limit:</p><formula xml:id="formula_206">lim x→∞ e (x+y) 2 2x erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x = 0<label>(256)</label></formula><p>The limit follows from Lemma 22. Since the function is monotonic increasing in x, it has to approach 0 from below. Thus,</p><formula xml:id="formula_207">e (x+y) 2 2x erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x<label>(257)</label></formula><p>is smaller than zero.</p><p>Behavior of the main subfunction with respect to y at minimal x. We now consider the derivative of sub-function Eq. (101) with respect to y. We proofed that sub-function Eq. (101) is strictly monotonically increasing independent of y. In the proof of Theorem 16, we need the minimum of sub-function Eq. (101). Therefore we are only interested in the derivative of sub-function Eq. (101) with respect to y for the minimum x = 12/10 = 1.2</p><p>Consequently, we insert the minimum x = 12/10 = 1.2 into the sub-function Eq. (101). The main terms become We obtained for the maximal absolute error the value 0.163052. We added an approximation error of 0.2 to the approximation of the derivative. Since we want to show that the approximation upper bounds the true expression, the addition of the approximation error is required here. We get a sequence of inequalities: </p><formula xml:id="formula_208">x + y √ 2 √ x = y + 1.2 √ 2 √ 1.2 = y √ 2 √ 1.2 + √ 1.2 √ 2 = 5y + 6 2 √ 15<label>(258)</label></formula><formula xml:id="formula_209">√ 15π</formula><formula xml:id="formula_210">      −1 &lt; 0 .</formula><p>We explain this sequence of inequalities.</p><p>• First inequality: The approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref> and then adding the error bound to ensure that the approximation is larger than the true value.</p><p>• • Last inequality &lt; 0 is proofed in the following sequence of inequalities.</p><p>We look at the numerator of the last term in Eq. (264). We have to proof that this numerator is smaller than zero in order to proof the last inequality of Eq. For the first inequality we choose y in the roots, so that positive terms maximally increase and negative terms maximally decrease. The second inequality just removed the y 2 term which is always negative, therefore increased the expression. For the last inequality, the term in brackets is negative for all settings of y. Therefore we make the brackets as negative as possible and make the whole term positive by multiplying with y = −0.1.</p><p>Consequently</p><formula xml:id="formula_211">e (x+y) 2 2x erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x<label>(268)</label></formula><p>is strictly monotonically decreasing in y for the minimal x = 1. x − 2e</p><formula xml:id="formula_212">(2x+y) 2 2x 4x 2 − y 2 erfc 2x+y √ 2 √ x + √ 2 √ x(3x − y) 2 √ πx 2 = (271) √ π e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x − 2e (2x+y) 2 2x (2x + y)(2x − y) erfc 2x+y √ 2 √ x + √ 2 √ x(3x − y) 2 √ πx 2 = √ π   e (x+y) 2 2x (x−y)(x+y) erfc x+y √ 2 √ x √ 2 √ x − 2e (2x+y) 2 2x (2x+y)(2x−y) erfc 2x+y √ 2 √ x √ 2 √ x   + (3x − y) √ 22 √ π √ xx 2 .</formula><p>We consider the numerator</p><formula xml:id="formula_213">√ π    e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x √ 2 √ x − 2e (2x+y) 2 2x (2x + y)(2x − y) erfc 2x+y √ 2 √ x √ 2 √ x    + (3x − y) .<label>(272)</label></formula><p>For bounding this value, we use the approximation</p><formula xml:id="formula_214">e z 2 erfc(z) ≈ 2.911 √ π(2.911 − 1)z + √ πz 2 + 2.911 2 .<label>(273)</label></formula><p>from Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>. We start with an error analysis of this approximation. According to Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>  <ref type="figure">(Figure 1</ref>), the approximation error is both positive and negative in the range [0.175, <ref type="bibr">1.33</ref>]. This range contains all possible arguments of erfc that we consider in this subsection.</p><p>Numerically we maximized and minimized the approximation error of the whole expression</p><formula xml:id="formula_215">E(x, y) =    e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x √ 2 √ x − 2e (2x+y) 2 2x (2x − y)(2x + y) erfc 2x+y √ 2 √ x √ 2 √ x    − (274)       2.911(x − y)(x + y) √ 2 √ x √ π(2.911−1)(x+y) √ 2 √ x + π x+y √ 2 √ x 2 + 2.911 2 − 2 • 2.911(2x − y)(2x + y) √ 2 √ x √ π(2.911−1)(2x+y) √ 2 √ x + π 2x+y √ 2 √ x 2 + 2.911 2       .</formula><p>We numerically determined −0.000228141 E(x, y) 0.00495688 for 0.08 x 0.875 and −0.01 y 0.01. We used different numerical optimization techniques like gradient based constraint BFGS algorithms and non-gradient-based Nelder-Mead methods with different start points. Therefore our approximation is smaller than the function that we approximate.</p><p>We use an error gap of −0.0003 to countermand the error due to the approximation. We have the sequences of inequalities using the approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>:   We explain this sequence of inequalities:</p><formula xml:id="formula_216">(3x − y) +    e (x+y) 2 2x (x − y)(x + y) erfc x+y √ 2 √ x √ 2 √ x − 2e (2x+y) 2x (2x − y)(2x + y) erfc 2x+y √ 2 √ x √ 2 √ x    √ π<label>(275)</label></formula><p>• First inequality: The approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref> and then subtracting an error gap of 0.0003.</p><p>• Equalities: The factor √ 2 √</p><p>x is factored out and canceled.</p><p>• Second inequality: adds a positive term in the first root to obtain a binomial form. The term containing the root is positive and the root is in the denominator, therefore the whole term becomes smaller.</p><p>• Equalities: solve for the term and factor out.</p><p>• Bringing all terms to the denominator (x + y)</p><formula xml:id="formula_217">+ 2.911 π (2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.911 2 x π .</formula><p>• Equalities: Multiplying out and expanding terms.</p><p>• Last inequality &gt; 0 is proofed in the following sequence of inequalities.</p><p>We look at the numerator of the last expression of Eq. (275), which we show to be positive in order to show &gt; 0 in <ref type="bibr">Eq. (275</ref> We obtain the inequalities:</p><p>x We used x 0.007 and x 0.875 (reducing the negative x 4 -term to a x 3 -term). We have proofed the last inequality &gt; 0 of Eq. (275).</p><p>Consequently the derivative is always positive independent of y, thus</p><formula xml:id="formula_218">e (x+y) 2 2x erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x<label>(282)</label></formula><p>is strictly monotonically increasing in x.</p><p>Next we show that the sub-function Eq. (111) is smaller than zero. We consider the limit:</p><formula xml:id="formula_219">lim x→∞ e (x+y) 2 2x erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x = 0<label>(283)</label></formula><p>The limit follows from Lemma 22. Since the function is monotonic increasing in x, it has to approach 0 from below. Thus,</p><formula xml:id="formula_220">e (x+y) 2 2x erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x<label>(284)</label></formula><p>is smaller than zero.</p><p>We now consider the derivative of sub-function Eq. (111) with respect to y. We proofed that subfunction Eq. (111) is strictly monotonically increasing independent of y.   For the first inequality we applied Lemma 24 which states that the function xe x 2 erfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = −0.01 to make the positive term less positive. </p><formula xml:id="formula_221">erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x<label>(293)</label></formula><p>is strictly monotonically increasing in y for x = 0.128.</p><p>Next, we consider x = 0.24 • 0.9 = 0.216, which is the minimal τ = 0.9 (here we consider 0.9 as lower bound for τ ). We insert the minimum x = 0. </p><formula xml:id="formula_222">erfc x + y √ 2 √ x − 2e (2x+y) 2 2x erfc 2x + y √ 2 √ x<label>(296)</label></formula><p>is strictly monotonically increasing in y for x = 0.216. </p><formula xml:id="formula_223">2 erfc µω + ντ √ 2 √ ντ − 2e µω+2•ντ √ 2 √ ντ 2 erfc µω + 2 • ντ √ 2 √ ντ .<label>(297)</label></formula><p>The derivative of the equation above with respect to</p><p>• ν is larger than zero;</p><p>• τ is smaller than zero for maximal ν = 0.7, ν = 0.16, and ν = 0.24 (with 0.9 τ );</p><p>• y = µω is larger than zero for ντ = 0.00875   We now consider the derivative with respect to τ , which is not trivial since τ is a factor of the whole expression. The sub-expression should be maximized as it appears with negative sign in the mapping for ν.</p><p>First, we consider the function for the largest ν = 0.7 and the largest y = µω = 0.01 for determining the derivative with respect to τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The expression becomes</head><formula xml:id="formula_224">τ   e 7τ + 1 √ 2 √ 7τ 10 2 erfc   7τ + 1 √ 2 7τ 10   − 2e 2•7τ + √ 2 √ 7τ 10 2 erfc   2•7τ + 1 √ 2 7τ 10      .<label>(300)</label></formula><p>The derivative with respect to τ is</p><formula xml:id="formula_225">√ π e (70τ +1) 2 14000τ (700τ (7τ + 20) − 1) erfc 70τ + 1 20 √ √ τ − (301) 2e (140τ +1) 2 14000τ (2800τ (7τ + 5) − 1) erfc 140τ + 1 20 √ √ τ + 20 √ 35(210τ − 1) √ τ 14000 √ πτ −1 .</formula><p>We are considering only the numerator and use again the approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>.</p><p>The error analysis on the whole numerator gives an approximation error 97 &lt; E &lt; 186. Therefore we add 200 to the numerator when we use the approximation Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>. We obtain the inequalities: with respect to τ is smaller than zero for maximal ν = 0.7.</p><formula xml:id="formula_226">√ π e (70τ +1) 2 14000τ (700τ (7τ + 20) − 1) erfc 70τ + 1 20 √ √ τ − (302) 2e (140τ +1) 2 14000τ (2800τ (7τ + 5) − 1) erfc 140τ + 1 20 √ √ τ + 20 √ 35(210τ − 1) √ τ √ π     2.911(700τ (7τ + 20) − 1) √ π(2.911−1)(70τ +1) √ √ τ + π 70τ +1 √ √ τ 2 + 2.911 2 − 2 • 2.911(2800τ (7τ + 5) − 1) √ π(2.911−1)(140τ +1) √ √ τ + π 140τ +1 √ √ τ 2 + 2.911 2     + 20 √ 35(210τ − 1) √ τ + 200 = √ π   (700τ (7τ + 20) − 1) 20 • √ 35 • 2.911 √ τ √ π(2.911 − 1)(70τ + 1) + 20 • 2.911 √ 35 √ τ 2 + π(70τ + 1) 2 − 2(2800τ (7τ + 5) − 1) 20 • √ • 2.911 √ τ √ π(2.911 − 1)(140τ + 1) + 20 • √ • 2.911 √ τ 2 + π(140τ + 1) 2   + 20 √ 35(210τ − 1) √ τ + 200 = 20 √ 35(210τ − 1) √ τ +</formula><p>Next, we consider the function for the largest ν = 0.16 and the largest y = µω = 0.01 for determining the derivative with respect to τ . </p><formula xml:id="formula_227">+ 1 √ 2 16τ 100      .<label>(309)</label></formula><p>The derivative with respect to τ is √ π e (16τ +1) 2 3200τ</p><formula xml:id="formula_228">(128τ (2τ + 25) − 1) erfc 16τ + 1 40 √ √ τ − (310) 2e (32τ +1) 2 3200τ (128τ (8τ + 25) − 1) erfc 32τ + 1 40 √ √ τ + 40 √ 2(48τ − 1) √ τ 3200 √ πτ −1 .</formula><p>We are considering only the numerator and use again the approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>.</p><p>The error analysis on the whole numerator gives an approximation error 1.1 &lt; E &lt; 12. Therefore we add 20 to the numerator when we use the approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>. We obtain the inequalities: .</p><formula xml:id="formula_229">√ π e (16τ +1) 2 3200τ (128τ (2τ + 25) − 1) erfc 16τ + 1 √ 2 √ τ − (311) 2e (32τ +1) 3200τ (128τ (8τ + 25) − 1) erfc 32τ + 1 40 √ √ τ + 40 √ 2(48τ − 1) √ τ √ π     2.911(128τ (2τ + 25) − 1) √ π(2.911−1)(16τ +1) √ 2 √ τ + π 16τ +1 40 √ 2 √ τ 2 + 2.911 2 − 2 • 2.911(128τ (8τ + 25) − 1) √ π(2.911−1)(32τ +1) √ 2 √ τ + π 32τ +1 √ √ τ 2 + 2.911 2     + 40 √ 2(48τ − 1) √ τ + 20 = √ π   (128τ (2τ + 25) − 1)</formula><p>After applying the approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref> and adding 20, we first factored out √ √ τ . Then we brought all terms to the same denominator.</p><p>We now consider the numerator: with respect to τ is smaller than zero for maximal ν = 0.16.</p><formula xml:id="formula_230">40 √ 2(48τ − 1) √ τ + 20 √ π(2.911 − 1)(16τ + 1) + 40 √ 22.911 √ τ 2 + π(16τ + 1) 2 (312) √ π(2.911 − 1)(32τ + 1) + 40 √ 22.911 √ τ 2 + π(32τ + 1) 2 + 2.911 • 40 √ √ π(128τ (2τ + 25) − 1) √ τ √ π(2.911 − 1)(32τ + 1) + 40 √ 22.911 √ τ 2 + π(32τ + 1) 2 − 2 √ π40 √</formula><p>Next, we consider the function for the largest ν = 0.24 and the largest y = µω = 0.01 for determining the derivative with respect to τ . However we assume 0.9 τ , in order to restrict the domain of τ .</p><p>The expression becomes τ   e</p><formula xml:id="formula_231">24τ 100 + 1 100 √ 2 √ 24τ 2 erfc   24τ 100 + 1 100 √ 2 24τ   − e 2 24τ + 1 100 √ 2 √ 24τ 2 erfc   2 24τ 100 + 1 100 √ 2 24τ      .<label>(314)</label></formula><p>The derivative with respect to τ is √ π e (24τ +1) 2 4800τ</p><formula xml:id="formula_232">(192τ (3τ + 25) − 1) erfc 24τ + 1 √ 3 √ τ − (315) 2e (48τ +1) 2 4800τ (192τ (12τ + 25) − 1) erfc 48τ + 1 √ 3 √ τ + 40 √ 3(72τ − 1) √ τ 4800 √ πτ −1 .</formula><p>We are considering only the numerator and use again the approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>.</p><p>The error analysis on the whole numerator gives an approximation error 14 &lt; E &lt; 32. Therefore we add 32 to the numerator when we use the approximation of Ren and MacKenzie <ref type="bibr" target="#b34">[30]</ref>. We obtain the inequalities:  First we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for τ for some positive terms and value of 0.9 for negative terms. These terms are then expanded at the =-sign. The next equality factors the terms under the squared root. We decreased the negative term by setting τ = τ + 0.0000769518 under the root. We increased positive terms by setting τ + 0.0000769518 = 1.0000962τ and τ + 0.0000766694 = 1.0000962τ under the root for positive terms. The positive terms are increase, since 0.8+0.0000769518 0.8 &lt; 1.0000962, thus τ + 0.0000766694 &lt; τ + 0.0000769518 1.0000962τ . For the next inequality we decreased negative terms by inserting τ = 0.9 and increased positive terms by inserting τ = 1. <ref type="bibr" target="#b29">25</ref>. The next equality expands the terms. We use upper bound of 1.25 and lower bound of 0.9 to obtain terms with corresponding exponents of τ . with respect to τ is smaller than zero for maximal ν = 0.24 and the domain 0.9 τ 1. <ref type="bibr" target="#b29">25</ref>. </p><formula xml:id="formula_233">√ π e (<label>24τ +1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consequently</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4 Additional information on experiments</head><p>In this section, we report the hyperparameters that were considered for each method and data set and give details on the processing of the data sets.    <ref type="table" target="#tab_4">Table A13</ref>: UCI comparison reporting the average rank of a method on 46 classification task of the UCI machine learning repository with more than 1000 data points. For each dataset, the 24 compared methods, were ranked by their accuracy and the ranks were averaged across the tasks. The first column gives the method group, the second the method, the third the average rank , and the last the p-value of a paired Wilcoxon test whether the difference to the best performing method is significant. For the Tox21 data set, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using the validation set defined by the challenge winners <ref type="bibr" target="#b32">[28]</ref>. The hyperparameter space was chosen to be similar to the hyperparameters that were tested by Mayr et al. <ref type="bibr" target="#b32">[28]</ref>. The early stopping parameter was determined on the smoothed learning curves of epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested "rectangular" and "conic" layers -rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. All methods had the chance to adjust their hyperparameters to the data set at hand.  Distribution of network inputs. We empirically checked the assumption that the distribution of network inputs can well be approximated by a normal distribution. To this end, we investigated the density of the network inputs before and during learning and found that these density are close to normal distributions (see <ref type="figure" target="#fig_59">Figure A8</ref>).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5 Other fixed points</head><p>A similar analysis with corresponding function domains can be performed for other fixed points, for example for µ =μ = 0 and ν =ν = 2, which leads to a SELU activation function with parameters α 02 = 1.97126 and λ 02 = 1.06071.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6 Bounds determined by numerical methods</head><p>In this section we report bounds on previously discussed expressions as determined by numerical methods (min and max have been computed). </p><formula xml:id="formula_234">(</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 )</head><label>3</label><figDesc>has a stable and attracting fixed point depending on (ω, τ ) in the (µ, ν)-domain: µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617]. All points within the (µ, ν)-domain converge when iteratively applying the mapping Eq. (3) to this fixed point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 3 (. 1 .</head><label>31</label><figDesc>Increasing ν). We consider λ = λ 01 , α = α 01 and the domain Ω − : −0.1 µ 0.1, and −0.1 ω 0For the domain 0.02 ν 0.16 and 0.8 τ 1.25 as well as for the domain 0.02 ν 0.24 and 0.9 τ 1.25, the mapping of the varianceν(µ, ω, ν, τ, λ, α) given in Eq. (5) increases:ν(µ, ω, ν, τ, λ 01 , α 01 ) &gt; ν.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The affine transformation a(xd + α (1 − d)) + b allows to determine parameters a and b such that mean and variance are kept to their values: E(a(xd + α (1 − d)) + b) = µ and Var(a(xd + α (1 − d)) + b) = ν . In contrast to dropout, a and b will depend on µ and ν, however our SNNs converge to activations with zero mean and unit variance. With µ = 0 and ν = 1, we obtain a = q + α 2 q(1 − q) −1/2 and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>A2. 1 1 )</head><label>11</label><figDesc>Theorem 1: Stable and Attracting Fixed Points Close to (0,Theorem 1 shows that the mapping g defined by Eq. (4) and Eq. (5) exhibits a stable and attracting fixed point close to zero mean and unit variance. Theorem 1 establishes the self-normalizing property of self-normalizing neural networks (SNNs). The stable and attracting fixed point leads to robust learning through many layers. Theorem 1 (Stable and Attracting Fixed Points). We assume α = α 01 and λ = λ 01 . We restrict the range of the variables to the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.95, 1.1]. For ω = 0 and τ = 1, the mapping Eq. (4) and Eq. (5) has the stable fixed point (µ, ν) = (0, 1). For other ω and τ the mapping Eq. (4) and Eq. (5) has a stable and attracting fixed point depending on (ω, τ ) in the (µ, ν)-domain: µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617]. All points within the (µ, ν)-domain converge when iteratively applying the mapping Eq. (4) and Eq. (5) to this fixed point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. Consequently, the variance mapping Eq. (5) and Eq. (4) ensures a lower bound on the variance ν. A3 Proofs of the Theorems A3.1 Proof of Theorem 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.95, 1.1]. For ω = 0 and τ = 1, the mapping Eq. (4) and Eq. (5) has the stable fixed point (µ, ν) = (0, 1). For other ω and τ the mapping Eq. (4) and Eq. (5) has a stable and attracting fixed point depending on (ω, τ ) in the (µ, ν)-domain: µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617]. All points within the (µ, ν)-domain converge when iteratively applying the mapping Eq. (4) and Eq. (5) to this fixed point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>π − a 2</head><label>2</label><figDesc>+ 2a(x + y) 0. We have ∂ ∂x 8x π − a 2 + 2a(x + y) = 8 π − 2a &gt; 0 and ∂ ∂y 8x π − a 2 + 2a(x + y) = −2a &lt; 0. Therefore the minimum is at border for minimal x and maximal y:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Finally a quadratic equations was solved. The sub-function has its minimal value for minimal x = ντ = 1.5 • 0.8 = 1.2 and minimal y = µω = −1 • 0.1 = −0.1. We further minimize the function µωe µ 2 ω 2 2ντ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>8 ν 1 . 5 ,</head><label>15</label><figDesc>and 0.8 τ 1.25 for α = α 01 and λ = λ 01 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 πντ e − µ 2 ω 2 2ντ</head><label>22</label><figDesc>is bounded by |J 12 | 0.194145 in the domain −0.1 µ 0.1, −0.1 ω 0.1, 0.8 ν 1.5, and 0.8 τ 1.25 for α = α 01 and λ = λ 01 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Lemma 12 (</head><label>12</label><figDesc>Largest Singular Value Smaller Than One). We set α = α 01 and λ = λ 01 and restrict the range of the variables to µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>94) 0.32112 • 0.0068097371 + 2.63690 • 0.0008292885+ 2.28242 • 0.0009580840 + 2.98610 • 0.0007323095 &lt; 0.008747 . For a grid with grid length ∆µ = 0.0068097371, ∆ω = 0.0008292885, ∆ν = 0.0009580840, and ∆τ = 0.0007323095, we evaluated the function Eq. (61) for the largest singular value in the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].We did this using a computer. According to Subsection A3.<ref type="bibr" target="#b8">4</ref>.5 the precision if regarding error propagation and precision of the implemented functions is larger than 10 −13 . We performed the evaluation on different operating systems and different hardware architectures including CPUs and GPUs. In all cases the function Eq. (61) for the largest singular value of the Jacobian is bounded by 0.9912524171058772.We obtain from Eq. (94):S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ 01 , α 01 ) 0.9912524171058772 + 0.008747 &lt; 1 .(95)A3.4.2 Lemmata for proofing Theorem 1 (part 2): Mapping within domain We further have to investigate whether the the mapping Eq. (4) and Eq. (5) maps into a predefined domains. Lemma 13 (Mapping into the domain). The mapping Eq. (4) and Eq. (5) map for α = α 01 and λ = λ 01 into the domain µ ∈ [−0.03106, 0.06773] and ν ∈ [0.80009, 1.48617] with ω ∈ [−0.1, 0.1] and τ ∈ [0.95, 1.1]. Proof. We use Lemma 8 which states that with given sign the derivatives of the mapping Eq. (4) and Eq. (5) with respect to α = α 01 and λ = λ 01 are either positive or have the sign of ω. Therefore with given sign of ω the mappings are strict monotonic and the their maxima and minima are found at the borders. The minimum ofμ is obtained at µω = −0.01 and its maximum at µω = 0.01 and σ and τ at their minimal and maximal values, respectively. It follows that: −0.03106 &lt;μ(−0.1, 0.1, 0.8, 0.95, λ 01 , α 01 ) μ μ(0.1, 0.1, 1.5, 1.1, λ 01 , α 01 ) &lt; 0.06773, (96) and thatμ ∈ [−0.1, 0.1]. Similarly, the maximum and minimum ofξ( is obtained at the values mentioned above: 0.80467 &lt;ξ(−0.1, 0.1, 0.8, 0.95, λ 01 , α 01 ) ξ ξ (0.1, 0.1, 1.5, 1.1, λ 01 , α 01 ) &lt; 1.48617. (97) Since |ξ −ν| = |μ 2 | &lt; 0.004597, we can conclude that 0.80009 &lt;ν &lt; 1.48617 and the variance remains in [0.8, 1.5]. Corollary 14. The image g(Ω ) of the mapping g : (µ, ν) → (μ,ν) (Eq. (8)) and the domain Ω = {(µ, ν)| − 0.1 µ 0.1, 0.8 µ 1.5} is a subset of Ω :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>for all ω ∈ [−0.1, 0.1] and τ ∈ [0.95, 1.1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>For x and y</head><label></label><figDesc>we obtain: 0.8 • 1.5 = 1.2 x 20 = 1.25 • 16 and 0.1 • (−1) = −0.1 y 0.1 = 0.1 • 1. In the following we assume to remain within this domain. Lemma 15 (Main subfunction). For 1.2 x 20 and −0.1 y 0.1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure A3 :x</head><label>A3</label><figDesc>Left panel: Graphs of the main subfunction f (x, y) = e treated in Lemma 15. The function is negative and monotonically increasing with x independent of y. Right panel: Graphs of the main subfunction at minimal x = 1.2. The graph shows that the function f (1.2, y) is strictly monotonically decreasing in y. Theorem 16 (Contraction ν-mapping). The mapping of the varianceν(µ, ω, ν, τ, λ, α) given in Eq. (5) is contracting for λ = λ 01 , α = α 01 and the domain Ω + : −0.1 µ 0.1, −0.1 ω 0.1, 1.5 ν 16, and 0.8 τ 1.25, that is,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>-</head><label></label><figDesc>Second inequality: We apply Lemma 15. According to Lemma 15 the function Eq. (101) is negative. The largest contribution is to subtract the most negative value of the function Eq. (101), that is, the minimum of function Eq. (101). According to Lemma 15 the function Eq. (101) is strictly monotonically increasing in x and strictly monotonically decreasing in y for x = 1.2. Therefore the function Eq. (101) has its minimum at minimal x = ντ = 1.5 • 0.8 = 1.2 and maximal y = µω = 1.0 • 0.1 = 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>= 1.5 • 0.8 = 1.2 and the maximal y = µω = 1.0 • 0.1 = 0.1. -Sixth inequality: evaluation of the terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>For λ = λ 01 and α = α 01 , we consider the domain −0.1 µ 0.1, −0.1 ω 0.1 0.00875 ν 0.7, and 0.8 τ 1.25. For x and y we obtain: 0.8 • 0.00875 = 0.007 x 0.875 = 1.25 • 0.7 and 0.1 • (−0.1) = −0.01 y 0.01 = 0.1 • 0.1. In the following we assume to be within this domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>17 (</head><label>17</label><figDesc>Main subfunction Below). For 0.007 x 0.875 and −0.01 y 0.01, the function e (x+y) 2 2x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>111) smaller than zero, is strictly monotonically increasing in x and strictly monotonically increasing in y for the minimal x = 0.007 = 0.00875 • 0.8, x = 0.56 = 0.7 • 0.8, x = 0.128 = 0.16 • 0.8, and x = 0.216 = 0.24 • 0.9 (lower bound of 0.9 on τ ). Proof. See proof 45. Lemma 18 (Monotone Derivative). For λ = λ 01 , α = α 01 and the domain −0.1 µ 0.1, −0.1 ω 0.1, 0.00875 ν 0.7, and 0.8 τ 1.25. We are interested of the derivative of τ e µω+ντ √ 2 √ ντ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>= 0.007, ντ = 0.70.8 = 0.56, ντ = 0.160.8 = 0.128, and ντ = 0.24 • 0.9 = 0.216. Proof. See proof 46. A3.4.5 Computer-assisted proof details for main Lemma 12 in Section A3.4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Lemma 19 (</head><label>19</label><figDesc>Mean value theorem). For a real-valued function f which is differentiable in the closed interval [a, b], there exists t ∈ [0, 1] with f (a) − f (b) = ∇f (a + t(b − a)) • (a − b) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Lemma 20 .</head><label>20</label><figDesc>If the values µ, ω, ν, τ have a precision of , the singular value (Eq. (61)) evaluated with the formulas given in Eq. (54) and Eq. (61) has a precision better than 292 .This means for a machine with a typical precision of 2 −52 = 2.220446 • 10 −16 , we have the rounding error ≈ 10 −16 , the evaluation of the singular value (Eq. (61)) with the formulas given in Eq. (54) and Eq. (61) has a precision better than 10 −13 &gt; 292 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>5</head><label>5</label><figDesc>ulps." According to https://www.mirbsd.org/htman/i386/man3/exp.htm and http: //man.openbsd.org/OpenBSD-current/man3/exp.3: "exp(x), log(x), expm1(x) and log1p(x) are accurate to within an ulp"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>2 *</head><label>2</label><figDesc>exp(−x^2)/(sqrt(pi)*(sqrt(x^2+4/pi)+x)) 2*exp(−x^2)/(sqrt(pi)*(sqrt(x^2+2)+x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>min , ν max ] = [0.8, 1.5], and τ ∈ [τ min , τ max ] = [0.8, 1.25]. For bounding different partial derivatives we need properties of different functions. We will bound a the absolute value of a function by computing an upper bound on its maximum and a lower bound on its minimum. These bounds are computed by upper or lower bounding terms. The bounds get tighter if we can combine terms to a more complex function and bound this function. The following lemmata give some properties of functions that we will use in bounding complex functions. Throughout this work, we use the error function erf(x) := 1 √ π x −x e −t 2 and the complementary error function erfc(x) = 1 − erf(x). Lemma 21 (Basic functions). exp(x) is strictly monotonically increasing from 0 at −∞ to ∞ at ∞ and has positive curvature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure A5 :</head><label>A5</label><figDesc>Graphs of the functions e x 2 erfc(x) (left) and xe x 2 erfc(x) (right) treated in Lemma 23 and Lemma 24, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Lemma 24 (</head><label>24</label><figDesc>Properties of xe x 2 erfc(x)). The function xe x 2 erfc(x) has the sign of x and is monotonically increasing to 1 √ π .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>√ π . Lemma 25 (</head><label>25</label><figDesc>Function µω). h 11 (µ, ω) = µω is monotonically increasing in µω. It has minimal value t 11 = −0.01 and maximal value T 11 = 0.01. Proof. Obvious. Lemma 26 (Function ντ ). h 22 (ν, τ ) = ντ is monotonically increasing in ντ and is positive. It has minimal value t 22 = 0.64 and maximal value T 22 = 1.875. Proof. Obvious.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>−2x(−1 + α) + ντ α &gt; −2 • 0.01 • (−1 + α 01 ) + 0.8 • 0.8α 01 &gt; 1.0574 &gt; 0, the derivative is larger than zero. Consequently, the original function is increasing in µω. The maximal value is obtained with the minimal ντ = 0.8 • 0.8 and the maximal µω = 0.1 • 0.1. The maximal value is 2 π</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>since −3α 2 x</head><label>2</label><figDesc>− µω(−1 + α 2 ) &lt; −3α 2 01 0.8 • 0.8 + 0.1 • 0.1(−1 + α 2 01 ) &lt; −5.35764.The derivative of the function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>•</head><label></label><figDesc>decreasing. The minimal value that is larger than 0.4349 is taken on at the maximal values ντ = 1.5 • 1.25 and µω = 0.1 • 0.1. The second inequality uses 1 0.4349 √ 2π = 0.545066 &gt; 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>••</head><label></label><figDesc>5 • 1.25 and µω = 0.1 • 0.1. 0.261772 √ π &gt; 0.463979. The equalities are just algebraic reformulations. The last inequality follows from µω (√ ντ − 0.463979µω) + 0.85592(ντ ) 2 − 0.0720421ντ &gt; 0.85592 • (0.8 • 0.8) 2 − 0.1 • 0.1 √ 1.5 • 1.25 + 0.1 • 0.1 • 0.463979 − 0.0720421 • 1.5 • 1.25 &gt; 0.201766.Therefore the function is increasing in ντ .Decreasing in µω follows from decreasing of e x 2 erfc(x) according to Lemma 23. Positivity follows from the fact that erfc and the exponential function are positive and that ντ &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>2 √</head><label>2</label><figDesc>ντ in brackets. An upper bound on the maximum is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Lemma 40 (</head><label>40</label><figDesc>Derivatives of the Mapping). We assume α = α 01 and λ = λ 01 . We restrict the range of the variables to the domain µ ∈ [−0.1, 0.1], ω ∈ [−0.1, 0.1], ν ∈ [0.8, 1.5], and τ ∈ [0.8, 1.25].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>-−-</head><label></label><figDesc>Second inequality part 1: we applied 0 &lt; 2y =⇒ (2x + y) 2 + 4x + 1 &lt; (2x + y) 2 + 2(2x + y) + 1 = (2x + y + 1) 2 . (216) -Second inequality part 2: we show that for a = 1 20 2048+169π π − 13 following holds: 8x π − a 2 + 2a(x + y) 0. We have ∂ ∂x 8x π − a 2 + 2a(x + y) = 8 π −2a &gt; 0 and ∂ ∂y 8x π − a 2 + 2a(x + y) = −2a &gt; 0. Therefore the minimum is at border for minimal x and maximal y: 13 &gt; 0.782. Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x + y) + 0.782). -We set α = α 01 and multiplied out. Thereafter we also factored out x in the numerator. Finally a quadratic equations was solved. The sub-function has its minimal value for minimal x and minimal y x = ντ = 0.8 • 0.8 = 0.64 and y = µω = −0.1 • 0.1 = −0.01. We further minimize the function µωe µ 2 ω 2 2ντ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head></head><label></label><figDesc>2(2(x + y) + 0.782) − (2(2x + y) + 1)) √ π((2(x + y) + 0.782)(2(2x + y) x + y) + 0.782)(2(2x + y) + 1)) &gt; 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>---</head><label></label><figDesc>Second inequality part 1: we applied 0 &lt; 2y =⇒ (2x + y) 2 + 4x + 1 &lt; (2x + y) 2 + 2(2x + y) + 1 = (2x + y + 1)<ref type="bibr" target="#b6">2</ref> .(223) Second inequality part 2: we show that for a = π − a 2 + 2a(x + y) 0. We have ∂ ∂x 8x π − a 2 + 2a(x + y) =8   π −2a &gt; 0 and ∂ ∂y 8x π − a 2 + 2a(x + y) = −2a &lt; 0. Therefore the minimum is at border for minimal x and maximal y: Equalities only solve square root and factor out the resulting terms (2(2x + y) + 1) and (2(x + y) + 0.782).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Lemma 41 (√</head><label>41</label><figDesc>Mean at low variance). The mapping of the meanμ (Eq. (4)) µ(µ, ω, ν, τ, λ, α) ντ e − µ 2 ω 2 2ντ + 2µω in the domain −0.1 µ −0.1, −0.1 ω −0.1, and 0.02 ντ 0.5 is bounded by |μ(µ, ω, ν, τ, λ 01 , α 01 )| &lt; 0.289324 (228) and lim ν→0 |μ(µ, ω, ν, τ, λ 01 , α 01 )| = λµω.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>xFigure A6 :</head><label>A6</label><figDesc>The graph of functionμ for low variances x = ντ for µω = 0.01, where x ∈ [0, 3], is displayed in yellow. Lower and upper bounds based on the Abramowitz bounds (Lemma 22) are displayed in green and blue, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>|λω| 2 −</head><label>2</label><figDesc>e 0.0353553 2 erfc (0.0353553) + αe 0.106066 2 erfc (0.106066) &lt; 0.133</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Figure A7 :</head><label>A7</label><figDesc>The graph of the function h(x) =μ 2 (0.1, −0.1, x, 1, λ 01 , α 01 ) is displayed. It has a local maximum at x = ντ ≈ 0.187342 and h(x) ≈ 0.00451457 in the domain x ∈ [0, 1].We use the argumentation that the term with the error function is monotonically decreasing (Lemma 23) again for the expression Sinceμ is symmetric in ν and τ , we only have to chance outermost term<ref type="bibr" target="#b5">1</ref> 4 λτ to 1 4 λν to obtain the estimate ∂ ∂τμ &lt; 0.11. Lemma 43 (Tight bound onμ 2 in Ω − ). The functionμ 2 (µ, ω, ν, τ, λ 01 , α 01 ) (Eq. (4)) is bounded by μ 2 &lt; 0.005 (236) (237) in the domain Ω − = {µ, ω, ν, τ | − 0.1 µ 0.1, −0.1 ω 0.1, 0.05 ν 0.24, 0.8 τ 1.25}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head></head><label></label><figDesc>+ 0.001498041 • 0.08101072 + 0.001498041 • 0.08101072+ 0.0004033190 • 0.30089696 + 0.0019065994 • 0.06365128 &lt; 0.005.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head></head><label></label><figDesc>For 1.2 x 20 and −0.1 y 0.1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head></head><label></label><figDesc>(101) with respect to x. The derivative of the function e (x+y) 2 2x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>− 2 ( 2 − 2 (− 2 ( 1 =(</head><label>22221</label><figDesc>911 2 x + (2.911 − 1)(2x + y) √ π   √ π − 0.0131259 = (3x − y) + 2.911   (x − y)(x + y)(2.911 − 1)(x + y) + (x + y) 2 + 2•2.911 2 x π 2x − y)(2x + y) (2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.− y)(2x + y) (2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.y)(x + y) (2.911 − 1)(x + y) + x + y + 2.911 2 π 2x − y)(2x + y) (2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.911 2 x π   − 0.0131259 = (3x − y) + 2.911   (x − y)(x + y) 2.911(x + y) + 2.911 2 π − 2(2x − y)(2x + y) (2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.911 2 x π   − 0.0131259 = (3x − y) + (x − y)(x + y) (x + y) + 2.911 π 2x − y)(2x + y)2.911(2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.y − 0.0131259) (2.911 − 1)(2x + y) + (2x + y) 2 + 2 • 2.911 2 x π + (x − y)(x + y) (2.911 − 1)(2x + y) + (2x + y) 2 + 2 • 2.911 2 x π (x + y) + 2.911 π (2.911 − 1)(2x + y) + (2x + y) 2 + 2 • 2.911 2 x π −(x − y)(x + y) + (3x − y − 0.0131259)(x + y + 0.9266)) (2x + y) 2 + 5.39467x + 3.822x + 1.911y − (249) 5.822(2x − y)(x + y + 0.9266)(2x + y)) (x + y) + 2.911 π (2.911 − 1)(2x + y) + (2x + y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head></head><label></label><figDesc>, which we show to be positive in order to show &gt; 0 in Eq. (248). The numerator is ((x − y)(x + y) + (3x − y − 0.0131259)(x + y + 0.9266)) (2x + y) 2 + 5.39467x + 3.822x + 1.911y − (250) 5.822(2x − y)(x + y + 0.9266)(2x + y) = − 5.822(2x − y)(x + y + 0.9266)(2x + y) + (3.822x + 1.911y)((x − y)(x + y)+ (3x − y − 0.0131259)(x + y + 0.9266)) + ((x − y)(x + y)+ (3x − y − 0.0131259)(x + y + 0.9266)) (2x + y) 2 + 5.39467x = − 8.0x 3 + 4x 2 + 2xy + 2.76667x − 2y 2 − 0.939726y − 0.0121625 (2x + y) 2 + 5.39467x− 8.0x 2 y − 11.0044x 2 + 2.0xy 2 + 1.69548xy − 0.0464849x + 2.0y 3 + 3.59885y 2 − 0.0232425y = − 8.0x 3 + 4x 2 + 2xy + 2.76667x − 2y 2 − 0.939726y − 0.0121625 (2x + y) 2 + 5.39467x− 8.0x 2 y − 11.0044x 2 + 2.0xy 2 + 1.69548xy − 0.0464849x + 2.0y 3 + 3.59885y 2 − 0.0232425y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>2 .</head><label>2</label><figDesc>Lemma 45 (Main subfunction below). For 0.007 x 0.875 and −0.01 y 0.01, the function e smaller than zero, is strictly monotonically increasing in x and strictly monotonically increasing in y for the minimal x = 0.007 = 0.00875 • 0.8, x = 0.56 = 0.7 • 0.8, x = 0.128 = 0.16 • 0.8, and x = 0.216 = 0.24 • 0.9 (lower bound of 0.9 on τ ). Proof. We first consider the derivative of sub-function Eq. (111) with respect to x. y)(x + y) erfc x+y √ 2 √</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>( 2 .− 2 (− 2 ( 2 π 2 − 2 (− 2 (− 2 (− 2 (</head><label>222222222</label><figDesc>911 − 1)(x + y) + (x + y) 2 + 2•2.911 2 x π 2x − y)(2x + y)(2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.911 2 x π   − 0.0003 (3x − y) + 2.911   (x − y)(x + y) (2.911 − 1)(x + y) + 2.911 2 π 2 + (x + y) 2 + 2•2.911 2 x π + 2•2.911 2 y π 2x − y)(2x + y) (2.911 − 1)(2x + y) + (2x + y) + 2•2.y)(x + y)(2.911 − 1)(x + y) + x + y + 2.911 2x − y)(2x + y) (2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.911 2 x π   − 0.0003 = (3x − y) + 2.911   (x − y)(x + y) 2.911(x + y) + 2.911 2 π 2x − y)(2x + y) (2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.911 2 x π   − 0.0003 = (3x − y) + (x − y)(x + y) (x + y) + 2.911 π 2x − y)(2x + y)2.911 (2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.911 2 x π − 0.0003 = (3x − y) + (x − y)(x + y) (x + y) + 2.911 π 2x − y)(2x + y)2.911 (2.911 − 1)(2x + y) + (2x + y) 2 + 2•2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>• 4 .</head><label>4</label><figDesc>168614250 • 10 −7 − y 2 2.049216091 • 10 −7 − 0.0279456x 5 + (281) 43.0875x 4 y + 30.8113x 4 + 43.1084x 3 y 2 + 68.989x 3 y + 41.6357x 3 + 10.7928x 2 y 3 − 13.1726x 2 y 2 − 27.8148x 2 y − 0.00833715x 2 + 0.0139728xy 4 + 5.47537xy 3 + 4.65089xy 2 + 0.00277916xy − 10.7858y 5 − 12.2664y 4 + 0.00436492y 3 &gt; x • 4.168614250 • 10 −7 − (0.01) 2 2.049216091 • 10 −7 − 0.0279456x 5 + 0.0 • 43.0875x 4 + 30.8113x 4 + 43.1084(0.0) 2 x 3 + 0.0 • 68.989x 3 + 41.6357x 3 + 10.7928(0.0) 3 x 2 − 13.1726(0.01) 2 x 2 − 27.8148(0.01)x 2 − 0.00833715x 2 + 0.0139728(0.0) 4 x + 5.47537(0.0) 3 x + 4.65089(0.0) 2 x+ 0.0 • 0.00277916x − 10.7858(0.01) − 12.2664(0.01) 4 + 0.00436492(0.0) 3 = x • 4.168614250 • 10 −7 − 1.237626189 • 10 −7 − 0.0279456x 5 + 30.8113x 4 + 41.6357x 3 − 0.287802x 2 &gt; • 10 −7 + 30.8113x 4 − (0.875) • 0.0279456x 4 + 41.6357x 3 − (0.287802x)x 2 0.007 = 30.7869x 4 + 0.160295x 3 &gt; 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head></head><label></label><figDesc>In the proof of Theorem 3, we need the minimum of sub-function Eq. (111). First, we are interested in the derivative of subfunction Eq. (111) with respect to y for the minimum x = 0.007 = 7/1000.Consequently, we insert the minimum x = 0.007 = 7/1000 into the sub-function Eq. (111):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>For</head><label></label><figDesc>the first inequality, we use Lemma 24. Lemma 24 says that the function xe x 2 erfc(x) has the sign of x and is monotonically increasing to 1 √ π . Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = −0.01 to make the positive term less positive. increasing in y for the minimal x = 0.007. Next, we consider x = 0.7 • 0.8 = 0.56, which is the maximal ν = 0.7 and minimal τ = 0.8. We insert the minimum x = 0.56 = 56/100 into the sub-function Eq. (111):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>Lemma 46 (</head><label>46</label><figDesc>Monotone Derivative). For λ = λ 01 , α = α 01 and the domain −0.1 µ 0.1, −0.1 ω 0.1, 0.00875 ν 0.7, and 0.8 τ 1.25. We are interested of the derivative of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><head></head><label></label><figDesc>• 0.8 = 0.007, ντ = 0.7 • 0.8 = 0.56, ντ = 0.16 • 0.8 = 0.128, and ντ = 0.24 • 0.9 = 0.216. Proof. We consider the domain: −0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head></head><label></label><figDesc>respect to y = µω is larger than zero for ντ = 0.00875 • 0.8 = 0.007, ντ = 0.7 • 0.8 = 0.56, ντ = 0.16 • 0.8 = 0.128, and ντ = 0.24 • 0.9 = 0.216, which also follows directly from Lemma 17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>− 5 .τ 2 − 5 .09079 × 10 6 τ 3 × 10 7 τ 7/ 2 − 3 .5539 × 10 7 τ 3 − 3 .19193 × 10 8 τ 2 =− 5 .</head><label>5253233325</label><figDesc>22.561  −250764.τ 3/2 + 1.8055 × 10 7 τ 5/2 + 115823.τ + √ 1.25 + 5.66103 5.20199 × 10 7 τ 3 + 1.46946 × 10 8 τ 2 + 238086. √ τ + √ 0.9 + 22.561 −3.55709 × 10 7 τ 3 − 1.45741 × 10 8 τ 2 + 304097. √ τ − 5.8118510 6 τ 3/2 − 1.67707 × 10 6 τ 5/2 − 3.44998 × 10 7 τ 7/2 + 422935.τ 2 + 33874.33874.τ 3/2 0.9 3/2 − 9.02866 × 10 6 τ 3/2 + 2.29933 × 10 8 τ 5/2 − 3.44998 × 10 7 τ 7/2 − 3.5539 × 10 7 τ 3 − 3.19193 × 10 8 τ 2 + 1.48578 × 10 6 √ 09079 × 10 6 τ 3/2 + 2.29933 × 10 8 τ 5/2 − 3.44998 × 10 7 τ 7/2 − 3.5539 × 10 7 τ 3 − 3.19193 × 10 8 09079 × 10 6 τ 3/2 − 3.44998 × 10 7 τ 7/2 − 3.5539 × 10 7 τ 3 − 6.21197 × 10 7 τ 2 &lt; 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head>Lemma 47 . 1 2 1 (√ 2 0 ⇐⇒ 1 .</head><label>4711201</label><figDesc>In the domain −0.01 y 0.01 and 0.64 x 1.875, the function f (x, y) = e (2y+x) erfc x+y √ 2x has a global maximum at y = 0.64 and x = −0.01 and a global minimum at y = 1.875 and x = 0.01. Proof. f (x, y) = e 2y+x) erfc x+y √ 2x is strictly monotonically decreasing in x, since its derivative with respect to x is negative: inqualities come from applying Abramowitz bounds 22 and from the fact that the expression does not change monotonicity in the domain and hence the maximum must be found at the border. For x = 0.64 that maximizes the function f (x, y) is monotonically in y, because its derivative w.r.t. y at x = 0.64 is e y 1.37713 erfc(0.883883y + 0.565685) − 1.37349e −0.78125(y+0.64) 2 &lt; 37713 erfc(0.883883y + 0.565685) − 1.37349e −0.78125(y+0.64) 2 &lt; 0 1.37713 erfc(0.883883y + 0.565685) − 1.37349e −0.78125(y+0.64) 2 1.37713 erfc(0.883883 • −0.01 + 0.565685) − 1.37349e −0.78125(0.01+0.64) 2 = 0.5935272325870631 − 0.987354705867739 &lt; 0. (320) Therefore, the values y = 0.64 and x = −0.01 give a global maximum of the function f (x, y) in the domain −0.01 y 0.01 and 0.64 x 1.875 and the values y = 1.875 and x = 0.01 give the global minimum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>Figure A8 :</head><label>A8</label><figDesc>Distribution of network inputs of an SNN for the Tox21 data set. The plots show the distribution of network inputs z of the second layer of a typical Tox21 network. The red curves display a kernel density estimator of the network inputs and the black curve is the density of a standard normal distribution. Left panel: At initialization time before learning. The distribution of network inputs is close to a standard normal distribution. Right panel: After 40 epochs of learning. The distributions of network inputs is close to a normal distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. If mean and variance of x are already within</figDesc><table><row><cell>10 1 10 0</cell><cell></cell><cell></cell><cell>BatchNorm Depth 8 BatchNorm Depth 16 BatchNorm Depth 32 SNN Depth 8 SNN Depth 16 SNN Depth 32</cell><cell>10 1 10 0</cell><cell></cell><cell>BatchNorm Depth 8 BatchNorm Depth 16 BatchNorm Depth 32 SNN Depth 8 SNN Depth 16 SNN Depth 32</cell></row><row><cell>10 3 10 2 Training loss</cell><cell></cell><cell></cell><cell></cell><cell>10 3 10 2 Training loss</cell><cell></cell><cell></cell></row><row><cell>10 4</cell><cell></cell><cell></cell><cell></cell><cell>10 4</cell><cell></cell><cell></cell></row><row><cell>10 5</cell><cell></cell><cell></cell><cell></cell><cell>10 5</cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>250</cell><cell>500</cell><cell>750 1000 1250 1500 1750 2000 Iterations</cell><cell>250</cell><cell>500</cell><cell>750 1250 1500 2000 Iterations</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of FNNs at the Tox21 challenge dataset in terms of AUC. The rows represent different methods and the columns different network depth and for ResNets the number of residual blocks ("na": 32 blocks were omitted due to computational constraints). The deeper the networks, the more prominent is the advantage of SNNs. The best networks are SNNs with 8 layers. ± 0.3 84.4 ± 0.5 84.2 ± 0.4 83.9 ± 0.5 84.5 ± 0.2 83.5 ± 0.5 82.5 ± 0.7 Batchnorm 80.0 ± 0.5 79.8 ± 1.6 77.2 ± 1.1 77.0 ± 1.7 75.0 ± 0.9 73.7 ± 2.0 76.0 ± 1.1 WeightNorm 83.7 ± 0.8 82.9 ± 0.8 82.2 ± 0.9 82.5 ± 0.6 81.9 ± 1.2 78.1 ± 1.3 56.6 ± 2.6 LayerNorm 84.3 ± 0.3 84.3 ± 0.5 84.0 ± 0.2 82.5 ± 0.8 80.9 ± 1.8 78.7 ± 2.3 78.8 ± 0.8 Highway 83.3 ± 0.9 83.0 ± 0.5 82.6 ± 0.9 82.4 ± 0.8 80.3 ± 1.4 80.3 ± 2.4 79.6 ± 0.8 MSRAinit 82.7 ± 0.4 81.6 ± 0.9 81.1 ± 1.7 80.6 ± 0.6 80.9 ± 1.1 80.2 ± 1.1 80.4 ± 1.9 ResNet 82.2 ± 1.1 80.0 ± 2.0 80.5 ± 1.2 81.2 ± 0.7 81.8 ± 0.6 81.2 ± 0.6 na</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">#layers / #blocks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>method</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>SNN</cell><cell>83.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Astronomy: Prediction of pulsars in the HTRU2 dataset. Since a decade, machine learning methods have been used to identify pulsars in radio wave signals</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of FNNs and reference methods at HTRU2 in terms of AUC. The first, fourth and seventh column give the method, the second, fifth and eight column the AUC averaged over 10 cross-validation folds, and the third and sixth column the p-value of a paired Wilcoxon test of the AUCs against the best performing method across the 10 folds. FNNs achieve better results than Naive Bayes (NB), C4.5, and SVM. SNNs exhibit the best performance and set a new record.</figDesc><table><row><cell></cell><cell>FNN methods</cell><cell></cell><cell></cell><cell>FNN methods</cell><cell></cell><cell cols="2">ref. methods</cell></row><row><cell>method</cell><cell>AUC</cell><cell cols="2">p-value method</cell><cell>AUC</cell><cell cols="3">p-value method AUC</cell></row><row><cell>SNN</cell><cell>0.9803 ± 0.010</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MSRAinit</cell><cell>0.9791 ± 0.010</cell><cell>3.5e-01</cell><cell cols="2">LayerNorm 0.9762* ± 0.011</cell><cell>1.4e-02</cell><cell>NB</cell><cell>0.976</cell></row><row><cell cols="2">WeightNorm 0.9786* ± 0.010</cell><cell>2.4e-02</cell><cell cols="2">BatchNorm 0.9760 ± 0.013</cell><cell>6.5e-02</cell><cell>C4.5</cell><cell>0.946</cell></row><row><cell>Highway</cell><cell>0.9766* ± 0.009</cell><cell>9.8e-03</cell><cell>ResNet</cell><cell>0.9753* ± 0.010</cell><cell>6.8e-03</cell><cell>SVM</cell><cell>0.929</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>01 2•0.05•0.8  . Since erfc is monotonically decreasing we inserted the smallest argument via erfc − in order to obtain the maximal negative contribution. Thus, applying Lemma 18, we obtain the lower bound on the derivative:</figDesc><table><row><cell>√</cell><cell>0.01 √ 2 0.05•0.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>01•0.01 2•0.05•0.9 . Since erfc is monotonically decreasing we inserted the smallest argument via erfc −</figDesc><table><row><cell>√</cell><cell>0.01 √ 2 0.05•0.9 in order to obtain the maximal</cell></row><row><cell>negative contribution.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>which can easily be seen by maximizing or minimizing the arguments of the exponential or the square root function. The first term scaled by α is 0.727780 αe µω+ ντ 2 erfc µω+ντ Therefore, the absolute difference between these terms is at most 0.983247 − 0.392294 leading to the derived bound.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ω 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2ντ</cell></row><row><cell cols="4">|λ||τ | |0.983247 − 0.392294|</cell></row><row><cell>0.194035</cell><cell></cell><cell></cell><cell>(65)</cell></row><row><cell cols="4">For the first term we have 0.434947 e µω+ ντ 2 erfc µω+ντ √ 2 √ ντ</cell><cell>0.587622 after Lemma 47 and for</cell></row><row><cell cols="2">the second term 0.582677</cell><cell cols="2">2 πντ e − µ 2 ω 2 2ντ 0.997356, √ 2 √ ντ 0.983247 and the second term scaled by α − 1 is</cell></row><row><cell>0.392294 (α − 1)</cell><cell cols="2">2 πντ e − µ 2 ω 2 2ντ</cell><cell>0.671484.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>11 − H 22 ) 2 + (H 12 + H 21 ) 2 + H 11 + H 22 (H 11 + H 22 ) 2 + (H 21 − H 12 ) 2 &lt; H 12 + H 21 (H 11 − H 22 ) 2 + (H 12 + H 21 ) 2 − H 21 − H 12 (H 11 + H 22 ) 2 + (H 21 − H 12 ) 2 &lt; H 21 − H 12 (H 11 + H 22 ) 2 + (H 21 − H 12 ) 2 + H 12 + H 21 (H 11 − H 22 ) 2 + (H 12 + H 21 ) 2 &lt; |J 12 | |J 11 | 0.0031049101995398316 + 0.031242911235461816 + 0.02220441024325437 + 0.14983446469110305+ 2 • 0.104497 • 0.087653 + 2 • 0.104497 2 + 2 • 0.194035 • 0.087653 + 2 • 0.104497 • 0.194035 &lt; 0.32112, where we used the results from the lemmata 5, 6, 7, and 9. • 1.055872374194189 • 0.087653 + 2 • 0.104497 2 + 2 • 0.031242911235461816 • 0.087653 + 2 • 0.194035 • 0.104497 &lt; 2.63690 , where we used the results from the lemmata 5, 6, 7, and 9 and thatμ is symmetric for µ, ω. |J 11 | |J 12 | + 2 ∂J 12 ∂ν |μ| + 2 |J 12 | 2 2.19916 + 2 • 0.031242911235461816 • 0.087653 + 2 • 0.104497 • 0.194035+ 2 • 0.21232788238624354 • 0.087653 + 2 • 0.194035 2 &lt; 2.28242 ,where we used the results from the lemmata 5, 6, 7, and 9. Derivative of the singular value w.r.t. τ : • 0.03749149348255419 • 0.087653 + 2 • 0.104497 • 0.194035+ 2 • 0.2124377655377270 • 0.087653 + 2 • 0.194035 2 &lt; 2.98610 , where we used the results from the lemmata 5, 6, 7, and 9 and thatμ is symmetric for ν, τ . Lemma 11 (Mean Value Theorem Bound on Deviation from Largest Singular Value). We set α = α 01 and λ = λ 01 and restrict the range of the variables to µ ∈ [µ min , µ max ] = [−0.1, 0.1], ω ∈ [ω min , ω max ] = [−0.1, 0.1], ν ∈ [ν min , ν max ] = [0.8, 1.5], and τ ∈ [τ min , τ max ] = [0.8, 1.25].</figDesc><table><row><cell cols="12">We have ∂S ∂H 11 ∂H 11 ∂ν ∂J 11 ∂ν from which follows using the bounds from Lemma 5: ∂S ∂µ = ∂S ∂H 11 ∂H 11 ∂µ + ∂S ∂H 12 ∂H 12 ∂µ + ∂H 11 ∂ν + ∂S ∂H 12 ∂H 12 ∂ν + ∂S ∂H 21 ∂H 21 ∂ν ∂S ∂ω = ∂S ∂H 11 ∂H 11 ∂ω + ∂S ∂H 12 ∂H 12 ∂ω + + ∂H 12 ∂ν + ∂H 21 ∂ν ∂H 22 + ∂ν ∂S ∂ν = ∂S ∂H 11 ∂H 11 ∂ν + ∂S ∂H 12 ∂H 12 ∂ν + + ∂J 12 ∂ν + ∂J 21 − 2μJ 11 ∂ν ∂J 22 − 2μJ 12 ∂S ∂H 21 ∂S ∂H 21 ∂µ ∂H 22 + ∂H 22 ∂S + ∂H 22 ∂ν ∂S ∂H 21 ∂H 21 ∂ω + ∂S ∂H 22 ∂S ∂H 21 ∂H 21 ∂ν + ∂S ∂H 22 + ∂ν ∂S ∂τ = ∂S ∂H 11 ∂H 11 ∂τ + ∂S ∂H 12 ∂H 12 ∂τ + ∂S ∂H 21 ∂H 21 ∂τ + ∂S ∂H 22 Derivative of the singular value w.r.t. µ: ∂S ∂µ ∂S ∂H 11 ∂H 11 ∂µ + ∂S ∂H 12 ∂H 12 ∂µ + ∂S ∂H 21 ∂H 21 ∂µ + ∂S ∂H 22 ∂H 22 ∂µ ∂J 11 ∂ν + ∂J 12 ∂ν + ∂J 21 ∂ν + ∂J 22 ∂ν + 2 ∂J 11 ∂ν |μ| + 2 ∂S ∂τ</cell><cell>∂H ∂µ ∂H ∂ω ∂H ∂ν ∂H ∂τ</cell><cell>(79) (80) (81) (82) (83) (84) (88)</cell></row><row><cell>∂H 11 ∂µ</cell><cell>+</cell><cell>∂H 12 ∂µ ∂S ∂H 11</cell><cell cols="3">+ ∂H 11 ∂H 21 ∂µ ∂τ +</cell><cell cols="3">+ ∂S ∂H 22 ∂µ ∂H 12 ∂H 12 ∂τ</cell><cell>+</cell><cell cols="2">∂S ∂H 21</cell><cell>∂H 21 ∂τ</cell><cell>+</cell><cell>∂S ∂H 22</cell><cell>∂H 22 ∂τ</cell></row><row><cell>∂J 11 ∂µ</cell><cell>+</cell><cell>∂J 12 ∂µ ∂H 11 ∂τ</cell><cell>+ +</cell><cell cols="4">∂J 21 − 2μJ 11 ∂µ ∂H 12 ∂τ + ∂H 21 + ∂τ</cell><cell cols="4">∂J 22 − 2μJ 12 ∂µ ∂H 22 + ∂τ</cell></row><row><cell cols="12">We obtain ∂S ∂H 11 ∂S ∂ω ∂J 11 ∂µ + |μ| + Derivative of the singular value w.r.t. ω: = 1 2 H − H 22 (H (75) (85) ∂J 12 ∂µ + ∂J 21 ∂µ + ∂J 22 ∂µ + 2 ∂J 11 ∂µ |μ| + 2 |J 11 | 2 + 2 ∂J 12 ∂µ ∂J 11 ∂τ + ∂J 12 ∂τ + ∂J 21 − 2μJ 11 ∂τ + ∂J 22 − 2μJ 12 ∂τ ∂J 11 ∂τ + ∂J 12 ∂τ + ∂J 21 ∂τ + ∂J 22 ∂τ + 2 ∂J 11 ∂τ |μ| + 2 |J 11 | ∂μ ∂τ + 2 ∂J 12 ∂τ |μ| + 2 |J 12 | ∂μ ∂τ (89) 2.82643 + 2</cell></row><row><cell cols="5">1 ∂H 11   ∂S and analogously 1 (H12+H21) 2 (H11−H22) 2 + 1 ∂H 11 ∂ω + ∂S ∂H 12 ∂H 11 ∂ω + ∂H 12 ∂ω + ∂H 21 + ∂ω</cell><cell cols="6">1 (H21−H12) 2 ∂H 12 ∂ω + ∂S ∂H 21 (H11+H22) 2 + 1 ∂H 21   &lt; ∂ω ∂H 22 + ∂ω</cell><cell>1 + 1 2 ∂S + ∂H 22 = 1 ∂H 22 ∂ω</cell></row><row><cell>∂S ∂J 11 ∂ω ∂H 12 ∂J 11 ∂ω</cell><cell>+ = +</cell><cell>1 ∂J 12 ∂ω ∂J 12 ∂ω</cell><cell>+ +</cell><cell cols="3">∂J 21 − 2μJ 11 ∂ω ∂J 21 ∂ω + ∂J 22 ∂ω</cell><cell cols="5">∂J 22 − 2μJ 12 ∂ω + 2 + ∂J 11 ∂ω |μ| + 2 |J 11 |</cell><cell>∂μ ∂ω</cell><cell>+</cell><cell>(76)</cell></row><row><cell cols="8">and ∂S ∂H 21 2 ∂J 12 ∂ω and ∂S 2.38392 + 2 Derivative of the singular value w.r.t. ν: |μ| + 2 |J 12 | ∂μ ∂ω 1 = 1 H 11 + H 22 = ∂H 22 ∂S ∂ν</cell><cell></cell><cell></cell><cell></cell><cell>(86) (77) (78) (87)</cell></row></table><note>(H 11 + H 22 ) 2 + (H 21 − H 12 ) 2 − H 11 − H 22 (H 11 − H 22 ) 2 + (H 12 + H 21 ) 2 &lt; .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>90) 0.32112 |∆µ| + 2.63690 |∆ω| + 2.28242 |∆ν| + 2.98610 |∆τ | . Proof. The mean value theorem states that a t ∈ [0, 1] exists for which S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ 01 , α 01 ) − S(µ, ω, ν, τ, λ 01 , α 01 ) = t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ 01 , α 01 ) ∆µ + 01 , α 01 ) ∆τ from which immediately follows that |S(µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ 01 , α 01 ) − S(µ, ω, ν, τ, λ 01 , α 01 )| (92) ∂S ∂µ (µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ 01 , α 01 ) |∆µ| + ∂S ∂ω (µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ 01 , α 01 ) |∆ω| + ∂S ∂ν (µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ 01 , α 01 ) |∆ν| + ∂S ∂τ (µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ 01 , α 01 ) |∆τ | .</figDesc><table><row><cell>(91)</cell></row><row><cell>∂S ∂µ (µ + ∂S</cell></row></table><note>∂ω (µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ 01 , α 01 ) ∆ω + ∂S ∂ν (µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ 01 , α 01 ) ∆ν + ∂S ∂τ (µ + t∆µ, ω + t∆ω, ν + t∆ν, τ + t∆τ, λ</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>1.5 ν 16, and 0.8 τ 1.25, we first show that the derivative is positive and then upper bound it. of the derivative Eq. (104) is also positive according to Lemma 21. All factors outside the brackets in Eq. (104) are positive. Hence, the derivative Eq. (104) is positive.</figDesc><table><row><cell cols="8">The upper bound of the derivative is:</cell><cell></cell></row><row><cell>1 2</cell><cell cols="5">λ 2 01 τ α 2 01 −e µω+ ντ 2</cell><cell>erfc</cell><cell cols="3">µω + ντ √ 2 √ ντ</cell><cell>+</cell><cell>(107)</cell></row><row><cell cols="5">2α 2 01 e 2µω+2ντ erfc</cell><cell cols="3">µω + 2ντ √ 2 √ ντ</cell><cell cols="2">− erfc</cell><cell>√</cell><cell>µω 2 √ ντ</cell><cell>+ 2 =</cell></row><row><cell>1 2</cell><cell cols="5">λ 2 01 τ α 2 01 −e − µ 2 ω 2 2ντ</cell><cell>e</cell><cell cols="2">(µω+ντ ) 2 2ντ</cell><cell>erfc</cell><cell>µω + ντ √ 2 √ ντ</cell><cell>−</cell></row><row><cell cols="2">2e</cell><cell cols="2">(µω+2ντ ) 2 2ντ</cell><cell>erfc</cell><cell cols="2">µω + 2ντ √ 2 √ ντ</cell><cell></cell><cell cols="2">− erfc</cell><cell>√</cell><cell>µω 2 √ ντ</cell><cell>+ 2</cell></row><row><cell>1 2</cell><cell cols="6">1.25λ 2 01 α 2 01 −e − µ ω 2 2ντ</cell><cell>e</cell><cell cols="2">(µω+ντ ) 2 2ντ</cell></row><row><cell cols="9">According to Lemma 15, the expression</cell></row><row><cell></cell><cell></cell><cell>e</cell><cell cols="2">(µω+ντ ) 2 2ντ</cell><cell>erfc</cell><cell cols="3">µω + ντ √ 2 √ ντ</cell><cell>− 2e</cell><cell>(µω+2ντ ) 2 2ντ</cell><cell>erfc</cell><cell>µω + 2ντ √ 2 √ ντ</cell><cell>(105)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">2 − erfc</cell><cell>√</cell><cell>µω √ ντ</cell><cell>(106)</cell></row></table><note>is negative. This expression multiplied by positive factors is subtracted in the derivative Eq. (104), therefore, the whole term is positive. The remaining term</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Statements follow directly from elementary functions square root and division. Proof. Statements follow directly from Lemma 21 and erfc.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">√</cell><cell></cell><cell></cell></row><row><cell></cell><cell>).</cell><cell></cell><cell>π (α−1) √ ντ</cell><cell cols="3">&gt; 0 and decreasing in ντ .</cell></row><row><cell cols="2">Proof. Lemma 32 (Function 2 − erfc</cell><cell>√</cell><cell cols="2">µω 2 √ ντ ). 2 − erfc</cell><cell>√</cell><cell>µω 2 √ ντ</cell><cell>&gt; 0 and decreasing in ντ and</cell></row><row><cell>increasing in µω.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lemma 33 (Function</cell><cell>π</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>√ ντ &lt; 0 is decreasing in ντ and increasing in µω.</figDesc><table><row><cell cols="2">Lemma</cell><cell cols="2">(Function</cell><cell>2 π</cell><cell></cell><cell>(α 2 −1)µω √ ντ</cell><cell>− 3α 2 √</cell><cell>ντ ).</cell><cell>The</cell><cell>function</cell></row><row><cell>2 π</cell><cell>(α 2 −1)µω √ ντ</cell><cell>− 3α 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">The maximal value is 2 π</cell><cell cols="2">0.1•0.1(α 2 01 −1) (1.5•1.25) 3/2 −</cell><cell>√</cell><cell>3α 2 01 1.5•1.25</cell><cell cols="2">= −4.88869. Therefore the function is</cell></row><row><cell cols="2">negative.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>5 • 1.25 and the maximal µω = 0.1 • 0.1.Proof. The derivative of the function</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>Next we go through all the derivatives, where we useLemma  25, Lemma 26, Lemma 27, Lemma 28, Lemma 29, Lemma 30, Lemma 21, and Lemma 23 without citing. Furthermore, we use the bounds on the simple expressions t 11 ,t 22 , ..., and T 4 as defined the aforementioned lemmata:</figDesc><table><row><cell>• ∂J11 ∂µ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>We use Lemma 31 and consider the expression αe</cell><cell>(µω+ντ ) 2 2ντ</cell><cell>erfc µω+ντ √ 2 √ ντ −</cell><cell>√</cell><cell>2 π (α−1) √ ντ</cell><cell>in</cell></row><row><cell>brackets. An upper bound on the maximum of is</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>We use Lemma 37 to obtain an lower bound on the minimum of this expression:0.8 • 0.8eThe sum of the minimal values of the terms is −2.23019+0.62046+1.45560 = −0.154133. The sum of the maximal values of the terms is −1.72295 + 1.37380 + 1.96664 = 1.61749. −T4 = −0.359403 .Thus, an upper bound on the maximal absolute value isWe use the fact that ∂J22 ∂µ = ∂J21 ∂ν . Thus, an upper bound on the maximal absolute value is</figDesc><table><row><cell>(0.8•0.8+0.1•0.1) 2 2•0.8•0.8 Next we apply Lemma 23 for 2αe α 01 erfc (µω+ντ ) 2 0.8 • 0.8 + 0.1 • 0.1 √ 2 √ 0.8 • 0.8 2ντ erfc µω+ντ √ 2 √ ντ . An upper bound on this expres-= 0.620462 . (185) sion is 2e (0.8•0.8−0.1•0.1) 2 20.8•0.8 α 01 erfc 0.8 • 0.8 − 0.1 • 0.1 √ 2 √ 0.8 • 0.8 = 1.96664 . 2 π √ • ∂J22 ∂µ • ∂J22 ∂τ We apply Lemma 36 to the expression 2 π (α 2 −1)µω √ ντ − 3α 2 √ ντ . t 22 e λ 2 01 α 2 01 (2T + 1)e t 2 2 e −t4 erfc(t 2 ) + 2T 11 (2 − erfc(T 3 )) + (194) α 2 01 (t 11 + 1)e T 2 1 −e −T4 erfc(T 1 ) + 2 π 1 2 λ 2 01 τ max ω max e −t4   α 2 01 −e T 2 1 erfc(T 1 ) + 4α 2 01 e t 2 2 erfc(t 2 ) + 2 π (−1) α 2 − 1 √ T 22  We apply Lemma 37 to the expression ντ e (µω+ντ ) 2 2ντ erfc µω+ντ √ 2 √ ντ . We apply Lemma 38 to  = (201) the expression ντ e (µω+2ντ ) 2 2ντ erfc µω+2ντ √ 2 √ ντ . T 22 e −t4 = 1.146955401845684 . 0.14983446469110305 . We combine the results of these lemmata to obtain an upper bound on the maximum: (186) A lower bound on this expression is • ∂J21 ∂ν • ∂J22 ∂ω 1 4 λ 2 01 −α 2 01 t 22 e −T4 e (T 11 +t 22 ) 2 2t 22 erfc T 11 + t 22 √ 2 √ t 22 + (208)</cell></row><row><cell>2e An upper bound on the maximum is (1.5•1.25+0.1•0.1) 2 2•1.5•1.25 α 01 erfc 1 λ 2 01 τ max ω max e −t4   α 2 01 −e T 2 1 erfc(T 1 ) + 4α 2 1.5 • 1.25 + 0.1 • 0.1 √ 2 √ 1.5 • 1.25 01 e t 2 2 erfc(t 2 ) + = 1.4556 . π (−1) α 2 01 − 1 (187) √ T 22   = An upper bound on the maximum is 1 2 λ 2 µ max τ max e −t4   α 2 01 −e T 1 erfc(T 1 ) + 4α 2 01 e t 2 2 erfc(t 2 ) + π (−1) α 2  8α 2 01 T 22 e −t4 e (t 11 +2T 22 ) 2 2T 22 erfc t 11 + 2T 22 √ √ − T 22 − 1 √ T 22  = 2α 2 01 e T 2 1 e −T4 erfc(T 1 ) + 4α 2 01 e t 2 2 e −t4 erfc(t 2 ) + 2(2 − erfc(T 3 )) +</cell></row><row><cell>(195) (202) (188) 2 e t4 erfc(t 2 ) − erfc(T 3 ) + 2 = (189) t 22 = 2.39669 . Thus, an upper bound on the maximal absolute value is 0.149834 . π e −T4 α 2 01 − T √ − 3α 2 01 √ t 22 0.149834 . 1 8 λ 01 e t4 α T 22 e (t 11 +T 22 ) 2 2T 22 erfc A lower bound on the minimum is A lower bound on the minimum is We combine the results of these lemmata to obtain an lower bound on the minimum: t 11 + T 22 √ 2 √ T 22 + 2α 01 e t 2 1 erfc(t 1 ) + 2 π − (α 01 − 1)T 2 11 t 3/2 22 + −α 01 + α 01 T 11 + 1 √ t 22 − α √ t 22 = 0.2124377655377270 . • ∂J21 ∂µ An upper bound on the maximum is λ 2 01 ω 2 max α 2 01 e T 2 1 −e −T4 erfc(T 1 ) + 2α 2 01 e t 2 0.0222044 . 1 λ 2 01 τ max ω max e −t4   α 2 01 −e t 2 1 erfc(t 1 ) + 4α 2 01 e T 2 2 erfc(T ) + 2 π (−1) α 2 01 − 1 √ t 22   = (196) 1 2 λ 2 01 µ max τ max e −t4   α 2 01 −e t 2 1 erfc(t 1 ) + 4α 2 01 e T 2 2 erfc(T 2 ) + π (−1) α 2 − 1 √   = 1 4 λ 2 01 8α 2 01 t 22 e −T4 e (T 11 +2t 22 ) 2 2t 22 erfc T 11 + 2t 22 √ 2 √ + (209) t 22 t 22 (203) α 2 01 T 22 e −t4 e (t 11 +T 22 ) 2 2T 22 erfc t 11 + T 22 √ 2 √ − T 22 − 0.0351035 . Thus, an upper bound on the maximal absolute value is 1 2 λ 2 01 τ max ω max e −t4   α 2 −e T 2 1 erfc(T 1 ) + 4α 2 01 e t 2 2 erfc(t 2 ) + 2 π (−1) α 2 01 − 1 √ T 22   = − 0.0351035 . 2α 2 01 e t 2 1 e −t4 erfc(t 1 ) + 4α 2 2 e −T4 erfc(T 2 ) + 01 e T 2 Thus, an upper bound on the maximal absolute value is 2 λ 2 01 µ max τ max e −t4   α 2 01 −e T 2 1 erfc(T 1 ) + 4α 2 01 e t 2 2 erfc(t 2 ) + π (−1) α 2 − 1 √  2(2 − erfc(t 3 )) + 2 π e −t4 α 2 01 − 1 t 11 √ − 3α 2 01 T 22 = −1.17154 . T 22  = T 22 Thus, an upper bound on the maximal absolute value is (197) 0.14983446469110305 . (204) 0.14983446469110305 . 4 λ 2 01 −α 2 01 t 22 e −T4 e (T 11 +t 22 ) 2 2t 22 erfc T 11 + t 22 √ 2 √ t 22 + (210)</cell></row><row><cell>5 • 1.25) 3/2 Next we apply Lemma 37 for the expression ντ e − √ 1.5 • 1.25α 01 + (µω+ντ ) 2 (−0.1 • 0.1)α 01 − α 01 + 1 √ 1.5 • 1.25 2ντ erfc µω+ντ √ 2 √ ντ . We use Lemma 37 = −2.2302 . (183) to obtain an upper bound on the maximum of this expression: 1.5 • 1.25e (1.5•1.25−0.1•0.1) 2 2•1.5•1.25 α 01 erfc 1.5 • 1.25 − 0.1 • 0.1 √ 2 √ 1.5 • 1.25 = 1.37381 . (184) A upper bound on the absolute minimum is λ 2 01 ω 2 max α 2 01 e t 2 1 −e −t4 erfc(t 1 ) + 2α 2 01 e T 2 2 e T4 erfc(T 2 ) − erfc(t 3 ) + 2 = (190) 0.00894889 . Thus, an upper bound on the maximal absolute value is λ 2 01 ω 2 max α 2 01 e T 2 1 −e −T4 erfc(T 1 ) + 2α 2 01 e t 2 0.02220441024325437 . • ∂J21 ∂ω An upper bound on the maximum is λ 2 01 α 2 01 (2T 11 + 1)e t 2 2 e −t4 erfc(t 2 ) + 2T 11 (2 − erfc(T 3 )) + (192) α 2 01 (t 11 + 1)e T 2 1 −e −T4 erfc(T 1 ) + A lower bound on the minimum is λ 2 01 α 2 01 (T 11 + 1)e t 2 (193) α 2 )+ (200) 0.17980135762932363 . 2 π α 2 01 − 1 t 11 t 3/2 22 − 01 √ t 22 = 1.805740052651535 . 3α 2 1 −e −t4 erfc(t 1 ) + 1 2 λ 2 01 ν max ω max e −t4   α 2 01 −e T 2 1 erfc(T 1 ) + 4α 2 01 e t 2 2 erfc(t 2 ) + π (−1) α 2 √ T 22  = − 1 λ 2 01 τ 2 max e −t4 α 2 01 −e t 2 1 erfc(t 1 ) + 8α 2 2 erfc(T 2 ) + (207) 01 e T 2 01 − 1  Thus, an upper bound on the maximal absolute value is π T 22 e −t4 = 1.14696 . A lower bound on the minimum is 1 2 λ 2 01 ν max ω max e −t4   α 2 01 −e t 2 1 erfc(t 1 ) + 4α 2 01 e T 2 2 erfc(T 2 ) + 2 π (−1) α 2 01 − 1 √ t 22 − 0.0421242 . Thus, an upper bound on the maximal absolute value is 2 π 01 − 1 t 11 t 3/2 22 − 3α 2 01 √ t 22 = −1.80574 . α 2 (199) 4 λ 2 01 τ 2 max e −t4 α 2 01 −e t 2 1 erfc(t 1 ) + 8α 2 2 erfc(T 2 ) + (206) 01 e T 2  = Using Lemma 35, a lower bound on the minimum is  2 π α 2 01 − 1 T 11 T 3/2 22 − 3α 2 √ T 22 = 1.19441 . 01 0.179801 . 2 e t4 erfc(t 2 ) − erfc(T 3 ) + 2 = (191) • ∂J21 ∂τ An upper bound on the maximum is 1 2 λ 2 01 ν max ω max e −t4   α 2 01 −e T 2 1 erfc(T 1 ) + 4α 2 01 e t 2 2 erfc(t 2 ) + 2 π (−1) α 2  • ∂J22 ∂ν We apply Lemma 35 to the expression 2 π 8α 2 01 T 22 e −t4 e (t 11 +2T 22 ) 2 2T 22 erfc t 11 + 2T 22 √ 2 √ − T 22 (α 2 −1)µω (ντ ) 3/2 − 3α 2 √ ντ . Using Lemma 35, an 2α 2 01 e T 2 1 e −T4 erfc(T 1 ) + 4α 2 01 e t 2 2 e −t4 erfc(t 2 ) + 2(2 − erfc(T 3 )) + 01 − 1 √ T 22  = (198) upper bound on the maximum is 1 λ 2 01 τ 2 max e −t4 α 2 01 −e T 2 1 erfc(T 1 ) + 8α 2 01 e t 2 2 erfc(t 2 ) + (205) 2 π e −T4 α 2 01 − 1 T 11 √ t 22 − 3α 2 01 √ t 22 = 2.396685907216327 .</cell></row></table><note>01 (2t 11 + 1)e T 2 2 e −T4 erfc(T 2 ) + 2t 11 (2 − erfc(T 3 )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc>Lemma 23 says e x 2 erfc(x) is decreasing in µω+ντ √ 2 √ντ . The first term (negative) is increasing in ντ since it is proportional to minus one over the squared root of ντ .We obtain a lower bound by setting µω+ντ (α 01 − 1) = 0.056 Consequently, the function is larger than zero.</figDesc><table><row><cell></cell><cell cols="3">• ∂ ∂νμ (µ, ω, ν, τ, λ, α)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>√</cell><cell cols="2">2 √</cell><cell cols="4">ντ = 1.5•1.25+0.1•0.1 √ 2 √ 1.5•1.25</cell><cell>for the e x 2 erfc(x)</cell></row><row><cell></cell><cell cols="8">term. The term in brackets is larger than e</cell><cell cols="4">1.5•1.25+0.1•0.1 √ √ 1.5•1.25</cell><cell>2</cell><cell>α 01 erfc 1.5•1.25+0.1•0.1 √ 2 √ 1.5•1.25</cell><cell>−</cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">π0.8•0.8 • ∂ ∂µξ (µ, ω, ν, τ, λ, α)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">We consider the sub-function</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2 π</cell><cell>√</cell><cell>ντ − α 2 e</cell><cell>µω+ντ √ 2 √ ντ</cell><cell>2</cell><cell>erfc</cell><cell cols="4">µω + ντ √ 2 √ ντ</cell><cell>− e</cell><cell cols="2">µω+2ντ √ 2 √ ντ</cell><cell>2</cell><cell>erfc</cell><cell>µω + 2ντ √ 2 √ ντ</cell><cell>.</cell></row><row><cell cols="7">The derivative ∂ ∂νξ (µ, ω, ν, τ, λ, α) is positive.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Proof.</cell><cell cols="4">• ∂ ∂µμ (µ, ω, ν, τ, λ, α)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(2 − erfc(x) &gt; 0 according to Lemma 21 and e x 2 erfc(x) is also larger than zero according to Lemma 23. Consequently, has ∂ ∂µμ (µ, ω, ν, τ, λ, α) the sign of ω.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>0.05 ν 0.24, 0.8 τ 1.25} can be bounded as follows:</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">∂ ∂µμ</cell><cell cols="3">&lt; 0.14</cell><cell>(232)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">∂ ∂ωμ</cell><cell cols="3">&lt; 0.14</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">∂ ∂νμ</cell><cell cols="3">&lt; 0.52</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">∂ ∂τμ</cell><cell cols="3">&lt; 0.11.</cell></row><row><cell cols="6">Proof. The expression</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>∂ ∂µμ</cell><cell>= J 11 =</cell><cell>1 2</cell><cell cols="2">λωe</cell><cell>−(µω) 2 2ντ</cell><cell>2e</cell><cell cols="2">(µω) 2 2ντ</cell><cell>− e</cell><cell cols="3">(µω) 2 2ντ erfc</cell><cell>√</cell><cell>µω 2 √ ντ</cell><cell>+ αe</cell><cell>(µω+ντ ) 2 2ντ</cell><cell>erfc</cell><cell>µω + ντ √ 2 √ ντ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(233)</cell></row><row><cell cols="4">contains the terms e</cell><cell cols="2">(µω) 2 2ντ erfc</cell><cell cols="2">√</cell><cell cols="3">µω 2 √ ντ and e</cell><cell cols="2">(µω+ντ ) 2 2ντ</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head></head><label></label><figDesc>ν, τ, λ , α 01 ) −μ 2 (µ + ∆µ, ω + ∆ω, ν + ∆ν, τ + ∆τ, λ 01 , α 01 ) • 0.289324 • 0.11 = 0.06365128</figDesc><table><row><cell cols="2">∂ ∂νμ 2 = 2 |μ|</cell><cell cols="2">∂ ∂νμ</cell><cell>&lt; 2 • 0.289324 • 0.52 = 0.30089696</cell></row><row><cell cols="2">∂ ∂τμ 2 = 2 |μ|</cell><cell cols="2">∂ ∂τμ</cell><cell>&lt; 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(238)</cell></row><row><cell>∂ ∂µμ 2 |∆µ| +</cell><cell cols="2">∂ ∂ωμ 2 |∆ω| +</cell><cell cols="2">∂ ∂νμ 2 |∆ν| +</cell><cell>∂ ∂τμ 2 |∆τ |.</cell></row><row><cell cols="5">We use Lemma 42 and Lemma 41, to obtain</cell></row><row><cell cols="2">∂ ∂µμ 2 = 2 |μ|</cell><cell cols="2">∂ ∂µμ</cell><cell>&lt; 2 • 0.289324 • 0.14 = 0.08101072</cell><cell>(239)</cell></row><row><cell cols="2">∂ ∂ωμ 2 = 2 |μ|</cell><cell cols="2">∂ ∂ωμ</cell><cell>&lt; 2 • 0.289324 • 0.14 = 0.08101072</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head></head><label></label><figDesc>− −8.0x 3 − 8.0x 2 y − 11.0044x 2 + 2.xy 2 + 1.69548xy − 0.0464849x + 2.y 3 + 3.59885y 2 − 0.0232425y &lt; Therefore the squares of the root term have to be larger than the square of the other term to show &gt; 0 in Eq. (248). Thus, we have the inequality: −8.0x 3 − 8.0x 2 y − 11.0044x 2 + 2.xy 2 + 1.69548xy − 0.0464849x + 2.y 3 + 3.59885y 2 − 0.0232425y − 8.0x 2 y − 11.0044x 2 + 2.0xy 2 + 1.69548xy − 0.0464849x + 2.0y 3 + 3.59885y 2 − 0.0232425y 2 = − 1.2227x 5 + 40.1006x 4 y + 27.7897x 4 + 41.0176x 3 y 2 + 64.5799x 3 y + 39.4762x 3 + 10.9422x 2 y 3 − 13.543x 2 y 2 − 28.8455x 2 y − 0.364625x 2 + 0.611352xy 4 + 6.83183xy 3 + 5.46393xy 2 + 0.121746xy + 0.000798008x − 10.6365y 5 − 11.927y 4 + 0.190151y 3 − 0.000392287y 2 .</figDesc><table><row><cell></cell><cell></cell><cell>(251)</cell></row><row><cell>4x 2 + 2xy + 2.76667x − 2y 2 − 0.939726y − 0.0121625</cell><cell cols="2">(2x + y) 2 + 5.39467x .</cell></row><row><cell></cell><cell></cell><cell>2 &lt;</cell></row><row><cell></cell><cell></cell><cell>(252)</cell></row><row><cell>4x 2 + 2xy + 2.76667x − 2y 2 − 0.939726y − 0.0121625</cell><cell cols="2">2 (2x + y) 2 + 5.39467x .</cell></row><row><cell>This is equivalent to</cell><cell></cell></row><row><cell cols="2">0 &lt; 4x 2 + 2xy + 2.76667x − 2y 2 − 0.939726y − 0.0121625</cell><cell>2 (2x + y) 2 + 5.39467x −</cell></row><row><cell></cell><cell></cell><cell>(253)</cell></row><row><cell>−8.0x 3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head></head><label></label><figDesc>= − 1.2227x 5 + 27.7897x 4 + 41.0176x 3 y 2 + 39.4762x 3 − 13.543x 2 y 2 − 0.364625x 2 + y 40.1006x 4 + 64.5799x 3 + 10.9422x 2 y − 28.8455x 2 + 6.83183xy 2 + 0.121746x − 10.6365y 4 + 0.190151y 2 + 0.611352xy 4 + 5.46393xy 2 + 0.000798008x − 11.927y 4 − 0.000392287y 2 &gt; − 1.2227x + 27.7897x 4 + 41.0176 • (0.0) 2 x 3 + 39.4762x 3 − 13.543 • (0.1) 2 x 2 − 0.364625x 2 − 0.1 • 40.1006x 4 + 64.5799x 3 + 10.9422 • (0.1) 2 x 2 − 28.8455x 2 + 6.83183 • (0.1) 2 x + 0.121746x + 10.6365 • (0.1) 4 + 0.190151 • (0.1) 2 + 0.611352 • (0.0) 4 x + 5.46393 • (0.0) 2 x + 0.000798008x − 11.927 • (0.1) 4 − 0.000392287 • (0.1) 2 = − 1.2227x 5 + 23.7796x 4 + (20 + 13.0182)x 3 + 2.37355x 2 − 0.0182084x − 0.000194074 − 1.2227x 5 + 24.7796x 4 + 13.0182x 3 + 2.37355x − 0.0182084x − 0.000194074 &gt; 13.0182x 3 + 2.37355x 2 − 0.0182084x − 0.000194074 &gt; 0 . We used 24.7796 • (20) 4 − 1.2227 • (20) 5 = 52090.9 &gt; 0 and x 20. We have proofed the last inequality &gt; 0 of Eq. (248).</figDesc><table><row><cell cols="2">Consequently the derivative is always positive independent of y, thus</cell></row><row><cell>e</cell><cell>(x+y) 2 2x</cell></row></table><note>40.1006x 4 y + 27.7897x 4 + 41.0176x 3 y 2 + 64.5799x 3 y + 39.4762x 3 + 10.9422x 2 y 3 − (254) 13.543x 2 y 2 − 28.8455x 2 y − 0.364625x 2 + 0.611352xy 4 + 6.83183xy 3 + 5.46393xy 2 + 0.121746xy + 0.000798008x − 10.6365y 5 − 11.927y 4 + 0.190151y 3 − 0.000392287y 2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head></head><label></label><figDesc>− 8x 2 y + 4x 2 (2x + y) + 5.39467x − 10.9554x 2 + 2xy 2 − 2y 2 (2x + y) 2 + 5.39467x + 1.76901xy + 2xy (2x + y) 2 + 5.39467x + 2.7795x (2x + y) 2 + 5.39467x − 0.9269y (2x + y) 2 + 5.39467x − 0.00027798 (2x + y) 2 + 5.39467x − 0.00106244x +</figDesc><table><row><cell cols="4">−8x 3 2y + 3.62336y − 0.00053122y</cell><cell></cell><cell></cell></row><row><cell cols="2">(x + y) +</cell><cell>2.911 π</cell><cell cols="2">(2.911 − 1)(2x + y) + (2x + y) 2 +</cell><cell cols="2">2 • 2.911 2 x π</cell><cell>−1</cell><cell>=</cell></row><row><cell cols="6">−8x 3 + 4x 2 + 2xy + 2.7795x − 2y 2 − 0.9269y − 0.00027798</cell><cell>(2x + y) 2 + 5.39467x −</cell></row><row><cell cols="7">8x 2 y − 10.9554x 2 + 2xy 2 + 1.76901xy − 0.00106244x + 2y 3 + 3.62336y 2 − 0.00053122y</cell></row><row><cell cols="2">(x + y) +</cell><cell>2.911 π</cell><cell cols="2">(2.911 − 1)(2x + y) + (2x + y) 2 +</cell><cell cols="2">2 • 2.911 2 x π</cell><cell>−1</cell><cell>&gt; 0 .</cell></row><row><cell>x + y) +</cell><cell cols="2">2.911 π</cell><cell cols="4">(3x − y − 0.0003) (2.911 − 1)(2x + y) + (2x + y) 2 +</cell><cell>2 • 2.911 2 x π</cell><cell>+</cell></row><row><cell cols="4">(x − y)(x + y) (2.911 − 1)(2x + y) + (2x + y) 2 +</cell><cell cols="2">2 • 2.911 2 x π</cell></row><row><cell cols="2">(x + y) +</cell><cell>2.911 π</cell><cell cols="2">(2.911 − 1)(2x + y) + (2x + y) 2 +</cell><cell cols="2">2 • 2.911 2 x π</cell><cell>−1</cell><cell>=</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head></head><label></label><figDesc>− 10.9554x 2 + 2xy 2 + 1.76901xy − 0.00106244x + 2y 3 + 3.62336y 2 − 0.00053122y .The factor 4x 2 + 2xy + 2.7795x − 2y 2 − 0.9269y − 0.00027798 in front of the root is positive:4x 2 + 2xy + 2.7795x − 2y 2 − 0.9269y − 0.00027798 &gt;(277)−2y 2 + 0.007 • 2y − 0.9269y + 4 • 0.007 2 + 2.7795 • 0.007 − 0.00027798 = −2y 2 − 0.9129y + 2.77942 = −2(y + 1.42897)(y − 0.972523) &gt; 0 . If the term that does not contain the root would be positive, then everything is positive and we have proofed the the numerator is positive. Therefore we consider the case that the term that does not contain the root is negative. The term that contains the root must be larger than the other term in absolute values. − −8x 3 − 8x 2 y − 10.9554x 2 + 2xy 2 + 1.76901xy − 0.00106244x + 2y 3 + 3.62336y 2 − 0.00053122y &lt; (278) 4x 2 + 2xy + 2.7795x − 2y 2 − 0.9269y − 0.00027798 (2x + y) 2 + 5.39467x . Therefore the squares of the root term have to be larger than the square of the other term to show &gt; 0 in Eq. (275). Thus, we have the inequality: −8x 3 − 8x 2 y − 10.9554x 2 + 2xy 2 + 1.76901xy − 0.00106244x + 2y 3 + 3.62336y 2 − 0.00053122y + 2xy + 2.7795x − 2y 2 − 0.9269y − 0.00027798 2 (2x + y) 2 + 5.39467x . This is equivalent to 0 &lt; 4x 2 + 2xy + 2.7795x − 2y − 0.9269y − 0.00027798 2 (2x + y) 2 + 5.39467x − − 8x 2 y − 10.9554x 2 + 2xy 2 + 1.76901xy − 0.00106244x + 2y 3 + 3.62336y 2 − 0.00053122y 2 = x • 4.168614250 • 10 −7 − y 2 2.049216091 • 10 −7 − 0.0279456x 5 + 43.0875x 4 y + 30.8113x 4 + 43.1084x 3 y 2 + 68.989x 3 y + 41.6357x 3 + 10.7928x 2 y 3 − 13.1726x 2 y 2 − 27.8148x 2 y − 0.00833715x 2 + 0.0139728xy 4 + 5.47537xy 3 + 4.65089xy 2 + 0.00277916xy − 10.7858y 5 − 12.2664y 4 + 0.00436492y 3 .</figDesc><table><row><cell cols="2">4x 2 (280)</cell></row><row><cell>−8x 3</cell><cell></cell></row><row><cell>). The numerator is</cell><cell></cell></row><row><cell>− 8x 3 + 4x 2 + 2xy + 2.7795x − 2y 2 − 0.9269y − 0.00027798</cell><cell>(2x + y) 2 + 5.39467x −</cell></row><row><cell></cell><cell>(276)</cell></row><row><cell cols="2">8x 2 y 2 &lt;</cell></row><row><cell></cell><cell>(279)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40"><head></head><label></label><figDesc>For the first inequality we applied Lemma 24 which states that the function xe x 2 erfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = −0.01 to make the positive term less positive.</figDesc><table><row><cell cols="2">Consequently</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">e</cell><cell cols="4">(x+y) 2 2x</cell><cell cols="2">erfc</cell><cell cols="2">x + y √ 2 √ x</cell><cell>− 2e</cell><cell>(2x+y) 2 2x</cell><cell>erfc</cell><cell>2x + y √ 2 √ x</cell><cell>(290)</cell></row><row><cell cols="17">is strictly monotonically increasing in y for x = 0.56.</cell></row><row><cell cols="17">Next, we consider x = 0.16 • 0.8 = 0.128, which is the minimal τ = 0.8. We insert the minimum</cell></row><row><cell cols="17">x = 0.128 = 128/1000 into the sub-function Eq. (111):</cell></row><row><cell></cell><cell>e</cell><cell></cell><cell></cell><cell>√</cell><cell cols="2">y √ 2</cell><cell cols="3">128 1000</cell><cell cols="2">+</cell><cell cols="2">√ √ 128 2</cell><cell>2</cell><cell cols="2">erfc</cell><cell> </cell><cell>√</cell><cell>y 2 128 1000</cell><cell>+</cell><cell>128 1000 √ 2</cell><cell>  −</cell><cell>(291)</cell></row><row><cell></cell><cell cols="3">2e</cell><cell cols="4">y √ 2 √</cell><cell cols="3">128 1000</cell><cell cols="2">+ √</cell><cell>√</cell><cell>128 1000</cell><cell>2</cell><cell>erfc</cell><cell> </cell><cell>√</cell><cell>y 2 128 1000</cell><cell>+</cell><cell>√</cell><cell>2</cell><cell>128 1000</cell><cell>  =</cell></row><row><cell></cell><cell>e</cell><cell cols="4">125y 2</cell><cell cols="8">+y+ 8 125 erfc</cell><cell cols="3">125y + 16 20 √</cell><cell>− 2e</cell><cell>(125y+32) 2</cell><cell>erfc</cell><cell>125y + 32 20 √</cell><cell>.</cell></row><row><cell cols="15">The derivative with respect to y is:</cell><cell></cell></row><row><cell>1</cell><cell cols="2">e</cell><cell cols="14">125y 2 32 +y+ 8 125 (125y + 16) erfc</cell><cell>125y + 16 20 √</cell><cell>−</cell><cell>(292)</cell></row><row><cell>2e</cell><cell cols="6">(125y+32) 2</cell><cell cols="9">(125y + 32) erfc</cell><cell>125y + 32 20 √</cell><cell>+ 20</cell><cell>π</cell><cell>&gt;</cell></row><row><cell>1</cell><cell cols="16">(16 + 125(−0.01))e −0.01+ 8</cell><cell>+</cell><cell>125(−0.01) 2</cell><cell>erfc</cell><cell>16 + 125(−0.01) 20 √</cell><cell>−</cell></row><row><cell>2e</cell><cell cols="8">(32+1250.01) 2</cell><cell cols="8">(32 + 1250.01) erfc</cell><cell>32 + 1250.01 √</cell><cell>+ 20</cell><cell>π</cell><cell>&gt; 0.4468 .</cell></row><row><cell cols="2">Consequently</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">e</cell><cell cols="4">(x+y) 2 2x</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head></head><label></label><figDesc>For the first inequality we applied Lemma 24 which states that the function xe x 2 erfc(x) is monotonically increasing. Consequently, we inserted the maximal y = 0.01 to make the negative term more negative and the minimal y = −0.01 to make the positive term less positive.</figDesc><table><row><cell></cell><cell></cell><cell>e</cell><cell cols="8">(125y+27) 2 6750</cell><cell cols="3">erfc</cell><cell></cell><cell cols="2">125y + 27 15 √ 30</cell><cell>− 2e</cell><cell>(125y+54) 2 6750</cell><cell>erfc</cell><cell>125y + 54 15 √ 30</cell></row><row><cell cols="15">The derivative with respect to y is:</cell><cell></cell></row><row><cell>1 27</cell><cell>e</cell><cell cols="5">(125y+27) 2 6750</cell><cell cols="10">(125y + 27) erfc</cell><cell>125y + 27 15 √ 30</cell><cell>−</cell><cell>(295)</cell></row><row><cell>2e</cell><cell cols="3">(125y+54) 2 6750</cell><cell cols="13">(125y + 54) erfc</cell><cell>125y + 54 15 √ 30</cell><cell>+ 15</cell><cell>30 π</cell><cell>&gt;</cell></row><row><cell>1 27</cell><cell cols="14">(27 + 125(−0.01))e</cell><cell cols="2">(27+125(−0.01)) 2 6750</cell><cell>erfc</cell><cell>27 + 125(−0.01) 15 √ 30</cell><cell>−</cell></row><row><cell>2e</cell><cell cols="5">(54+1250.01) 2 6750</cell><cell cols="11">(54 + 1250.01) erfc</cell><cell>54 + 1250.01 15 √ 30</cell><cell>+ 15</cell><cell>30 π</cell><cell>) &gt; 0.211288 .</cell></row><row><cell cols="2">Consequently</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">e</cell><cell cols="4">(x+y) 2 2x</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>216 = 216/1000 into the sub-function Eq. (111):</cell></row><row><cell></cell><cell></cell><cell>e</cell><cell cols="2">√</cell><cell cols="3">y √ 2</cell><cell cols="3">1000</cell><cell cols="2">+</cell><cell cols="3">√ √ 1000 2</cell><cell>2</cell><cell>erfc</cell><cell> </cell><cell>√</cell><cell>y 2 216 1000</cell><cell>+</cell><cell>216 1000 √ 2</cell><cell>  −</cell><cell>(294)</cell></row><row><cell></cell><cell></cell><cell cols="2">2e</cell><cell cols="6">y √ 2 √</cell><cell cols="2">1000</cell><cell cols="2">+ √</cell><cell cols="2">√</cell><cell>216 1000</cell><cell>2</cell><cell>erfc</cell><cell> </cell><cell>√</cell><cell>y 2 216 1000</cell><cell>+</cell><cell>√</cell><cell>2</cell><cell>216 1000</cell><cell>  =</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_42"><head></head><label></label><figDesc>We use Lemma 17 to determine the derivatives. Consequently, the derivative of</figDesc><table><row><cell>τ e</cell><cell>µω+ντ √ 2 √ ντ</cell><cell>2</cell><cell>erfc</cell><cell>µω + ντ √ 2 √ ντ</cell><cell>− 2e</cell><cell>µω+2ντ √ 2 √ ντ</cell><cell>2</cell><cell>erfc</cell><cell>µω + 2ντ √ 2 √ ντ</cell><cell>(298)</cell></row><row><cell cols="11">with respect to ν is larger than zero, which follows directly from Lemma 17 using the chain rule.</cell></row><row><cell cols="4">Consequently, the derivative of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>τ e</cell><cell>µω+ντ √ 2 √ ντ</cell><cell>2</cell><cell>erfc</cell><cell>µω + ντ √ 2 √ ντ</cell><cell>− 2e</cell><cell>µω+2ντ √ 2 √ ντ</cell><cell>2</cell><cell>erfc</cell><cell>µω + 2ντ</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_43"><head></head><label></label><figDesc>× 10 6 π(140τ + 1) 2 + 118635τ τ 3/2 − 2.89498 × 10 7 τ 3/2 − 1.21486 × 10 7 π(70τ + 1) 2 + 118635τ τ 5/2 + 8.8828 × 10 6 π(140τ + 1) 2 + 118635τ τ 5/2 − 2.43651 × 10 7 τ 5/2 − 1.46191 × 10 9 τ 7/2 + 2.24868 × 10 7 τ 2 + 94840.5 π(70τ + 1) 2 + 118635τ τ + − 2.89498 × 10 7 τ 3/2 − 2.43651 × 10 7 τ 5/2 − 1.46191 × 10 9 τ 7/2 + −1.70658 × 10 7 τ 3/2 − 1.21486 × 10 7 τ5/2 + 94840.5τ + 820.213 × 10 7 τ 2 + 481860.τ + 710.354 √ τ + 2294.57 − 2.89498 × 10 7 τ 3/2 − 2.43651 × 10 7 τ 5/2 − 1.46191 × 10 9 τ 7/2 + −1.70658 × 10 7 τ 3/2 − 1.21486 × 10 7 τ 5/2 + 820.213 √ 1.25 + 1.25 • 94840.5 + 677.432 π(70τ + 1) 2 + 118635τ + 8.60302 × 10 6 τ 3/2 + 8.8828 × 10 6 τ 5/2 − 1011.27 √ 0.8 + 1.25 • 47420.2 + 677.432 − 2.89498 × 10 7 τ 3/2 − 2.43651 × 10 7 τ 5/2 − 1.46191 × 10 9 τ 7/2 + −1.70658 × 10 7 τ 3/2 − 1.21486 × 10 7 τ 5/2 + 120145. − 2.89498 × 10 7 τ 3/2 − 2.43651 × 10 7 τ 5/2 − 1.46191 × 10 9 τ 7/2 + 8.60302 × 10 6 τ 3/2 + 8.8828 × 10 6 τ 5/2 + 59048.2 19600π(τ + 1.94093)(τ + 0.0000262866)+ −1.70658 × 10 7 τ 3/2 − 1.21486 × 10 7 τ 5/2 + 120145. × 10 7 τ 2 + 605413 − 2.89498 × 10 7 τ 3/2 − 2.43651 × 10 7 τ 5/2 − 1.46191 × 10 9 τ 7/2 + 8.60302 × 10 6 τ 3/2 + 8.8828 × 10 6 τ × 10 7 τ 2 + = − 2.89498 × 10 7 τ 3/2 − 2.43651 × 10 7 τ 5/2 − 1.46191 × 10 9 τ 7/2 + −3.64296 × 10 6 τ 3/2 + 7.65021 × 10 8 τ 5/2 + 6.15772 × 10 6 τ √ τ + 1.94093 √ τ + 7.73521 + 2.24868 × 10 7 τ 2 + 2.20425 × 10 9 τ 3 + 2.13482 × 10 9 τ 2 + 1.46527 × 10 7 √ τ √ τ + 1.94093+ −1.5073 × 10 9 τ 3 − 2.11738 × 10 9 τ 2 + 1.49066 × 10 7 √ .73521 −3.64296 × 10 6 τ 3/2 + 7.65021 × 10 8 τ 5/2 + 6.15772 × 10 6 τ + √ 1.25 + 1.94093 2.20425 × 10 9 τ 3 + 2.13482 × 10 9 τ 2 + 1.46527 × 10 7 √ τ + √ 0.8 + 7.73521 −1.5073 × 10 9 τ 3 − 2.11738 × 10 9 τ 2 + 1.49066 × 10 7 √ τ −</figDesc><table><row><cell cols="3">200 π(2.911 − 1)(140τ + 1) + 2.911 • 20 √ √ √ π(700τ (7τ + 20) − 1) √ π(2.911 − 1)(70τ + 1) + 20 • √ • 2.911 √ τ 2 + π(140τ + 1) 2 + 20 • √ • 2.911 √ τ √ π2 • 20 • √ • 2.911(2800τ (7τ + 5) − 1) √ τ √ π(2.911 − 1)(70τ + 1) + 20 • √ • 2.911 √ τ 2 + π(70τ + 1) 2 √ π(2.911 − 1)(70τ + 1) + 20 √ • 2.911 • √ τ 2 + π(70τ + 1) 2 √ π(2.911 − 1)(140τ + 1) + 20 √ • 2.911 • √ τ 35(210τ − 1) √ τ + 200 √ π(2.911 − 1)(70τ + 1) + 20 • √ • 2.911 √ √ τ √ π(2.911 − 1)(140τ + 1) + 20 • √ • 2.911 √ τ 2 + π(140τ + 1) 2 + 2.911 • 20 √ √ π(700τ (7τ + 20) − 1) √ τ √ π(2.911 − 1)(140τ + 1) + 20 • √ • 2.911 √ τ 2 + π(140τ + 1) 2 − √ π2 • 20 • √ • 2.911(2800τ (7τ + 5) − 1) √ τ √ π(2.911 − 1)(70τ + 1) + 20 • √ • 2.911 √ τ 2 + π(70τ + 1) 2 = − 1.70658 × 10 7 π(70τ + 1) 2 + 118635τ τ 3/2 + 4200 √ π(70τ + 1) 2 + 118635τ π(140τ + 1) 2 + 118635τ τ 3/2 + 8.60302 47420.2 π(140τ + 1) 2 + 118635τ τ + 481860τ + 710.354 √ τ + 820.213 √ τ π(70τ + 1) 2 + 118635τ + 677.432 π(70τ + 1) 2 + 118635τ − 1011.27 √ τ π(140τ + 1) 2 + 118635τ − 20 √ √ τ π(70τ + 1) 2 + 118635τ π(140τ + 1) 2 + 118635τ + 200 π(70τ + 1) 2 + 118635τ π(140τ + 1) 2 + 118635τ + 677.432 π(140τ + 1) 2 + 118635τ + 2294.57 = √ τ + 677.432 π(70τ + 1) 2 + 118635τ + 8.60302 × 10 6 τ 3/2 + 8.8828 × 10 6 τ 5/2 + 47420.2τ − 1011.27 τ + 677.432 √ 2.24868 π(140τ + 1) + 118635τ + 4200 √ 35τ 3/2 − 20 √ 35 √ τ + 200 π(70τ + 1) 2 + 118635τ π(140τ + 1) 2 + 118635τ + 2.24868 × 10 7 τ 2 + 710.354 √ 1.25 + 1.25 • 481860 + 2294.57 = π(70τ + 1) 2 + 118635τ + τ 2 8.60302 × 10 6 τ 3/2 + 8.8828 × 10 6 τ 5/2 + 59048.2 π(140τ + 1) 2 + 118635τ + 4200 √ 35τ 3/2 − 20 √ √ τ + 200 π(70τ + 1) 2 + 118635τ π(140τ + 1) 2 + 118635τ + 2 + π(70τ + 1) 2 + π(70τ + 1) 2 (303) 2.24868 × 10 7 τ 2 + 605413 = 4900π(τ + 7.73521)(τ + 0.0000263835)+ 4200 √ 35τ 3/2 − 20 √ √ τ + 200 19600π(τ + 1.94093)(τ + 0.0000262866) 4900π(τ + 7.73521)(τ + 0.0000263835)+ 2.24868 5/2 + 59048.2 19600π(τ + 1.94093)τ + −1.70658 × 10 7 τ 3/2 − 1.21486 × 10 7 τ 5/2 + 120145. 4900π1.00003(τ + 7.73521)τ + 4200 √ 35τ 3/2 − 20 √ √ τ + 200 19600π1.00003(τ + 1.94093)τ 4900π1.00003(τ + 7.73521)τ + 2.24868 τ √ τ + 7.73521 + 605413 √ 1.25 + 1.94093 √ 1.25 + 7Consequently, the derivative of τ e µω+ντ √ 2 √ ντ 2 erfc µω + ντ √ 2 √ ντ − 2e µω+2ντ √ 2 √ ντ 2 erfc µω + 2ντ √ 2 √ ντ (308)</cell></row><row><cell>√ π(140τ + 1) 2 + 118635τ + π(2.911 − 1)(140τ + 1) + 4200 √ 35τ 3/2 − 20 √ √ τ + 200</cell><cell>20 •</cell><cell>√ π(70τ + 1) 2 + 118635τ π(140τ + 1) 2 + 118635τ + • 2.911 √ τ 2 + π(140τ + 1) 2 −</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_46"><head></head><label></label><figDesc>+ 27116.5τ τ 3/2 − 3.16357 × 10 6 τ 3/2 − 303446 π(16τ + 1) 2 + 27116.5τ τ 5/2 + 221873 π(32τ+ 1) 2 + 27116.5τ τ 5/2 − 608588τ 5/2 − 8.34635 × 10 6 τ 7/2 + 117482.τ 2 + 2167.78 π(16τ + 1) 2 + 27116.5τ τ + − 3.16357 × 10 6 τ 3/2 − 608588τ 5/2 − 8.34635 × 10 6 τ 7/2 + −1.86491 × 10 6 τ 3/2 − 303446τ5/2 + 2167.78τ + 392.137 + 27116.5τ π(32τ + 1) 2 + 27116.5τ + 117482.τ 2 + 11013.9τ + 339.614 √ τ + 229.457 − 3.16357 × 10 6 τ 3/2 − 608588τ 5/2 − 8.34635 × 10 6 τ 7/2 + − 3.16357 × 10 6 τ 3/2 − 608588τ 5/2 − 8.34635 × 10 6 τ 7/2 + − 3.16357 × 10 6 τ 3/2 − 608588τ 5/2 − 8.34635 × 10 6 τ 7/2 + −91003τ 3/2 + 4.36814 × 10 6 τ 5/2 + 32174.4τ √ τ + 8.49155 √ τ + 33.8415 + 117482.τ 2 + 1.25852 × 10 7 τ 3 + 5.33261 × 10 7 τ 2 + 56165.1 + 4.36814 × 10 6 τ 5/2 + 32174.4τ + √ 1.25 + 8.49155 1.25852 × 10 7 τ 3 + 5.33261 × 10 7 τ 2 + 56165.1 √ τ + √ 0.8 + 33.8415 −8.60549 × 10 6 τ 3 − 5.28876 × 10 7 τ 2 + 91200.4 √ τ − 3.16357 × 10 6 τ 3/2 − 608588τ 5/2 − 8.34635 × 10 6 τ 7/2 + 117482.τ 2 + 14376.6 =</figDesc><table><row><cell cols="8">π(32τ + 1) 2 + 27116.5τ +</cell></row><row><cell cols="8">1920 π(16τ + 1) 2 −1.86491 × 10 6 τ 3/2 − 303446τ 5/2 + 392.137 √ 2τ 3/2 − 40 √ 2 √ τ + 20 √</cell><cell>1.25 + 1.252167.78 + 67.7432</cell></row><row><cell cols="8">π(16τ + 1) 2 + 27116.5τ +</cell></row><row><cell cols="8">940121τ 3/2 + 221873τ 5/2 − 483.478</cell><cell>√</cell><cell>0.8 + 1.251083.89 + 67.7432</cell></row><row><cell cols="8">π(32τ + 1) 2 + 27116.5τ +</cell></row><row><cell>1920</cell><cell>√</cell><cell>2τ 3/2 − 40</cell><cell cols="2">√</cell><cell cols="2">2 √</cell><cell>τ + 20</cell><cell>π(16τ + 1) 2 + 27116.5τ π(32τ + 1) 2 + 27116.5τ +</cell></row><row><cell cols="4">117482.τ 2 + 339.614</cell><cell cols="2">√</cell><cell cols="2">1.25 + 1.2511013.9 + 229.457 =</cell></row><row><cell cols="8">− 3.16357 × 10 6 τ 3/2 − 608588τ 5/2 − 8.34635 × 10 6 τ 7/2 +</cell></row><row><cell cols="8">−1.86491 × 10 6 τ 3/2 − 303446τ 5/2 + 3215.89</cell><cell>π(16τ + 1) 2 + 27116.5τ +</cell></row><row><cell cols="8">940121τ 3/2 + 221873τ 5/2 + 990.171</cell><cell>π(32τ + 1) 2 + 27116.5τ +</cell></row><row><cell>1920</cell><cell>√</cell><cell>2τ 3/2 − 40</cell><cell cols="2">√</cell><cell cols="2">2 √</cell><cell>τ + 20</cell><cell>π(16τ + 1) 2 + 27116.5τ π(32τ + 1) 2 + 27116.5τ +</cell></row><row><cell cols="7">117482τ 2 + 14376.6 =</cell></row><row><cell cols="8">940121τ 3/2 + 221873τ 5/2 + 990.171</cell><cell>1024π(τ + 8.49155)(τ + 0.000115004)+</cell></row><row><cell cols="8">22.911(128τ (8τ + 25) − 1) π(2.911 − 1)(16τ + 1) + 40 √ √ 22.911 τ − 1.86491 × 10 6 π(16τ + 1) 2 + 27116.5τ τ 3/2 + √ √ τ 2 1920 √ π(16τ + 1) 2 + 27116.5τ π(32τ + 1) 2 + 27116.5τ τ 3/2 + + π(16τ + 1) 2 = 940121 π(32τ + 1) 2 1083.89 π(32τ + 1) 2 + 27116.5τ τ + 11013.9τ + 339.614 √ τ + 392.137 √ τ π(16τ + 1) 2 + 27116.5τ + 67.7432 π(16τ + 1) 2 + 27116.5τ − 483.478 √ τ π(32τ + 1) 2 + 27116.5τ − 40 √ 2 √ τ π(16τ + 1) 2 + 27116.5τ π(32τ + 1) 2 + 27116.5τ + 20 π(16τ + 1) 2 + 27116.5τ π(32τ + 1) 2 + 27116.5τ + 67.7432 π(32τ + 1) 2 + 27116.5τ + 229.457 = −1.86491 × 10 6 τ 3/2 − 303446τ 5/2 + 3215.89 256π(τ + 33.8415)(τ + 0.000115428)+ 1920 √ 2τ 3/2 − 40 √ 2 √ τ + 20 1024π(τ + 8.49155)(τ + 0.000115004) 256π(τ + 33.8415)(τ + 0.000115428)+ 117482.τ 2 + 14376.6 − 3.16357 × 10 6 τ 3/2 − 608588τ 5/2 − 8.34635 × 10 6 τ 7/2 + 940121τ 3/2 + 221873τ 5/2 + 990.171 1024π1.00014(τ + 8.49155)τ + 1920 √ 2τ 3/2 − 40 √ √ τ + 20 256π1.00014(τ + 33.8415)τ 1024π1.00014(τ + 8.49155)τ + −1.86491 × 10 6 τ 3/2 − 303446τ 5/2 + 3215.89 256π(τ + 33.8415)τ + 117482.τ 2 + 14376.6 = √ τ √ τ + 8.49155+ −8.60549 × 10 τ − 5.28876 × 10 7 τ 2 + 91200.4 √ τ √ τ + 33.8415 + 14376.6 √ 1.25 + 8.49155 √ 1.25 + 33.8415 −91003τ 3/2 Consequently, the derivative of τ e µω+ντ √ √ ντ 2 erfc µω + ντ √ 2 √ ντ − 2e µω+2ντ √ 2 √ ντ 2 erfc µω + 2ντ √ 2 √ ντ (313)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>√</cell><cell>τ + 67.7432</cell></row><row><cell cols="8">π(16τ + 1) 2 + 27116.5τ +</cell></row><row><cell cols="8">940121τ 3/2 + 221873τ 5/2 + 1083.89τ − 483.478</cell><cell>√</cell><cell>τ + 67.7432</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_47"><head></head><label></label><figDesc>+ 40674.8τ π(48τ + 1) 2 + 40674.8τ τ 3/2 + 1.72711 × 10 6 π(48τ + 1) 2 + 40674.8τ τ 3/2 − 5.81185 × 10 6 τ 3/2 − + 40674.8τ + 367.131 = − 5.81185 × 10 6 τ 3/2 − 1.67707 × 10 6 τ 5/2 − 3.44998 × 10 7 τ 7/2 + −3.42607 × 10 6 τ 3/2 − 836198τ 5/2 + 5202.68τ + 480.268 − 5.81185 × 10 6 τ 3/2 − 1.67707 × 10 6 τ 5/2 − 3.44998 × 10 7 τ 7/2 + − 5.81185 × 10 6 τ 3/2 − 1.67707 × 10 6 τ 5/2 − 3.44998 × 10 7 τ 7/2 + − 5.8118510 6 τ 3/2 − 1.67707 × 10 6 τ 5/2 − 3.44998 × 10 7 τ 7/2 + 1.72711 × 10 6 τ 3/2 + 611410τ 5/2 + 2798.− 5.8118510 6 τ 3/2 − 1.67707 × 10 6 τ 5/2 − 3.44998 × 10 7 τ 7/2 + −250764.τ 3/2 + 1.8055 × 10 7 τ 5/2 + 115823.τ √ τ + 5.66103 √ τ + 22.561 + 422935.τ 2 + 5.20199 × 10 7 τ 3 + 1.46946 × 10 8 τ 2 + 238086. √ τ √ τ + 5.66103+ −3.55709 × 10 7 τ 3 − 1.45741 × 8 τ 2 + 304097.</figDesc><table><row><cell cols="8">2e √ (48τ +1) 2 4800τ π(2.911 − 1)(48τ + 1) + (192τ (12τ + 25) − 1) erfc √</cell><cell>48τ + 1 40 √ 3 √ τ 32.911 √ τ</cell><cell>2</cell><cell>+ 40 + π(48τ + 1) 2 − √ 3(72τ − 1) √</cell><cell>τ</cell></row><row><cell cols="8">√ 2 √ π √     π40 √ √ π(2.911−1)(24τ +1) 32.911(192τ (12τ + 25) − 1) 2.911(192τ (3τ + 25) − 1) √ τ 40 √ 3 √ τ + π 24τ +1 40 √ 3 √ τ 2 π(2.911 − 1)(24τ + 1) + √ 32.911 + 2.911 2 √ τ 2 + π(24τ + 1) 2 = − √ 3τ 3/2 − √ √ τ + 32 2304π(τ + 5.66103)(τ + 0.0000766694)</cell></row><row><cell cols="8">2 • 2.911(192τ (12τ + 25) − 1) π(2.911−1)(48τ +1) 40 √ 3 √ τ + π 48τ +1 40 √ 3 √ τ 2 + 2.911 2 √ √ 40 3(72τ − 1) √ τ + 32 = 576π(τ + 22.561)(τ + 0.0000769518)+ − 3.42607 × 10 6 π(24τ + 1) 2 + 40674.8τ τ 3/2 +     + 2880 √ 422935τ 2 + 33874 π(24τ + 1) 2 836198 π(24τ + 1) 2 + 40674.8τ τ 5/2 + 611410 π(48τ + 1) 2 + 40674.8τ τ 5/2 − 31 2304π1.0001(τ + 5.66103)τ +</cell></row><row><cell cols="8">√ 1.67707 × 10 6 τ 5/2 − π   (192τ (3τ + 25) − 1) 40 √ √ π(2.911 − 1)(24τ + 1) + 40 √ 32.911 32.911 √ τ 2880 √ 3τ 3/2 − 40 √ √ τ + 32 2304π1.0001(τ + 5.66103)τ 576π1.0001(τ + 22.561)τ + √ τ 2 + π(24τ + 1) 2 − 2(192τ (12τ + 25) − 1) 40 √ 32.911 √ τ √ π(2.911 − 1)(48τ + 1) + 40 √ 32.911 √ τ 2 + π(48τ + 1) 2 3.44998 × 10 7 τ 7/2 + 422935.τ 2 + 5202.68 π(24τ + 1) 2 + 40674.8τ τ + −3.42607 × 10 6 τ 3/2 − 836198τ 5/2 + 7148.69 2601.34 π(48τ + 1) 2 + 40674.8τ τ +   + 40 √ 3(72τ − 1) √ τ + 32 = 26433.4τ + 415.94 √ τ + 480.268 √ 576π(τ + 22.561)τ + τ π(24τ + 1) 2 + 40674.8τ + 108.389 π(24τ + 1) 2 + 40674.8τ − 592.138 √ 40 3 √ τ π(24τ + 1) 2 + 40674.8τ π(48τ + 1) 2 + 40674.8τ + √ τ π(48τ + 1) 2 + 40674.8τ − 422935τ 2 + 33874. =</cell></row><row><cell cols="8">40 32 π(24τ + 1) 2 + 40674.8τ π(48τ + 1) 2 + 40674.8τ + √ 3(72τ − 1) √ τ + 32 √ π(2.911 − 1)(24τ + 1) +</cell><cell>40 √</cell><cell>32.911</cell><cell>√</cell><cell>τ</cell><cell>2</cell><cell>+ π(24τ + 1) 2</cell></row><row><cell cols="8">√ 2.911 • 40 π(2.911 − 1)(48τ + 1) + √ 3 √ π(192τ (3τ + 25) − 1) 40 √ 108.389 π(48τ + 1) 2 √ 32.911 √ τ 2 + π(48τ + 1) 2 + τ + 108.389 √ τ π(24τ + 1) 2 + 40674.8τ +</cell></row><row><cell cols="8">√ 1.72711 × 10 6 τ 3/2 + 611410τ 5/2 + 2601.34τ − 592.138 π(2.911 − 1)(48τ + 1) + 40 √ 32.911 √ τ 2 + π(48τ + 1) 2 − √ τ + 108.389</cell></row><row><cell cols="8">2 √ π(48τ + 1) 2 + 40674.8τ + π40 √ 32.911(192τ (12τ + 25) − 1) √ τ √ π(2.911 − 1)(24τ + 1) + 40 √ 2880 √ 3τ 3/2 − 40 √ 3 √ τ + 32 π(24τ + 1) 2 + 40674.8τ π(48τ + 1) 2 + 40674.8τ + 32.911 √ τ 2 + π(24τ + 1) 2 422935.τ 2 + 26433.4τ + 415.94 √ τ + 367.131</cell></row><row><cell cols="8">√ −3.42607 × 10 6 τ 3/2 − 836198τ 5/2 + 480.268 π(2.911 − 1)(24τ + 1) + 40 √ 32.911 √ √ τ</cell><cell>2 1.25 + 1.255202.68 + 108.389 + π(24τ + 1) 2</cell></row><row><cell cols="8">√ π(24τ + 1) 2 + 40674.8τ + π(2.911 − 1)(48τ + 1) + 1.72711 × 10 6 τ 3/2 + 611410τ 5/2 − 592.138 40 √ 32.911 √ √ τ 0.9 + 1.252601.34 + 108.389</cell></row><row><cell cols="7">π(48τ + 1) 2 + 40674.8τ +</cell></row><row><cell>2880 √</cell><cell cols="2">3τ 3/2 − 40</cell><cell cols="2">√</cell><cell>3 √</cell><cell>τ + 32</cell><cell>π(24τ + 1) 2 + 40674.8τ π(48τ + 1) 2 + 40674.8τ +</cell></row><row><cell cols="2">422935τ 2 + 415.94</cell><cell cols="2">√</cell><cell cols="4">1.25 + 1.2526433.4 + 367.131 =</cell></row><row><cell cols="8">−3.42607 × 10 6 τ 3/2 − 836198τ 5/2 + 7148.69</cell><cell>π(24τ + 1) 2 + 40674.8τ +</cell></row><row><cell cols="8">1.72711 × 10 6 τ 3/2 + 611410τ 5/2 + 2798.31</cell><cell>π(48τ + 1) 2 + 40674.8τ +</cell></row><row><cell>2880 √</cell><cell cols="2">3τ 3/2 − 40</cell><cell cols="2">√</cell><cell>3 √</cell><cell>τ + 32</cell><cell>π(24τ + 1) 2 + 40674.8τ π(48τ + 1) 2 + 40674.8τ +</cell></row><row><cell cols="8">) 2 4800τ 422935τ 2 + 33874 = (192τ (3τ + 25) − 1) erfc</cell><cell>24τ + 1 √ 3 √ τ</cell><cell>−</cell><cell>(316)</cell></row></table><note>√ τ √ τ + 22.561 + 33874.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_49"><head>Table A7 :</head><label>A7</label><figDesc>Hyperparameters considered for weight normalized networks in the UCI data sets.</figDesc><table><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell>Number of hidden units</cell><cell>{1024, 512, 256}</cell></row><row><cell cols="2">Number of hidden layers {2, 3, 4, 8, 16, 32}</cell></row><row><cell>Learning rate</cell><cell>{0.01, 0.1, 1}</cell></row><row><cell>Normalization</cell><cell>{Weightnorm}</cell></row><row><cell>Layer form</cell><cell>{rectangular, conic}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_50"><head>Table A8 :</head><label>A8</label><figDesc>Hyperparameters considered for layer normalized networks in the UCI data sets.</figDesc><table><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell>Number of hidden units</cell><cell>{1024, 512, 256}</cell></row><row><cell cols="2">Number of hidden layers {2, 3, 4, 8, 16, 32}</cell></row><row><cell>Learning rate</cell><cell>{0.01, 0.1, 1}</cell></row><row><cell>Normalization</cell><cell>{Layernorm}</cell></row><row><cell>Layer form</cell><cell>{rectangular, conic}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_51"><head>Table A9 :Table A10 :</head><label>A9A10</label><figDesc>Hyperparameters considered for Highway networks in the UCI data sets. Hyperparameters considered for Residual networks in the UCI data sets.</figDesc><table><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell cols="2">Number of hidden layers {2, 3, 4, 8, 16, 32}</cell></row><row><cell>Learning rate</cell><cell>{0.01, 0.1, 1}</cell></row><row><cell>Dropout rate</cell><cell>{0, 0.5}</cell></row><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell>Number of blocks</cell><cell>{2, 3, 4, 8, 16}</cell></row><row><cell cols="2">Number of neurons per blocks {1024, 512, 256}</cell></row><row><cell>Block form</cell><cell>{rectangular, diavolo}</cell></row><row><cell>Bottleneck</cell><cell>{25%, 50%}</cell></row><row><cell>Learning rate</cell><cell>{0.01, 0.1, 1}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_52"><head></head><label></label><figDesc>SNNs are ranked first having outperformed diverse machine learning methods and other FNNs.</figDesc><table><row><cell cols="2">A4.3 Tox21 challenge data set: Hyperparameters</cell><cell></cell></row><row><cell>methodGroup</cell><cell>method</cell><cell>avg. rank p-value</cell></row><row><cell>SNN</cell><cell>SNN</cell><cell>5.8</cell></row><row><cell>SVM</cell><cell>LibSVM_weka</cell><cell>6.1 5.8e-01</cell></row><row><cell>RandomForest</cell><cell>RRFglobal_caret</cell><cell>6.6 2.1e-01</cell></row><row><cell>MSRAinit</cell><cell>MSRAinit</cell><cell>7.1 4.5e-03</cell></row><row><cell>LayerNorm</cell><cell>LayerNorm</cell><cell>7.2 7.1e-02</cell></row><row><cell>Highway</cell><cell>Highway</cell><cell>7.9 1.7e-03</cell></row><row><cell>ResNet</cell><cell>ResNet</cell><cell>8.4 1.7e-04</cell></row><row><cell>WeightNorm</cell><cell>WeightNorm</cell><cell>8.7 5.5e-04</cell></row><row><cell>BatchNorm</cell><cell>BatchNorm</cell><cell>9.7 1.8e-04</cell></row><row><cell>MARS</cell><cell>gcvEarth_caret</cell><cell>9.9 8.2e-05</cell></row><row><cell>Boosting</cell><cell>LogitBoost_weka</cell><cell>12.1 2.2e-07</cell></row><row><cell>LMR</cell><cell>SimpleLogistic_weka</cell><cell>12.4 3.8e-09</cell></row><row><cell>Rule-based</cell><cell>JRip_caret</cell><cell>12.4 9.0e-08</cell></row><row><cell>Bagging</cell><cell>ctreeBag_R</cell><cell>13.5 1.6e-05</cell></row><row><cell cols="2">DiscriminantAnalysis mda_R</cell><cell>13.9 1.4e-10</cell></row><row><cell>Nearest Neighbour</cell><cell>NNge_weka</cell><cell>14.1 1.6e-10</cell></row><row><cell>DecisionTree</cell><cell>rpart2_caret</cell><cell>15.5 2.3e-08</cell></row><row><cell>OtherEnsembles</cell><cell>Dagging_weka</cell><cell>16.1 4.4e-12</cell></row><row><cell>NeuralNetworks</cell><cell>lvq_caret</cell><cell>16.3 1.6e-12</cell></row><row><cell>Bayesian</cell><cell>NaiveBayes_weka</cell><cell>17.9 1.6e-12</cell></row><row><cell>OtherMethods</cell><cell>pam_caret</cell><cell>18.3 2.8e-14</cell></row><row><cell>GLM</cell><cell>bayesglm_caret</cell><cell>18.7 1.5e-11</cell></row><row><cell>PLSR</cell><cell>simpls_R</cell><cell>19.0 3.4e-11</cell></row><row><cell>Stacking</cell><cell>Stacking_weka</cell><cell>22.5 2.8e-14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_53"><head>Table A14 :Table A15 :Table A16 :Table A17 :Table A18 :Table A19 :Table A20 :</head><label>A14A15A16A17A18A19A20</label><figDesc>Hyperparameters considered for self-normalizing networks in the Tox21 data set. Hyperparameters considered for ReLU networks with MS initialization in the Tox21 data set. Hyperparameters considered for batch normalized networks in the Tox21 data set. Hyperparameters considered for weight normalized networks in the Tox21 data set. Hyperparameters considered for layer normalized networks in the Tox21 data set. Hyperparameters considered for Highway networks in the Tox21 data set. Hyperparameters considered for Residual networks in the Tox21 data set.</figDesc><table><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell>Number of hidden units</cell><cell>{1024, 2048}</cell></row><row><cell>Number of hidden layers</cell><cell>{2, 3, 4, 6, 8, 16, 32}</cell></row><row><cell>Learning rate</cell><cell>{0.01, 0.05, 0.1}</cell></row><row><cell>Normalization</cell><cell>{Weightnorm}</cell></row><row><cell>Dropout rate</cell><cell>{0, 0.5}</cell></row><row><cell>Layer form</cell><cell>{rectangular, conic}</cell></row><row><cell cols="2">L2 regularization parameter {0.001,0.0001,0.00001}</cell></row><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell>Number of hidden units</cell><cell>{1024, 2048}</cell></row><row><cell>Number of hidden layers</cell><cell>{2,3,4,6,8,16,32}</cell></row><row><cell>Learning rate</cell><cell>{0.01, 0.05, 0.1}</cell></row><row><cell>Dropout rate</cell><cell>{0.05, 0.10}</cell></row><row><cell cols="2">Layer form L2 regularization parameter {0.001,0.0001,0.00001} {rectangular, conic} Hyperparameter Considered values</cell></row><row><cell>Number of hidden units</cell><cell>{1024, 2048}</cell></row><row><cell>Number of hidden layers</cell><cell>{2, 3, 4, 6, 8, 16, 32}</cell></row><row><cell>Learning rate</cell><cell>{0.01, 0.05, 0.1}</cell></row><row><cell>Normalization</cell><cell>{Layernorm}</cell></row><row><cell>Dropout rate</cell><cell>{0, 0.5}</cell></row><row><cell cols="2">Hyperparameter Layer form Number of hidden units L2 regularization parameter {0.001,0.0001,0.00001} {rectangular, conic} Considered values {1024, 2048}</cell></row><row><cell>Number of hidden layers</cell><cell>{2,3,4,6,8,16,32}</cell></row><row><cell>Learning rate</cell><cell>{0.01, 0.05, 0.1}</cell></row><row><cell>Dropout rate</cell><cell>{0.5, 0}</cell></row><row><cell>Layer form</cell><cell>{rectangular, conic}</cell></row><row><cell cols="2">L2 regularization parameter {0.001,0.0001,0.00001}</cell></row><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell>Number of hidden layers</cell><cell>{2, 3, 4, 6, 8, 16, 32}</cell></row><row><cell>Hyperparameter Learning rate</cell><cell>Considered values {0.01, 0.05, 0.1}</cell></row><row><cell cols="2">Number of hidden units Dropout rate Number of hidden layers L2 regularization parameter {0.001,0.0001,0.00001} {0, 0.5} {1024, 2048} {2, 3, 4, 6, 8, 16, 32}</cell></row><row><cell>Learning rate</cell><cell>{0.01, 0.05, 0.1}</cell></row><row><cell>Normalization</cell><cell>{Batchnorm}</cell></row><row><cell>Layer form</cell><cell>{rectangular, conic}</cell></row><row><cell cols="2">L2 regularization parameter {0.001,0.0001,0.00001}</cell></row><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell>Number of blocks</cell><cell>{2, 3, 4, 6, 8, 16}</cell></row><row><cell cols="2">Number of neurons per blocks {1024, 2048}</cell></row><row><cell>Block form</cell><cell>{rectangular, diavolo}</cell></row><row><cell>Bottleneck</cell><cell>{25%, 50%}</cell></row><row><cell>Learning rate</cell><cell>{0.01, 0.05, 0.1}</cell></row><row><cell>L2 regularization parameter</cell><cell>{0.001,0.0001,0.00001}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_54"><head>Table A24 :</head><label>A24</label><figDesc>Hyperparameters considered for WeightNorm networks on the HTRU2 data set.</figDesc><table><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell>Number of hidden units</cell><cell>{256, 512, 1024}</cell></row><row><cell cols="2">Number of hidden layers {2, 4, 8, 16, 32}</cell></row><row><cell>Learning rate</cell><cell>{0.1, 0.01, 1}</cell></row><row><cell>Normalization</cell><cell>{Weightnorm}</cell></row><row><cell>Layer form</cell><cell>{rectangular, conic}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_55"><head>Table A25 :</head><label>A25</label><figDesc>Hyperparameters considered for LayerNorm networks on the HTRU2 data set.</figDesc><table><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell>Number of hidden units</cell><cell>{256, 512, 1024}</cell></row><row><cell cols="2">Number of hidden layers {2, 4, 8, 16, 32}</cell></row><row><cell>Learning rate</cell><cell>{0.1, 0.01, 1}</cell></row><row><cell>Normalization</cell><cell>{Layernorm}</cell></row><row><cell>Layer form</cell><cell>{rectangular, conic}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_56"><head>Table A26 :Table A27 :</head><label>A26A27</label><figDesc>Hyperparameters considered for Highway networks on the HTRU2 data set. Hyperparameters considered for Residual networks on the HTRU2 data set.</figDesc><table><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell cols="2">Number of hidden layers {2, 4, 8, 16, 32}</cell></row><row><cell>Learning rate</cell><cell>{0.1, 0.01, 1}</cell></row><row><cell>Dropout rate</cell><cell>{0, 0.5}</cell></row><row><cell>Hyperparameter</cell><cell>Considered values</cell></row><row><cell>Number of hidden units</cell><cell>{256, 512, 1024}</cell></row><row><cell cols="2">Number of residual blocks {2, 3, 4, 8, 16}</cell></row><row><cell>Learning rate</cell><cell>{0.1, 0.01, 1}</cell></row><row><cell>Block form</cell><cell>{rectangular, diavolo}</cell></row><row><cell>Bottleneck</cell><cell>{0.25, 0.5}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_57"><head></head><label></label><figDesc>µ=0.06,ω=0,ν=1.35,τ =1.12) &lt; ∂J 11 ∂µ &lt; .00182415 (µ=−0.1,ω=0.1,ν=1.47845,τ =0.883374) (321) 0.905413 (µ=0.1,ω=−0.1,ν=1.5,τ =1.25) &lt; ∂J 11 ∂ω &lt; 1.04143 (µ=0.1,ω=0.1,ν=0.8,τ =0.8) −0.0151177 (µ=−0.1,ω=0.1,ν=0.8,τ =1.25) &lt; ∂J 11 ∂ν &lt; 0.0151177 (µ=0.1,ω=−0.1,ν=0.8,τ =1.25) −0.015194 (µ=−0.1,ω=0.1,ν=0.8,τ =1.25) &lt; ∂J 11 ∂τ &lt; 0.015194 (µ=0.1,ω=−0.1,ν=0.8,τ =1.25)−0.0151177 (µ=−0.1,ω=0.1,ν=0.8,τ =1.25) &lt; ∂J 12 ∂µ &lt; 0.0151177 (µ=0.1,ω=−0.1,ν=0.8,τ =1.25) −0.0151177 (µ=0.1,ω=−0.1,ν=0.8,τ =1.25) &lt; ∂J 12 ∂ω &lt; 0.0151177 (µ=0.1,ω=−0.1,ν=0.8,τ =1.25) −0.00785613 (µ=0.1,ω=−0.1,ν=1.5,τ =1.25) &lt; ∂J 12 ∂ν &lt; 0.0315805 (µ=0.1,ω=0.1,ν=0.8,τ =0.8)0.0799824 (µ=0.1,ω=−0.1,ν=1.5,τ =1.25) &lt; ∂J 12 ∂τ &lt; 0.110267 (µ=−0.1,ω=0.1,ν=0.8,τ =0.8) 0 (µ=0.06,ω=0,ν=1.35,τ =1.12) &lt; ∂J 21 ∂µ &lt; 0.0174802 (µ=0.1,ω=0.1,ν=0.8,τ =0.8) 0.0849308 (µ=0.1,ω=−0.1,ν=0.8,τ =0.8) &lt; ∂J 21 ∂ω &lt; 0.695766 (µ=0.1,ω=0.1,ν=1.5,τ =1.25) −0.0600823 (µ=0.1,ω=−0.1,ν=0.8,τ =1.25) &lt; ∂J 21 ∂ν &lt; 0.0600823 (µ=−0.1,ω=0.1,ν=0.8,τ =1.25) −0.0673083 (µ=0.1,ω=−0.1,ν=1.5,τ =0.8) &lt; ∂J 21 ∂τ &lt; 0.0673083 (µ=−0.1,ω=0.1,ν=1.5,τ =0.8) −0.0600823 (µ=0.1,ω=−0.1,ν=0.8,τ =1.25) &lt; ∂J 22 ∂µ &lt; 0.0600823 (µ=−0.1,ω=0.1,ν=0.8,τ =1.25) −0.0600823 (µ=0.1,ω=−0.1,ν=0.8,τ =1.25) &lt; ∂J 22 ∂ω &lt; 0.0600823 (µ=−0.1,ω=0.1,ν=0.8,τ =1.25) −0.276862 (µ=−0.01,ω=−0.01,ν=0.8,τ =1.25) &lt; ∂J 22 ∂ν &lt; −0.084813 (µ=−0.1,ω=0.1,ν=1.5,τ =0.8) 0.562302 (µ=0.1,ω=−0.1,ν=1.5,τ =1.25) &lt; ∂J 22 ∂τ &lt; 0.664051 (µ=0.1,ω=0.1,ν=0.8,τ =0.8)</figDesc><table><row><cell>Brief index</cell><cell>∂J ∂τ</cell><cell>&lt; 0.015194(0.03749149348255419)</cell></row><row><cell cols="3">∂J 12 ∂µ ∂J 12 ∂ω Banach Fixed Point Theorem, 13 &lt; 0.0151177(0.031242911235461816) Abramowitz bounds, 37 definition, 11 mapping in domain, 29 &lt; 0.0151177(0.031242911235461816) bounds self-normalizing neural networks, 2</cell></row><row><cell cols="3">derivatives of Jacobian entries, 21 ∂J 12 &lt; 0.0315805(0.21232788238624354) SELU Jacobian entries, 23 definition, 3 ∂ν mean and variance, 24 parameters, 4, 11</cell></row><row><cell cols="2">∂J 12 singular value, 25, 27 ∂τ central limit theorem, 6 ∂J 21 complementary error function ∂µ bounds, 37</cell><cell>&lt; 0.110267(0.2124377655377270) Theorem 1, 5, 12 proof, 13 proof sketch, 5 &lt; 0.0174802(0.02220441024325437) Theorem 2, 6, 12</cell></row><row><cell cols="2">∂J 21 computer-assisted proof, 33 definition, 37 ∂ω ∂ν definitions, 2 ∂J 21 contracting variance, 29</cell><cell>proof, 14 &lt; 0.695766(1.146955401845684) Theorem 3, 6, 12 &lt; 0.0600823(0.14983446469110305) proof, 18</cell></row><row><cell>domain singular value, 19 Theorem 1, 12</cell><cell>∂J 21 ∂τ</cell><cell>&lt; 0.0673083(0.17980135762932363)</cell></row><row><cell>Theorem 2, 12 Theorem 3, 13</cell><cell>∂J 22 ∂µ</cell><cell>&lt; 0.0600823(0.14983446469110305)</cell></row><row><cell>dropout, 6 erf, 37</cell><cell>∂J 22 ∂ω</cell><cell>&lt; 0.0600823(0.14983446469110305)</cell></row><row><cell>erfc, 37 error function bounds, 37 definition, 37 properties, 39</cell><cell>∂J 22 ∂ν ∂J 22 ∂τ</cell><cell>&lt; 0.562302(1.805740052651535) &lt; 0.664051(2.396685907216327)</cell></row><row><cell>expanding variance, 32</cell><cell></cell><cell></cell></row><row><cell>experiments, 7, 85</cell><cell></cell><cell></cell></row><row><cell>astronomy, 8</cell><cell></cell><cell></cell></row><row><cell>HTRU2, 8, 95</cell><cell></cell><cell></cell></row><row><cell cols="2">hyperparameters, 95</cell><cell></cell></row><row><cell cols="2">methods compared, 7</cell><cell></cell></row><row><cell>Tox21, 7, 92</cell><cell></cell><cell></cell></row><row><cell cols="2">hyperparameters, 8, 92</cell><cell></cell></row><row><cell>UCI, 7, 85</cell><cell></cell><cell></cell></row><row><cell>details, 85</cell><cell></cell><cell></cell></row><row><cell cols="2">hyperparameters, 85</cell><cell></cell></row><row><cell>results, 86</cell><cell></cell><cell></cell></row><row><cell>initialization, 6</cell><cell></cell><cell></cell></row><row><cell>Jacobian, 20</cell><cell></cell><cell></cell></row><row><cell>bounds, 23</cell><cell></cell><cell></cell></row><row><cell>definition, 20</cell><cell></cell><cell></cell></row><row><cell>derivatives, 21</cell><cell></cell><cell></cell></row><row><cell cols="2">entries, 20, 23 singular value, 21 singular value bound, 25 ∂J 11 ∂µ</cell><cell>&lt; 0.00182415(0.0031049101995398316)</cell><cell>(322)</cell></row><row><cell>lemmata, 19 Jacobian bound, 19</cell><cell>∂J 11 ∂ω</cell><cell>&lt; 1.04143(1.055872374194189)</cell></row><row><cell>mapping g, 2, 4</cell><cell>∂J 11 ∂ν</cell><cell>&lt; 0.0151177(0.031242911235461816)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b6">2</ref><p>.89498 × 10 7 τ 3/2 − 2.43651 × 10 7 τ 5/2 − 1.46191 × 10 9 τ 7/2 + 2.24868 × 10 7 τ 2 + 605413 = − 4.84561 × 10 7 τ 3/2 + 4.07198 × 10 9 τ 5/2 − 1.46191 × 10 9 τ 7/2 − 4.66103 × 10 8 τ 3 − 2.34999 × 10 9 τ 2 + 3.29718 × 10 7 τ + 6.97241 × 10 7 √ τ + 605413 605413τ First we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for τ for some positive terms and value of 0.8 for negative terms. These terms are then expanded at the =-sign.</p><p>The next equality factors the terms under the squared root. We decreased the negative term by setting τ = τ + 0.0000263835 under the root. We increased positive terms by setting τ + 0.000026286 = 1.00003τ and τ + 0.000026383 = 1.00003τ under the root for positive terms. The positive terms are increase, since <ref type="bibr">0.8+0.000026383</ref> 0.8 = 1.00003, thus τ + 0.000026286 &lt; τ + 0.000026383 1.00003τ . For the next inequality we decreased negative terms by inserting τ = 0.8 and increased positive terms by inserting τ = 1. <ref type="bibr" target="#b29">25</ref>. The next equality expands the terms. We use upper bound of <ref type="bibr" target="#b5">1</ref> </p><p>First we expanded the term (multiplied it out). The we put the terms multiplied by the same square root into brackets. The next inequality sign stems from inserting the maximal value of 1.25 for τ for some positive terms and value of 0.8 for negative terms. These terms are then expanded at the =-sign.</p><p>The next equality factors the terms under the squared root. We decreased the negative term by setting τ = τ + 0.00011542 under the root. We increased positive terms by setting τ + 0.00011542 = 1.00014τ and τ + 0.000115004 = 1.00014τ under the root for positive terms. The positive terms are increase, since 0.8+0.00011542 0.8 &lt; 1.000142, thus τ + 0.000115004 &lt; τ + 0.00011542 1.00014τ . For the next inequality we decreased negative terms by inserting τ = 0.8 and increased positive terms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.1 121 UCI Machine Learning Repository data sets: Hyperparameters</head><p>For the UCI data sets, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using 15% of the training data as validation set. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested "rectangular" and "conic" layers -rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. If multiple hyperparameters provided identical performance on the validation set, we preferred settings with a higher number of layers, lower learning rates and higher dropout rates. All methods had the chance to adjust their hyperparameters to the data set at hand.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.4 HTRU2 data set: Hyperparameters</head><p>For the HTRU2 data set, the best hyperparameter setting was determined by a grid-search over all hyperparameter combinations using one of the 9 non-testing folds as validation fold in a nested cross-validation procedure. Concretely, if M was the testing fold, we used M − 1 as validation fold, and for M = 1 we used fold 10 for validation. The early stopping parameter was determined on the smoothed learning curves of 100 epochs of the validation set. Smoothing was done using moving averages of 10 consecutive values. We tested "rectangular" and "conic" layers -rectangular layers have constant number of hidden units in each layer, conic layers start with the given number of hidden units in the first layer and then decrease the number of hidden units to the size of the output layer according to the geometric progession. All methods had the chance to adjust their hyperparameters to the data set at hand.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">121 UCI Machine Learning Repository data sets: detailed results Methods compared. We used data sets and preprocessing scripts by</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernández-Delgado</surname></persName>
		</author>
		<idno>A4.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">With several flaws in the method comparison[37] that we avoided, the authors compared 179 machine learning methods of 17 groups in their experiments. The method groups were defined by Fernández-Delgado et al. [10] as follows: Support Vector Machines, RandomForest, Multivariate adaptive regression splines (MARS), Boosting, Rule-based, logistic and multinomial regression, Discriminant Analysis (DA), Bagging, Nearest Neighbour, DecisionTree, other Ensembles</title>
	</analytic>
	<monogr>
		<title level="m">for data preparation and defining training and test sets</title>
		<imprint/>
	</monogr>
	<note>Neural Networks. generalized linear models (GLM. Partial least squares and principal component regression (PLSR), and Stacking</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">However, many of methods assigned to those groups were merely different implementations of the same method. Therefore, we selected one representative of each of the 17 groups for method comparison. The representative method was chosen as the group&apos;s method with the median performance across all tasks. Finally, we included 17 other machine learning methods of Fernández-Delgado et al. [10], and 6 FNNs</title>
		<imprint>
			<pubPlace>BatchNorm, WeightNorm, LayerNorm, Highway</pubPlace>
		</imprint>
	</monogr>
	<note>Residual and MSRAinit networks, and self-normalizing neural networks (SNNs</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">We assigned each of the 121 UCI data sets into the group &quot;large datasets&quot; or &quot;small datasets&quot; if the had more than 1,000 data points or less, respectively. We expected that Deep Learning methods require large data sets to competitive to other machine learning methods</title>
		<imprint/>
	</monogr>
	<note>Small and large data sets</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The results of the method comparison are given in Tables A12 and A13 for small and large data sets, respectively. On small data sets, SVMs performed best followed by RandomForest and SNNs. On large data sets, SNNs are the best method followed by SVMs and Random Forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Results</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>image-segmentation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Handbook of Mathematical Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abramowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stegun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics Series. National Bureau of Standards</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
	<note>10th edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep learning of representations: Looking forward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Statistical Language and Speech Processing</title>
		<meeting>the First International Conference on Statistical Language and Speech Processing<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Consider the lowly 2×2 matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Central limit theorems under weak dependence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for offline handwritten chinese character classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Phase 4: Dcl system using deep learning approaches for land-based or ship-based real-time recognition and localization of marine mammals-distributed processing and big data applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Parijs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.00982</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Do we need hundreds of classifiers to solve real world classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fernández-Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cernadas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amorim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3133" to="3181" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What every computer scientist should know about floating-point arithmetic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">223</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="48" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Coram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Widner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Madams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cuadros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A machine-checked theory of floating point arithmetic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bertot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dowek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hirschowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Théry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theorem Proving in Higher Order Logics: 12th International Conference, TPHOLs&apos;99</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">1690</biblScope>
			<biblScope unit="page" from="113" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An empirical evaluation of deep learning on highway driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tandon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01716</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A logarithm too clever by half</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An improvement of the Berry-Esseen inequality with applications to Poisson and mixed Poisson random sums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Korolev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shevtsova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Actuarial Journal</title>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The GNU C Library: Application Fundamentals. GNU Press, Free Software Foundation, 51 Franklin St, Fifth Floor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Loosemore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Stallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcgrath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Drepper</surname></persName>
		</author>
		<idno>MA 02110-1301</idno>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Boston; USA, 2</pubPlace>
		</imprint>
	</monogr>
	<note>24 edition</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fifty years of pulsar candidate selection: From simple filters to a new principled real-time classification approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stappers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of the Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">459</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1104" to="1123" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepTox: Toxicity prediction using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Environmental Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">On the definition of ulp(x)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Muller</surname></persName>
		</author>
		<idno>RR2005-09</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report Research report</note>
	<note>Laboratoire de l&apos;Informatique du Parallélisme</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Closed-form approximations to the error and complementary error functions and their applications in atmospheric science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mackenzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmos. Sci. Let</title>
		<imprint>
			<biblScope unit="page" from="70" to="73" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06947</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Are random forests truly the best classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">110</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">List of Figures 1 FNN and SNN trainin error curves</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A3 Graph of the main subfunction of the derivative of the second moment</title>
		<imprint>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A4 Graph of the Abramowitz bound for the complementary error function</title>
		<imprint>
			<biblScope unit="page">37</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">A5 Graphs of the functions e x 2 erfc(x) and xe x 2 erfc(x)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . . . . . . . . . . . .</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A6 The graph of functionμ for low variances</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A7 Graph of the function h(x) =μ 2 (0.1, −0.1, x, 1, λ 01 , α 01 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . . . . .</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">57</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">94 List of Tables 1 Comparison of seven FNNs on 121 UCI tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . . . . . . . . . . . . . . .</forename><surname>Snns</surname></persName>
			<affiliation>
				<orgName type="collaboration">. . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">A8 Distribution of network inputs in Tox21</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Comparison of FNNs at the Tox21 challenge dataset</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Comparison of FNNs and reference methods at HTRU2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . . . . . . . . . .</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A4 Hyperparameters considered for self-normalizing networks in the UCI data sets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A5 Hyperparameters considered for ReLU networks in the UCI data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . .</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A6 Hyperparameters considered for batch normalized networks in the UCI data sets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A7 Hyperparameters considered for weight normalized networks in the UCI data sets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A8 Hyperparameters considered for layer normalized networks in the UCI data sets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A9 Hyperparameters considered for Highway networks in the UCI data sets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A10 Hyperparameters considered for Residual networks in the UCI data sets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A11 Comparison of FNN methods on all 121 UCI data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . . . . . . . . . . .</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A12 Method comparison on small UCI data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . . . . . . . . . . . . . . . .</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A13 Method comparison on large UCI data sets</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">A14 Hyperparameters considered for self-normalizing networks in the Tox21 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A15 Hyperparameters considered for ReLU networks in the Tox21 data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">. . .</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m">A16 Hyperparameters considered for batch normalized networks in the Tox21 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A17 Hyperparameters considered for weight normalized networks in the Tox21 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A18 Hyperparameters considered for layer normalized networks in the Tox21 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A19 Hyperparameters considered for Highway networks in the Tox21 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">A20 Hyperparameters considered for Residual networks in the Tox21 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">A21 Hyperparameters considered for self-normalizing networks on the HTRU2 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">A22 Hyperparameters considered for ReLU networks on the HTRU2 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">A23 Hyperparameters considered for BatchNorm networks on the HTRU2 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m">A24 Hyperparameters considered for WeightNorm networks on the HTRU2 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">A25 Hyperparameters considered for LayerNorm networks on the HTRU2 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">A26 Hyperparameters considered for Highway networks on the HTRU2 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">A27 Hyperparameters considered for Residual networks on the HTRU2 data set</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
