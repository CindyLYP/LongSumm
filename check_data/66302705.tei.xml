<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">STTR: A System for Tracking All Vehicles All the Time At the Edge of the Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuangdi</forename><surname>Xu</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshit</forename><surname>Gupta</surname></persName>
							<email>harshitg@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umakishore</forename><surname>Ramachandran</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<addrLine>12 pages</addrLine>
									<settlement>New York</settlement>
									<region>NY, USA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">STTR: A System for Tracking All Vehicles All the Time At the Edge of the Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3210284.3210291</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>• Computer systems organization → Distributed architectures</term>
					<term>Sensor networks</term>
					<term>Real-time systems</term>
					<term>• Information systems → Storage management</term>
					<term>• Networks → Network types</term>
					<term>multi-target multi-camera tracking, smart camera surveillance, fog computing, trajectory management</term>
				</keywords>
			</textClass>
			<abstract>
				<p>To fully exploit the capabilities of sensors in real life, especially cameras, smart camera surveillance requires the cooperation from both domain experts in computer vision and systems. Existing alert-based smart surveillance is only capable of tracking a limited number of suspicious objects, while in most real-life applications, we often do not know the perpetrator ahead of time for tracking their activities in advance. In this work, we propose a radically different approach to smart surveillance for vehicle tracking. Specifically, we explore a smart camera surveillance system aimed at tracking all vehicles in real time. The insight is not to store the raw videos, but to store the space-time trajectories of the vehicles. Since vehicle tracking is a continuous and geo-distributed task, we assume a geo-distributed Fog computing infrastructure as the execution platform for our system. To bound the storage space for storing the trajectories on each Fog node (serving the computational needs of a camera), we focus on the activities of vehicles in the vicinity of a given camera in a specific geographic region instead of the time dimension, and the fact that every vehicle has a &quot;finite&quot; lifetime. To bound the computational and network communication requirements for detection, re-identification, and inter-node communication, we propose novel techniques, namely, forward and backward propagation that reduces the latency for the operations and the communication overhead. STTR is a system for smart surveillance that we have built embodying these ideas. For evaluation, we develop a toolkit upon SUMO to emulate camera detections from traffic flow and adopt MaxiNet to emulate the fog computing infrastructure on Microsoft Azure.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A smart camera surveillance system <ref type="bibr" target="#b7">[8]</ref> has the potential for simultaneously reducing manual labor (which could be error-prone) and achieving better efficiency and effectiveness for surveillance tasks. Specifically, due to increase in urban terrorism there is a crying need to bring more automation to bear on the task of suspicious vehicle tracking. Without such automation help, security personnel have to watch tons of archived videos in order to track a suspicious vehicle, which is extremely demanding and error-prone due to lapses in attention <ref type="bibr" target="#b17">[18]</ref>. Existing smart camera surveillance systems, such as IBM S3 <ref type="bibr" target="#b19">[20]</ref>, are largely alert-based. In other words, users need to register the suspicious vehicles in advance and then once the system detects them, it will start tracking and sending alerts. However, in reality, it is not always the case that we know which vehicle we want to track in advance. For example, after a traffic accident, the police would want to find where the perpetrators escaped, which there is no way to know in advance. Alert-based smart surveillance cannot help a lot in these situations. The state-of-the-art is to store raw video streams from cameras and if a situation warrants it, to analyze the recorded videos to extract the track of a suspicious vehicle postmortem.</p><p>In this work, we propose a radically different approach to smart surveillance for vehicle tracking. Specifically, we explore a smart camera surveillance system aimed at tracking all vehicles in real time. The insight is not to store the raw videos, but store the spacetime trajectories of the vehicles. Queries could then be answered directly from such recorded space-time tracks of all the vehicles. In this work, we do not focus on the querying side of the problem but on demonstrating that it is feasible to construct a system that would store the trajectories of all vehicles for their lifetime.</p><p>We adopt Fog/edge computing <ref type="bibr" target="#b2">[3]</ref> as the platform for smart camera-based vehicle tracking. Processing sensor streams (especially cameras) at the edge of the network is advantageous for three reasons: (a) reducing the latency for processing the streams, (b) reducing the backhaul bandwidth needed to send raw sensor streams to the Cloud; and (c) preserving privacy concerns for the collected sensor data. Specifically, for our work in building a smart surveillance system for 24x7 vehicle tracking, Fog computing offers the following advantages:</p><p>• For a large-scale smart camera surveillance system, sending thousands of camera streams to the Cloud is not practical and a waste of network resources.</p><p>• Vehicle tracking is a continuous and local task, and geodistributed Fog infrastructure is an ideal fit for such a task.</p><p>• Storing the vehicle trajectories at the edge of the network would enable fast query processing.</p><p>The biggest challenge for our approach is the limited storage on each fog node, since the vision in this work is to store the space-time track of all vehicles for their lifetime. Considering the infinite time dimension, it appears very hard (even impossible) to construct such a system, since cameras are continuously generating new vehicle detections. Based on this impression, the solution usually lies in discarding old trajectories or pushing the heavy lifting to the Cloud (under the assumption of unlimited resources in the Cloud) and reducing the role of the Fog to simply serve as a cache for recent trajectories. In our work, we look at the problem of trajectory generation differently. Instead of the time dimension, we focus on activity of a vehicle in a geographical region and formulate an upper bound for the storage space needed per vehicle for recording its activity in its entire lifetime. While the time dimension is infinite, the number of simultaneous activities (number of vehicles) of a finite geographical region is finite and the activity (lifetime) of given vehicle is also finite, which implies the storage required for a given finite geographical region to store all its activities is also finite. The assumption is that after the useful life of a vehicle (i.e., it is no longer being used in the roadways either because it has been totaled and/or has been sent to a junkyard), the tracks will no longer be hosted on the Fog nodes. Most likely it will be archived in the Cloud for legal reasons should such information be needed at some future date for law enforcement purposes. Considering the tag/title registrations for our vehicles every year, the task of tracking all vehicles is like an advanced vehicle registration system, and instead of on a yearly basis, the granularity is finer (say every second). And each camera (and its associated Fog node) is like a tag office, which is responsible of recording all activities occurring in its region.</p><p>The second challenge for large-scale camera-based tracking is the size of computation, where the system should make its best effort to detect and re-identify the vehicles as they are moving from cameras to cameras in real time. In other words, we want to reduce the size of computation or search search space such that we can minimize the latency of processing each vehicle detection. The smart camera system that we propose in this paper exploits the locality of vehicle tracking problem, and restricts the space to nearby cameras that vehicles must pass through. Specifically we propose two strategies to bound the computation that each camera has to do: forward propagation which progressively multicasts the signature of a detected vehicle to the downstream cameras in the forward path of the vehicle (using the topology of camera deployment in a given neighborhood) so that vehicle re-identification can be immediately triggered at a camera when a vehicle is sighted; and backward propagation to reach back to the upstream cameras upon detecting a new vehicle that was not informed by the forward propagation.</p><p>Based on the above two strategies, we propose our smart camera surveillance system dubbed, "STTR" (short for Space Time Trajectory Registration). STTR is aimed at solving the system side challenge, i.e., distributed computation, communication, and storage, for real-time vehicle tracking using camera networks. We rely on domain expertise from computer vision for multi-camera tracking algorithms including vehicle detection and re-identification. Also, in this work, we only focus on demonstrating the generation and storage of the space-time tracks of vehicles in real time. Efficient indexing structure of the space-time tracks for fast query processing is outside the scope of this work and will be explored in the future.</p><p>We summarize the contribution of this work as follows:</p><p>• We present details of the activity-based tracking of all the vehicles all the time in a given geographical area that is at the intellectual core of STTR, which enables bounding the storage space requirements at each Fog node.</p><p>• We present the details of forward and backward propagation that enable bounding the computation and communication requirements of STTR.</p><p>• We implement STTR using ZeroMQ for inter-camera communication and Redis persistent key-value store for recording the space-time trajectories.</p><p>• We build a toolkit on top of SUMO <ref type="bibr" target="#b12">[13]</ref>, with which we are able to import OpenStreetMap [21], generate traffic flows and detectors, simulate vehicle movements, and the corresponding camera streams.</p><p>• We evaluate STTR with the above tool kit and MaxiNet <ref type="bibr" target="#b22">[24]</ref> on Microsoft Azure to experimentally verify the theoretical assertions about finite storage space requirements for activity-based space-time tracking of vehicles.</p><p>The rest of the paper is organized as follows: Section 2 covers related work, Section 3 presents the problem definition and notations, Section 4 summarizes the theoretical upper bounds of computation and storage for the problem, Section 5 presents the details of the forward and backward strategies, Section 6 gives the architecture of STTR, Section 7 gives the implementation details of STTR, Section 8 presents the experimental setup for emulating the Fog computing infrastructure and the camera streams on Microsoft Azure using MaxiNet, Section 9 summarizes the evaluation of STTR, and Section 10 concludes with directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multi-target multi-camera tracking (MTMC). Wu, et al. <ref type="bibr" target="#b24">[26]</ref> present a new evaluation measure to isolate the trackbased multi-camera tracking (T-MCT) errors from single camera tracking (SCT) errors. Gou, et al. <ref type="bibr" target="#b5">[6]</ref> introduce DukeMTMC4ReID, a new large-scale real-world person re-identification dataset, which uses 8 disjoint surveillance camera views covering parts of the Duke University campus. Similarly, VeRi <ref type="bibr" target="#b15">[16]</ref> is a dataset for vehicle re-identification in urban surveillance scenario, which contains over 40,000 bounding boxes of 619 vehicles captured by 20 cameras. These datasets are more specialized for computer vision algorithms, while in our work, traffic flow and the road network are more influential. That's why we adopt SUMO <ref type="bibr" target="#b12">[13]</ref> for simulation and evaluation. Wang <ref type="bibr" target="#b20">[22]</ref> discusses topics related to intelligent multi-camera video surveillance such as multi-camera calibration, and computing the topology of camera networks. We believe that our work can be integrated with these ideas as long as they are local feature or detector based. Particularly we leave the detection and matching modules to domain experts, to allow integration of such algorithms into our system. Kawanishi, et al. <ref type="bibr" target="#b11">[12]</ref> study failures caused by appearance change or environmental illumination if matching only happens across adjacent cameras and introduce "random camera drop" and "trajectory ensemble" to integrate incomplete trajectory result. This work is a very good complement for extending our system when detections are missed due to occlusion (e.g., a truck hiding a vehicle from the camera's field of view). Smart video surveillance. Arth's <ref type="bibr" target="#b1">[2]</ref> work optimizes the object re-acquisition and tracking when there are limited resources. Alsmirat <ref type="bibr" target="#b0">[1]</ref> utilizes the cloud and mobile edge computing (MEC) to optimize the network bandwidth for wireless surveillance system. Fan <ref type="bibr" target="#b4">[5]</ref> presents an event-driven visualization mechanism fusing multi-modal information for a largescale intelligent video surveillance system. Mobile fog <ref type="bibr" target="#b10">[11]</ref> proposes a programming model for developing large-scale distributed situation awareness applications, launching the application components on Fog nodes at the edge of the network. Vehicle tracking by cameras is used as an example application for evaluation in mobile fog, as the intent is to showcase the features of the programming model. Arun <ref type="bibr" target="#b8">[9]</ref> proposes the use of smart surveillance to shift from "investigation of incidents" to "prevention of potentially catastrophic incidents". Their system architecture involves object detection, multi-object tracking, object classification and real time alert. Their work is a pioneering effort in smart surveillance. IBM s3 <ref type="bibr" target="#b19">[20]</ref> smart surveillance system involves a smart engine for video/image analysis and middleware for large scale surveillance management. Their work focuses more on openness and extensibility of the framework, and cross-indexed data model for correlation across multiple sensors and event types. Trajectory management. There exists many research efforts on trajectory compression or simplification <ref type="bibr" target="#b14">[15]</ref>[4] <ref type="bibr" target="#b16">[17]</ref>. While our work does not involve any trajectory simplification, these technologies can be very helpful if used with our system to further reduce actual storage usage. Another major topic on trajectory management is efficient query support <ref type="bibr" target="#b13">[14]</ref>[7] <ref type="bibr" target="#b18">[19]</ref>, which usually balances the trade-off between spatio-temporal range query and retrieval by distance or time interval. At the current stage of our work, our focus is on building the large-scale smart camera surveillance system and storing all the trajectories. Efficient query support (e.g., spatial and temporal indexing into the stored trajectories) is in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION AND NOTATIONS</head><p>This section defines the problem addressed in this work and all the notations used in the rest of the paper which we summarize in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="figure" target="#fig_0">Figure 1 1</ref> shows a pictorial representation of the road network and the computational infrastructure assumed in this work. We assume that vehicles are continuously moving on the road network, and there are no shortcuts hidden from the cameras for the vehicles to "disappear" from camera observation. We expect that most Icons made by Smashicons, Freepik from www.flaticon.com is licensed by CC 3.0 BY </p><formula xml:id="formula_0">d (v x , c i , t )</formula><p>The detected object of vehicle x at camera i and time stamp t.</p><formula xml:id="formula_1">V (d x ), C (d x ), T (d x ) Reverse functions. V (d x ) = v x , C (d x ) = c i and T (d x ) = t. reid (d x , p)</formula><p>The vehicle re-identification algorithm. Given d</p><p>x and a candidate pool (list) p of vehicles' detected objects, it returns i such that</p><formula xml:id="formula_2">V (d x ) = V (p[i]). head (d x )</formula><p>Getting the vehicle's heading information from</p><formula xml:id="formula_3">d x . f orward (c i )</formula><p>The candidate pool maintained for forward propagation on camera i, which is composed of detected objects received from other cameras' forward action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>backward (c i )</head><p>The candidate pool maintained for backward propagation on camera i. Detected objects in backward (c i ) meet the following requirement:</p><formula xml:id="formula_4">∀d x ∈ backward (c i ), V (d x ) ∈ c i . д k : t k</formula><p>One trajectory record generated from one detected object. In this paper, we use location and time stamp as an example.</p><formula xml:id="formula_5">u (д k 1 : t k 1 , д k 2 : t k 2 , . . . )|c i</formula><p>One trajectory vertex is the unit for vehicles' trajectory stored on one camera. It can consist of many successive trajectory records.</p><p>u x |c i Same as the above. We emit the trajectory records for simplicity.</p><formula xml:id="formula_6">u x t |c i → u x t +1 |c j</formula><p>The edge connected the trajectory vertices for the same vehicle on two different cameras. The first trajectory record of u x t +1 |c j should be the successor of the last one of u x t |c i . cameras are at road intersections and a few along the road. We assume a non-overlapping camera network in this paper, although there is nothing inherent in the system that we have built that will preclude it being used to a situation where the field of view (FOV) of the cameras overlap. We do not assume that cameras are at each road intersection, and in Sections 5 and 9, we will show how our system reacts to different density of camera distributions. Cameras are connected to nearby fog nodes, so camera streams can be processed there. For simplicity of illustration, we will assume each camera is connected to a unique fog node and refer to c i and f i interchangeably. In other words, in this paper we will use the word "camera" generally to include not only the capability of videoing but also computation, storage and network. In reality, it is more common that multiple cameras are connected to one nearby fog node, which can be done by running the application instances in the docker containers. We assume nearby cameras are connected by cables/fibers for network communication such that a low-latency local area network is set up among cameras.</p><p>With this setup, the smart camera surveillance system will launch application instances running on each camera, which cooperate with each other to track all passing vehicles in real time and store the trajectories such that the system is able to answer track-related queries immediately such as where did the red vehicle at c 2 come from? or where does the blue vehicle at c 9 head to? In the rest of the paper, we assume a perfect vehicle detection and re-identification algorithm and our work focuses on designing distributed computation, communication, and storage in this system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ACTIVITY AND STORAGE UPPER BOUND</head><p>In this section, we will elaborate how we use physical restrictions to find the upper bound of storage required on each camera to store vehicles' trajectories. To start with, we first define two terminologies, vehicle activity and camera's activity region. </p><formula xml:id="formula_7">v x , c i , t )∧(∀c i ′ ∄t ′ (t ′ &gt; t ) ∧ d (v x , c i ′ , t ′ )), then we call vehicle x is active under camera i from time t, shortened as v x ∈ c i |[t, now]. Similarly, if we have ∃d (v x , c i , t r ) ∧ (∃c j t p (t p &gt; t r ) ∧d (v x , c j , t p )) ∧ (∀c i ′ ∄t q (t p &gt; t q &gt; t r ) ∧ d (v x , c i ′ , t q ))</formula><p>, then we call vehicle x was active under camera i between t r and t p , shortened as</p><formula xml:id="formula_8">v x ∈ c i |[t r , t p ].</formula><p>In other words, at any given time point, each vehicle is active under the camera that last detected it. As the last camera detecting the vehicle changes with the movement of the vehicle, so we have a chain of vehicle activities. For example, in figure 1, we have</p><formula xml:id="formula_9">v r ∈ c 2 → v r ∈ c 3 for the red vehicle and v b ∈ c 9 → v b ∈ c 5 → v b ∈ c 6 → v b ∈ c 7 for the blue vehicle 2 . Let's consider a specific stage of chain v x ∈ c i |[t r , t p ] → v x ∈ c j |[t p , t q ]</formula><p>. We know vehicle x is at camera i at t r and at camera j at t p , but we have no information where v x is exactly between t r and t p . For example, it could be moving on the road, or it could have come to a stop (say in a parking garage, or on the roadside). But we do know it must be at some place that cannot be detected by any other cameras during [t r , t p ]. The collection of these places are called camera i's activity region.</p><p>Definition 4.2. For camera i, its activity region is defined as:</p><formula xml:id="formula_10">∀v x ,c j v x ∈c i |[t r ,t p ]→v x ∈c j |[t p ,t q ] location(v x )</formula><p>For example, in <ref type="figure" target="#fig_0">Figure 1</ref>  In the rest of the paper, we only focus on interior cameras unless stated otherwise. For a closed geographical region, almost all cameras are interior cameras, and for a large-scale camera surveillance system, majority of the cameras are interior. In Section 7, we will revisit boundary cameras and discuss options therein for bounding the storage requirement.</p><p>The finite size of the camera's activity region gives us a very good property, because at any given time, the number of vehicles that are active under this camera has to be finite, for vehicles need to occupy space. Meanwhile, each vehicle's life is also finite which indicates there exists trajectory upper bound for a given camera. Putting these facts together, we have the following simple idea -At any time point, each camera stores the trajectory of vehicles that are active under its region. And we have the storage space required on a camera i at time t is:</p><formula xml:id="formula_11">disksize (c i , t ) = ∀v x ,v x ∈c i |[t,t ] sizeo f (traj (v x ))</formula><p>where traj is the trajectory of a given vehicle and sizeof is the number of bytes to store them. By applying the activity region and the trajectory upper bound, we have the following theorem: Theorem 4.4. For each camera i, its storage space over time disksize (c i , t ) has the following upper bound:</p><formula xml:id="formula_12">∀t, disksize (c i , t ) ≤ ρ i size (c i )#maximum size of trajectory</formula><p>where ρ i size (c i ) is the maximum number of vehicles that can be simultaneously active under c i .</p><p>Since this upper bound is conditioned by ρ i size (c i ), which is governed by the physical environment near the camera and is unlikely to change frequently, this also gives a good reference for system administrators to know the storage capacity needed for each camera.</p><p>We show the storage required on each camera is finite, but is it small enough to be practical? We make the discussion concrete with a hypothetical but realistic example. Considering that the average life of a vehicle is 150000 miles <ref type="bibr" target="#b21">[23]</ref>, with a deployment of 5 cameras every mile, using two 64-bit longitude-latitude to represent each car detection, and assuming 100 (maximum number of simultaneous vehicles) activities for a given camera, the upper bound for storage space needed on each camera can be calculated as 150000 * 5 * 2 * 64 * 100 which is around 1.12GB <ref type="bibr" target="#b2">3</ref> . In reality, we will have to store more meta data besides longitude-latitude, but at the same time it is also unlikely that ALL the vehicles in one camera's activity region are close to the end of their active life.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">PROPAGATING VEHICLE DETECTIONS</head><p>In this section, we are looking into the detection propagation models to limit the storage and computational needs for vehicle reidentification. Upon vehicle detection by a camera, the detections have to be propagated for re-identification by neighboring cameras. While domain experts provide detection and re-identification algorithms, our system will handle the underlying detection propagation to reduce the resource requirements. Broadcasting to all the cameras in the region of interest is not only wasteful of networking resources but also burdening ALL the cameras with unnecessary additional computational work (leading to latency for re-identification). Instead, we propose two detection propagation models, detection forward and backward, which exploit the topology of camera deployment in the roadways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Forward Propagation</head><p>Upon detection of a vehicle, the camera propagates the detected vehicle's signature to the set of cameras that are likely candidates for this vehicle to pass through next. We call this forward propagation. This way the downstream cameras are already primed to run the re-identification procedure when the vehicle is sighted without any additional communication. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, when the blue vehicle is detected by c 5 , c 6 is the only candidate camera downstream in the direction in which the blue car is headed, so we can safely forward d (v b , c 5 , t ) to c 6 . For a more complicated case, let us remove c 6 from the camera surveillance system. In this case, since the direction of travel of the vehicle cannot be predicted, we have to forward d (v b , c 5 , t ) to a set of cameras (c 3 , c 7 , c 10 ). Eventually one camera will detect the vehicle again, which is c 7 in this case, and c 7 will send a confirmation to (c 3 , c 10 ), so they can safely discard</p><formula xml:id="formula_13">d (v b , c 5 , t ).</formula><p>In general, until such confirmation is received from one of the downstream cameras that are in the plausible set for the next sighting of the vehicle, all cameras have to hold on to the objects received by this forward propagation.</p><p>Theorem 5.1. The resource for each camera to store all the detection objects received by forward propagation has the following upper bound:</p><formula xml:id="formula_14">∀c i , sizeo f ( f orward (c i )) ≤ ρ i size (c i ) * sizeo f (d x )</formula><p>This upper bound is predicated on clearing out the trajectories of "destroyed vehicles" from the fog nodes either by archiving them in the Cloud or deleting them permanently as discussed in Section 7.2.3.</p><p>where f orward (c i ) is the candidate pool for detection objects received by forward propagation and we assume the format of detected object is fixed, so sizeo f (d x ) is constant.</p><p>The rationale behind theorem 5.1 has similarity with how we discuss the activity and storage upper bound. When the detected object</p><formula xml:id="formula_15">d (v x , c i , t r ) stays in the f orward (c j ) at time t p , it means v x ∈ c i |[t r , t p ]</formula><p>. In other words, v x must be somewhere between the c i and c j , which we call the shared activity region between camera i and j, shortened as c i ∩ c j . By summing up all cameras that have shared activity region with c j , we have size</p><formula xml:id="formula_16">( ∀c i c j c i ∩ c j ) ≤ size (c j ). And we also have ∀d x ∈ f orward (c j ) ⇒ V (d x ) ∈ ∀c i c j c i ∩ c j .</formula><p>Combining these terms together, if a detection object is in the camera's forward propagation candidate pool, then the corresponding vehicle must occupy the space in the camera's activity region. So the storage resource of the camera's candidate pool is capped by the product of maximum number of vehicles in the camera's activity region and size of each detected object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Backward Propagation</head><p>Backward Propagation would be warranted if a vehicle is detected by a camera for which no forward detection notification was received from some upstream camera in the direction of travel of the vehicle <ref type="bibr" target="#b3">4</ref> . In this case, the camera has to guess whence from (i.e., which upstream camera) the vehicle came. Therefore, it sends the detected object "backwards" to a candidate set of upstream cameras once again taking into account the deployment topology for possible re-identification by one of those cameras. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, considering removing c 6 from the camera surveillance system, if the blue vehicle is detected by c 7 , (without any prior forwarding information), we will backward d (v b , c 7 , t ) to a set of cameras (c 3 , c 5 , c 10 ). One camera should confirm it saw v b before, which is c 5 in this case. Different from forward propagation, cameras do not need to keep the detected objects received from backward propagation. Instead, if the camera cannot re-identify the detected object in the backward propagation, it simply discards the object. On the other hand, the candidate pool for backward propagation is composed of detected objects that have not been re-identified by any other upstream cameras, which is ∀d x ∈ backward (c i )</p><formula xml:id="formula_17">⇒ V (d x ) ∈ c i where backward (c i )</formula><p>is the backward candidate pool of camera i.</p><p>Theorem 5.2. The resource for each camera to store all the detected objects for backward propagation has the following upper bound:</p><formula xml:id="formula_18">∀c i , sizeo f (backward (c i )) ≤ ρ i size (c i ) * sizeo f (d x )</formula><p>Again the storage space is capped by the product of the maximum number of vehicles in the camera's activity region and size of each detected object, as backward (c i ) is actually the collection of detected objects of vehicles that are active under camera i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">STTR SYSTEM ARCHITECTURE</head><p>STTR (short for Space Time Trajectories Registration) is the system that embodies the ideas presented thus far for registering the spacetime tracks of all the vehicles all the time. <ref type="figure" target="#fig_4">Figure 2</ref> shows the STTR In the current implementation of STTR, failure of forward propagation is caused by the optimizations in section 7.1, while in reality it could also be due to the fact that an upstream camera failed to detect the vehicle. system architecture. Each application instance running on the camera contains 7 modules: Detection, Matching, Forward, Backward, TrajectoryStore, CameraMap and Policy. Among them, Detection and Matching provide interfaces for domain experts to plug in unique algorithms. Specifically, we expect the input of Detection will be the raw camera stream (frames) and the output of Detection will be the stream of vehicle's detected objects. Each detected object is a JSON object that at least includes time stamp, unique signature and direction of travel information. Matching implements the reidentification algorithm reid (d x , p), which inputs a detected object and a candidate pool and outputs the object from the candidate pool. For the remaining 5 modules, CameraMap maintains the geographical relationship between cameras, which is mainly used with the direction of travel to form the set of cameras for forward and backward propagation; Forward/Backward implements the vehicle detection propagation in Section 5, and real-time requirement is also taken into account by dynamically monitoring the traffic flow and truncating/flushing the forward candidate pool; Trajectory-Store distributes the trajectory according to vehicle activities as discussed in Section 4 and optimizes network consumption; Policy module allows each camera to configure parameters and disable specific modules, for example only a subset of cameras are permitted to archive the trajectories at the cloud. There could be services running in the cloud to reach into the space-time trajectories stored in the camera nodes. For example, we show the HistoryTrajectory service in the cloud in <ref type="figure" target="#fig_4">Figure 2</ref>. This service may provide the ability to archive some of the trajectories (e.g., vehicles no longer in active service, trajectories that are older than some delta, etc.) from the camera nodes to the cloud. Similarly, we envision other services, e.g., QueryEngine that may embody our future work with supporting query processing of the stored trajectories in the cameras.</p><p>The flow of processing each detected objects is presented as green lines in <ref type="figure" target="#fig_4">Figure 2</ref>. After getting detected objects d x from Detection, d</p><p>x is given to the Forward to multicast it to candidate cameras, and Matching to reid (d x , f orward (c i )). If a re-identification is successfully found, d x and the previous detected object is given to TrajectoryStore for writing. Otherwise, d x will be given to the Backward to multicast to backward candidate cameras. Meanwhile, Backward will re-identify the detected objects it received with backward (c i ) through Matching, and similarly if a re-identification is found, TrajectoryStore will be used for recording.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">IMPLEMENTATION DETAILS</head><p>STTR is written in Python, communication between cameras is based on ZeroMQ, and persistent key-value store is based on Redis. In this section, we go into some implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Matching</head><p>The Matching module brings in the re-identification algorithm provided by domain experts, which accepts a detected object and a candidate pool. In this section, we show some tricks in STTR to help further reducing the latency of re-identification. In other words, long-term stopped vehicle will produce an unwelcome detected object in the top of following cameras' forward candidate pool, and we want to discard it (when the corresponding vehicle moves again later, its re-identification can be found through backward propagation) to reduce the latency for majority moving vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Forward_candidate_pool_flush</head><formula xml:id="formula_19">1: k ← reid (d x , f orward (c i )) 2: if k &gt; α * len( f orward (c i )) then 3: f orward (c i ) ← f orward (c i )[k + 1 : end] 4: else 5: f orward (c i ) ← f orward (c i )[0 : k − 1, k + 1 : end] 6: end if</formula><p>Algorithm 1 illustrates the idea of flush operation of camera's forward candidate pool. After we get the re-identification result for every detected object, we check the position of the result in the candidate pool, where α is a parameter between 0-1 for tolerance of short-term stopped vehicles and overtaking between vehicles. If we find the result is after first α * len( f orward (c i )) objects, we flush the candidate pool by removing all detected objects before the re-identification result. Otherwise, we only remove the result itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Adaptive truncated candidate pool.</head><p>Considering a longterm stopped vehicle moves again, when it gets detected by the following camera, unfortunately its forwarded detected object has been discarded. However, there is no way for Matching module to know this, so it will try to re-identify the vehicle with the whole forward candidate pool, fail and fall back to Backward module. When the re-identification algorithm is efficient and rate of traffic flow is low, we can afford doing this, but this is not often the case. Instead we want to consider the rate of traffic flow, efficiency <ref type="figure">Figure 3</ref>: Trajectory aggregation example. The red vehicle starts at intersection G 0 at time T 0 , then moves through intersection G 1 at time T 1 and finally ends at intersection G 2 at time T 2 . Correspondingly we have camera C i and fog node F i at each road intersection.</p><p>of re-identification algorithm and then decide whether to give a truncated candidate pool to the Matching module. Intuitively, the slower the algorithm is, the more we need to truncate the candidate pool. Similarly, if the traffic flow is high, we have to spend less time on each detected object, so the candidate pool needs to include few objects. By these observations, Algorithm 2 shows how we monitor the rate of traffic flow, time spent on re-identifying the last detected object, and then update the size of the candidate pool given to the re-identification function.</p><formula xml:id="formula_20">Algorithm 2 Adaptive_truncated_candidate_pool 1: rate ← Detection(c i ) 2: L ← L r at e * t ime 3: (k, time) ← reid (d x , f orward (c i )[0 : L])</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">TrajectoryStore</head><p>In this subsection, we go into the detail of trajectory implementation in STTR and how to bring the activity and storage upper bound in Section 4 into life and optimize it. We implement the trajectories as vertices and edges linking two vertices on the key-value store. Each trajectory vertex is a list of trajectory records of when and which camera detected the corresponding vehicle. Each edge links trajectory vertices stored on different fog nodes and heads to the vertex with large time stamp. For simplicity, we will use the following notation u (д 1 : t 1 , д 2 : t 2 )|c 2 → u (д 3 : t 3 )|c 3 to represent one trajectory vertex on camera 2 storing two trajectory records (д 1 , t 1 ) and (д 2 , t 2 ), while another trajectory vertex on camera 3 stores the trajectory record (д 3 , t 3 ) and a directed edge links them together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Greedy trajectory aggregation.</head><p>The most intuitive way to implement the idea -At any time point, each camera stores the trajectory of vehicles that are active under its region -is to aggregate the trajectories as vehicle moves from one camera to another. Take the red vehicle in <ref type="figure">Figure 3</ref>  <ref type="bibr" target="#b4">5</ref> as an example:</p><p>• When the red vehicle is detected by C 0 , we generate the trajectory vertex u (G 0 : T 0 )|F 0 .</p><p>• When the red vehicle is then detected by C 1 , we get and remove the previous trajectory vertex u (G 0 : T 0 )|F 0 from F 0 , and aggregate the new trajectory vertex at F 1 , so we have</p><formula xml:id="formula_21">u (G 0 : T 0 , G 1 : T 1 )|F 1 .</formula><p>Icons made by Smashicons, Freepik from www.flaticon.com is licensed by CC 3.0 BY</p><p>• Similarly, when the red vehicle is finally detected by C 2 , we get and remove the previous trajectory vertex u (G 0 : T 0 , G 1 :</p><p>T 1 )|F 1 from F 1 , and aggregate the new trajectory vertex at F 2 , so we have u (G 0 :</p><formula xml:id="formula_22">T 0 , G 1 : T 1 , G 2 : T 2 )|F 2 .</formula><p>Algorithm 3 Greedy_trajectory_aggregation</p><formula xml:id="formula_23">1: d x ← Detection(c i ) 2: d ′ x ← Matchinд(d x , c i ) 3: u ′ x ← pullAndRemoveTrajectory(d ′ x ) 4: store (append (u ′</formula><p>x , c i :</p><formula xml:id="formula_24">T (d i ))|c i )</formula><p>Algorithm 3 summarizes the procedure of greedy trajectory aggregation. Upon receiving the new detected object d x and its reidentification result d ′</p><p>x , we pull and remove the trajectory vertex u ′</p><p>x from the camera</p><formula xml:id="formula_25">C (d ′ x )</formula><p>, which is the previous camera that detected the vehicle. Then we store and append the new trajectory record to u ′</p><p>x . The greedy trajectory aggregation is a direct translation from the activity upper bound idea, but it suffers from the increasing network consumption over time. Because the trajectories of vehicle will become larger and larger over the life of the vehicle, and it will be not practical to keep pulling the whole trajectories from one camera to another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Lazy trajectory aggregation.</head><p>The idea is that each camera only aggregates the trajectory if its storage is under pressure and a new trajectory vertex is coming. We again use the example in <ref type="figure">Figure 3</ref> and assume no camera's storage is under pressure at the beginning.</p><p>• When the red vehicle is detected by C 0 , we generate the trajectory vertex u (G 0 : T 0 )|F 0 .</p><p>• When the red vehicle is then detected by C 1 , we generate the new trajectory vertex and create an edge from the previous trajectory vertex, which is u (G 0 :</p><formula xml:id="formula_26">T 0 )|F 0 → u (G 1 : T 1 )|F 1 .</formula><p>• Similarly, when the red vehicle is finally detected by C 2 , we will have u (G 0 :</p><formula xml:id="formula_27">T 0 )|F 0 → u (G 1 : T 1 )|F 1 → u (G 2 : T 2 )|F 2 .</formula><p>• After a period time, F 1 's storage is under pressure and a new trajectory vertex is coming, so u (G 1 : T 1 )|F 1 is chosen to be aggregated.</p><p>• u (G 1 : T 1 )|F 1 will be aggregated to F 2 , so we have u (G 0 :</p><formula xml:id="formula_28">T 0 )|F 0 → u (G 1 : T 1 , G 2 : T 2 )|F 2</formula><p>as the trajectory of the red vehicle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 Lazy_trajectory_aggregation</head><formula xml:id="formula_29">1: d x ← Detection(c i ) 2: d ′ x ← Matchinд(d x , c i ) 3: u y ← randomPick (c i ) 4: aддreдate (u y ) 5: store (trajectoryV ertex (d ′ x )− &gt; u (c i : T (d x ))|c i )</formula><p>Algorithm 4 summarizes the procedure of lazy trajectory aggregation. Upon receiving the new detected object d x and its reidentification result d ′</p><p>x , if the storage of the camera is under pressure <ref type="bibr" target="#b5">6</ref> , a random victim trajectory vertex with outgoing edge is <ref type="bibr" target="#b5">6</ref> Idea of lazy trajectory aggregation also works without setting a threshold for storage pressure, and actually in evaluation, in order to see the lazy trajectory aggregation in picked u y . We aggregate u y to the trajectory vertex which the outgoing edge points to. Then we create an edge from the trajectory vertex of d ′</p><p>x to the new trajectory vertex generated from d x . Under lazy trajectory aggregation, each camera also owns the activity upper bound and corresponding storage upper bound. It is because whenever a new trajectory vertex is coming, either the activity region of the camera hasn't been full of vehicles or one old trajectory vertex is available to be aggregated to another camera. In the worst case, for example, all cameras are under pressure, and then lazy trajectory aggregation will be eventually consistent with greedy aggregation as it will keep aggregating the trajectory vertex with outgoing edges such that trajectory data leaving on one camera will only belong to the vehicles that are still active under this camera. Moreover, lazy trajectory aggregation is network friendly for two reasons. Firstly, the rate of the trigger of lazy aggregation is the rate of incoming vehicles detected by the camera which is slow compared to the network speed <ref type="bibr" target="#b6">7</ref> . Secondly, the minimum aggregation only needs to transmit one trajectory record as there is only one trajectory record written in. Yet, for network efficiency, it is usually better to pack multiple trajectory records into one transmission. On the other hand, the downside of lazy trajectory aggregation is the potential increased in query processing time, e.g., for a range search. Exploring indexing techniques for efficient query processing is part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">General discussion</head><p>. Does the infinite time line contradict with the activity upper bound we give for each camera? The answer is no. Let's consider a closed region first. In a closed region, every camera is the interior camera except for those at the sink and the source. Source is where the vehicle is created, such as the vehicle factory, the entrance of the region, while the sink is where the vehicle is "destroyed" (e.g, vehicle is "totaled", or sent to the junkyard), the exit of the region. In other words, when the vehicle is created from the source, we start tracking the vehicle, and when the vehicle is destroyed at the sink, we stop tracking the vehicle. So more precisely, we are able to track all alive vehicles in the region over their lifetime with each camera owning limited resources. With time elapsing, there will be new vehicles born from the source and we start to allocate resource for them, old vehicles gone at the sink and we "free" their resources (either by deletion or archiving them permanently in the cloud). Notice we do not need explicit garbage collection, because the trajectory aggregation will eventually aggregate the whole trajectory to the sink, as the sink is the last camera detecting those retired vehicles. On the other hand, for a general non-closed region, boundary cameras are different from sink and source, because vehicles can leave from one boundary camera and enter the region from another. One interesting option we can try is to create virtual cameras that connect all boundary cameras together to manually force a closed region. Although all cameras turn into interior, these virtual cameras cover the infinite activity region in terms of storing activities. Theoretically it is possible any vehicle from around the world could frequently leave and enter a given region. However, in reality, we believe vehicles active in a specified geographic region are largely "return customers" to the STTR the three-hour simulation, we omit the threshold and start the aggregation from the beginning. <ref type="bibr" target="#b6">7</ref> Physical world is slow compared to the cyber world! system every day. The example cloud service (HistoryTrajectory) shown in <ref type="figure" target="#fig_4">Figure 2</ref> would come in handy to archive trajectories from the boundary cameras (or the virtual cameras) into the cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Other implementation details</head><p>For performance and our best effort to guarantee the real-time property, we adopt a lock-free system design and manually handle the collisions between threads and cameras. For example, considering the trajectory u x t |c i → u x t +1 |c j , in lazy trajectory aggregation, it is possible that C i and C j will independently make the decision to aggregate u x t |c i and u x t +1 |c j at the same time. When the u x t |c i aggregation request reaches C j , u x t +1 |c j may not exist anymore. A "fail-wait-retry" protocol is used, as c j is responsible for updating the trajectory edge information correctly. The trajectory vertex will be chosen as the victim for lazy trajectory aggregation only if it has both incoming and outgoing edges, which indicates its role in re-identification has completed. Otherwise the trajectory vertex and corresponding detected object have to be kept on the birth camera for the re-identification purpose. Backward propagation is multi-thread supported. Instead of blocking and processing one vehicle detected object each time, multi-threading can be used to improve the overall throughput, which is very helpful when the re-identification is slow and we have redundant computational resources. A simple example cloud service is implemented, which can be used to achieve the trajectory based on LIVEness factor. For example, LIV Eness = 3600 which means for each vehicle, the recent one-hour trajectory will be kept at the Fog while the trajectory older than one hour will be pushed to the Cloud. This is implemented by checking the trajectory's timestamp upon each trajectory aggregation, and establishing a trajectory edge from Cloud to the Fog to guarantee the integrity of each vehicle's trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EXPERIMENTAL SETUP</head><p>In this section, we present the experimental setup for performance evaluation of STTR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Traffic flow and camera stream of detections</head><p>SUMO is an open source, highly portable, microscopic and continuous road traffic simulation package designed to handle large road  networks <ref type="bibr" target="#b12">[13]</ref>. We build a tool kit upon SUMO to generate camera stream of detections given the simulated traffic flow. Specifically, we import and simplify the campus map from OpenStreetMap [21] as shown in <ref type="figure" target="#fig_6">Figure 4</ref>. Vehicles are generated on each road following the distribution shown in the <ref type="table" target="#tab_1">Table 2</ref>, where sigma is the driver's imperfectness. Number of vehicles started on each road is proportional to the length of the roads. Rerouters which keep vehicles moving are generated at the end of each lane. And detectors which record the entering and leaving of vehicles are generated at beginning and end of each lane. Finally, we cluster the output of detectors by road intersections, extract the vehicle identifier, time stamp and direction of travel and publish them to cameras' application instances as stream of detected objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Fog computing topology</head><p>We emulate the fog computing topology using MaxiNet <ref type="bibr" target="#b22">[24]</ref> on a Microsoft Azure virtual machine with 32 cores, 128 GB memory. <ref type="figure" target="#fig_7">Figure 5 8</ref> shows the example network topology. Each fog node owns two hosts (docker containers), one for STTR's application instance and the other for the redis key-value store, and one network switch which connects these two components. Nearby fog nodes' switches are connected such that the network topology is isomorphic with the road network in <ref type="figure" target="#fig_6">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">EVALUATION</head><p>All experiments are based on a 10000-second traffic flow and camera stream emulation. Following metrics are collected during the emulation: storage usage from the Redis's info command, network usage using Linux utilities tcpdump and tcpstat <ref type="bibr" target="#b9">[10]</ref>, computation latency for each detected object. Following variables are changed between experiments: set of cameras enabled to see how density of cameras <ref type="bibr" target="#b7">8</ref> Icons made by Smashicons, Freepik from www.flaticon.com is licensed by CC 3.0 BY </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Storage</head><p>In this subsection, we quantify the storage space usage on each camera over time. There are three factors that influence the storage usage: the road network and camera density, the traffic flow, and the trajectory aggregation. As shown in Section 4, the road network and camera density determine the maximum activity region and therefore the upper bound of storage usage on a particular camera. Meanwhile the actual traffic flow determines the distribution of routing storage usage across cameras. Finally, the trajectory aggregation will migrate data around cameras to guarantee that no camera is overloaded. 9.1.1 Activity vs. storage usage. <ref type="figure" target="#fig_8">Figure 6</ref> shows the storage usage and number of activities accumulated on each camera at the end of the experiment. As mentioned earlier, an activity denotes that a particular vehicle is active under the camera. Hence the number of accumulated activities on a camera implies the number of vehicles whose journey ended in the camera's activity region. Firstly, we can see that in general, the storage usage increases with the increase in the number of activities, because each camera needs to store all activities ending in its activity region. However, the storage usage on a camera does not increase linearly with the number of accumulated activities because of the size of each activity (trajectory) is not</p><p>We consider the optimal case is comparing two license plate numbers for which the overhead is negligible.  necessarily the same. Secondly, we observe that the number of accumulated activities on each camera increases with the decrease in the density of cameras. This is because a lower density of cameras leads to each camera being responsible for a large activity region, making them responsible for a higher number of activities. However, the storage usage does not necessarily increase, because with fewer cameras, we also have fewer detections and correspondingly the size of a trajectory is smaller. This demonstrates that the storage usage on each camera is determined by the number of accumulated vehicle activities and the size of vehicle's trajectory together. 9.1.2 Camera density vs. storage usage. To better see how density affects the storage usage, we pick camera1 and plot its storage usage and the number of activities over time for the three experimental settings (C h , C m and C l ) in <ref type="figure" target="#fig_10">Figure 7</ref>. Firstly, we can see the storage usage is slowly increasing, which is expected because each trajectory becomes longer over time. Meanwhile, there exists dramatic fluctuations on storage usage, which is caused by the greedy trajectory aggregation. When a large trajectory gets migrated and aggregated to another camera, the storage usage declines rapidly, and when a large trajectory gets migrated and aggregated in, the storage usage roars up. Secondly, we observe that the storage usage pattern with a medium density of cameras (green line) is lower than that of high camera density (red line). The storage usage pattern with low camera density (blue line) is further lesser than the medium density pattern. This means the storage usage is lower with fewer cameras, and the reason is that each trajectory is shorter due to fewer number of detection points. On the other hand, from the right parts of the figure, we can clearly see the general increase in the number of activities during the whole simulation when we choose fewer cameras. 9.1.3 Greedy/lazy aggregation vs. storage usage. <ref type="figure" target="#fig_11">Figure 8</ref> compares the storage usage between greedy and lazy trajectory aggregation. We plot the cameras with the maximum and minimum average storage usage, so we can expect that other cameras' storage variation would fall between them. We also plot a baseline (yellow and purple line in the figure), which represent cases when no aggregation is triggered and every camera simply stores the detections in its activity region. The baseline, though not practical, serves as the worst case upper bound for storage need at each camera.</p><p>We can see that both greedy and lazy trajectory aggregation techniques incur much less storage usage over time than the baseline approach, which is because by aggregating a vehicle's trajectory and storing them on one camera, we save on the duplication of the vehicle meta data. Moreover, both greedy and lazy trajectory aggregation also balance the storage usage better across cameras than the baseline. With the baseline approach, where trajectory aggregation is not enabled, we see that the storage gap (between maximum and minimum storage use) keeps increasing over time because some cameras see more activities than others, which would eventually result in storage hot spots on some cameras. Meanwhile, with greedy or lazy trajectory aggregation, the storage usage will uniformly increase on each camera over time.</p><p>Next, we focus on the behavior of storage usage for greedy and lazy aggregation. Although the storage usage fluctuates a lot due to continuous data movement caused by trajectory aggregation, we still see that the distribution of storage usage under lazy and greedy trajectory do not differ significantly, which follows our claim that the lazy trajectory can also satisfy the storage upper bound in Section 4. If we compare the red line and green line, which are respectively camera13's storage usage under lazy and greedy aggregation, we do see that the lazy aggregation leads to more storage use. It is because lazy trajectory aggregation does need to maintain more meta data for the trajectory that is not fully aggregated on one camera, such as edges linking distributed trajectory vertices. Finally, we find that lazy trajectory aggregation's slow data movement can better balance the storage across cameras, as the storage usage pattern of least and most loaded cameras are quite close.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Network</head><p>In this set of evaluations, we look into the network usage of each camera over time. Similar to storage usage variation, the network usage will be affected by the road network, the traffic flow and the trajectory aggregation. However, the road network and actual traffic flow are accountable for network usage mainly due to the dependence of message flow patterns between cameras (forward propagation/backward) which is negligible over time compared to the data movement caused by trajectory aggregation. So we are going to focus on the difference in network usage between greedy and lazy trajectory aggregation. 9.2.1 Greedy/lazy aggregation vs. network usage. In <ref type="figure" target="#fig_12">Figure 9</ref>, we pick two cameras with maximum and minimums total network usage during the experiment from greedy and lazy trajectory aggregation, and we plot their network usage in kbytes/second over the whole simulation. As we can see the greedy aggregation will cause the network usage to increase continuously over time because of the growing size of the trajectory, while on the other hand, the lazy aggregation almost has a constant network usage over time. In addition, we observe that the maximum and minimum network usage for cameras under lazy aggregation are roughly the same. This result shows that all cameras place roughly the same load on the network infrastructure with lazy aggregation, which is because each lazy aggregation is composed of fixed number of trajectory records 10 due to the increased activity in the camera's region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Latency</head><p>In this subsection, we are exploring the factors that affect the latency of processing each detected vehicle. This latency is defined as the time interval between receiving a "detection notification" and completion of the re-identification and writing the trajectory into the key-value store. Since this latency is unaffected by the choice of lazy Vs. greedy aggregation strategy, we show the results only for the lazy aggregation. 9.3.1 Forward/Backward vs. latency. A detected object can be processed either through forward propagation or backward propagation. <ref type="figure" target="#fig_0">Figure 10</ref> shows the latency difference between these two strategies. We can see forward propagation always outperforms</p><p>We aggregate 5 trajectory records in the implementation, although 1 also works! backward propagation and the difference increases with the increasing latency in re-identification algorithm which we inject synthetically. This is because the system is optimized towards forward propagation as a fast path trying to get an immediate re-identification result and backward propagation as a slow path to make sure not losing the track of vehicles. 9.3.2 Density of camera and efficiency of re-identification vs. latency. The optimal case for computing the similarity between two detected vehicles is comparing their license plates for which the overhead is negligible and represented by the E h experimental setting. However realistically, the re-identification algorithm may use some appearance-base similarity as its basis for comparison (e.g., the L 2 distance between two large signature vectors), We use E m and E l to simulate this extra latency. Similarly, we might not be able to afford cameras at each road intersection, and we use C m and C l to simulate the sparse camera distribution. From <ref type="figure" target="#fig_0">Figure 11</ref>, we can see that the extra latency in comparing the signature dominates the latency of processing each detected object. There is a clear latency increase between experiments under E h to E m and E m to E l . On the other hand, the density of cameras has much less influence, which is further declining when we have less extra latency. For example, we can see under E m = 100ms, C m takes 2.15x time to process a vehicle detection than C h . However, under E h = 0ms, the camera density does not affect the result. All these experimental results demonstrate that an efficient re-identification algorithm is critical for the smart camera surveillance's real-time property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSION</head><p>STTR is a distributed smart camera-based system for registering the trajectories of all the vehicles all the time built assuming a geo-distributed Fog infrastructure. The key insight is to focus on the activities of vehicles in the vicinity of a camera rather than the time dimension to bound the storage requirement for storing the aggregated trajectories of the vehicles. The other insight is to judiciously communicate the vehicle detections at a given camera (forwards and backwards) exploiting the deployment knowledge of the cameras on the roadways, reducing the latency for detection and re-identification tasks, and bounding the inter-node communication requirements. We have built a toolkit on top of SUMO to generate traffic flows and detectors, simulate vehicle movements, and the corresponding camera streams. We implement and evaluate STTR with the above tool kit and Maxinet <ref type="bibr" target="#b22">[24]</ref> on Microsoft Azure to experimentally verify the theoretical assertions about finite storage space requirement for activity-based space-time tracking of vehicles. There are two areas of future work. Our current system focuses purely on generating the space-time tracks with low latency and storing them with finite storage space on the Fog nodes. An immediate future work is creating efficient spatial and temporal index structures for answering queries on the stored trajectories. Another avenue for future work is increasing the confidence in the accuracy of the space-time tracks from the systems side. While the detection and re-identification algorithms are the forte of domain experts (computer vision), there are opportunities from the system side to bound errors in detection and re-identification by adopting a probabilistic approach to trajectory generation and maintenance. We are also planning to work with the campus police department to perform in situ studies using STTR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Conceptual Picture: This figure shows an example road network and camera surveillance system, where c i represent cameras, f i represent Fog nodes and orange lines represent bidirectional network communication between Fog nodes. Multiple vehicles are moving in this region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 4 . 1 .</head><label>41</label><figDesc>For vehicle x, if we have ∃d (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, the activity region of c 5 is the 4 roads towards c 2 , c 4 , c 6 and c 9 . In reality, camera's activity region can be more diverse including roads, garage, plaza, estate. But what matters is the size of activity region and whether it is finite. With reference to Figure 1, there is a practical upper bound for the size of the activity regions owned by cameras c 2 , c 3 , c 5 , c 6 , c 9 , c 10 ; while it is unbounded for the cameras c 1 , c 4 , c 8 , c 7 , c 11 , since these cameras are at the periphery of a geographical region of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Definition 4 . 3 .</head><label>43</label><figDesc>For a given geographical region and camera surveillance system, cameras that have a finite activity region are regarded as interior cameras, while cameras that have an infinite activity region are regarded as boundary cameras, and camera i's activity region is shortened as size (c i ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>STTR System Architecture. The red rectangle represents the application instances running on each camera. The orange lines represent the fog-fog or fog-cloud network communication. The green lines represent the flow of processing one vehicle's detected object from camera stream to trajectory storage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7. 1 . 1</head><label>11</label><figDesc>Forward candidate pool flush. Consider a vehicle stream v 1 v 2 . . . v 2n from camera c a to c b , where the odd sequence v 1 v 3 . . . v 2n−1 stops in a plaza in the middle and the even sequence v 2 v 4 . . . v 2n directly heads to c b . However, by forward propagation, the detected objects of the odd sequence, which is d 1 d 3 . . . d 2n−1 , will stay in the f orward (c b ). So for vehicle v 2k from the even sequence, the re-identification on c b has to go through first k detected objects in f orward (c b ), and this phenomenon will accumulate over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>This figure shows the map used for traffic flow simulation and 25 cameras' deployment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>This figure shows the network topology we build using MaxiNet. Purple lines represent the network communication. Red dashed square represents one fog node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>This figure shows the storage usage and number of activities accumulated on each camera at the end of the experiment. C h , C m and C l are three different camera sets we choose above. This experiment is run under E h and greedy trajectory aggregation.affects the system, and synthetically inject latency to emulate varying efficiency of re-identification<ref type="bibr" target="#b8">9</ref> to see how efficiency of matching affects the system. Specifically, we have three camera sets:C h = {all cameras}, C m = {c 1 , c 2 , c 3 , c 4 , c 5 , c 6 , c 14 , c 15 , c 16 , c 17 , c 18 , c 19 , c 21 , c 22 , c 23 , c 24 , c 25 } and c l = {c 1 , c 3 , c 6 , c 15 , c 18 , c 21 , c 25 }.And we have three latency settings: E h = 0ms, E m = 100ms and E l = 250ms, used when comparing the object signatures of the detected vehicle with the candidate pool during the forward and backward processing. The performance will be best for the experiment that uses all the cameras with negligible cost for re-identification (C h , E h ) to use as the gold standard for all the other experimental settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>DEBS ' 18 ,</head><label>18</label><figDesc>June 25-29, 2018, Hamilton, New Zealand Zhuangdi Xu, Harshit Gupta, and Umakishore Ramachandran 50</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Storage use for a representative camera. We use Camera1 as the representative camera since it belongs to all three camera sets to show how density affects the storage usage and the number of activities. The left figure is storage usage over time, while the right figure is the box plot of the number of activities over time. This experiment is run under E h and greedy trajectory aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>,min) camera19(lazy,max) camera13(greedy,min) camera18(greedy,max) camera13(baseline,min) camera8(baseline,max) Lazy Vs. Greedy aggregation. Storage usage under greedy and lazy trajectory aggregation strategies are shown for representative cameras that display maximum and minimum storage use. A baseline with no aggregation is also plotted as yellow and purple line. The experiment is run under (C h , E h ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Network Usage for Greedy/Lazy aggregation. Results for representative cameras are shown that have the maximum and minimum network usage. The experiment is conducted under (C h , E h ) setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Latency for Forward/Backward Processing. The left subfigure shows the average latency for processing a detected object by forward/backward propagation. The green and yellow lines are under (C h , E m ) setting, while the red and blue lines are under (C h , E h ) setting. The right subfigure confirms that there is no latency difference between greedy and lazy aggregation. The x axis is the camera numbers 1 to 25 shown in the deployment Figure 4. (C_h, E_h) (C_m, E_h) (C_l, E_h) (C_h, E_m)(C_m, E_m) (C_l, E_m) (C_h, E_l) Latency under different experimental configurations. This figure summarizes the average latency for processing a detected object under different experimental settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notation Table ci \f i Camera i and its corresponding fog node. They are used interchangeably.</figDesc><table><row><cell>v x v x ∈ c i |[t r , t p ] Vehicle x was active under camera i during time Vehicle x. interval [t r , t p ]. v x ∈ c i Vehicle x is active under camera i now. size (c i ) Size of camera i's activity region. s x The signature of vehicle x, such as license plate number or a vector of features returned by the vehicle detection algorithm. d x The detected object of vehicle x, which usually includes the signature and other helpful infor-mation.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Vehicle configuration</figDesc><table><row><cell>type passenger passenger passenger bus</cell><cell>length accel decel[25] sigma probability 5 2.6 4.5 1.0 0.3 5 2.6 4.5 0.5 0.3 2.6 4.5 0.2 0.3 1.2 2.5 0.1 0.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Time interval is omitted for simplicity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was funded in part by an NSF Award (NSF-CPS-1446801) and a grant from Microsoft Corp. We thank members of Georgia Tech's Embedded Pervasive Lab and the anonymous reviews for helping to improve the presentation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Internet of surveillance: a cloud supported large-scale wireless surveillance system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Mohammad A Alsmirat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Jararweh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obaidat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Brij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Supercomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object reacquisition and tracking in large-scale smart camera networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Arth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed Smart Cameras, 2007. ICDSC&apos;07. First ACM/IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="156" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fog computing and its role in the internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flavio</forename><surname>Bonomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolfo</forename><surname>Milito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sateesh</forename><surname>Addepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first edition of the MCC workshop on Mobile cloud computing</title>
		<meeting>the first edition of the MCC workshop on Mobile cloud computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Spatio-temporal data reduction with deterministic error bounds. The VLDB Journal-The International Journal on Very Large Data Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouri</forename><surname>Hu Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goce</forename><surname>Wolfson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trajcevski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="211" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Heterogeneous information fusion and visualization for a large-scale intelligent video surveillance system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Tang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cai-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="593" to="604" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dukemtmc4reid: A large-scale multi-camera person re-identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengran</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavia</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vassilis J Tsotras, and Dimitrios Gunopulos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Hadjieleftheriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kollios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="164" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Indexing spatiotemporal archives</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Smart video surveillance: exploring the concept of multiscale spatiotemporal tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Merkl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharath</forename><surname>Pankanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="38" to="51" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Smart surveillance: applications, technologies and implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharat</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">and Fourth Pacific Rim Conference on Multimedia. Proceedings of the 2003 Joint Conference of the Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1133" to="1138" />
		</imprint>
	</monogr>
	<note>Information</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Herman</surname></persName>
		</author>
		<ptr target="https://frenchfries.net/paul/tcpstat/" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mobile fog: A programming model for large-scale applications on the internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirak</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lillethun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umakishore</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beate</forename><surname>Ottenwälder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Koldehofe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second ACM SIGCOMM workshop on Mobile cloud computing</title>
		<meeting>the second ACM SIGCOMM workshop on Mobile cloud computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Trajectory Ensemble: Multiple Persons Consensus Tracking across Non-overlapping Multiple Cameras over Randomly Dropped Camera Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutomo</forename><surname>Kawanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Deguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="56" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recent Development and Applications of SUMO -Simulation of Urban MObility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Krajzewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Erdmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Behrisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Bieker</surname></persName>
		</author>
		<ptr target="http://elib.dlr.de/80483/" />
	</analytic>
	<monogr>
		<title level="j">International Journal On Advances in Systems and Measurements</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="128" to="138" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable processing of trajectory-based queries in space-partitioned moving objects databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Dürr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Rothermel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGSPATIAL international conference on Advances in geographic information systems</title>
		<meeting>the 16th ACM SIGSPATIAL international conference on Advances in geographic information systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient real-time trajectory tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Dürr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Rothermel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal-The International Journal on Very Large Data Bases</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="671" to="694" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale vehicle re-identification in urban surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyuan</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatiotemporal compression techniques for moving point objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirvana</forename><surname>Meratnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Extending Database Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="765" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccauley</surname></persName>
		</author>
		<title level="m">CREW FATIGUE AND PERFORMANCE ON US COAST GUARD CUTTERS</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Indexing spatio-temporal trajectories with efficient polynomial approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chinya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravishankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="663" to="678" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ibm smart surveillance system (s3): a open and extensible framework for event based surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiao-Fe</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal Based Surveillance, 2005. AVSS 2005. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="318" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Intelligent multi-camera video surveillance: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">What&apos;s the life expectancy of my car?</title>
		<ptr target="http://www.nbcnews.com/id/12040753/ns/business-consumer_news/t/whats-life-expectancy-my-car/" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maxinet: Distributed emulation of softwaredefined networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Wette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Schwabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wallaschek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Networking Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Mohammad Hassan Zahraee, and Holger Karl</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumo Wiki</surname></persName>
		</author>
		<ptr target="http://sumo.dlr.de/wiki/Tutorials/SUMOlympics" />
		<title level="m">SUMO vehicle types</title>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Track-clustering Error Evaluation for Track-based Multi-Camera Tracking System Employing Human Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Ting</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Kuang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Yi</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
