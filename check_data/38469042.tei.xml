<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Same-different problems strain convolutional neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-25">25 May 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Ricci</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkyung</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
						</author>
						<title level="a" type="main">Same-different problems strain convolutional neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-25">25 May 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1802.03390v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Relations</term>
					<term>Convolutional Neural Networks</term>
					<term>Deep Learning</term>
					<term>Visual Attention</term>
					<term>Perceptual Grouping</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The robust and efficient recognition of visual relations in images is a hallmark of biological vision. We argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible, as when intraclass variability exceeds network capacity. Motivated by the comparable success of biological vision, we argue that feedback mechanisms including attention and perceptual grouping may be the key computational components underlying abstract visual reasoning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Consider the images in <ref type="figure" target="#fig_1">Fig. 1</ref>. The image on the left was correctly classified as a flute by a deep convolutional neural network (CNN; <ref type="bibr" target="#b10">He et al., 2015)</ref>. This is quite a remarkable feat for such a complicated image. After the network was trained on millions of photographs, this and many other images were accurately categorized into one thousand natural object categories, surpassing, for the first time, the accuracy of a human observer on the ImageNet classification challenge. Now, consider the image in the middle. On its face, it is quite simple compared to the image on the left. It is just a binary image containing two curves. Further, it has a rather distinguishing property, at least to the human eye: both curves are the same. The relation between the two items in this simple scene is rather intuitive and immediately obvious to a human observer. Yet, the CNN failed to learn this relation even after seeing millions of training examples.</p><p>Why is it that a CNN can accurately detect the flute while struggling to recognize the simple relation depicted in the middle panel of <ref type="figure" target="#fig_1">Fig. 1</ref>? That such task is extremely difficult for contemporary computer vision algorithms like CNNs, is known <ref type="bibr" target="#b3">(Fleuret et al., 2011;</ref><ref type="bibr" target="#b9">Gülçehre &amp; Bengio, 2016;</ref><ref type="bibr" target="#b1">Ellis et al., 2015;</ref><ref type="bibr" target="#b22">Stabinger et al., 2016)</ref>. However, these results, which often relied on a single architecture, were not entirely conclusive: does the inability of CNNs to solve various visual-relation problems reflect a poor choice of network hyperparameters or rather a systematic failure of the entire class of models? To our knowledge, there has been no systematic exploration of the limits of contemporary machine learning  The image in the left panel can be classified confidently as containing a flute by modern vision algorithms. However, these same algorithms struggle to learn the concept of "sameness" as exemplified by the image with the two curves shown in the middle panel. The image in the right panel panel depicts a spatial relation: three objects arranged in a line with the largest in the middle. Middle and are right images are from SVRT <ref type="bibr" target="#b3">(Fleuret et al., 2011)</ref>. algorithms on relational reasoning problems.</p><p>In this study, we will probe the limits of CNNs on visualrelation tasks. In Experiment 1, we perform a systematic performance analysis of CNN architectures on each of the twenty-three synthetic visual reasoning test (SVRT) problems, which reveals a dichotomy of visual-relation tasks: hard same-different problems vs. easy spatial-relation problems. In Experiment 2, we describe a novel, controlled, visualrelation challenge which convincingly shows that CNNs solve same-different tasks via rote memorization. With these experiments, we hope to motivate the computer vision community to reconsider existing visual question answering challenges and turn to cognitive science and neuroscience for inspiration in the design of visual reasoning architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 1: SVRT</head><p>The synthetic visual reasoning test (SVRT) is a collection of twenty-three binary classification problems in which opposing classes differ based on whether their stimuli obey an abstract rule <ref type="bibr" target="#b3">(Fleuret et al., 2011)</ref>. For example, in problem number 1, positive examples feature two items which are the same up to translation <ref type="figure" target="#fig_1">(Fig. 1, middle panel)</ref>, whereas negative examples do not. In problem 9, positive examples have three items, the largest of which is in between the two smaller ones <ref type="figure" target="#fig_1">(Fig. 1, right panel)</ref>. All stimuli depict simple, closed, black curves on a white background. Methods. We tested nine different CNNs of three different depths (2, 4 and 6 convolutional layers) and with three different convolutional filter sizes (2×2, 4×4 and 6×6) in the first layer. This initial receptive field size effectively determines the size of receptive fields throughout the network. The number of filters in the first layer was 6, 12 or 18, respectively, for <ref type="figure">Figure 2</ref>: SVRT results. Multiple CNNs were trained on each of the twenty-three SVRT problems. Shown are the ranked accuracies of the best-performing network for each problem. The x-axis shows the problem ID. CNNs were found to produce uniformly lower accuracies on same-different problems (red bars) than on spatial-relation problems (blue bars). The purple bar represents a problem which required detecting both a same-different relation and a spatial relation. each choice of initial receptive field size. In the other convolutional layers, filter size was fixed at 2×2 with the number of filters doubling every layer. All convolutional layers had strides of 1 and used ReLU activations. Pooling layers were placed after every convolutional layer, with pooling kernels of size 3×3 and strides of 2. On top of the retinotopic layers, all nine CNNs had three fully connected layers with 1,024 hidden units in each layer, followed by a 2-dimensional classification layer. All CNNs were trained on all problems. Network parameters were initialized using Xavier initialization <ref type="bibr" target="#b8">(Glorot &amp; Bengio, 2010)</ref> and were trained using the Adaptive Moment Estimation (Adam) optimizer <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2015)</ref> with base learning rate of η = 10 −4 . All experiments were run using TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>.</p><p>We obtained the accuracy from the best network for each problem individually. Then, we organized the results into a bar plot, sorted the problems by accuracy and colored the bars red or blue according to the SVRT problem descriptions in <ref type="bibr" target="#b3">(Fleuret et al., 2011)</ref>. Problems whose descriptions had words like "same" or "identical" were colored red. These Same-Different (SD) problems had items that are congruent up to some transformation (e.g., middle panel, <ref type="figure" target="#fig_1">Fig. 1</ref>). Spatial-Relation (SR) problems, whose descriptions have phrases like "left of", "next to" or "touching," were colored blue (e.g., right panel, <ref type="figure" target="#fig_1">Fig. 1</ref>).</p><p>Results. The resulting dichotomy across the SVRT problems is striking <ref type="figure">(Fig. 2)</ref>. CNNs fare uniformly worse on SD problems than they do on SR problems. Many SR problems were learned satisfactorily, whereas some SD problems (e.g., problems 20 and 7) resulted in accuracy not substantially above chance. From this analysis, it appears as if SD tasks pose a particularly difficult challenge to CNNs. This result matches earlier evidence for a visual-relation dichotomy hypothesized by <ref type="bibr" target="#b22">Stabinger et al. (2016)</ref>. Additionally, our search revealed that SR problems are equally well-learned across all net- work configurations, with less than 10% difference in final accuracy between the worst case and the best case. On the other hand, larger networks yielded significantly higher accuracy than smaller ones on SD problems, suggesting that SD problems are more capacity-sensitive than SR problems. Experiment 1 corroborates earlier studies <ref type="bibr" target="#b3">(Fleuret et al., 2011;</ref><ref type="bibr" target="#b9">Gülçehre &amp; Bengio, 2016;</ref><ref type="bibr" target="#b1">Ellis et al., 2015;</ref><ref type="bibr" target="#b19">Santoro et al., 2017)</ref> which found that CNNs perform badly on many visualrelation problems and additionally suggests that low performance cannot be simply attributed to a poor choice of hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment 2: PSVRT</head><p>Though useful for surveying many types of relations, the SVRT challenge has two important limitations. First, different problems have different visual structure; e.g., problem 1 requires that an image have two items <ref type="figure" target="#fig_1">(Fig. 1, middle)</ref>, while problem 9 requires that an image have three <ref type="figure" target="#fig_1">(Fig. 1, right)</ref>. Therefore, image features, not abstract relational rules, might make some problems harder than others. Second, the ad hoc procedure used to generate simple, closed curves as items in SVRT prevents quantification of image variability and its effect on task difficulty. As a result, even within a single problem in SVRT, it is unclear whether its difficulty is inherent to the classification rule itself or rather the choice of image generation parameters unrelated to the rule.</p><p>To address these limitations, we constructed a new visualrelation benchmark consisting of two idealized problems <ref type="figure" target="#fig_2">(Fig. 3</ref>) from the dichotomy that emerged from Experiment 1: Spatial Relations (SR) and Same-Different (SD). Critically, both problems used exactly the same images, but with different labels. Further, we parameterized the dataset so that we could systematically control the size of scene items, the number of scene items, and the size of the whole image. Items were binary bit patterns placed on a blank background.</p><p>For each configuration of image parameters, we trained a new instance of a single CNN architecture and measured the ease with which it fit the data. Our goal was to examine how hard it is for a CNN architecture to learn relations for visually different but conceptually equivalent problems. If CNNs can truly learn the "rule" underlying these problems, then one would expect the models to learn all problems with more-orless equal ease. However, if the CNNs only memorize the distinguishing features of the two image classes, then learning should be affected by the variability of the example images in each category. For example, when image size and items size are large, there are simply more possible samples, which might put a strain on the representational capacity of a CNN trying to learn by rote memorization.</p><p>Methods. Our image generator uses three parameters to control image variability: the size (m) of each bit pattern or item, the size (n) of the input image and the number (k) of items in an image. Our parametric construction allows a dissociation between two possible factors that may affect problem difficulty: classification rules vs. image variability. To highlight the parametric nature of the images, we call this new challenge the parametric SVRT or PSVRT.</p><p>The image generator is designed such that each image can be used to pose both problems by simply labeling it according to different rules <ref type="figure" target="#fig_2">(Fig. 3)</ref>. In SR, an image is classified according to whether scene items are arranged horizontally or vertically as measured by the orientation of the line joining their centers (with a 45 • threshold). In SD, an image is classified according to whether or not it contains at least two identical items. When k ≥ 3, the SR category label is determined according to whether the average orientation of the displacements between all pairs of items is greater than or equal to 45 • . Each image can be labeled according to either the SR or SD rules, so we can ensure the image distribution is identical between the two problem types.</p><p>We trained the same CNN repeatedly from scratch over multiple subsets of the data in order to see if learnability depends on the dataset's image parameters. Training accuracy was sampled at regular intervals and samples were averaged across the length of a training run as well as over multiple trials for each condition, yielding a scalar measure of learnability called "mean area under the learning curve" (mean ALC). ALC is high when accuracy increases earlier and more rapidly throughout the course of training and/or when it converges to a higher final accuracy by the end of training.</p><p>First, we found a baseline architecture which could easily learn both same-different and spatial-relation PSVRT problems for one parameter configuration (item size m = 4, image size n = 60 and item number k = 2). Then, for a range of combinations of item size, image size and number of items, we trained an instance of this architecture from scratch.</p><p>The baseline CNN we used in this experiment had four convolutional layers. The first layer had 8 filters with a 4×4 receptive field size. In the rest of convolutional layers, filter size was fixed at 2×2 with the number of filters in each layer doubling from the immediately preceding layer. All convolutional layers had ReLU activations with strides of 1. Pooling layers were placed after every convolutional layer, with pool-ing kernels of size 3×3 and strides of 2. On top of retinotopic layers were three fully connected layers with 256 hidden units each, followed by a 2-dimensional classification layer. We initialized all parameters with the Xavier method, optimized the network with Adam with base rate η = 10 −4 and ran all experiments in Tensorflow.</p><p>To understand the effect of network size on learnability, we also used two control networks in this experiment: (1) a "wide" control that had the same depth as the baseline but twice as many filters in the convolutional layers and four times as many hidden units in the fully connected layers and (2) and a "deep" control which had twice as many convolutional layers as the baseline, by adding a convolutional layer of filter size 2×2 after each existing convolutional layer. Each extra convolutional layer had the same number of filters as the immediately preceding convolutional layer.</p><p>We separately varied the three image parameters to examine their effects on learnability. This resulted in three subexperiments (n was varied between 30 and 180 while m and k were fixed at 4 and 2, respectively; m was varied between 3 and 7, while n and k were fixed at 60 and 2, respectively; k was varied between 2 and 6 while n and m were fixed at 60 and 4, respectively). The baseline CNN was trained from scratch in each condition with 20 million training images and a batch size of 50.</p><p>Results. In all cases where learning occurred, training accuracy eventually jumped from chance-level and gradually plateaued. In other cases, accuracy remained at chance throughout a training session and the ALC was 0.5. Within a single condition, the CNN often only learned for a fraction of 10 randomly initialized trials. This led us to use two different quantities for describing a model's performance: (1) mean ALC obtained from learned trials (in which accuracy crossed 55%) and (2) the number of trials in which the learning event never took place (non-learned). Note that these two quantities are independent, computed from two complementary subsets of 10 trials.</p><p>In all conditions, we found a strong dichotomy between SD and SR conditions. In SR, across all image parameters and in all trials, the model immediately learned at the start of training and quickly approached 100% accuracy, producing consistently high and flat mean ALC curves <ref type="figure">(Fig. 4, blue dotted lines)</ref>. In SD, however, we found that the overall ALC was significantly lower than SR <ref type="figure">(Fig. 4, red dotted lines)</ref>. We also identified two ways in which image variability affects learnability. First, among the trials in which learning occurred, the final accuracy achieved by the CNN decreased as image size (n) and number of items (k) increased. This caused ALC to decrease from around 0.95 to 0.8. Second, increasing n also decreased the chance of learning altogether, with more than half of the trials failing to escape chance level when image size was greater than 60 ( <ref type="figure">Fig. 4, gray bars)</ref>. In contrast, increasing item size never strained CNN performance. Similar to SR, learnability, both in terms of the proportion of successful trials as well as final accuracy, did not change significantly over the range of item sizes.  <ref type="figure">Figure 4</ref>: Mean area under the learning curve (ALC) over PSVRT image parameters. ALC is the normalized area under a training accuracy curve over 20 million images. Colored dots are the mean ALCs for learned trials (in which validation accuracy exceeded 55%) out of 10 randomly initialized trials. Shaded regions around the colored dots indicate the intervals between the maximum and the minimum ALC among learned trials. Gray bars denote the number of non-learned trials, out of 10 trials. Three model-task combinations (CNN on SR (blue), CNN on SD (red), wide CNN control on SD (violet) and deep CNN control on SD (brown)) are plotted, and each combination is shown for three image parameters: item size, image size and number of items.</p><p>The fact that straining is only observed in SD, and not in SR and that it is only observed along some of the image parameters, n and k, suggests that straining is not simply a direct outcome of an increase in image variability. Using a CNN with more than twice the number of kernels <ref type="figure">(Fig. 4</ref>, purple dotted lines) or with twice as many convolutional lay-ers <ref type="figure">(Fig. 4, brown dotted lines)</ref> as the control did not qualitatively change the trend observed in the baseline model. Although increasing network size did result in improved learned accuracy in general, it also made learning less likely, yielding more non-learned trials than the baseline CNN.</p><p>We also rule out the possibility of the loss of spatial acuity from pooling or subsampling operations as a possible cause of straining. Our CNNs achieved the best overall accuracy when image size was smallest. If the loss of spatial acuity was the source of straining, increasing image size should have improved the network's performance instead of hurting it because items would have tended to be placed farther apart from each other. Moreover, in other experiments <ref type="bibr">(Kim et al.,</ref> in press), we found that networks with identical spatial acuity exhibited no straining as long as items were segregated into different channels.</p><p>The weak effects of item size and item number shed light on the computational strategy used by CNNs to solve SD. We hypothesize that CNNs learn "subtraction templates", filters with one positive region and one negative region (like a Haar or Gabor wavelet), in order to detect the similarity between two image regions. A different subtraction template is required for each relative arrangement of items, since each item must lie in one of the template's two regions. When identical items lie in these opposing regions, they are subtracted by the synaptic weights. This difference is then used to choose the appropriate same/different label. This strategy does not require memorizing specific items, so increasing item size (and therefore total number of possible items) should not make the task appreciably harder. Further, a single subtraction template can be used even in scenes with more than two items, since images are classified as "same" when they have at least two identical items. So, any straining effect from item number should be negligible as well. Instead, the principal straining effect with this strategy should arise from image size, which exponentially increases the possible number arrangements of items.</p><p>Taken together, these results suggest that, when CNNs learn a PSVRT problem, they are simply building a feature set tailored to the relative positional arrangements of items in a particular data set, instead of learning the abstract "rule" per se.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Our results indicate that visual-relation problems can quickly exceed the representational capacity of feedforward networks. While learning templates for individual objects appears to be tractable for today's deep networks, learning templates for arrangements of objects becomes rapidly intractable because of the combinatorial explosion in the requisite number of features to be stored. That stimuli with a combinatorial structure are difficult to represent with feedforward networks has been long acknowledged by cognitive scientists <ref type="bibr" target="#b4">(Fodor &amp; Pylyshyn, 1988)</ref>.</p><p>Compared to the feedforward networks in this study, biological visual systems excel at detecting relations. <ref type="bibr" target="#b3">Fleuret et al. (2011)</ref> found that humans can learn rather complicated visual rules and generalize them to new instances from just a few SVRT training examples. Their participants could learn the rule underlying the hardest SVRT problem for CNNs in our Experiment 1, problem 20, from an average of about 6 examples. Problem 20 is rather complicated, involving two shapes such that "one shape can be obtained from the other by reflection around the perpendicular bisector of the line joining their centers." In contrast, the best performing network for this problem could not get significantly above chance after one million training examples.</p><p>Visual reasoning ability is not just found in humans. Birds and primates can be trained to recognize same-different relations and then transfer this knowledge to novel objects <ref type="bibr" target="#b25">(Wright &amp; Katz, 2006)</ref>. A striking example of same-different learning in animals comes from <ref type="bibr" target="#b15">Martinho III &amp; Kacelnik (2016)</ref> who showed that newborn ducklings can learn the abstract concept of sameness from a single example. In contrast, we have found in follow-up work that state-of-the-art neural networks demonstrated no ability to transfer the concept of same-different to novel objects even after hundreds of thousands of training examples <ref type="bibr">(Kim et al., in press)</ref>.</p><p>It is relatively well accepted that, despite the widespread presence of feedback connections in our visual cortex, certain visual recognition tasks, including the detection of natural object categories, are possible in the near absence of cortical feedback -based primarily on a single feedforward sweep of activity through our visual cortex <ref type="bibr" target="#b20">(Serre, 2016)</ref>. However, psychophysical evidence suggests that this feedforward sweep is too spatially coarse to localize objects even when they can be recognized <ref type="bibr" target="#b2">(Evans &amp; Treisman, 2005)</ref>. The implication is that object localization in clutter requires attention <ref type="bibr" target="#b26">(Zhang et al., 2011)</ref>. It is difficult to imagine how one could recognize a relation between two objects without spatial information. Indeed, converging evidence <ref type="bibr" target="#b14">(Logan, 1994;</ref><ref type="bibr" target="#b16">Moore et al., 1994;</ref><ref type="bibr" target="#b18">Rosielle et al., 2002;</ref><ref type="bibr" target="#b11">Holcombe et al., 2011;</ref><ref type="bibr" target="#b5">Franconeri et al., 2012;</ref><ref type="bibr" target="#b23">van der Ham et al., 2012)</ref> suggests that the processing of spatial relations between pairs of objects in a cluttered scene requires attention, even when individual items can be detected pre-attentively.</p><p>In follow-up work <ref type="bibr">(Kim et al.,</ref> in press), we argued that perceptual grouping, a mechanism for binding features into discrete objects <ref type="bibr" target="#b17">(Roelfsema, 2006)</ref>, is another key nonfeedforward process supporting visual relation detection. We found that relational networks <ref type="bibr" target="#b19">(Santoro et al., 2017)</ref>, CNN extensions that exhaustively attend to all unbound features in a deep layer, are strained just like CNNs and tend to easily overfit. In contrast, we showed that a network which simulates the effects of perceptual grouping by forcing scene items into separate channels can easily learn our PSVRT tasks without straining. This toy network simulates in a feedforward manner the dynamic sequence of attention shifts between perceptually grouped features believed to underlie visual relation detection <ref type="bibr" target="#b5">(Franconeri et al., 2012)</ref>. These dynamic representations built "on-the-fly" circumvent the combinatorial explosion associated with the storage of synaptic templates for all possible relations, helping to prevent the capacity overload associated with feedforward neural networks.</p><p>Humans can easily detect when two objects are the same up to some transformation <ref type="bibr" target="#b21">(Shepard &amp; Metzler, 1971)</ref> or when objects exist in a given spatial relation <ref type="bibr" target="#b3">(Fleuret et al., 2011;</ref><ref type="bibr" target="#b5">Franconeri et al., 2012)</ref>. More generally, humans can effort-lessly construct an unbounded set of structured descriptions about their visual world <ref type="bibr" target="#b6">(Geman et al., 2015)</ref>. Given the vast superiority of humans over modern computers in their ability to detect visual relations, we see the exploration of attentional and grouping mechanisms as an important next step in our computational understanding of visual reasoning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>To appear in the Proceedings of theAnnual Meeting of the Cognitive Science Society, 2018 † These authors contributed equally to this work. * Brown University, Department of Cognitive, Linguistic and Psychological Sciences. 190 Thayer Street. Providence, RI 029012</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Three images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The PSVRT challenge. Four images show the joint categories of SD and SR problems. An image is Same or Different depending on whether it contains identical (left column) or different (right column) square bit patterns. An image is Horizontal or Vertical depending on the average angular displacement between the items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Mean ALC of learned trials Number of non-learned trials Image parameters Item Size Image Size Number of Items CNN on SR CNN on SD CNN (Wide) on SD CNN (Deep) on SD</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Drs. Drew Linsley and Sven Eberhardt for their advice, along with Dan Shiebler for earlier work. This research was supported by NSF early career award (IIS-1252951) and DARPA young faculty award (YFA N66001-14-1-4037). Additional support was provided by the Center for Computation and Visualization (CCV) at Brown University. This material is based upon work supported by author MR's National Science Foundation Graduate Research  Fellowship under Grant No. 1644760.   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">.</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX conference on operating systems design and implementation</title>
		<meeting>the 12th USENIX conference on operating systems design and implementation<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised Learning by Program Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="973" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perception of objects in natural scenes: is it really attention free?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol. Hum. Percept. Perform</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1476" to="1492" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparing machines and humans on a visual categorization test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dubout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Wampler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yantis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="17621" to="17626" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connectionism and cognitive architecture: A critical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="71" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flexible visual processing of spatial relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Franconeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Scimeca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Helseth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Kahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual Turing test for computer vision systems</title>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3618" to="3623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on artificial intelligence and statistics</title>
		<editor>Y. W. Teh &amp; M. Titterington</editor>
		<meeting>the 13th international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge Matters : Importance of Prior Information for Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of achine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-02" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceiving spatial relations via attentional tracking and shifting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Holcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Linares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaziri-Pashkam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Biol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1135" to="1139" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Not-so-CLEVR: Learning same-different relations strains feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Soc. Interface, Special issue on Understanding images in biological and computer vision</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial attention and the apprehension of spatial relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1015" to="1036" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ducklings imprint on the relational concept of same or different</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Martinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kacelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="issue">6296</biblScope>
			<biblScope unit="page" from="286" to="288" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual attention and the apprehension of spatial relations: The case of depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Elsinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lleras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Exp. Psychol. Hum. Percept. Perform</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1015" to="1036" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cortical algorithms for perceptual grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="203" to="227" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attentional coding of categorical relations in scene perception: evidence from the flicker paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Rosielle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Crabb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychon. Bull. Rev</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="319" to="345" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G T</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Models of visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdiscip. Rev. Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="213" />
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mental Rotation of Three-Dimensional Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">3972</biblScope>
			<biblScope unit="page" from="701" to="703" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">25 years of CNNs: Can we compare to human abstraction capabilities?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stabinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodríguez-Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN, 9887 LNCS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="380" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J M</forename><surname>Van Der Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J A</forename><surname>Duijndam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raemaekers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J A</forename><surname>Van Wezel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oleksiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Postma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Retinotopic mapping of categorical and coordinate spatial relation processing in early visual cortex</title>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mechanisms of same/different concept learning in primates and avians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Katz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="234" to="254" />
		</imprint>
	</monogr>
	<note type="report_type">Behav. Processes</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object decoding with attention in inferior temporal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Bichot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desimone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. U. S. A</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="8850" to="8855" />
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
