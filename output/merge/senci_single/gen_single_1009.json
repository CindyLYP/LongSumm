{"1009": "this paper presents a high dimensional encoded phonetic similarity algorithm for chinese. the encodings are learned from annotated data to separately map initial and phonemes into n-dimensional coordinates. pinyin phonetic similarities are then calculated by aggregating the similarities of initial , final and tone. dimsim demonstrates a 7.5x improvement on mean reciprocal rank over mean phonetic similarity approaches. this paper proposes a supervised learning approach to learn n dimensional encodings for finals and initials where n can be easily extended from one to two or higher dimensions. the learning model derives accurate encodings by jointly considering pinyin linguistic characteristics, such as place of articulation and pronunciation methods, as well as high quality annotated training data sets. we compare dimsim to double metaphone ( dm) , minimum edit distance ( med) and aline demonstrating that dimsim outperforms these algorithms by 7.5x on mean reciprocal rank, 1.4x on precision and 1.5x on recall on a real world dataset. a package release of the implemented algorithm and a constructed dataset of chinese words with phonetic corrections.2 dimsim generates ranked candidate words with similar pronunciation to a seed word. this paper presents a supervised machine learning approach that uses pinyin linguistic characteristics combined with manually labeled data sets of phonetic similarity. the training data sets consist of word pairs that highlight a pair of initials ( or finals), and are used as the context for an annotator-provided phonetic similarity score. the set of initials ( or finals) is then mapped to the n-dimensional encodings by minimizing the difference between the resulting pairwise distances, and the distances obtained from the training data sets. generating similar word pairs phonetically similar word pairs are used to create annotations representing the phonetic similarity of a pair of initials , or finals. chinese has pairs of initials and pairs of finals. manually annotating each pair similarity requires a very large number of examples: assuming ten or twenty word pairs are provided as context for each pair, the task quickly blows up to eighteen thousand annotations. leveraging known pinyin linguistic characteristics can improve the accuracy of our model and reduce the size of the annotation task. the aim of this paper is to generate word pairs whose pinyins only differ in these pairs. for each word pair, the annotators give a label on a point scale representing their agreement. equation equation inverts the labels so that the output can be used as a distance metric ( phonetically similar initials or finals are closer together, and scales the result to more accurately measure phonetic similarities. parameters a and b are set and by default, but we also show that the performance of our method is not sensitive to the parameter settings. the final goal is to map each initial ( or final) to an ndimensional point. based on the structured structured of table 2, we intuit that extending beyond one dimension will yield more accurate encodings. figures and visualize the computed encodings of initials when setting the computed encodings of initials to be and 0.54, representing the inter-annotator agreement. for each word pair, we use equation to calculate the distance with the average value of labels across the annotators. this paper studies the impact of varying the pairwise phonetic distance between a word and a chinese pinyin dictionary (dict) on the accuracy and recall of the generated candidates. a larger th generates more candidates, increasing recall while decreasing precision.3 finally, we output the candidates ranked in ascending order by similarity distance. since downstream applications will only consider a limited number of candidates in practice, we evaluate precision via a manual annotation task on the top-ranked candidates generated by each approach. dm considers word spelling, pronunciation and other miscellaneous characteristics to encode the word into a primary and a secondary code. dm as one of the baselines is known to perform poorly at ranking the candidates since only two codes are used. we therefore use our method to rank the dm-generated candidates, to create a second baseline, dm-rank.4 the third baseline, aline, measures phonetic similarity based on manually coded multi-valued articulatory features weighted by their relative importance with respect to feature salience. dimsim is an improved phonetic similarity algorithm that encodes the phonetic distance between a pair of words as a pair of pairs of pairs of pairs of one- and two-dimensional encodings. there is a plethora of work focusing on phonetic similarities between words and characters these algorithms encode words with similar pronunciation into the same code. these algorithms fail to capture chinese phonetic similarity since pinyin has its own specific characteristics, which do not easily map to english, for determining phonetic similarity. dimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components. using a real world dataset, we demonstrate that dimsim effectively improves mrr by 7.5x, recall by 1.5x and precision by 1.4x over existing approaches. this paper presents dimsim, a learned ndimensional phonetic encoding for chinese along with a phonetic similarity algorithm, which uses the encoding to generate and rank phonetically similar words. dimsim demonstrates a 7.5x improvement on mean reciprocal rank over the state-of-the - art. this paper proposes a new method to learn chinese word encodings based on annotators. the model aims to minimize the sum of the absolute differences between the euclidean distances of component pairs and the average distances obtained from the annotated training data across all pairs for initials ( or finals) c. the parameters a and b are set and by default, but we also show that the performance of our model is not sensitive to the parameter settings. we also incorporate a penalty function, p, for pairs deviating from the manually annotated distance so that more phonetically similar pairs are penalized more highly. we observe that when n 2, the locations of initial coordinates align well with table 2, . in particular, the twelve groups are clustered in a pattern that is defined in section for example, bp, gk, jqx are separated into different clusters. however, while table indicated the basic clusters for the initials, our learned model goes further than table by actually quantifying the inter- and intra-cluster similarities. dimsim is a chinese language counterpart of the named entity translation algorithm soundex ( archives and administration, 2007 ) . the original motivation for this work was to improve the quality of downstream nlp tasks , such as named entity identification, text normalization and spelling correction. these tasks all share a dependency on reliable phonetic similarity as an intermediate step, especially for languages such as chinese where incorrect homophones and synophones abound.", "1000": "###", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1007": "###", "1008": "###", "1010": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1018": "###", "1019": "###", "1020": "###", "1021": "###"}