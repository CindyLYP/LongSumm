<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Twitter Heron: Towards Extensible Streaming Engines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maos</forename><surname>Ng Fu</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Twitter</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashvin</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Twitter</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrilia</forename><surname>Floratou</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Twitter</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Graham</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Twitter</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jorgensen</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Twitter</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Twitter</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neng</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Twitter</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Ramasamy</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Twitter</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Rao</surname></persName>
							<email>sriramra@microsoft.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Twitter</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Twitter</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Twitter Heron: Towards Extensible Streaming Engines</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ICDE.2017.161</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Twitter&apos;s data centers process billions of events per day the instant the data is generated. To achieve real-time performance, Twitter has developed Heron, a streaming engine that provides unparalleled performance at large scale. Heron has been recently open-sourced and thus is now accessible to various other organizations. In this paper, we discuss the challenges we faced when transforming Heron from a system tailored for Twitter&apos;s applications and software stack to a system that efficiently handles applications with diverse characteristics on top of various Big Data platforms. Overcoming these challenges required a careful design of the system using an extensible, modular architecture which provides flexibility to adapt to various environments and applications. Further, we describe the various optimizations that allow us to gain this flexibility without sacrificing performance. Finally, we experimentally show the benefits of Heron&apos;s modular architecture.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Twitter's daily operations rely heavily on real-time processing of billions of events per day. To efficiently process such large data volumes, Twitter has developed and deployed Heron <ref type="bibr" target="#b0">[1]</ref>, a streaming engine tailored for large scale environments that is able to meet Twitter's strict performance requirements. Heron has been shown to outperform Storm <ref type="bibr" target="#b1">[2]</ref>, the first generation streaming engine used at Twitter and at the same time provides better manageability. Heron is now the de facto stream data processing system in Twitter and is used to support various types of applications such as spam detection, real time machine learning and real time analytics, among others.</p><p>Twitter has recently open sourced Heron. As many organizations rely on stream processing for various applications such as IOT and finance among others, Heron has already attracted contributors from multiple institutions, including Microsoft. However, making Heron publicly available poses significant challenges. Heron must now be able to operate on potentially different environments (private/public cloud), various software stacks, and at the same time support applications with diverse requirements without sacrificing performance. For example, Twitter deploys Heron on top of the Aurora scheduler <ref type="bibr" target="#b2">[3]</ref> in its private cloud. However, many organizations already manage a solutions stack based on the YARN scheduler <ref type="bibr" target="#b3">[4]</ref> since it efficiently supports batch processing systems such as Apache Hive <ref type="bibr" target="#b4">[5]</ref>. Co-locating Heron along with these batch processing systems on top of YARN, can reduce the total cost of ownership and also improves maintenance and manageability of the overall software stack. Similarly, depending on the application requirements, one might want to optimize for performance or for deployment cost especially in the context of a cloud environment which employs a pay-as-you-go model. Along the same lines, a data scientist might want to use Python to implement a machine learning model in a streaming system, whereas a financial analyst might prefer to use C++ in order to fully optimize the performance of her application.</p><p>To address these challenges, we carefully designed Heron using a modular architecture. In this approach, each component of the system provides the minimum functionality needed to perform a particular operation. The various modules of the system communicate and provide services to each other through well-specified, communication protocols. An important aspect of Heron is that it allows extensibility of the modules. More specifically, Heron allows the application developer or the system administrator to create a new implementation for a specific Heron module (such as the scheduler, resource manager, etc) and plug it in the system without disrupting the remaining modules or the communication mechanisms between them. The benefit of this approach is that the developer needs to understand and implement only the basic functionality of a particular module, using wellspecified APIs, and does not need to interact with other system components. Another important aspect of this architecture is that different Heron applications can seamlessly operate on the same resources using different module implementations. For example, one application might manage resources optimizing for load balancing while another application might optimize for total cost. Providing such functionality is critical for the adoption of Heron across different organizations.</p><p>To the best of our knowledge, Heron is the first streaming system that adopts and implements a modular architecture. Note that although other systems such as Storm <ref type="bibr" target="#b1">[2]</ref> face similar challenges, they have taken the approach of implementing specialized versions of the system for different software stacks. For example, there are separate repositories for Storm on Mesos <ref type="bibr" target="#b5">[6]</ref>, Storm on YARN <ref type="bibr" target="#b6">[7]</ref>, and Storm on Slider <ref type="bibr" target="#b7">[8]</ref>. Heron, on the other hand, has a more flexible and scalable architecture that allows it to easily integrate with different Big Data platforms, as long as the appropriate module implementations are available. Despite the benefits of general-purpose architectures, such as Heron's modular architecture, a common belief is that specialized solutions tend to outperform generalpurpose ones because they are optimized for particular environments and applications. In this paper, we show that by carefully optimizing core components of the system, Heron's generalpurpose architecture can actually provide better performance than specialized solutions such as Storm.</p><p>The contributions of this paper are the following:</p><p>• We describe Heron's modular architecture and present in detail various important modules of the system. • We compare and contrast Heron's general-purpose architecture to specialized approaches such as Storm <ref type="bibr" target="#b1">[2]</ref> and Spark Streaming <ref type="bibr" target="#b8">[9]</ref>.</p><p>• We experimentally highlight the performance advantages of Heron's general-purpose architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HERON'S GENERAL-PURPOSE ARCHITECTURE</head><p>In this section, we provide a high-level overview of Heron's modular and extensible architecture. Heron's architecture is inspired by that of microkernel-based operating systems. Unlike monolithic kernel-based operating systems, microkernel-based operating systems provide a core set of services each one implementing one basic mechanism such as IPC and scheduling among others. As pointed in <ref type="bibr" target="#b9">[10]</ref>, microkernel-based operating systems have been developed to facilitate the adoption of new device drivers, protocol stacks, and file systems. These components were normally located in the monolithic kernel and as result their adoption required a considerable amount of work. In microkernel-based systems, these components run in user space, outside the kernel. Along the same lines, due to the heterogeneity of today's cloud environments and Big Data platforms, we decided to design Heron using extensible, self-contained modules that operate on top of a kernel which provides the basic functionality needed to build a streaming engine. This architecture facilitates the adoption of Heron to run on different underlying software stacks. Furthermore, it simplifies the detection of performance problems as well as tuning and maintainance of the overall software stack. <ref type="figure">Figure 1</ref> shows the various Heron modules and the interactions between them. As opposed to a monolithic architecture, Heron consists of several modules (in blue) that communicate through basic inter/intra process communication mechanisms <ref type="bibr">(IPC)</ref>. The IPC mechanisms essentially constitute the kernel of the system. Every other Heron module is extensible and easily pluggable to the system, similar to microkernel-based operating systems.</p><p>As described in <ref type="bibr" target="#b0">[1]</ref>, a Heron topology is a directed graph of spouts and bolts. The spouts are sources of input data such as a stream of Tweets, whereas the bolts perform computations on the streams they receive from spouts or other bolts. When a topology is submitted to Heron, the Resource Manager first determines how many containers should be allocated for the topology. The first container runs the Topology Master which is the process responsible for managing the topology throughout its existence. The remaining containers each run a Stream Manager, a Metrics Manager and a set of Heron Instances which are essentially spouts or bolts that run on their own JVM. The Stream Manager is the process responsible for routing tuples among Heron Instances. The Metrics Manager collects several metrics about the status of the processes in a container. As we will discuss later, the Resource Manager determines the allocation of Heron Instances to containers based on some resource management policy. It then passes this information to the Scheduler which is responsible for allocating the required resources from the underlying scheduling framework such as YARN or Aurora. The Scheduler is also responsible for starting all the Heron processes assigned to the container. In the following sections we discuss how the various modules behave when a failure is detected or when a user updates the topology configuration.</p><p>Heron's architecture provides flexibility to the application developers. For example, a Heron Instance can execute user code that can be written in Java or Python. Along the same lines, an application developer can use the YARN implementation of the Scheduler, if she operates on top of a YARN cluster. If her environment changes, she can easily switch to another Scheduler implementation (e.g., Aurora) without having to change her topology or her Heron setup. Similarly, the application developer can pick the Resource Manager implementation that better suits her needs or even implement new policies. The user can configure the Heron modules either at topology submission time through the command line or using special configuration files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GENERAL-PURPOSE VS. SPECIALIZED STREAMING ARCHITECTURES</head><p>Heron's modular architecture provides flexibility to incorporate various cloud environments and Big Data platforms. In that sense, Heron's architecture is a general-purpose architecture as opposed to specialized streaming engines which have tight dependencies between various parts of the system. In this section, we compare Heron's general-purpose architecture with the architectures of Storm <ref type="bibr" target="#b1">[2]</ref> and Spark Streaming <ref type="bibr" target="#b8">[9]</ref>. More specifically, we highlight the major differences between Heron and these two systems focusing on their resource isolation mechanisms in shared environments as well as their data processing and communication layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison with Apache Storm</head><p>Storm is also a large-scale stream processing system but has a significantly different architecture than Heron. Although it provides some extensibility points, like support for different topology specification languages, Storm has a much more rigid architecture than Heron.</p><p>Scheduling and Resource Sharing. Heron is designed with the goal of operating in a cloud environment on top of a scheduling framework such as Aurora or YARN (although it can also run on local mode). As a result, it leverages the resource isolation mechanisms implemented by these frameworks. Storm, on the other hand implements parts of the functionality of the Heron Resource Manager, the Heron Scheduler and the underlying scheduling framework in the same abstraction. More specifically, Storm is responsible for deciding how resources will be allocated to a given topology and how resources will be allocated across different topologies without taking into account the underlying scheduling framework. Apart from the extensibility issues, this approach has several drawbacks. First, it is not clear what is the relationship between Storm's scheduling decisions and that of the underlying scheduling framework if one is part of the software stack. Second, in the absence of a scheduling framework, it becomes very hard to provide resource isolation across different topologies in an environment where resources are shared. Finally, unlike Heron where resources are acquired when the topologies are deployed, the resources for a Storm cluster must be acquired before any topology can be submitted. As a result, the system administrator needs to know a priori how many topologies will run on the cluster and their expected resource requirements in order to preallocate resources accordingly.</p><p>Data Processing Layer. Heron's modular architecture provides resource isolation not only across different topologies (through the underlying scheduling framework) but also among the processes of the same topology. This is because every spout and bolt run as separate Heron Instances and thus do not share the same JVM. This architecture helps isolating performance problems and also recover from failures. Storm, on the other hand, packs multiple spout and bolt tasks into a single executor. Each executor shares the same JVM with other executors. As a result, it is very difficult to provide resource isolation in such an environment.</p><p>Communication Layer. In Heron's modular architecture, a dedicated process named Stream Manager is responsible for all the data transfers among the Heron Instances. Separating this component from the processing units (Heron Instances) makes the system scalable, allows various optimizations at the data transfer level and simplifies the manageability of the system. In Storm, on the other hand, the threads that perform the communication operations and the actual processing tasks share the same JVM <ref type="bibr" target="#b1">[2]</ref>. As a result, it is much harder to isolate performance bottlenecks and thus optimize the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with Spark Streaming</head><p>Spark Streaming <ref type="bibr" target="#b8">[9]</ref> is another popular open-source streaming engine. Its architecture differs from that of Heron and Storm as it uses a separate processing framework (Spark <ref type="bibr" target="#b10">[11]</ref>) to process data streams. Because of its architecture, it operates on small batches of input data and thus it is not suitable for applications with latency needs below a few hundred milliseconds <ref type="bibr" target="#b11">[12]</ref>.</p><p>Scheduling and Resource Sharing. Spark Streaming depends on Spark for extensibility. For example, since Spark can support two different types of schedulers (YARN and Mesos), Spark Streaming is able to operate on top of these frameworks. Apart from streaming applications, Spark is typically also used for batch analytics, machine learning and graph analytics on the same resources. As a result, it is not easy to customize it particularly for streaming applications. Spark runs executor processes for each application submitted. Each executor process can run multiple tasks in different threads. Unlike Heron's Resource Manager, Spark Streaming does not provide a way to specify new policies for assigning tasks to executors.</p><p>Data Processing Layer. It is worth noting that Spark (and, as a result, Spark Streaming) has a similar architecture with Storm that limits the resource isolation guarantees it can provide. As noted above, each executor process can run multiple tasks in different threads. As opposed to Heron, this model does not provide resource isolation among the tasks that are assigned to the same executor. Overall, Heron's modular architecture makes it much easier for the application developer to configure, manage, and extend Heron. Although Storm and Spark Streaming provide some extensibility, their respective architectures contain more tightlycoupled modules and are less flexible than that of Heron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HERON MODULES</head><p>In this section, we present several important Heron modules and describe in detail their functionality. To further highlight the simplicity and extensibility of the Heron modules, we also provide the corresponding APIs. The source code of all the implemented modules can be found in the Heron code repository <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Resource Manager</head><p>The Resource Manager is the module that determines how resources (CPU, memory, disk) are allocated for a particular topology. Note that the Resource Manager is not a long-running Heron process but is invoked on-demand to manage resources when needed. More specifically, it is the component responsible for assigning Heron Instances to containers, namely generating a packing plan. The packing plan is essentially a mapping from containers to a set of Heron Instances and their corresponding resource requirements. The Resource Manager produces the packing plan using a packing algorithm. After the packing plan is generated, it is provided to the Scheduler which is the component responsible for requesting the appropriate resources from an underlying scheduling framework such as YARN, Mesos or Aurora.</p><p>When a topology is submitted for the first time, the Resource Manager generates an initial packing plan which is used to distribute the Heron Instances to a set of available containers. Heron provides the flexibility to the user to adjust the parallelism of the components of a running Heron topology. For example, a user may decide to increase the number of instances of a particular spout or bolt. This functionality is useful when load variations are observed. In such case, adjusting the parallelism of one or more topology components can help meet a performance requirement or better utilize the available resources. When the user invokes a topology scaling command, the Resource Manager adjusts the existing packing plan given the specific user requests.</p><p>The Resource Manager implements the basic functionality of assigning Heron Instances to containers through the following simple APIs: p u b l i c i n t e r f a c e ResourceManager { v o i d i n i t i a l i z e ( C o n f i g u r a t i o n conf , Topology t o p o l o g y ) P a c k i n g P l a n pack ( ) P a c k i n g P l a n r e p a c k ( P a c k i n g P l a n c u r r e n t P l a n , Map p a r a l l e l i s m C h a n g e s )</p><formula xml:id="formula_0">v o i d c l o s e ( ) }</formula><p>The pack method is the core of the Resource Manager. It implements a packing algorithm which generates a packing plan for the particular topology and it is invoked the first time a topology is submitted. The Resource Manager can implement various policies through the pack method. For example, a user who wants to optimize for load balancing can use a simple Round Robin algorithm to assign Heron Instances to containers. A user who wants to reduce the total cost of running a topology in a pay-as-you go environment can choose a Bin Packing algorithm that produces a packing plan with the minimum number of containers for the given topology <ref type="bibr" target="#b13">[14]</ref>. Note that Heron's architecture is flexible enough to incorporate user-defined resource management policies which may require different inputs than the existing algorithms.</p><p>The repack method is invoked during topology scaling operations. More specifically, it adjusts the existing packing plan by taking into account the parallelism changes that the user requested. Various algorithms can be implemented to specify the logic of the repack operation. Heron currently attempts to minimize disruptions to the existing packing plan while still providing load balancing for the newly added instances. It also tries to exploit the available free space of the already provisioned containers. However, the users can implement their own policies depending on the requirements of their particular application.</p><p>It is worth noting that the Resource Manager allows the user to specify different resource management policies for different topologies running on the same cluster. This is an important feature of the general-purpose architecture which differentiates it from specialized solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scheduler</head><p>The Scheduler is the module responsible for interacting with the underlying scheduling framework such as YARN or Aurora and allocate the necessary resources based on the packing plan produced by the Resource Manager.</p><p>The Scheduler can be either stateful or stateless depending on the capabilities of the underlying scheduling framework. A stateful Scheduler regularly communicates with the underlying scheduling framework to monitor the state of the containers of the topology. In case a container has failed, the stateful Scheduler takes the necessary actions to recover from the failure. For example, when Heron operates on top of YARN, the Heron Scheduler monitors the state of the containers by communicating with YARN through the appropriate YARN APIs. When a container failure is detected, the Scheduler invokes the appropriate commands to restart the container and its associated tasks. A stateless Scheduler, on the other hand, is not aware of the state of the containers while the topology is running. More specifically, it relies on the underlying scheduling framework to detect container failures and take the necessary actions to resolve them. For example, the Heron Scheduler is stateless when Aurora is the underlying scheduling framework. In case of a container failure, Aurora invokes the appropriate command to restart the container and its corresponding tasks.</p><p>The Scheduler implements the following APIs: The onSchedule method is invoked when the initial packing plan for the topology is received from the Resource Manager. The method interacts with the underlying scheduling framework to allocate the resources specified in the packing plan. The onKill and onRestart methods are invoked when the Scheduler receives a request to kill or restart a topology, respectively. Finally, the onUpdate method is invoked when a request to update a running topology has been submitted. For example, during topology scaling the Scheduler might need to update the resources allocated to the topology given a new packing plan. In this case, the Scheduler might remove existing containers or request new containers from the underlying scheduling framework.</p><p>It is worth noting that the various underlying scheduling frameworks have different strengths and limitations. For example, YARN can allocate heterogeneous containers whereas Aurora can only allocate homogeneous containers for a given packing plan. Depending on the framework used, the Heron Scheduler determines whether homogeneous or heterogeneous containers should be allocated for the packing plan uncder consideration. This architecture abstracts all the low level details from the Resource Manager which generates a packing plan irrespective of the underlying scheduling framework.</p><p>Heron currently supports several scheduling frameworks. More particularly, it has been tested with Aurora and YARN. The Heron community is currently extending the Scheduler component by implementing the above APIs for various other frameworks such as Mesos <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, Slurm <ref type="bibr" target="#b16">[17]</ref> and Marathon <ref type="bibr">[18]</ref>. Note that since Heron's architecture is able to seamlessly incorporate scheduling frameworks with different capabilities, there is no need to create separate specialized versions of Heron for each new scheduling framework. the State Manager including the topology definition, the packing plan generated by the Resource Manager, the host and port information of all the containers, and the URL of the underlying scheduling framework among others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. State Manager</head><p>The State Manager abstraction allows easy integration with various filesystems and coordination frameworks. Heron provides a State Manager implementation using Apache Zookeeper <ref type="bibr" target="#b17">[19]</ref> for distributed coordination in a cluster environment and also an implementation on the local file system for running locally in a single server. Both implementations currently operate on tree-structured storage where the root of the tree is supplied by the Heron administrator. However, Heron provides the flexibility to extend the State Manager in order to incorporate other state management mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. OPTIMIZING THE COMMUNICATION LAYER</head><p>One of the concerns with general-purpose architectures, such as Heron's modular architecture, is the potential overhead of data transfer between the different modules of the system. As opposed to tighter architectures, such as Storm, where the data transfer and the processing threads share the same JVM, the Stream Manager is a separate process in Heron and thus incurs inter-process instead of intra-process communication.</p><p>In this section, we first discuss how we enable fast data transfers between Heron Instances by applying various optimizations to the Stream Manager module, and as a result, outperform other tightly coupled architectures (such as Storm). We then provide more insights about the Stream Manager by discussing two important configuration parameters that affect the overall performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stream Manager Optimizations</head><p>The Stream Manager is an extensible and pluggable module. It can potentially support different inter-process communication mechanisms (sockets, shared memory, etc) and different data serialization formats (e.g., binary encoding, Protocol Buffers <ref type="bibr" target="#b18">[20]</ref>). The module is implemented in C++ because it provides tighter control on the memory and cpu footprint and to avoid the overhead of copying data between the native heap and the JVM heap.</p><p>The Stream Manager implementation relies on two major techniques to achieve high performance. First, it avoids data copies as much as possible. Currently, the Stream Manager implementation uses Protocol Buffers to serialize and exchange data between different processes. Our implementation allows reusability of the Protocol Buffer objects by using memory pools to store dedicated objects and thus avoid the expensive new/delete operations. Second, the Stream Manager performs in-place updates of Protocol Buffer objects and uses lazy deserialization whenever possible. For example, when one Stream Manager process receives a message from another Stream Manager process, it parses only the destination field that determines the particular Heron Instance that must receive the tuple. The tuple is not deserialized but is forwarded as a serialized byte array to the appropriate Heron Instance. In Section VI, we experimentally show the impact of these two optimizations in Heron's overall performance both in terms of throughput and latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Tuning</head><p>Although the current Stream Manager implementation can provide very good performance out-of-the-box due to the optimizations that we described above, Heron provides the flexibility to the user to further tune the communication layer. We now discuss two important configuration parameters that can be tuned by the user to actively control the data transfer rates in case this might be beneficial for a particular workload. As part of future work, we plan to automate the process of configuring the values for these parameters based on real-time observations of the workload performance.</p><p>The first parameter called max spout pending, represents the maximum number of tuples that can be pending on a spout task at any given time. A pending tuple is one that has been emitted from a spout but has not been acknowledged or failed yet. By configuring this parameter the user can determine how many tuples can be in flight at any point of time and thus The second parameter called cache drain frequency, is reated to the Stream Manager tuple cache. The tuple cache is a cache that temporarily stores the incoming and outcoming data tuples before routing them to the appropriate Heron Instances. The cache stores tuples in batches along with the Heron Instance id that is the recipient of the batch. The cache drain frequency determines the frequency in milliseconds to drain the tuple cache. As we show in Section VI, the value of this parameter can affect the end-to-end performance both in terms of latency and throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL EVALUATION</head><p>In this section, we present experiments that demonstrate the benefits of Heron's modular architecture. In all our experiments we use Heron version 0.14.4 and Storm version 1.0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparing Heron with Storm</head><p>In this experiment, we compare Heron's general-purpose architecture with Storm on top of the YARN scheduler. As in previous work <ref type="bibr" target="#b0">[1]</ref>, we use the Word Count topology since it is a good measure of the overhead introduced by either Storm or Heron. This is because its spouts and bolts do not perform significant work. In this topology, the spout picks a word at random from a set of 450K Engish words and emits it. Hence spouts are extremely fast, if left unrestricted. The spouts use hash partitioning to distribute the words to the bolts which in turn count the number of times each word was encountered.</p><p>The experiments were performed on Microsoft HDInsight <ref type="bibr" target="#b19">[21]</ref>. Each machine has one 8-core Intel Xeon E5-2673 CPU@2.40GHz and 28GB of RAM. In each experiment we use the same degree of parallelism for spouts and bolts and we perform four experiments with different degrees of parallelism. <ref type="figure" target="#fig_2">Figures 2 and 3</ref> present the overall throughput and latency results when acknowledgements are enabled. As shown in the figures, Heron outperforms Storm by approximately 3-5X in terms of throughput and at the same time has 2-4X lower latency. <ref type="figure" target="#fig_3">Figure 4</ref> presents the total throughput when acknowledgements are disabled. As shown in the figure, the throughput of Heron is 2-3X higher than that of Storm.</p><p>Our experiment demonstrates that although Heron employs an extensible and general-purpose architecture, it can significantly outperform Storm's more specialized architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Impact of Stream Manager Optimizations</head><p>In this section, we quantify the impact of the optimizations in the Stream Manager module presented in Section V. We use the Word Count topology and vary the parallelism of the spouts and bolts. All experiments were run on machines with dual Intel Xeon E5645@2.4GHZ CPUs, each consisting of 12 physical cores with hyper-threading enabled and 72GB of RAM.  <ref type="figure">Figure 6</ref> shows the throughput per cpu core provisioned. When the Stream Manager optimizations are enabled, there is approximately a 4-5X performance improvement per CPU core which shows that Heron is then able to utilize resources more efficiently.</p><p>The throughput results when acknowledgments are enabled are presented in <ref type="figure">Figures 7 and 8</ref>. As shown in <ref type="figure">Figure 7</ref>, the Stream Manager optimizations provide a 3.5-4.5X performance improvement. At the same time, as shown in <ref type="figure">Figure 8</ref> there is a substantial performance improvement per CPU core. <ref type="figure" target="#fig_5">Figure 9</ref> shows that the Stream Manager optimizations can also provide a 2-3X reduction in end-to-end latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Tuning</head><p>In this section, we present experiments that highlight the impact of the configuration parameters described in Section V-B on the overall performance. We use the same hardware setup as in the previous experiment and run the WordCount topology with acknowledgements enabled.</p><p>In this experiment, we quantify the performance impact of varying the maximum number of tuples that can be pending on a spout task (max spout pending). <ref type="figure">Figures 10 and 11</ref> show how the throughput and end-to-end latency vary for different degrees of parallelism. As shown in <ref type="figure">Figure 10</ref>, as the value of the parameter increases the overall throughput also increases until the topology cannot handle more in-flight tuples. This behavior is expected since a small number of pending tuples can result in underutilization of the provisioned resources. <ref type="figure">Figure 11</ref> shows that as the number of maximum pending tuples increases, the end-to-end latency also increases. This is because the system has to process a larger number of tuples and thus additional queuing delays are incurred. For the particular topology examined, a value of 20 Ktuples/s seems to provide the best tradeoff between latency and throughput.</p><p>We now explore the impact of the cache drain frequency on the overall performance. As mentioned in Section V-B, this parameter determines how often the Stream Manager tuple cache is flushed. <ref type="figure" target="#fig_2">Figures 12 and 13</ref> show how the throughput and end-to-end latency vary for different degrees of parallelism. As shown in <ref type="figure" target="#fig_2">Figure 12</ref>, as the time threshold to drain the cache increases the overall throughput gradually increases until it reaches a peak point. After that point, the throughput starts decreasing. This behavior is consistent across all the configurations tested and can be explained as follows: When the time threshold to drain the cache is low, the system pays a significant overhead in flushing the cache state. This overhead gets amortized as the time threshold increases since the flush operations become less frequent. However, further increasing the time threshold might actually reduce the throughput observed. As noted previously, the maximum inflight tuples that can be handled by the system is fixed (see max spout pending parameter). At the same time, increasing the time threshold to drain the cache will increase the end-toend latency. As a result, fewer tuples will be processed during a given time interval since the system has a bounded number of pending tuples and is not able to process more in-flight tuples. <ref type="figure">Figure 13</ref> shows that if the time threshold to drain the cache is low, the system pays a large overhead to flush the cache. Similar to the throughput curves, as the time threshold increases, the latency improves until the system reaches a point where the additional queuing delays hurt performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Heron Resource Usage Breakdown</head><p>In this experiment, we present resource usage statistics for Heron that highlight that Heron's general-purpose architecture manages resources efficienly. We used a real topology that reads events from Apache Kafka <ref type="bibr" target="#b20">[22]</ref> at a rate of 60-100 million events/min. It then filters the tuples before sending them to an aggregator bolt, which after performing aggregation, stores the data in Redis <ref type="bibr" target="#b21">[23]</ref>. We have provisioned 120 and 24 CPU cores for Heron and Redis, respectively. The memory provisioned is 200 GB and 48 GB for Heron and Redis, respectively.</p><p>We profiled the topology in stable state and tracked the resources used from each system component. <ref type="figure" target="#fig_3">Figure 14</ref> shows the overall resource consumption breakdown. As shown in the figure, Heron consumes only 11% of the resources. This number includes the overheads introduced due to data transfers, internal data serialization and deserialization, and metrics reporting. The remaining resources are used to fetch data from Kafka (60%), execute the user logic (21%) and write data to Redis (8%).</p><p>Overall, this experiment highlights that Heron can efficiently manage the available resources and shows that most of the processing overheads are related to data reading and writing to external services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this paper, we presented Heron's modular and extensible architecture that allows it to accommodate various environments and applications. We compared Heron's general-purpose architecture with other specialized approaches and showed that </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Communication Layer. Finally, unlike Heron that implements an extensible Stream Manager, all the Spark Streaming communication mechanisms rely on Spark and are not customizable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>p u b l i c i n t e r f a c e S c h e d u l e r { v o i d i n i t i a l i z e ( C o n f i g u r a t i o n conf ) v o i d o n S c h e d u l e ( P a c k i n g P l a n i n i t i a l P l a n ) ; v o i d o n K i l l ( K i l l T o p o l o g y R e q u e s t r e q u e s t ) ; v o i d o n R e s t a r t ( R e s t a r t T o p o l o g y R e q u e s t r e q u e s t ) ; v o i d onUpdate ( U p d a t e T o p o l o g y R e q u e s t r e q u e s t ) ; v o i d c l o s e ( ) }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Heron uses the State Manager module for distributed coordination and for storing topology metadata. More specifically, the Topology Master advertises its location through the State Manager to the Stream Manager processes of all the containers. As a result, in case the Topology Master dies, all the Stream Managers become immediately aware of the event. Heron stores several metadata in Throughput with acksFig. 3. End-to-end latency with acks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Throughput without acks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 . 6 .Fig. 7 . 8 .</head><label>5678</label><figDesc>Throughput without acks Fig. Throughput per CPU Core without acks Throughput with acks Fig. Throughput per CPU Core with acks affect both the throughput and the latency of the workload. Section VI provides an analysis of the performance observed as the value of this parameter varies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>End-to-end latency with acks Fig. 10. Throughput vs max spout pending Figures 5 and 6 present our results when acknowledgements are not enabled. As shown in Figure 5, our Stream Manager optimizations provide 5-6X performance improvement in throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Latency vs max spout pending Throughput vs cache drain frequencyFig. 13. Latency vs cache drain frequency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 14 .</head><label>14</label><figDesc>Resource Consumption with careful system design especially at the communication layer, general-purpose architectures can outperform their specialized counterparts without sacrificing manageability.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Twitter Heron: Stream Processing at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kedigehalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kellogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taneja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ryaboy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">@</forename><surname>Storm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Twitter</surname></persName>
		</author>
		<title level="m">ACM SIGMOD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurora</forename><surname>"apache</surname></persName>
		</author>
		<ptr target="http://aurora.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Apache Hadoop YARN: Yet Another Resource Negotiator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Vavilapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>O'malley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Baldeschwieler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th Annual Symposium on Cloud Computing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Apache Hive</title>
		<ptr target="http://hive.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Storm on Mesos</title>
		<ptr target="https://github.com/mesos/storm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Storm on YARN</title>
		<ptr target="https://github.com/yahoo/storm-yarn" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<ptr target="http://hortonworks.com/apache/storm/" />
	</analytic>
	<monogr>
		<title level="j">Storm on Apache Slider</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Spark Streaming</title>
		<ptr target="http://spark.apache.org/streaming/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Microkernel</title>
		<ptr target="https://en.wikipedia.org/wiki/Microkernel" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spark: Cluster Computing with Working Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing</title>
		<meeting>the 2nd USENIX Conference on Hot Topics in Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Discretized streams: Fault-tolerant streaming computation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="423" to="438" />
		</imprint>
	</monogr>
	<note>in In SOSP, 2013</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Heron Code Repository</title>
		<ptr target="https://github.com/twitter/heron" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Bin Packing Problem</title>
		<ptr target="http://mathworld.wolfram.com/Bin-PackingProblem.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mesos</forename><surname>"apache</surname></persName>
		</author>
		<ptr target="http://mesos.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mesos: A platform for fine-grained resource sharing in the data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation, ser. NSDI&apos;11</title>
		<meeting>the 8th USENIX Conference on Networked Systems Design and Implementation, ser. NSDI&apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="295" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Slurm Workload Manager</title>
		<ptr target="https://mesosphere.github.io/marathon/" />
		<imprint/>
	</monogr>
	<note>Marathon</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zookeeper</forename><surname>"apache</surname></persName>
		</author>
		<ptr target="http://zookeeper.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Protocol Buffers</title>
		<ptr target="https://developers.google.com/protocol-buffers/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hdinsight</forename><surname>"microsoft</surname></persName>
		</author>
		<ptr target="https://azure.microsoft.com/en-us/services/hdinsight/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>"apache Kafka</surname></persName>
		</author>
		<ptr target="http://kafka.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Redis</title>
		<ptr target="http://redis.io/" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
