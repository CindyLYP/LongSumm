{"1000": "unsupervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing , resulting in highly accurate systems . however , supervised methods rely on labeled training data , which is time-consuming and expensive to generate . to bridge this gap , we consider a practically motivated scenario , in which we want to leverage existing resources from a resource-rich language ( like english ) when building tools for resource-poor foreign languages.1 we assume that absolutely no labeled training data is available for the foreign language of interest , but that we have access to parallel data with a resource-rich language . algorithm bilingual pos induction require : parallel english and foreign language data de and df , unlabeled foreign training data f ; english tagger . ensure : f , a set of parameters learned using a constrained unsupervised model . this paper presents the design and evaluation of an unsupervised foreign language pos tagger based on the feature-hmm of berg-kirkpatrick et al . ( 2010 ) . our approach uses the labeled treebanks of petrov et al . ( 2011 ) to extract a vector tag ( tx ) for every word in the foreign language . this vector tag is then used to construct a supervised feature-hmm ( hmm ) for each language . the hmm is then used to extract the pos tag for each language . our full model outperforms the hitherto state - of - the - art feature-hmm baselines and performs better than the no lp and no lp baselines by on an average . our no lp model performs better than the no lp and no lp baselines for seven out of eight languages , and performs better than the no lp and no lp baselines for six out of eight languages . our full model performs better than the no lp and no lp baselines for seven out of eight languages , and performs better than the no lp and no lp baselines for six out of eight languages .", "1001": "fisher vectors have been shown to provide a significant performance gain on many different applications in the domain of computer vision . this paper presents a novel approach for representation of sequences using a recurrent neural network ( rnn ) . the rnn is trained to predict the next element of a sequence given the previous elements . the new representation is sensitive to ordering and therefore mitigates the disadvantage of using the standard fisher vector representation . it is applied to two different tasks : video action recognition and image annotation by sentences . in the video action recognition task , \n the rnn is trained to predict the next element of a sequence given the previous elements . in image annotation and image search , \n the rnn is trained to predict the embedding of the next frame given the previous ones . in this paper , we present a new approach to image classification based on the gradient of a loss function . this loss function is not used to update the network s weights during inference , but it may serve as a sequence representation if the loss is not interpretable as a likelihood . the loss function is used to predict the next element in the vector space of the input sequence . the network is trained using the cross-entropy loss and the gradient of the loss is used to predict the next element in the vector space of the input sequence . the network is used to predict the sequence u ( w1 , w2 , ... , wn ) from the sequence x ( x0 , x1 ) , where x is the number of unique symbols in the input sequence . the method is applied to two large action recognition benchmarks : ( i ) image-sentence similarity and ( ii ) image textual similarity . this paper introduces a novel fv representation for sequences that is derived from rnns . the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs . when used for representing sentences , the rnnfv representation surpasses the state-of-the - art or competitive results on image annotation and image search tasks . when used for representing images , the rnnfv representation achieves state-of-the - art or competitive results on image annotation and image search tasks . a transfer learning result from the image annotation task to the video action recognition task is shown . the con-ceptual distance between these two tasks makes this result both interesting and surprising .", "1002": "the rate of publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research . recently , academic conferences started publishing talks ( e.g. , acl , naacl , emnlp1 , icml2 , and more ) . in such talks , the presenter ( usually a co- author ) must describe their paper coherently and concisely ( since there is a time limit ) , provid- ing a good basis for generating summaries . based on this idea , in this paper , we propose a new method , talksumm ( acronym for talk- based summarization ) , to automatically generate extractive content-based summaries for scientific papers based on video talks . our approach uti- lizes the transcripts of conference talks , and treat them as spoken summaries of pa- pers. then , using algo- rithms , we map the transcripts to the text of the paper , and create extractive summaries . we make our dataset and related code publicly available . this paper presents a novel approach to automatically break complex sentences into several simple ones while preserving the semantics of the sentences . the model is trained on automatically- generated summaries composed of sentences from conference talks given by authors . we evaluate the model on both the cited text and abstract tasks .", "1003": "this work studied the use of pretrained word vectors for emotion detection from text . pretrained word vectors were used as an input to neural networks to improve emotion detection from text . this work proposes an ensemble approach that combines both a linear classi er based on bag-of - words ( bow) 1url and a non - linear classi er based on pretrained word vectors . in addition , we propose a new method for realizing a sentence level representation from the single words vectors .", "1004": "social media are becoming an important part of customer service . yet there are limited tools for assessing this . in this paper , we analyze customer support dialogues using the twitter platform and show the utility of such analyses . the particular aspect of such dialogues that we concentrate on is emotions . emotions are a cardinal aspect of inter- personal communication : they are an implicit or explicit part of essentially any communication , and of particular importance in the setting of customer service , as they relate directly to customer satisfaction and experience . typical emotions expressed by customers in the context of social media service dialogues include anger and frustration , as well as gratitude and more . on the other hand , customer service agents also express emotions in service conversations , for example apology or empathy . thus , the analysis of emotions in service dialogues can take two applications : ( 1 ) to discern and compute quality of service indicators and ( 2 ) to provide real- time clues to customer service agents regarding the cus- service-barometer-us . in this paper , we focus on the problem of detecting emotions in agent turns in social media . we present two models for agent turn emotion detection . in our first model , agent turn emotion detection is performed before its content is known . in our second model , \n agent turn emotion detection is performed after the feature vector representing the turn has been extracted from the timeline of the dialogue . we evaluate our models on a social media dataset containing over 4k user - generated agent turns .", "1005": "neural architecture search ( nas ) is a very active research field that has contributed significantly to overall improvement of the state of the art in supervised classification . some of the recent nas techniques , and in particular differentiable-nas ( d-nas ) , are capable of finding optimal ( and transferable ) architectures given a particular task using a single gpu in the course of 1-2 days . this is due to incorporating the architecture as an additional set of neural network parameters to be optimized , and solving this optimization using sgd . due to this use of additional architectural parameters , the training tends to over-fitting . d-nas optimization techniques are especially designed to mitigate over-fitting , making them attractive to extreme situations with the greatest risk of overfitting , such as in the case of few-shot classification ( fsc ) . to this end \n , we employ a set of small neural networks , metadapt controllers , responsible for controlling the connections in the dag given the current task . in bf3s \n auxiliary self-supervision tasks are added , such that if some operations are better for the current task they will get higher weights , thus , effectively modifying the architecture in this paper , we propose a new architecture for few-shot architecture search , based on a task-adaptive block and a metadapt controller architecture . the metadapt controller is responsible for predicting , given a few-shot task , the best way of adapting the mixing coefficients for the corresponding edge operations . the task-adaptive block can be appended to any backbone feature extractor . we use resnet9 followed by a single task-adaptive block with nodes ( v 4 ) in our experiments , resulting in about times more parameters compared with the original resnet12 ( due to large set of operations on all connections combined ) .", "1006": "automated conversational agents ( chatbots ) are becoming widely used for various tasks such as personal assistants or customer service agents . due to the increasing prevalence of chatbots , even a small fraction of such egregious3 conversations could be problematic for the companies deploying them and the providers of the services in this paper , we show how it is possible to detect egregious conversations using a combination of customer utterances , agent responses , and customer-agent interactional features . for this purpose , we first extracted conversation data from two commercial systems , and then used an svm model to classify conversations into two classes : ( 1 ) natural language understanding ( nlu ) error - the agent s intent is wrong - the agent s intent is correct - and ( 2 ) egregious conversations - the agent s intent is wrong - the agent s intent is correct - the agent s intent is not supported by the agent s data . our results show that it is possible to detect egregious conversations using a combination of customer utterances , agent responses , and customer-agent interactional features .", "1007": "convolutional neural networks ( cnns ) have shown strong performance on traditional natural language processing ( nlp ) tasks such as text classification . as with other architectures of neural networks , explaining the learned functionality of cnns is still an active research area . the problem of interpretability in machine learning can be divided into two concrete tasks : model interpretability aims to supply a structured explanation which captures what the model has learned , and prediction interpretability aims to explain how the model arrived at its prediction . in this work , we examine and attempt to understand how cnns process text , and then use this information for the more practical goals of improving model-level and prediction-level interpretability . we examine the relationship between the activation patterns of naturally occurring and observed ngrams in the elec model . we find that the top naturally occurring ngrams score less than the topscoring possible ngrams when averaged over all filters . this discrepancy can be explained by two hypotheses : ( i ) each filter captures multiple semantic classes of ngrams , and each class has some dominating slots ( which we define as a max-pooling layer ) , and ( ii ) a slot may not be maximized because it does not achieve maximum activation s value , but rather lack of existenceensuring that specific words do not occur . we also show that filters may not only identify good ngrams , but may also actively supress bad ones .", "1008": "in this work , we suggest a new idea of editorial network ( editnet ) a mixed extractive-abstractive summarization approach . a summary generated by editnet may include sentences that were either extracted , abstracted or of both types . moreover , per considered sentence , editnet may decide not to take either of these decisions and completely reject the sentence . using the cnn/dailymail dataset we demonstrate that , editnet s summarization quality is highly competitive to that obtained by both state-of-the-art abstractive-only and extractive-only baselines . example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore in this paper , we propose a novel alternative summarization approach that instead of solely applying extraction or abstraction , mixes both together . moreover , editnet implements a novel sentence rejection decision , allowing to correct initial selection decisions which are predicted to negatively effect summarization quality .", "1009": "incorrect homophones and synophones , whether used in error or in grammar , pose challenges for a wide range of nlp tasks , such as named entity identification , text normalization and spelling correction . incorrect homophones and synophones , whether used in error or in grammar , \n require phonetic encoding to generate phonetically similar words . this paper presents dimsim , a simple and effective algorithm to generate and rank phonetically similar words . words are generated by ignoring ( foremost ones ) , which is appropriate where phonetic consists of a sequence of phonemes , such as for english . words are encoded with a phonetic encoding to approximate phonetic presentations by ignoring ( foremost ones ) , which is appropriate where phonetic consists of a single syllable in pinyin consisting of two or three parts : an initial ( optional ) , a final ( optional ) and a compound ( compound ) part . this paper presents dimsim , an end - to - end solution to the chinese word encoding task of identifying phonetically similar initials and final - to - final pinyins . the word encodings are learned from a dictionary and a manually annotated list of all possible pinyins with component words and final - to - final pinyins . we identify and account for several confounding factors that may affect annotation : 1 ) the position of the character containing the initial or final being compared ; 2 ) the word length ; and 3 ) the combination of initials and finals . we also incorporate a penalty function , p , for pairs deviating from the manually annotated distance so that more phonetically similar pairs are penalized more highly . we compare dimsim to double metaphone ( dm ) , aline ( kondrak , 2003 ) and minimum edit distance ( med ) in terms of precision and mean reciprocal rank ( mrr ) . phonetic transcription is a widely observed phenomenon in chinese social media and informal language . we propose dimsim , a phonetic similarity algorithm that generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial , final , and tone components . using a real world dataset , we demonstrate that dimsim effectively improves mrr by 7.5x , recall by 1.5x and precision by 1.4x over existing approaches .", "1010": "knowledge base question answering ( kbqa ) systems answer questions by obtaining information from kb tuples , which can be executed to retrieve the answers from a kb in this paper , we propose a novel relation detection model called hierarchical residual bilstm ( hr-bilstm ) for deep bilstms . the hr-bilstm has two key features : ( 1 ) it is a hierarchical matching model , and ( 2 ) it is based on residual learning . we evaluate the hr-bilstm on two relation detection tasks : ( 1 ) webqsp and ( 2 ) simplequestions . the results show that the hr-bilstm outperformed the best baselines on both tasks by margins of ( 1 ) and ( 2 ) . furthermore , the hr-bilstm outperformed the best baselines on both tasks by margins of ( 1 ) and ( 2 ) . the hr-bilstm also outperformed the best baselines on both tasks by margins of ( 1 ) and ( 2 ) . kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks . we propose a novel kb relation detection model , hr-bilstm , that performs hierarchical matching between questions and kb relations . our model outperforms the previous methods on kb relation detection tasks and allows our kbqa system to achieve state-of-the-arts .", "1011": "in this work we propose a feature agnostic approach for dictionary expansion based on neural language models , such as word2 to prevent semantic drift during the dictionary expansion , we effectively include humanin-the-loop ( huml). given an input corpus and a set of examples , the proposed runs in two phases , the proposed runs in two phases , the exploit phase tries to identify similar instances to the input dictionary and the explore phase tries to identify more complex instances to the input dictionary . this paper proposes an interactive dictionary expansion tool using a lightweight neural language model . our algorithm is iterative and purely statistical , hence does not require any feature extraction beyond tokenization . it incorporates human feedback to improve performance and control semantic drift at every iteration cycle . the experiments show high importance of tight huml integration on discovery efficiency .", "1012": "grounding of textual phrases , i.e. , finding bounding boxes in images which relate to textual phrases , is an important problem for human-computer interaction , robotics and mining of knowledge bases , three applications that are of increasing importance when considering autonomous systems , augmented and virtual reality environments . while being easy to obtain , automatic extraction of region proposals is limiting , because the performance of the visual grounding is inherently constrained by the quality of the proposal generation procedure . in this work we describe an interpretable mechanism which additionally alleviates any issues arising due to a limited number of region proposals . in this paper , we present a novel approach for loss-augmented inference based on the concept of support vector machines ( svms ) . the approach is based on the idea that loss-augmented inference is equivalent to training a structured svm by minimizing the energy over the output space . we demonstrate the feasibility of this approach by training a loss-augmented inference model on the flickr 30k entities and referitgame datasets . we also demonstrate the effectiveness of the approach by training a loss-augmented inference model on the flickr 30k entities dataset . we also demonstrate the effectiveness of the approach by training a loss-augmented inference model on the referitgame dataset .", "1013": "deep convolutional neural networks ( dcnns ) pretrained on imagenet have led to important advances in computer vision . moment matching approaches for implicit generative models are based on the assumption that one can learn the data distribution by matching the moments of the model distribution to the empirical data distribution . however , current moment matching approaches suffer from the problematic min/max game . in this work \n we demonstrate that , by using perceptual features ( pfs ) to perform moment matching , one can overcome some of the difficulties found in current moment matching approaches . more specifically , we propose a simple but effective moment matching method that : ( 1 ) breaks away from the problematic min/max game completely ; ( 2 ) does not use online learning of kernel functions ; and ( 3 ) is very efficient with regard to both number of used moments and required minibatch size . generative moment matching network autoencoder ( gfmn ) is a nonadversarial feature matching based generative model that uses pretrained feature extractors ( pfs ) to perform moment matching in a pf space induced by a non - linear kernel function ( a dcnn ) that is orders of magnitude larger than the ae latent code . we show that gfmn is universal in the image domain . our proposed model differs from adversarial generative models ( ae ) in that it uses pretrained ae feature extractors to play the role of feature extractors , while these methods aim to impose a prior distrib . on the latent space of ae \n , gfmn performs feature matching using an encoder that maps from a prior uniform distribution to the latent code learned by a pretrained ae , and then uses the frozen pretrained decoder to map back to image space . our proposed model is trained using a generator g that maps from a prior uniform distribution to the latent code learned by a pretrained ae , and then uses a generator g that maps back to image space . we show that gfmn achieves better or similar results compared to the state - of in this paper , we investigate the use of moving average of the difference of features ( mse-) and moving average of the difference of gradients ( adam-) as a feature extractor for autoencoders and generative moment matching networks ( gfmn ) . we show that mse- and adam- based feature extractors have better performance than mse- and adam- based extractors in terms of accuracy and regret . we also show that mse- and adam- based feature extractors have better performance than mse- and adam- based extractors in terms of image fidelity ( fid ) . we also show that mse- and adam- based feature extractors have better performance than mse- and adam- based extractors in terms of image fidelity ( is ) . we also show that mse- and adam- based feature extractors have better performance in terms of is and fid than mse- and adam- based feature extractors in terms of image fidelity ( is ) . we also show that mse- and adam- based feature extractors have better performance in terms of is and fid than mse-", "1014": "e- recommender systems aim to present items with high utility to the consumers into form utility : the item is desired as it is , and time utility : the item is desired at the given point in time . in this paper , we study the problem of learning the combination of form utility and time utility for purchase triplets ( user , item , time ) . to this end , we first formulate the problem of learning users s form utility as the problem of learning users s time utility . to this end \n , we use an underlying third-order tensor to model implicit feedback . in addition , we employ a nonnegative vector d rr to measure the underlying inter-purchase durations in item categories . we show that the proposed model is extremely efficient and able to handle largescale recommendation problems . in this paper , we consider the problem of recommendation with respect to one- or two-sided sampling . the objective function is highly non - smooth with nested hinge losses , and a naive optimization algorithm will take at least o ( mnl) time , which is computationally infeasible when the data is large . to address this issue , we propose an extremely efficient optimization algorithm by effectively exploring the sparse structure of the tensor p and low - rank structure of the matrix x . we show that ( i ) the problem ( 4 ) can be solved within o ( p0 ( k log ( p0 ) ) time ) , where k is the rank of x , and ( ii ) the algorithm converges to the critical points of f ( x , d ) time . since there are o ( mnl) terms in the objective function , a naive implementation will take at least o ( mnl) time , which is computationally infeasible when the data is large . to address this issue , we use proximal gradient descent to solve the problem . experiments with simulated data verify that the proposed algorithm is extremely efficient and robust to noise . in this paper , we examine the problem of demand-aware recommendation in settings when users purchase within categories . we formulate the problem of demand-aware recommendation as nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter- purchase durations , and propose a scalable optimization algorithm with a tractable time complexity . on two real - world datasets , tmall and amazon review , \n we show that our algorithm outperforms six state-of-the-art recommendation algorithms on the tasks of category , item , and purchase time predictions .", "1015": "this paper presents a personality - based neural response generation model for automated conversational agents . personality traits are a set of traits that represent durable characteristics of a person . we build upon a sequence-to-sequence ( seq2seq ) architecture by adding an additional layer that represents the target set of personality traits , and a hidden layer that learns personality based features . the response is then generated conditioned on these features . we experimented with a dataset of 87.5k real customer-agent utterance pairs from social media . we find that leveraging personality encoding improves relative performance up to in bleu score , compared to a baseline seq2seq model .", "1016": "event - based sensors , such as dynamic vision sensor ( dvs ) 37 and atis 50 , encode pixel illumination changes as events and solve two major drawbacks of frame - based cameras . first , temporal resolution is limited by the camera frame rate , usually frames per second . second , consecutive frames in videos are usually highly redundant , which waste downstream data transfer , computing resources and power . to solve this problem \n , we propose a fully neuromorphic event- based stereo disparity algorithm . a live-feed version of the system running on nine truenorth chips is shown to calculate disparity maps per second , and the ability to increase this up to 2,000 disparities per second ( subject to certain trade - offs ) is demonstrated , for use with high speed event cameras , such as dvs . the main advantages of the proposed method are simultaneous end-to-end neuromorphic disparity calculation , low power , high throughput , low latency ( ms ) , and linear scalability to multiple neuromorphic processors for larger input sizes . we present a novel feed - forward neural network architecture that finds the largest disparity level at each time step . the winner-take-all ( wta ) system is a feed - forward neural network that takes as input d thermometer code representations of the hadamard products for d distinct candidate disparity levels , and finds the disparity with the largest value , at every tick . we introduce a novel encoding technique for inputs , where numbers can be efficiently coded using base4 representation where each digit is encoded using a 3-bits thermometer code . for designing a scalable and compact wta system on a neuromorphic hardware , we introduced a novel encoding technique for inputs , where numbers can be efficiently coded using base4 representation where each digit is encoded using a 3-bits thermometer code . we evaluate the performance of the system on sequences of random dot stereograms , and two real world sets of sequences , consisting of a fast rotating fan and a rotating toy butterfly . we present a new approach to event based disparity in neuromorphic stereo systems . our approach is based on a fully graph - based computation model , where no frames , arrays or other such data -structures are used . thus , no frames or arrays are used . furthermore , our approach is fully event- and graph - based , allowing for a fully end - to - end low - power , high throughput fully event - based neuromorphic stereo system capable of running on live input event streams , using a fully graph - based computation model , where no frames , arrays or other such data -structures are used .", "1017": "learning the causal structure of a complex system is one of the fundamental challenges in science . the most popular language to encode the invariances needed to reason about causal relations , for both learning and inference , is based on graphical models , and appears under the rubric of causal graphs 16 , 21 . the task of learning the causal structure entails a search over the space of causal graphs that are compatible with the observed data ; the collection of these graphs forms what is called an equivalence class . the most popular mark imprinted on the data by the underlying causal structure that is used to delineate an equivalence class are conditional independence ( ci ) relations . while cis are powerful and have been the driving force behind some of the most prominent structural learning algorithms in the field , these are constraints specific for one distribution . in this paper , we start by noting something very simple , albeit powerful , that happens when a combination of observational and experimental distributions are available : there are constraints over the graphical structure that emerge by comparing these different distributions , and which are not of ci-type2 . remarkably , and unknown until our work , the converse of the causal calculus developed by pearl offers a systematic way of reading these constraints in this paper , we define the notion of the d-separation statement relative to a specific interventional distribution ( do-do-, do-do-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx-, do-dx- algorithm presents a modification of the function createaugmentednodes ( fci ) algorithm to learn augmented pags .", "1018": "complex questions can be answered using structured knowledge bases ( kbqas ) , such as wikidata and freebase , but require an end - to - end complex reasoning process . this can be accomplished with feed-forward and seq2seq networks . however , more complex questions need to be answered as an expression graph over nodes representing kb set , logical , and arithmetic operators . an alternative to this is to generate an imperative program . we present complex imperative program induction ( cipitr ) that is able to answer complex questions by inducing programs of length up to 7 , using atomic operators and variable types . cipitr reduces the combinatorial program space to only semantically mitigating subgoals , thereby incorporating symbolic constraints guided by kb schema and inferred answer as ( very distant ) supervision for inducing programs . on one of the hardest class of programs of around steps ( i.e. , i fail at least higher f1 than both ) cipitr outperforms keyvalue memory networks ( nsmnet ) by a factor of at least higher f1 than both . this paper describes cipitr , a programming model for generating programs from natural language questions . the model incorporates both generic and task - specific constraints in its programming procedure . it also incorporates an oracle to link entities , relations , types , and integers in the query to the kb . experiments show that cipitr is able to produce programs that are consistent as per the kb schema and on execution . we present cipitr , an advanced npi framework that significantly pushes the frontier of complex program induction in absence of gold programs . cipitr uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic programming styles to constrain the combinatorial program space to only semantically correct programs", "1019": "the cross entropy summarizer ( ces ) is a state of the art unsupervised query-focused multi-document extractive summarizer . in this work \n , we propose dual-ces , an extended ces summarizer , which is an unsupervised query-focused multi-document extractive summarizer . to this end , like ces , dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents , whose combination is predicted to produce a good summary . yet , differently from ces , dual-ces does not attempt to address both saliency and focus goals in a single optimization step . instead , dual-ces implements a novel twostep dual-cascade optimization approach . using such an approach , \n dual-ces tries to handle the tradeoff by gradually shifting from generating a long summary that is more salient in the first step to generating a short summary that is more focused in the second step . moreover , dual-ces utilizes the long summary that was generated in the first step for saliency-based pseudo-feedback distillation , which allows to generate a final focused summary with better saliency . in this work , we evaluate the performance of two variants of the ce-method . the first one is the original ce-method , which is based on an unsupervised learning approach . the second one is a variant of the original ce-method , which is based on a pseudo-feedback distillation approach . our evaluation is based on the document understanding conferences ( duc ) 2005 , and benchmarks2 .", "1020": "in this paper we show that we can get a considerable quality improvement by modifying a tts system that produced the world vocoder parameters to predict parameters for lpcnet as in the previous work 6 , we conduct multiple adaptation experiments , applied on multiple vctk voices and show that the new system has much better quality and similarity to the target voices but can still run much faster than real-time in a single-cpu mode . this paper presents a new system for high - quality speech synthesis that uses a neural network ( nn ) model for generating prosody , acoustic features and the final speech signal . the system achieves state - of - the - art performance with a small number of training samples and is able to operate at much faster than real - time rate without the need for expensive gpus . the system is tested on two tts datasets and demonstrated to produce speech of quality comparable to state - of - the - art tts systems .", "1021": "in this paper we introduce the sobolev independence criterion ( sic ) , a dependency measure based on the gradient of a witness function , or critic , that is regularized integral probability metric ( ipm ) between the joint distribution and the product of marginals . sic is interpretable and can be reliably used to control the false discovery rate ( fdr ) in feature selection . sic can be parameterized as a homogeneous neural network that can be optimized using stochastic block coordinate descent . we validate sic and its fdr control on synthetic and real datasets . we introduce empirical non convex sic ( sic-l1 ) , which is the first dependency criterion that decomposes in the sum of contributions of each coordinate , and hence it is an interpretable dependency measure . moreover , j are normalized importance scores of each feature j , and their ranking can be used to assess feature importance . we define empirical non convex sic ( l1 ) 2 as follows : sic ( l1 ) 2 ( pxy , pxpy) ( epxyf ( x , y) epxpyf ( x , y) xj epxpyf,2 ( x , y) f2f moreover , epxpy f ( x , y) xj js , l1 ( f ) and dx j1 j 1 ( x , y) xj epxpyf,2 ( x , y) f2f in this paper , we propose a novel approach for iter and iter in stochastic binary classification ( sbc ) called rndpy . rndpy is based on the stochastic gradient descent algorithm hrt with sic inputs and the softmax end for output . we show that rndpy and iter can be trained using a neural network model with relu units and conditional batch normalization ( bcn ) . we also show that rndpy and iter can be trained using a neural network model with relu units and bcn . we show that rndpy and iter can be trained using a neural network model with relu units and bcn . in this paper , we investigate the performance of a standard three - layer relu network on the heldout mse regression task . the network is concatenated and processed by a final branch that contains a three - layer leakyrelu network ( named bigcritic ) . this network is trained with the same adam settings as above for updates ( liang ) and updates ( ccle ) . the regressor network is also concatenated and processed by a final branch that contains a standard three - layer relu network ( named bigcritic ) ."}