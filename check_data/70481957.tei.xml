<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning the Dimensionality of Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
							<email>enalisni@uci.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
							<email>sachinr@cs.princeton.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92697</postCode>
									<settlement>Irvine Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning the Dimensionality of Word Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We describe a method for learning word embeddings with data-dependent dimensionality. Our Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic Dimensionality Continuous Bag-of-Words (SD-CBOW) are nonparametric analogs of Mikolov et al.&apos;s (2013) well-known &apos;word2vec&apos; models. Vector dimensionality is made dynamic by employing techniques used by Côté &amp; Larochelle (2016) to define an RBM with an infinite number of hidden units. We show qualitatively and quantitatively that the SD-SG and SD-CBOW are competitive with their fixed-dimension counterparts while providing a distribution over embedding dimensionalities, which offers a window into how semantics distribute across dimensions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word Embeddings (WEs) <ref type="bibr" target="#b1">(Bengio et al., 2003;</ref><ref type="bibr" target="#b7">Mnih and Hinton, 2009;</ref><ref type="bibr" target="#b11">Turian et al., 2010;</ref><ref type="bibr" target="#b6">Mikolov et al., 2013)</ref> have received wide-spread attention for their ability to capture surprisingly detailed semantic information without supervision. However, despite their success, WEs still have deficiencies. One flaw is that the embedding dimensionality must be set by the user and thus requires some form of cross-validation be performed. Yet, this issue is not just one of implementation. Words naturally vary in their semantic complexity, and since vector dimensionality is standardized across the vocabulary, it is difficult to allocate an appropriate number of parameters to each word. For instance, the meaning of the word race varies with context (ex: competition vs anthropological classification), but the meaning of regatta is rather specific and * Authors contributed equally.</p><p>invariant. It seems unlikely that race and regatta's representations could contain the same number of parameters without one overfitting or underfitting.</p><p>To better capture the semantic variability of words, we propose a novel extension to the popular Skip-Gram and Continuous Bag-of-Words models <ref type="bibr" target="#b6">(Mikolov et al., 2013)</ref> that allows vectors to have stochastic, data-dependent dimensionality. By employing the same mathematical tools that allow the definition of an Infinite Restricted Boltzmann Machine <ref type="bibr" target="#b3">(Côté and Larochelle, 2016)</ref>, we define two log-bilinear energy-based models named Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic Dimensionality Continuous Bag-of-Words (SD-CBOW) after their fixed dimensional counterparts. During training, SD-SG and SD-CBOW allow word representations to grow naturally based on how well they can predict their context. This behavior, among other things, enables the vectors of specific words to use few dimensions (since their context is reliably predictable) and the vectors of vague or polysemous words to elongate to capture as wide a semantic range as needed. As far as we are aware, this is the first word embedding method that allows vector dimensionality to be learned and to vary across words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Fixed Dimension Word Embeddings</head><p>We first review the original Skip-Gram (SG) and Continuous Bag-of-Words (CBOW) architectures <ref type="bibr" target="#b6">(Mikolov et al., 2013)</ref> before describing our novel extensions. In the following model definitions, let w i ∈ R d be a d-dimensional, real-valued vector representing the ith input word w i , and let c k ∈ R d be a vector representing the kth context word c k appearing in a 2K-sized window around an instance of w i in some training corpus D. arXiv:1511.05392v3 [stat.ML] 13 Apr 2017 2.1 Skip-Gram SG learns a word's embedding via maximizing the log probability of that word's context (i.e. the words occurring within a fixed-sized window around the input word). Training the SG model reduces to maximizing the following objective function:</p><formula xml:id="formula_0">L SG = | D | i=1 i−K ≤k ≤i+K,k i log p(c k |w i ) = | D | i=1 i−K ≤k ≤i+K,k i log e c T k w i V v=1 e c T v w i (1)</formula><p>where V is the size of the context vocabulary. Stochastic gradient descent is used to update not only w i but also c k and c v . A hierarchical softmax or negative sampling is commonly used to approximate the normalization term in the denominator <ref type="bibr" target="#b6">(Mikolov et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Continuous Bag-of-Words</head><p>CBOW can be viewed as the inverse of SG: context words c 1 . . . c k serve as input in the prediction of a center word w i . The CBOW objective function is then written as</p><formula xml:id="formula_1">L C BOW = | D | i=1 log p(w i |c i−K . . . c i+K ) = | D | i=1 log e 1 2K −1 j c T j w i V v=1 e 1 2K −1 j c T j w v<label>(2)</label></formula><p>where c, w, and V are defined as above for SG. Again, the denominator is approximated via negative sampling or a hierarchical softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word Embeddings with Stochastic Dimensionality</head><p>Having introduced the fixed-dimensional embedding techniques SG and CBOW, we now define extensions that make vector dimensionality a random variable. In order for embedding growth to be unconstrained, word vectors w i ∈ R ∞ and context vectors c k ∈ R ∞ are considered infinite dimensional and initialized such that the first few dimensions are non-zero and the rest zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stochastic Dimensionality Skip-Gram</head><p>Define the Stochastic Dimensionality Skip-Gram (SD-SG) model to be the following joint Gibbs distribution over w i , c k , and a random positive integer z ∈ Z + denoting the maximum index over which to compute the embedding inner product: <ref type="bibr">w,c,z)</ref> , also known as the partition function. Define the energy function as</p><formula xml:id="formula_2">p(w i , c k , z) = 1 Z e −E(w i ,c k ,z) where Z = w c z e −E(</formula><formula xml:id="formula_3">E(w i , c k , z) = z log a − z j=1 w i, j c k, j − λw 2 i, j − λc 2 k, j</formula><p>where 1 &lt; a &lt; ∞, a ∈ R and λ is a weight on the L2 penalty. Notice that SD-SG is essentially the same as SG except for three modification: (1) the inner product index z is a random variable, (2) an L2 term is introduced to penalize vector length, and (3) an additional z log a term is introduced to ensure the infinite sum over dimensions results in a convergent geometric series <ref type="bibr" target="#b3">(Côté and Larochelle, 2016)</ref>. This convergence, in turn, yields a finite partition function; see the Appendix for the underlying assumptions and derivation. For SD-SG's learning objective, ideally, we would like to marginalize out z:</p><formula xml:id="formula_4">logp(c k |w i ) = log ∞ z=1 p(c k , z|w i ) = log l z=1 p(c k , z|w i ) + a a − 1 p(c k , l |w i )<label>(3)</label></formula><p>where l is the maximum index at which w or c contains a non-zero value. l must exist under the sparsity assumption that makes the partition function finite (see Appendix), but that assumption gives no information about l's value. One work-around is to fix l at some constant, but that would restrict the model and make l a crucial hyperparameter. A better option is to sample z values and rely on the randomness to make learning dynamic yet tractable. This way l can grow arbitrarily (i.e. its the observed maximum sample) while the vectors retain a finite number of non-zero elements. Thus we write the loss in terms of an expectation</p><formula xml:id="formula_5">L SD-SG = log p(c k |w i ) = E z |c k ,w i [log p(c k , z|w i ) − log p(z|c k , w i )] .<label>(4)</label></formula><p>Notice that this is the evidence bound widely used for variational inference except here there is equality, not a bound, because we have set the variational distribution q(z) to the posterior p(z|w, c), which is tractable. The sampling we desire then comes about via a score function estimate of the gradient:</p><formula xml:id="formula_6">∂ ∂w i L SD-SG ≈ 1 S S s=1 ∂ ∂w i log p(c k ,ẑ s |w i ) + [log p(c k |w i ) − 1] ∂ ∂w i log p(ẑ s |c k , w i )<label>(5)</label></formula><p>where S samples are drawn fromẑ s ∼ p(z|c k , w i ).</p><p>Note the presence of the p(c k |w i ) term-the very term that we said was problematic in Equation 3 since l was not known. We can compute this term in the Monte Carlo objective by setting l to be the largestẑ value sampled up to that point in training.</p><p>The presence of p(c k |w i ) is a boon because, since it does not dependẑ, there is no need for control variates to stabilize the typically high variance term</p><formula xml:id="formula_7">∂ ∂w i log p(ẑ s |c k , w i ).</formula><p>Yet there's still a problem in thatẑ ∈ [1, ∞) and therefore a very large dimensionality (say, a few thousand or more) could be sampled, resulting in the gradient incurring painful computational costs. To remedy this situation, if aẑ value greater than the current value of l is sampled, we setẑ = l + 1, restricting the model to grow only one dimension at a time. Constraining growth in this way is computationally efficient sinceẑ can be drawn from a (l + 1)-dimensional multinoulli distribution with parameters Θ = {θ 1 = p(z = 1|w, c), . . . , θ l+1 = a a−1 p(z = l |w, c)}. The intuition is the model can sample a dimension less than or equal to l if l is already sufficiently large or draw the (l +1)th option if not, choosing to increase the model's capacity. The hyperparameter a controls this growing behavior: as a approaches one from the right, P(z &gt; l |w) approaches infinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stochastic Dimensionality Continuous Bag-of-Words</head><p>The Stochastic Dimensionality Continuous Bag-of-Words (SD-CBOW) model is a conditional Gibbs distribution over a center word w i and a random positive integer z ∈ Z + denoting the maximum index as before, given multiple context words c k :</p><formula xml:id="formula_8">p(w i , z|c i−K , . . . , c i+K ) = 1 Z w, z e − 1 2K −1 j E(w i ,c j ,z)</formula><p>where <ref type="figure">c j ,z)</ref> . The energy function is defined just as for the SD-SG and admits a finite partition function using the same arguments. The primary difference between SD-SG and SD-CBOW is that the latter assumes all words appearing together in a window have the same vector dimensionality. The SD-SG, on the other hand, assumes just word-context pairs share dimensionality.</p><formula xml:id="formula_9">Z w,c = w z e − 1 2K −1 j E(w,</formula><p>Like with SD-SG, learning SD-CBOW's parameters is done via a Monte Carlo objective. Define the SD-CBOW objective L SD-CBOW as</p><formula xml:id="formula_10">L SD-CBOW = log p(w i |c i−K . . . c i+K ) = E z [log p(w i , z|c i−K . . . c i+K ) − log p(z|w i , c i−K . . . c i+K )].<label>(6)</label></formula><p>Again we use a score function estimate of the gradient to produce dynamic vector growth:</p><formula xml:id="formula_11">∂ ∂w i L SD-CBOW ≈ 1 S S s=1 ∂ ∂w i log p(w i ,ẑ s |c i−K , . . .)+ [log p(w i |c i−K , . . .) − 1] ∂ ∂w i log p(ẑ s |w i , c i−K , . . .) where S samples are drawn from z s ∼ p(z|w i , c i−K , . . . , c i+K ).</formula><p>Vectors are constrained to grow only one dimension at a time as done for the SD-SG by sampling from a l + 1th dimensional multinoulli with</p><formula xml:id="formula_12">θ l+1 = a a−1 p(z = l |w i , c i−K , . . . , c i+K ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>As mentioned in the Introduction, we are aware of no previous work that defines embedding dimensionality as a random variable whose distribution is learned from data. Yet, there is much existing work on increasing the flexibility of embeddings via auxiliary parameters. For instance, <ref type="bibr" target="#b12">Vilnis &amp; McCallum (2015)</ref> represent each word with a multivariate Gaussian distribution. The Gaussian embedding's mean parameter serves much like a traditional vector representation, and the method's novelty arises from the covariance parameter's ability to capture word specificity <ref type="bibr" target="#b12">(Vilnis and McCallum, 2015)</ref>. Other related work proposes using multiple embeddings per word in order to handle polysemy and homonymy <ref type="bibr" target="#b5">(Huang et al., 2012;</ref><ref type="bibr" target="#b9">Reisinger and Mooney, 2010;</ref><ref type="bibr" target="#b8">Neelakantan et al., 2014;</ref><ref type="bibr" target="#b10">Tian et al., 2014;</ref><ref type="bibr" target="#b0">Bartunov et al., 2016)</ref>. <ref type="bibr" target="#b0">Bartunov et al. (2016)</ref>'s AdaGram model is closest in spirit to SD-SG and SD-CBOW in that it uses the Dirichlet Process to learn an unconstrained, data-dependent number of embeddings for each word. Yet, in contrast to SD-SG/-CBOW, the dimensionality of each embedding is still userspecified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We evaluate SD-SG and SD-CBOW quantitatively and qualitatively against original SG and CBOW. For all experiments, models were trained on a one billion word subset of Wikipedia (6/29/08 snapshot).  The same learning rate (α = 0.05 for CBOW, α = 0.025 for SG), number of negative samples <ref type="formula" target="#formula_6">5</ref>, context window size (6), and number of training epochs (1) were used for all models. SD-SG and SD-CBOW were initialized to ten dimensions.</p><p>Quantitative Evaluation. We test each model's ability to rank word pairs according to their semantic similarity, a task commonly used to gauge the quality of WEs. We evaluate our embeddings on two standard test sets: WordSim353 <ref type="bibr" target="#b4">(Finkelstein et al., 2001</ref>) and MEN <ref type="bibr" target="#b2">(Bruni et al., 2014)</ref>. As is typical for evaluation, we measure the Spearman's rank correlation between the similarity scores produced by the model and those produced by humans. The correlations for all models are reported in Subtable (a) of <ref type="figure">Figure 1</ref>. We see that the SD-SG and SD-CBOW perform better than their 50 dimensional counterparts but worse than their 200 dimensional counterparts. All scores are relatively competitive though, separated by no more than 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Evaluation.</head><p>Observing that the SD-SG and SD-CBOW models perform comparatively to finite versions somewhere between 50 and 200 dimensions, we qualitatively examine their distributions over vector dimensionalities. Subfigure (b) of <ref type="figure">Figure 1</ref> shows a histogram of the expected dimensionality-i.e. E z |w,c [z]-of each vector after training the SD-CBOW model. As expected, the distribution is long-tailed, and vague words occupy the tail while specific words are found in the head. As shown by the annotations, the word photon has an expected dimensionality of 19 while the homograph race has 150. Note that expected dimensionality correlates with word frequency-due to the fact that multi-sense words, by definition, can be used more frequently-but does not follow it strictly. For instance, the word william is the 482nd most frequently occurring word in the corpus but has an expected length of 62, which is closer to the lengths of much rarer words (around 20-40 dimensions) than to similarly frequent words.</p><p>In subfigures (c) and (d) of <ref type="figure">Figure 1</ref>, we plot the quantity p(z|w) for two homographs, race (c) and player (d), as learned by SD-SG, in order to examine if their multiple meanings are conspicuous in their distribution over dimensionalities. For race, we see that the distribution does indeed have at least two modes: the first at around 70 dimensions represents car racing, as determined by computing nearest neighbors with that dimension as a cutoff, while the second at around 100 dimensions encodes the anthropological meaning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Semantic similarity (Spearman's rank correlation) (b) Histogram of the expected dimensionalities.(c) p(z|w = 'race') (d) p(z|w = 'player') Figure 1: Subfigure (a) shows results for semantic similarity tasks. The Spearman's rank correlation between model and human scores are calculated for the WordSim-353 and MEN datasets. Subfigure (b) shows a histogram of the expected vector dimensionalities after training SD-CBOW. Subfigures (c) and (d) show the distribution over dimensionalities SD-SG learned for the words race and player.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose modest modifications to SG and CBOW that allow embedding dimensionality to be learned from data in a probabilistically principled way. Our models preserve performance on semantic similar-ity tasks while providing a view-via the distribution p(z|w, c)-into how embeddings utilize their parameters and distribute semantic information.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Finite Partition Function</head><p>Stochastic Dimensionality Skip-Gram's partition function, containing a sum over all countably infinite values of z, would seem to be divergent and thus incomputable. However, it is not, due to two key properties first proposed by <ref type="bibr" target="#b3">(Côté and Larochelle, 2016)</ref> to define a Restricted Boltzmann Machine with an infinite number of hidden units (iRBM). They are:</p><p>1. Sparsity penalty: The L2 penalty in E(w i , c k , z) (i.e. the w 2 i, j and c 2 k, j terms) ensures the word and context vectors must have a finite two-norm under iterative gradient-based optimization with a finite initial condition. In other words, no proper optimization method could converge to the infinite solution if all w and c vectors are initialized to have a finite number of non-zero elements <ref type="bibr" target="#b3">(Côté and Larochelle, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Per-dimension constant penalty:</head><p>The energy function's z log a term results in dimensions greater than l becoming a convergent geometric series. This is discussed further below.</p><p>With those two properties in mind, consider the conditional distribution of z given an input and context word: <ref type="bibr">w,c,z )</ref> .</p><p>Again, the denominator looks problematic due to the infinite sum, but notice the following:</p><p>e −E(w,c,z )</p><p>e −E(w,c,z ) + e −E(w,c,l)</p><p>e −E(w,c,z ) + a a − 1 e −E(w,c,l) .</p><p>The sparsity penalty allows the sum to be split as it is in step #2 into a finite term ( l z =1 e −E(w,c,z ) ) and an infinite sum ( ∞ z =l+1 e −E(w,c,z ) ) at an index l such that w i, j = c k, j = 0 ∀ j &gt; l. After e −E <ref type="bibr">(w,c,l)</ref> is factored out of the second term, all remaining w i, j and c k, j terms are zero. A few steps of algebra then reveal the presence of a convergent geometric series. Intuitively, we can think of the second term, a a−1 e −E <ref type="bibr">(w,c,l)</ref> , as quantifying the data's need to expand the model's capacity given w and c.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Breaking sticks and ambiguities with adaptive skip-gram. International Conference on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kondrashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An infinite restricted boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on World Wide Web. WWW &apos;01</title>
		<meeting>the 10th International Conference on World Wide Web. WWW &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiprototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<title level="m">Word representations via gaussian embedding. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
