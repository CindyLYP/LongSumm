<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SimRank: A Measure of Structural-Context Similarity *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Jeh</surname></persName>
							<email>glenj@db.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
							<email>widom@db.stanford.edu</email>
						</author>
						<title level="a" type="main">SimRank: A Measure of Structural-Context Similarity *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>The problem of measuring &quot;similarity&quot; of objects arises in many applications, and many domain-specific measures have been developed, e.g., matching text across documents or computing overlap among item-sets. We propose a complementary approach, applicable in any domain with object-to-object relationships, that measures similarity of the structural context in which objects occur, based on their relationships with other objects. Effectively, we compute a measure that says &quot;two objects are similar if they are related to similar objects.&quot; This general similarity measure, called SimRank, is based on a simple and intuitive graph-theoretic model. For a given domain, Sim-Rank can be combined with other domain-specific similarity measures. We suggest techniques for efficient computation of SimRank scores, and provide experimental results on two application domains showing the computational feasibility and effectiveness of our approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many applications require a measure of "similarity" between objects. One obvious example is the "find-similar-document" query, on traditional text corpora or the World-Wide Web <ref type="bibr" target="#b0">[2]</ref>. More generally, a similarity measure can be used to cluster objects, such as for collaborative filtering in a recommender system <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b18">20]</ref>, in which "similar" users and items are grouped based on the users' preferences.</p><p>Various aspects of objects can be used to determine similarity, usually depending on the domain and the appropriate definition of similarity for that domain. In a document corpus, matching text may be used, and for collaborative filtering, similar users may be identified by common preferences. We propose a general approach that exploits the object-to-object relationships found in many domains of interest. On the Web, for example, we can say that two pages are related if there are hyperlinks between them. A similar approach can be applied to scientific papers and their citations, or to any other document corpus with cross-reference information. In the case of recommender systems, a user's preference for an item constitutes a relationship between the user and the item. Such domains are naturally modeled as graphs, with nodes representing objects and edges representing relationships. We present an algorithm for analyzing the (logical) graphs derived from such data sets to compute similarity scores between nodes (objects) based on the structural context in which they appear, a concept to be made clear shortly. The intuition behind our algorithm is that, in many domains, similar objects are related to similar objects. More precisely, objects a and b are similar if they are related to objects c and d, respectively, and c and d are themselves similar. The base case is that objects are similar to themselves.</p><p>As an example, consider the tiny Web graph G shown in <ref type="figure" target="#fig_0">Figure  1</ref>(a), representing the Web pages of two professors ProfA and ProfB, their students StudentA and StudentB, and the home page of their university Univ. Edges between nodes represent hyperlinks from one page to another. From the fact that both are referenced (linked to) by Univ, we may infer that ProfA and ProfB are similar, and some previous algorithms are based on this co-citation <ref type="bibr" target="#b19">[21]</ref> information. We generalize this idea by observing that once we have concluded similarity between ProfA and ProfB, and considering that ProfA and ProfB reference StudentA and StudentB respectively, we can also conclude that StudentA and StudentB are similar. Continuing forth, we can infer some similarity between Univ and ProfB, ProfA and StudentB, etc.</p><p>Let us logically represent the computation by using a node-pair graph G 2 , in which each node represents an ordered pair of nodes of G. A node (a, b) of G 2 points to a node (c, d) if, in G, a points to c and b points to d. A simplified view of G 2 is shown in <ref type="figure" target="#fig_0">Figure  1</ref>(b); scores will be explained shortly. As we shall see later, scores are symmetric, so for clarity in the figure we draw (a, b) and (b, a) as a single node {a, b} (with the union of their associated edges). Further simplifications in <ref type="figure" target="#fig_0">Figure 1</ref>(b) are explained in Section 4.</p><p>We run an iterative fixed-point algorithm on G <ref type="bibr" target="#b0">2</ref> to compute what we call SimRank scores for the node-pairs in G 2 . The SimRank score for a node v of G 2 gives a measure of similarity between the two nodes of G represented by v. Scores can be thought of as "flowing" from a node to its neighbors. Each iteration propagates scores one step forward along the direction of the edges, until the system stabilizes (i.e., scores converge). Since nodes of G 2 represent pairs in G, similarity is propagated from pair to pair. Under this computation, two objects are similar if they are referenced by similar objects. <ref type="bibr">1</ref> It is important to note that we are proposing a general algorithm that determines only the similarity of structural context. Our algorithm applies to any domain where there are enough relevant relationships between objects to base at least some notion of similarity on relationships. Obviously, similarity of other domain-specific aspects are important as well; these can-and should-be combined with relational structural-context similarity for an overall similarity measure. For example, for Web pages we can combine SimRank with traditional textual similarity; the same idea applies to scientific papers or other document corpora. For recommender systems, there may be built-in known similarities between items (e.g., both computers, both clothing, etc.), as well as similarities between users (e.g., same gender, same spending level). Again, these similarities can be combined with the similarity scores that we compute based on preference patterns, in order to produce an overall similarity measure.</p><p>The main contributions of this paper are as follows.</p><p>• A formal definition for SimRank similarity scoring over arbitrary graphs, several useful derivatives of SimRank, and an algorithm to compute SimRank scores (Section 4).</p><p>• A graph-theoretic model for SimRank that gives intuitive mathematical insight into its use and computation (Section 5).</p><p>• Experimental results using an initial in-memory implementation of SimRank over two different real data sets that show the effectiveness and feasibility of SimRank (Section 6).</p><p>Discussions of related work and our basic graph model are provided in Sections 2 and 3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Structural context has been used and analyzed in specific applications, such as bibliometrics, database schema-matching, and hypertext classification. The more general problem of finding similar objects has been studied in Information Retrieval and recommender systems, among other areas.</p><p>Bibliometrics studies the citation patterns of scientific papers (or other publications), and relationships between papers are inferred from their cross-citations. Most noteworthy from this field are the methods of co-citation <ref type="bibr" target="#b19">[21]</ref> and bibliographic coupling <ref type="bibr" target="#b7">[9]</ref>. In the co-citation scheme, similarity between two papers p and q is based on the number of papers which cite both p and q. In bibliographic coupling, similarity is based on the number of papers cited by both p</p><p>The recursive nature of our algorithm, and thus its name, resembles that of the PageRank algorithm, used by the Google [1] Web search engine to compute importance scores for Web pages <ref type="bibr" target="#b14">[16]</ref>. In Section 2 we discuss how PageRank and other iterative algorithms relate to our work. and q. These methods have been applied to cluster scientific papers according to topic <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b19">21]</ref>. More recently, the co-citation method has been used to cluster Web pages <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b15">17]</ref>. As discussed in Section 1, our algorithm can be thought of as a generalization of cocitation where the similarity of citing documents is also considered, recursively. In terms of graph structure, co-citation scores between any two nodes are computed only from their immediate neighbors, whereas our algorithm can use the entire graph structure to determine the similarity between any two nodes. This generalization is especially beneficial for nodes with few neighbors (e.g., documents rarely cited), a property we will discuss in Section 4.</p><p>Computing similarity recursively based on structure has also been explored in the specific context of database schema-matching <ref type="bibr" target="#b13">[15]</ref>. However, that work deals with the pairing of nodes across two graphs and relies on domain-specific metadata (e.g., node and edge labels) as well as structural relationships.</p><p>Iterative algorithms over the web graph have been used in <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b14">16]</ref> to compute "importance" scores for Web pages. Results show that the use of structure can greatly improve Web search versus text alone. The algorithms in <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b14">16]</ref> analyze individual pages with respect to the global structure, whereas our similarity measure analyzes relationships between pairs of pages.</p><p>In the classifier for Web pages presented in <ref type="bibr" target="#b3">[5]</ref>, the classification of the neighbors of a Web page p is used to improve upon the textual classification of p through a probabilistic model. In contrast, Sim-Rank computes scores for pairs of pages (instead of a single page) by comparing their neighbors. Our algorithm is not limited to discrete categories and it computes a purely structural score that is independent of domain-specific information.</p><p>Similarity of documents by textual content has been studied extensively in the field of Information Retrieval (IR) <ref type="bibr" target="#b0">[2]</ref>. As discussed in Section 1, our work addresses only similarity of structural information, and may be used in combination with textual methods.</p><p>The process of making recommendations to a user based on preference or purchase data from other users is known as collaborative filtering <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b18">20]</ref>. Many approaches to collaborative filtering rely on identifying similar users or similar items. A good overview of the techniques used, which include numerical methods like vector-cosine similarity <ref type="bibr" target="#b0">[2]</ref> and the Pearson correlation <ref type="bibr" target="#b9">[11]</ref>, can be found in <ref type="bibr" target="#b1">[3]</ref>. These methods compute similarity between sets of objects (e.g., preference lists for users, or preferred-by lists for items), whereas our algorithm deals with similarity of context in graph structures. Still other approaches take advantage of external information about the objects themselves, e.g., a hierarchy by which the items may be categorized <ref type="bibr" target="#b4">[6]</ref>. The intuitive underlying model for our similarity measure is based on "random surfers", a concept which is also used in <ref type="bibr" target="#b14">[16]</ref> to provide an intuitive model for the PageRank algorithm. For our purposes we formalize and extend the model using expected-f distances, a general graph-theoretic property that can apply in other structure-based applications, such as personalized Web search <ref type="bibr" target="#b6">[8]</ref>.</p><p>Finally, some of the graph-theoretic definitions and properties used in this work are surveyed in <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b12">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Basic Graph Model</head><p>We model objects and relationships as a directed graph G = (V, E) where nodes in V represent objects of the domain and edges in E represent relationships between objects. In Web pages or sci-entific papers, which are homogeneous domains, nodes represent documents, and a directed edge p, q from p to q corresponds to a reference (hyperlink or citation) from document p to document q. In a user-item domain, which is bipartite, we represent both users and items by nodes in V . A directed edge p, q corresponds to a purchase (or other expression of preference) of item q by person p. The result in this case is a bipartite graph, with users and items on either side. Note that edge weights may be used to represent varying degrees of preference, but currently they are not considered in our work.</p><p>For a node v in a graph, we denote by I(v) and O(v) the set of in-neighbors and out-neighbors of v, respectively. Individual inneighbors are denoted as Ii(v), for 1 ≤ i ≤ |I(v)|, and individual out-neighbors are denoted as</p><formula xml:id="formula_0">Oi(v), for 1 ≤ i ≤ |O(v)|.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SimRank</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motivation</head><p>Recall that the basic recursive intuition behind our approach is "two objects are similar if they are referenced by similar objects." As the base case, we consider an object maximally similar to itself, to which we can assign a similarity score of 1. (If other objects are known to be similar a-priori, such as from human input or text matching, their similarities can be preassigned as well.) Referring back to Figure 1, ProfA and ProfB are similar because they are both referenced by Univ (i.e., they are co-cited by Univ), and Univ is (maximally) similar to itself. Note in <ref type="figure" target="#fig_0">Figure 1</ref>(b) the similarity score of 1 on the node {Univ, Univ}, and the score of 0.414 on the node {ProfA, ProfB}. (How we obtained 0.414 will be described later.) StudentA and StudentB are similar because they are referenced by similar nodes ProfA and ProfB; notice the similarity score of 0.331 on the node for {StudentA, StudentB} in <ref type="figure" target="#fig_0">Figure 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Basic SimRank Equation</head><p>Let us denote the similarity between objects a and b by s(a, b) ∈ [0, 1]. Following our earlier motivation, we write a recursive equation for s(a, b). If a = b then s(a, b) is defined to be 1. Otherwise,</p><formula xml:id="formula_1">s(a, b) = C |I(a)||I(b)| |I(a)| i=1 |I(b)| j=1 s(Ii(a), Ij(b))<label>(1)</label></formula><p>where C is a constant between 0 and 1. A slight technicality here is that either a or b may not have any in-neighbors. Since we have no way to infer any similarity between a and b in this case, we should set s(a, b) = 0, so we define the summation in equation (1) to be 0 when I(a) = ∅ or I(b) = ∅. One SimRank equation of the form (1) is written for each (ordered) pair of objects a and b, resulting in a set of n 2 SimRank equations for a graph of size n. Let us defer discussion of the constant C for now. Equation (1) says that to compute s(a, b), we iterate over all in-neighbor pairs (Ii(a), Ij(b)) of (a, b), and sum up the similarity s(Ii(a), Ij(b)) of these pairs. Then we divide by the total number of in-neighbor pairs, |I(a)||I(b)|, to normalize. That is, the similarity between a and b is the average similarity between in-neighbors of a and in-neighbors of b. As discussed earlier, the similarity between an object and itself is defined to be 1.</p><p>In the Appendix we show that a simultaneous solution s( * , * ) ∈ [0, 1] to the n 2 SimRank equations always exists and is unique. Thus we can define the SimRank score between two objects a and b to be the solution s(a, b). From equation <ref type="formula" target="#formula_1">1</ref>, it is easy to see that SimRank scores are symmetric, i.e., s(a, b) = s(b, a).</p><p>We said in Section 1 that similarity can be thought of as "propagating" from pair to pair. To make this connection explicit, we consider the derived graph Similarity propagates in G 2 from node to node (corresponding to propagation from pair to pair in G), with the sources of similarity being the singleton nodes. Notice that cycles in G 2 , caused by the presence of cycles in G, allow similarity to flow in cycles, such as from {Univ, ProfB} back to {ProfA, ProfB} in the example. Similarity scores are thus mutually reinforced.</p><formula xml:id="formula_2">G 2 = (V 2 , E 2 ), where each node in V 2 = V × V</formula><p>Now let us consider the constant C, which can be thought of either as a confidence level or a decay factor. Consider a simple scenario where page x references both c and d, so we conclude some similarity between c and d. The similarity of x with itself is 1, but we probably don't want to conclude that s(c, d) = s(x, x) = 1. Rather, we let s(c, d) = C•s(x, x), meaning that we are less confident about the similarity between c and d than we are between x and itself. The same argument holds when two distinct pages a and b cite c and d. Viewed in terms of similarity flowing in G 2 , C gives the rate of decay (since C &lt; 1) as similarity flows across edges. In Sections and 6 we will discuss the empirical significance of C.</p><p>Though we have given motivation for the basic SimRank equation, we have yet to characterize its solution, which we take to be a measure of similarity. It would be difficult to reason about similarity scores, to adjust parameters of the algorithm (so far only C), or to recognize the domains in which SimRank would be effective, if we cannot get an intuitive feel for the computed values. Section 5 addresses this issue with an intuitive model for SimRank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Bipartite SimRank</head><p>Next we extend the basic SimRank equation (1) to bipartite domains consisting of two types of objects. We continue to use recommender systems as motivation. Suppose persons A and B purchased itemsets {eggs, frosting, sugar} and {eggs, frosting, flour} respectively. A graph of these relationships is shown in <ref type="figure" target="#fig_6">Figure 2</ref>(a). Clearly, the two buyers are similar: both are baking a cake, say, and so a good recommendation to person A might be flour. One reason we can conclude that A and B are similar is that they both purchased eggs and frosting. But moreover, A purchased sugar while B purchased flour, and these are similar items, in the sense that they are purchased by similar people: cake-bakers like A and B. Here, similarity of items and similarity of people are mutually-reinforcing notions:</p><p>• People are similar if they purchase similar items.</p><p>• Items are similar if they are purchased by similar people.</p><p>The mutually-recursive equations that formalize these notions are analogous to equation (1). Let s(A, B) denote the similarity between persons A and B, and let s(c, d) denote the similarity between items c and d. Since, as discussed in Section 3, directed edges go from people to items, for A = B we write the equation <ref type="bibr" target="#b0">(2)</ref> and for c = d we write</p><formula xml:id="formula_3">s(A, B) = C1 |O(A)||O(B)| |O(A)| i=1 |O(B)| j=1 s(Oi(A), Oj(B))</formula><formula xml:id="formula_4">s(c, d) = C2 |I(c)||I(d)| |I(c)| i=1 |I(d)| j=1 s(Ii(c), Ij(d))<label>(3)</label></formula><p>If A = B, s(A, B) = 1, and analogously for s(c, d). Neglecting C1 and C2, equation <ref type="bibr" target="#b0">(2)</ref> says that the similarity between persons A and B is the average similarity between the items they purchased, and equation <ref type="formula" target="#formula_4">3</ref>says that the similarity between items c and d is the average similarity between the people who purchased them. The constants C1, C2 have the same semantics as C in equation (1). <ref type="figure" target="#fig_6">Figure 2</ref>(b) shows the derived node-pairs graph G 2 for the graph G in <ref type="figure" target="#fig_6">Figure 2</ref>(a). Simplifications have been made to G 2 , as in <ref type="figure" target="#fig_0">Figure 1</ref>(b). Similarity scores for nodes of G 2 , computed using C1 = C2 = 0.8, are also shown. Notice how sugar and flour are similar even though they were purchased by different people, although not as similar as, say, frosting and eggs. The node {frosting, eggs} has the same score as, say, {sugar, eggs}, even though frosting and eggs have been purchased together twice, versus once for sugar and eggs, since the normalization in equations (2) and (3) says that we consider only the percentage of times that items are purchased together, not the absolute number of times. It is, however, easy to incorporate the absolute number if desired; see Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Bipartite SimRank in Homogeneous Domains</head><p>It turns out that the bipartite SimRank equations (2) and (3) can also be applied to homogeneous domains, such as Web pages and scientific papers. Although a bipartite distinction is not explicit in these domains, it may be the case that elements take on different roles (e.g., "hub" pages and "authority" pages for importance <ref type="bibr" target="#b8">[10]</ref>), or that in-references and out-references give different information. For example, two scientific papers might be similar as survey papers if they cite similar result papers, while two papers might be similar as result papers if they are cited by similar survey papers. In analogy with the HITS <ref type="bibr" target="#b8">[10]</ref> algorithm, we can associate a "points-to" similarity score s1(a, b) to each pair of nodes a and b, as well as a "pointed-to" similarity score s2(a, b), and write the same equations <ref type="bibr" target="#b0">(2)</ref> and <ref type="formula" target="#formula_4">3</ref>as if the domain were bipartite:</p><formula xml:id="formula_5">s1(a, b) = C1 |O(a)||O(b)| |O(a)| i=1 |O(b)| j=1 s2(Oi(a), Oj(b)) s2(a, b) = C2 |I(a)||I(b)| |I(a)| i=1 |I(b)| j=1 s1(Ii(a), Ij(b))</formula><p>Depending on the domain and application, either score or a combination may be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">The Minimax Variation</head><p>The basic SimRank equation (and the bipartite version) is one way to encode mathematically our recursive notion of structural-context similarity. We present another possibility here for bipartite domains, inspired by a real data set on which we experimented: finding similarity between undergraduate students and between courses based on the students' history of courses taken. Students often take groups of related courses due to curricular requirements. For example, two Computer Science (CS) majors A and B may both take a group of required CS courses, while one takes Sociology-related electives and the other English-related. In the formalization presented thus far, each of A's courses would be compared with each of B's. However, it may be meaningless to compare A's CS courses with B's electives, or vice versa, diluting the results. Instead, we are more interested in comparing A's elective choices with B's elective choices.</p><p>One approach that addresses this problem is to compare each of B's courses c with only the one course taken by A which is most similar to c. For notational convenience in writing the new equation for s(A, B), we define the intermediate terms sA <ref type="figure">(A, B)</ref> and sB(A, B) (for A = B) as follows:</p><formula xml:id="formula_6">sA(A, B) = C1 |O(A)| |O(A)| i=1 |O(B)| max j=1 s(Oi(A), Oj(B)) sB(A, B) = C1 |O(B)| |O(B)| j=1 |O(A)| max i=1 s(Oi(A), Oj(B))</formula><p>Intuitively, sA(A, B) gives a score for B's liking of the preferences of A, while sB(A, B) gives a score for A's liking of the preferences of B. Since we consider a good match to be one of common predilection, we take s(A, B) = min(sA(A, B), sB(A, B)) <ref type="bibr" target="#b2">(4)</ref> to be the similarity between students A and B, requiring that each must be interested in the other's interests (i.e., no one-sided relationships). For the similarity of courses s(c, d), equation <ref type="formula" target="#formula_4">3</ref>or a minimax variation analogous to (4) (or possibly other variations) can be used. Likewise, a minimax variation can also be applied to equation (1) for use in homogeneous domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computing SimRank</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Naive Method</head><p>A solution to the SimRank equations (or bipartite variations) for a graph G can be reached by iteration to a fixed-point. Let n be the number of nodes in G. For each iteration k, we can keep n 2 entries R k ( * , * ) of length n 2 , where R k <ref type="figure">(a, b)</ref> gives the score between a and b on iteration k. We successively compute R k+1 ( * , * ) based on R k ( * , * ). We start with R0( * , * ) where each R0(a, b) is a lower bound on the actual SimRank score s(a, b):</p><formula xml:id="formula_7">R0(a, b) = 0 (if a = b) 1 (if a = b)</formula><p>To compute R k+1 (a, b) from R k ( * , * ), we use equation <ref type="formula" target="#formula_1">1</ref>to get:</p><formula xml:id="formula_8">R k+1 (a, b) = C |I(a)||I(b)| |I(a)| i=1 |I(b)| j=1 R k (Ii(a), Ij(b)) (5)</formula><p>for a = b, and R k+1 (a, b) = 1 for a = b. That is, on each iteration k + 1, we update the similarity of (a, b) using the similarity scores of the neighbors of (a, b) from the previous iteration k according to equation (1). The values R k ( * , * ) are nondecreasing as k increases. We show in the Appendix that they converge to limits satisfying (1), the SimRank scores s( * , * ), i.e., for all a, b ∈ V , lim k→∞ R k (a, b) = s(a, b). In all of our experiments we have seen rapid convergence, with relative rankings stabilizing within 5 iterations (details are in Section 6), so we may choose to fix a number K ≈ 5 of iterations to perform.</p><p>Let us analyze the time and space requirements for this method of computing SimRank. The space required is simply O(n 2 ) to store the results R k . Let d2 be the average of |I(a)||I(b)| over all nodepairs (a, b). The time required is O(Kn 2 d2), since on each iteration, the score of every node-pair (n 2 of these) is updated with values from its in-neighbor pairs (d2 of these on average). As it corresponds roughly to the square of the average in-degree, d2 is likely to be a constant with respect to n for many domains. The resource requirements for bipartite versions are similar.</p><p>We mentioned that typically K ≈ 5, and in most cases we also expect the average in-degree to be relatively small. However, n 2 can be prohibitively large in some applications, such as the Web, where it exceeds the size of main memory. Specialized disk layout and indexing techniques may be needed in this case; such techniques are beyond the scope of this paper. However, in the next subsection we do briefly consider pruning techniques that reduce both the time and space requirements. Pruning has allowed us to run our experiments entirely in main memory, without the need for disk-based techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Pruning</head><p>One way to reduce the resource requirements is to prune the logical graph G 2 . So far we have assumed that all n 2 node-pairs of G 2 are considered, and a similarity score is computed for every node-pair. When n is significantly large, it is very likely that the neighborhood (say, nodes within a radius of 2 or 3) of a typical node will be a very small percentage (&lt; 1%) of the entire domain. Nodes far from a node v, whose neighborhood has little overlap with that of v, will tend to have lower similarity scores with v than nodes near v, an effect that will become intuitive in Section 5. Thus one pruning technique is to set the similarity between two nodes far apart to be 0, and consider node-pairs only for nodes which are near each other. If we consider only node-pairs within a radius of r from each other in the underlying undirected graph (other criteria are possible), and there are on average dr such neighbors for a node, then there will be ndr node-pairs. The time and space complexities become O(Kndrd2) and O(ndr) respectively, where d2 is the average of |I(a)||I(b)| for pages a, b close enough to each other. Since dr is likely to be much less than n and constant with respect to n for many types of data, we can think of the approximate algorithm as being linear with a possibly large constant factor.</p><p>Of course, the quality of the approximation needs to be verified experimentally for the actual data sets. For the case of scientific papers, our empirical results suggest that this is a good approximation strategy, and allows the computation to be carried out entirely in main memory for a corpus of n = 278, 626 objects. More details can be found in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Limited-Information Problem</head><p>In document corpora, there may be many "unpopular" documents, i.e., documents with very few in-citations. Although the scarcity of contextual information makes them difficult to analyze, these documents are often the most important, since they tend to be harder for humans to find. This is especially true for new documents, which are likely unpopular because it takes time for others to notice and cite them, but often we are most interested in new documents. Unlike the simple co-citation scheme, SimRank can effectively analyze documents with little contextual information.</p><p>The intermediate scores R1( * , * ) resulting from the first iteration of the basic SimRank algorithm (Section 4.4.1) are essentially weighted co-citation scores. To be precise, R1(a, b) gives the number of documents that cite both a and b, divided by the product of their in-degrees; |I(a)||I(b)|R1(a, b) is exactly the co-citation score between a and b. Successive iterations can be thought of as improving upon these scores, especially in the limited-information case. We demonstrate with an example.</p><p>Suppose we are trying to answer a "find-similar-document" query for document A, and A is cited by only one other document B, which also cites A1, . . . , Am. The situation is shown in <ref type="figure" target="#fig_4">Figure 3</ref>. Under the co-citation scheme, any of A1, . . . , Am appears equally similar to A. In reality, we expect that not all citations from B are equal. There may be some "outlier" documents among A1, . . . , Am which have relatively little to do with B or the rest of the group. In our algorithm, the citers of each of the pages A1, . . . , Am are also taken into account, and they affect the similarity scores between A and each of A1, . . . , Am. Those documents Ai which are cited by other documents similar to B will have higher similarity to A. In the figure, Am is shown as a better match for A than A1, since Am's other citer is B , which is similar to B.</p><p>The example demonstrates the case where we are interested in documents similar to document A about which there is little information. We can also consider the complementary case where we are interested in a general document C, and ask whether A should be included on a list of documents most similar to C. In our example A has only one in-citation, and it may be the case that this is an "outlier" citation. It would be safer to consider only documents for which we have more information. On the other hand, we don't want to eliminate unpopular documents from consideration or popular documents to be favored for every query. If we eliminated the constant factor 1 |I(b)| from equation <ref type="formula" target="#formula_1">1</ref>, then documents b with a very high popularity would have a high similarity score with any other document a. As a compromise, we can weigh the final results of the algorithm by popularity, using the asymmetric formula</p><formula xml:id="formula_9">sP (a, b) = s(a, b) • |I(b)| P<label>(6)</label></formula><p>where the constant P ∈ (0, 1) is a parameter adjustable by the end user. In Section 6 we discuss experimentation with this weighting scheme.</p><p>Note that although we have used documents as examples of unpopular objects, the same ideas apply in other domains, such as to items rarely purchased, courses rarely taken, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Random Surfer-Pairs Model</head><p>As discussed in Section 4.2, it is important to have an intuition for the similarity scores produced by the algorithm. For this we provide an intuitive model based on "random surfers". (Readers not interested in underlying models may proceed directly to Section on experimental results.) We will show that the SimRank score s(a, b) measures how soon two random surfers are expected to meet at the same node if they started at nodes a and b and randomly walked the graph backwards. The details involve some complexity, and are developed in the remainder of this section. The model is presented in the context of general directed graphs; variations for bipartite Sim-Rank (Section 4.3) are easy to derive and we leave them to the interested reader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Expected Distance</head><p>Let H be any strongly connected graph (in which a path exists between every two nodes). Let u, v be any two nodes in H. We define </p><formula xml:id="formula_10">d(u, v) = t:u v P [t]l(t)<label>(7)</label></formula><p>The summation is taken over all tours t (paths that may have cycles) which start at u and end at v, and do not touch v except at the end. For a tour t = w1, . . . , w k , the length l(t) of t is k−1, the number of edges in t. The probability</p><formula xml:id="formula_11">P [t] of traveling t is k−1 i=1 1 |O(w i )|</formula><p>, or 1 if l(t) = 0. Note that the case where u = v, for which d(u, v) = 0, is a special case of (7): only one tour is in the summation, and it has length 0. Because of the presence of cycles, there are infinitely many tours from u to v, and <ref type="formula" target="#formula_10">7</ref>is an (convergent) infinite sum. The expected distance from u to v is exactly the expected number of steps a random surfer, who at each step follows a random out-edge, would take before he first reaches v, starting from u.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Expected Meeting Distance</head><p>For our model, we extend the concept of expected distance to expected meeting distance (EMD). Intuitively, the expected meeting distance m(a, b) between a and b is the expected number of steps required before two surfers, one starting at a and the other at b, would meet if they walked (randomly) in lock-step. The EMD is symmetric by definition. Before formalizing EMD, let us consider a few examples. The EMD between any two distinct nodes in <ref type="figure" target="#fig_5">Figure 4</ref>(a) is (informally) ∞, since two surfers walking the loop in lock-step will follow each other forever. In <ref type="figure" target="#fig_5">Figure 4</ref>(b), m(u, v) = m(u, w) = ∞ (surfers will never meet) and m(v, w) = 1 (surfers meet on the next step), suggesting that v and w are much more similar to each other than u is to v or w. Between two distinct nodes of 4(c), the EMD is 3, suggesting a lower similarity than between v and w in 4(b), but higher than between u and v (or u and w).</p><p>To define EMD formally in G, we use the derived graph </p><formula xml:id="formula_12">P [t]l(t)<label>(8)</label></formula><p>In the literature this quantity, in undirected graphs, is known as the hitting time <ref type="bibr" target="#b12">[14]</ref>, but we will develop the idea differently and so choose to use another name for our presentation.</p><p>The sum is taken over all tours t starting from (a, b) which touch a singleton node at the end and only at the end. Unfortunately, G 2 may not always be strongly connected (even if G is), and in such cases there may be no tours t for (a, b) in the summation <ref type="bibr" target="#b6">(8)</ref>. The intuitive definition for m(a, b) in this case is ∞, as in <ref type="figure" target="#fig_5">Figure 4</ref>(b), discussed above. However, this definition would cause problems in defining distances for nodes from which some tours lead to singleton nodes while others lead to (a, b). We discuss a solution to this problem in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Expected-f Meeting Distance</head><p>There are various ways to circumvent the "infinite EMD" problem discussed in the previous section. For example, we can make each surfer "teleport" with a small probability to a random node in the graph (the solution suggested for PageRank in <ref type="bibr" target="#b14">[16]</ref>). Our approach, which as we will see yields equations equivalent to the SimRank equations, is to map all distances to a finite interval: instead of computing expected length l(t) of a tour, we can compute the expected f (l(t)), for a nonnegative, monotonic function f which is bounded on the domain [0, ∞). With this replacement we get the expectedf meeting distance. For our purposes, we choose the exponential function f (z) = c z , where c ∈ (0, 1) is a constant. The benefits of this choice of f , which has values in the range (0, 1] over domain [0, ∞), are:</p><p>• Equations generated are simple and easy to solve.</p><p>• Closer nodes have a lower score (meeting distances of go to 1 and distances of ∞ go to 0), matching our intuition of similarity.</p><p>We define s (a, b), the similarity between a and b in G based on expected-f meeting distance, as</p><formula xml:id="formula_13">s (a, b) = t:(a,b) (x,x) P [t]c l(t)<label>(9)</label></formula><p>where c is a constant in (0, 1). The summation is taken to be 0 if there is no tour from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Equivalence to SimRank</head><p>We now show that s ( * , * ) exactly models our original definition of SimRank scores by showing that s ( * , * ) satisfies the SimRank equations (1). To ease presentation, let us assume that all edges in our graph G have been reversed, so following an edge is equivalent to moving one step backwards in the original graph. <ref type="bibr" target="#b1">3</ref> First, to aid in understanding, we give an intuitive but informal argument about the expected distance d(u, v) in a graph; the same ideas can be applied to the expected-f meeting distance. Suppose a surfer is at u ∈ V . At the next time step, he chooses one of O1(u), . . . , O |O(u)| (u), each with probability 1 |O(u)| . Upon</p><p>Had we written equation (1) in terms of out-neighbors instead of inneighbors, as may be appropriate in some domains, this step would not be necessary.</p><p>choosing Oi(u), the expected number of steps he will still have to travel is d(Oi(u), v) (the base case is when Oi(u) = v, for which d(Oi(u), v) = 0). Accounting for the step he travels to get to Oi(u), we get:</p><formula xml:id="formula_14">d(u, v) = 1 + 1 |O(u)| |O(u)| i=1 d(Oi(u), v)</formula><p>With this intuition in mind, we derive similar recursive equations for s (a, b) which will show that s (a, b) = s(a, b). If a = b then s (a, b) = s(a, b) = 1. If there is no path in G 2 from (a, b) to any singleton nodes, in which case s (a, b) = 0, it is easy to see from equation <ref type="formula">5</ref>that s(a, b) = 0 as well, since no similarity would flow to (a, b) (recall that edges have been reversed). Otherwise, consider the tours t from (a, b) to a singleton node in which the first step is to the out-neighbor <ref type="figure">Oz((a, b)</ref>). There is a one-to-one correspondence between such t and tours t from <ref type="figure">Oz((a, b)</ref>) to a singleton node: for each t we may derive a corresponding t by appending the edge (a, b), <ref type="figure">Oz((a, b)</ref>) at the beginning. Let T be the bijection that takes each t to the corresponding t. If the length of t is l, then the length of t = T (t ) is l + 1. Moreover, the probability of traveling t is</p><formula xml:id="formula_15">P [t] = 1 |O((a,b))| P [t ] = 1 |O(a)||O(b)| P [t ].</formula><p>We can now split the sum in (9) according to the first step of the tour t to write</p><formula xml:id="formula_16">s (a, b) = |O((a,b))| z=1 t : Oz ((a,b)) (x,x) P [T (t )]c l(T (t )) = |O((a,b))| z=1 t : Oz ((a,b)) (x,x) 1 |O(a)||O(b)| P [t ]c l(t )+1 = c |O(a)||O(b)| |O((a,b))| z=1 t : Oz ((a,b)) (x,x) P [t ]c l(t) = c |O(a)||O(b)| |O(a)| i=1 |O(b)| j=1 s (Oi(a), Oj(b))<label>(10)</label></formula><p>Equation <ref type="formula" target="#formula_16">10</ref>is identical to the SimRank equation (1) with c = C and in-edges swapped for out-edges. Since the solution to (1) is unique, s (a, b) = s(a, b) for all a, b ∈ V . Thus we have the following theorem.</p><p>Theorem. The SimRank score, with parameter C, between two nodes is their expected-f meeting distance traveling back-edges, for f (z) = C z .</p><p>Thus, two nodes with a high SimRank score can be thought of as being "close" to a common "source" of similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>We have proposed an algorithm for computing SimRank similarity scores between nodes of a graph, a mathematical property that depends only on the graph structure and can be computed in any graph. In this section, we report on some preliminary experiments, whose primary purpose is to show that SimRank scores do in fact refine simpler notions of structural similarity for graph structures derived from practical data sets. The experiments also illustrate the effects of varying the parameters of the algorithm. Although performance and scalability issues obviously are extremely important, they are not the focus of this paper or of our experiments. Nevertheless, the fact that we were able to run experiments on relatively large data sets shows the general feasibility of our approach. We ran experiments on two data sets. The first is a corpus of scientific research papers from ResearchIndex (http://www.researchindex.com) <ref type="bibr" target="#b11">[13]</ref>, which crawls the Web for research papers and parses their contents for citation information and other metadata. We had information on 688,898 crossreferences among 278,628 papers, along with the titles for most papers. The second data set comes from the transcripts of 1030 undergraduate students in the School of Engineering at Stanford University. Each transcript lists all the courses that the student has taken so far in his undergraduate career, an average of about 40 courses per student.</p><p>The feature that distinguishes SimRank is its recursive definition of similarity, which computationally is manifested in the fixed-point iteration process (Section 4.4.1). Thus we should expect to see that the intermediate similarity scores R k ( * , * ) become more "accurate" on successive iterations. We can also compare our algorithm against the simple co-citation scheme, which as discussed in Section 4.5 is similar to using just one iteration of SimRank.</p><p>A good evaluation of SimRank or any other method of measuring similarity in any domain is difficult without performing extensive user studies or having a reliable external measure of similarity to compare against. For the results reported in this paper, we take a simple approach that uses domain-specific properties as rough metrics of similarity. Although admittedly not definitive or exhaustive, this approach does serve to illustrate empirically important aspects of SimRank.</p><p>We consider objects p in our domain of interest, generating lists of objects similar to p. The procedure for evaluating the similarity scores for an object p by algorithm A is as follows:</p><p>1. Generate a set top A,N (p) of the top N objects most similar to p (except p itself), according to algorithm A.</p><p>2. For each q ∈ top A,N (p), compute σ(p, q), where σ is a coarse domain-specific similarity measure. Return the average σA,N (p) of these scores.</p><p>The number σA,N (p) gives the average "actual" similarity to p of the top N objects that algorithm A decides are similar to p. Different values of N were tried in our experiments, ranging from 5 to 50, in increments of 5. Since only the set of co-cited objects c(p) ever appear as candidates in the simple co-citation scheme, for proper comparison we restricted our experiments to those objects p for which co-citation had at least 50 candidates to consider, or |c(p)| ≥ 50. As a baseline, let σR,N (p) be the average of σ(p, q) for N objects q randomly chosen from c(p). We measure the performance of algorithm A on object p using the difference δA,N (p) = σA,N (p) − σR,N (p), which is the amount of "improvement" of A over a random assignment of similarity. The average of δA,N (p) over all p, ∆A,N , is the final score for algorithm A. In our experiments, algorithm A is either SimRank or the simple co-citation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Scientific Papers</head><p>For scientific papers, we based our external similarity metric σ on citations and titles. Similar papers should be more likely to cite common papers and have common words in their titles. More precisely, we used the evaluation functions σ C (p, q) = fraction of q's citations also cited by p σ T (p, q) = fraction of words in q's title also in p's title where title words were first stemmed using the standard Porter algorithm <ref type="bibr" target="#b17">[19]</ref>. At first one might question the use of citations to evaluate algorithms that are themselves based on citation structure. To be fair, we ran the bipartite variant (Section 4.3.1) of the SimRank algorithm and used only the pointed-to similarity scores, which for papers p and q are computed using only their in-citations.</p><p>We experimented with various values for the parameters C1 and C2 in equations (2) and (3), and found little difference in the rankings, although there were differences in absolute magnitudes of scores. This behavior is to be expected since C1 and C2 can be viewed as the bases of exponential functions whose only purpose is to map distances to finite intervals (Section 5.3). For efficiency, we pruned considerably, creating node-pairs only for nodes sharing  a common in-edge for pointed-to similarity calculations, or sharing a common out-edge for points-to similarity calculations (Section 4.4.2). Our experiments showed little difference in relative rankings as we increased the radius of consideration for node-pair creation, which confirms that this is a good approximation to make. We tried different values for P (Section 4.5), and found that P = 0.5 seems to be the best setting for this corpus. Results here are shown for C1 = C2 = 0.8 and P = 0.5. Iteration 2, which computes R2( * , * ) based on iteration-1 scores R1( * , * ), can be thought of as the first iteration that takes advantage of the recursive definition of similarity. Subsequent changes become increasingly minor, suggesting a rapid convergence, at least in terms of the two metrics. This result is in agreement with the numerical differences a,b |R k+1 (a, b) − R k (a, b)| that we observed. <ref type="figure" target="#fig_10">Figure 6</ref> plots the final scores ∆ C A,N (left) and ∆ T A,N (right) for different values of N . The value of N is on the x-axis and the score is on the y-axis. Across all N , the average improvement of SimRank over simple co-citation under these coarse evaluation measures is about 45% (citations) and 36% (titles). The downward curves show a decrease in score (for both algorithms) as N increases, which is expected since higher-ranking papers are more similar. We note that the high resemblance between the plots of the citation-based and title-based metrics confirms the appropriateness of these metrics as indicators of "actual" similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Students and Courses</head><p>For our second data set, students and courses, we used an external similarity metric for courses only, not having any reasonable metric for students. The external course-similarity metric is based on departments: σ D (p, q) = 1 if p and q are courses from the same  department, and σ D (p, q) = 0 otherwise. We found that for this bipartite domain, the minimax variation of SimRank (Section 4.3.2) performs best. Again, we found that the parameters C1 and C2 have little effect on relative rankings. The results shown here are for the minimax variation (used for both students and courses) with parameters C1 = C2 = 0.8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>To summarize the contributions of this paper:</p><p>• We started with the basic premise that in many domains with object-to-object relationships, "structural-context similarity" be-tween two objects can be inferred by considering recursively the similarity of their neighbors. This approach generalizes previous approaches that compute similarity by common neighbors alone.</p><p>• We wrote mathematical equations to formalize our recursive notion of structural-context similarity, and defined SimRank scores in terms of these equations. We also presented variations of Sim-Rank that are applicable to different domains.</p><p>• We presented a fixed-point algorithm for computing SimRank scores, as well as methods to reduce its time and space requirements.</p><p>• We defined a "random-surfer" model by which to interpret solutions to the SimRank equations, relating SimRank scores to intuitive graph-theoretic properties. The model is based on the concept of expected meeting distance (EMD).</p><p>• We ran experiments on two representative data sets. Results confirm the applicability of the algorithm in these domains, showing significant improvement over simpler co-citation measures.</p><p>There are a number of avenues for future work. Foremost, we must address efficiency and scalability issues, including additional pruning heuristics and disk-based algorithms. One possible approximation that differs from the neighborhood-based pruning heuristic in Section 4.4.2 is to divide a corpus into chunks, computing accurate similarity scores separately for each chunk and then combining them into a global solution. A second area of future work is to consider ternary (or more) relationships in computing structural-context similarity. For example, in the student-course domain we might also include the professors who taught the courses and the grades received by the students. Extending our entire framework to encompass such relationships should be possible, but it is not straightforward. Finally, we believe that structural-context similarity is only one component of similarity in most domains, so we plan to explore the combination of SimRank with other domain-specific similarity measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We prove the existence and uniqueness of a simultaneous solution to the n 2 SimRank equations (1). The unique solution is actually constructed in Section 4.4.1, and the correctness of the iterative algorithm follows. First, a simple fact about the values R k ( * , * ), which follows by induction using equation <ref type="formula">5</ref> Since 0 &lt; C &lt; 1, it follows that M = 0.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A small Web graph G and simplified node-pairs graph G 2 . SimRank scores using parameter C = 0.8 are shown for nodes in G 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b). In Section 4.2 we state and justify the basic equation that formalizes SimRank as motivated above. Section 4.3 modifies the equation for bipartite graphs, such as graphs for recommender systems as discussed in Section 3. The actual computation of SimRank values is discussed in Section 4.4, including pruning techniques to make the algorithm more efficient. Finally, Section 4.5 discusses the benefits of SimRank in scenarios where information is limited.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>represents a pair (a, b) of nodes in G, and an edge from (a, b) to (c, d) exists in E 2 iff the edges a, c and b, d exist in G. Figure 1(b) shows a simplified version of the derived graph G 2 for the graph G in Figure 1(a), along with similarity scores computed using C = 0.8. As mentioned earlier, we have drawn the symmetric pairs (a, b) and (b, a) as a single node {a, b}. Two types of nodes are omitted from the figure. The first are those singleton nodes which have no effect on the similarity of other nodes, such as {ProfA, ProfA}. The second are the nodes with 0 similarity, such as {ProfA, StudentA}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 GFigure 2 :</head><label>22</label><figDesc>Shopping graph G and a simplified version of the derived node-pairs graph G 2 . Bipartite SimRank scores are shown for G 2 using C 1 = C 2 = 0.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Little information is available for A, which is cited only by B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Sample graph structures. the expected distance 2 d(u, v) from u to v as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>G 2</head><label>2</label><figDesc>of node-pairs. Each node (a, b) of V 2 can be thought of as the present state of a pair of surfers in V , where an edge from (a, b) to (c, d) in G 2 says that in the original graph G, one surfer can move from a to c while the other moves from b to d. A tour in G 2 of length n represents a pair of tours in G also having length n. The EMD m(a, b) is simply the expected distance in G 2 from (a, b) to any singleton node (x, x) ∈ V 2 , since singleton nodes in G 2 represent states where both surfers are at the same node. More precisely, m(a, b) = t:(a,b) (x,x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a, b) to any singleton nodes. Note from (9) that s (a, b) ∈ [0, 1] for all a, b, and that s (a, b) = 1 if a = b. Let us consider these similarity scores on Figure using C = 0.8 as an example. Between any two distinct nodes a, b in Figure 4(a), s (a, b) = 0. In Figure 4(b), s (v, w) = 0.8 while s (u, v) = s (u, w) = 0. For any two distinct nodes in the complete graph of Figure 4(c), s (a, b) ≈ 0.47, a lower score than between v and w in Figure 4(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>SimRank and co-citation on scientific papers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>SimRank and co-citation on scientific papers for varying N.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5</head><label>5</label><figDesc>plots the scores ∆ C A,N (left) and ∆ T A,N (right) for N = 5 and N = 10, over a total of 13,481 objects p, for various intermediate SimRank values R k ( * , * ) computed after k iterations. The iteration number k is on the x-axis, and the scores ∆A,N are on the y-axis. The co-citation scores are also shown for comparison. Scores for the initial vector R0( * , * ) are not shown because they provide random rankings and are equivalent to the random baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>SimRank on courses for increasing iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 7</head><label>7</label><figDesc>plots the scores ∆ D A,N using N = 5 and N = 10 for intermediate results R k ( * , * ), over a total of 3,193 trials. Co-citation scores, which are very poor (≈ 0.161 for N = 5 and ≈ 0.147 for N = 10), are not shown in the graph. Scores corresponding to the first-iteration results R1( * , * ) are also plotted across the x-axis to show the improvements of successive iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>:</head><label></label><figDesc>Fact. Monotonicity: 0 ≤ R k (a, b) ≤ R k+1 (a, b) ≤ 1 for all a, b ∈ V , k ≥ 0.This says for every a, b, the sequence {R k (a, b)} is bounded and nondecreasing as k increases without bound. By the Completeness Axiom of calculus, each sequence {R k (a, b)} converges to a limit R(a, b) ∈ [0, 1]. But lim k→∞ R k+1 (a, b) = lim k→∞ R k (a, b) = R(a, b), and the limit of a sum is the sum of the limits, so we have from(5): i (a), I j (b)) (if a = b) 1 (if a = b) which shows that the limits R( * , * ) satisfy the SimRank equations. Now we show uniqueness. Suppose s 1 ( * , * ) and s 2 ( * , * ) are two solutions to the n 2 SimRank equations. For all a, b ∈ V , let δ(a, b) = s 1 (a, b) − s 2 (a, b) be their difference. Let M = max (a,b) |δ(a, b)| be the maximum absolute value of any difference. We need to show that M = 0. Let |δ(a, b)| = M for some a, b ∈ V . Certainly M = 0 if a = b, in which case s 1 (a, b) = s 2 (a, b) = 1, or if a or b have no out-neighbors, in which case s 1 (a, b) = s 2 (a, b) = 0. Otherwise, s 1 (a, b) and s 2 (a, b) are the average scores of their in-neighbors. That is, from equation(1),s 1 (a, b) = C |I(a)||I(b)| |I(a)| i=1 |I(b)| j=1 s 1 (I i (a), I j (b)) s 2 (a, b) = C |I(a)||I(b)| |I(a)| i=1 |I(b)| j=1 s 2 (I i (a), I j (b))In terms of δ(a, b), δ(a, b) = s 1 (a, b) − s 2 (a, b) I i (a), I j (b)) − s 2 (I i (a), I j (b)) = C |I(a)||I(b)| |I(a)| i=1 |I(b)| j=1 δ(I i (a), I j (b)) Thus, M = |δ(a, b)| i (a), I j (b)) )||I(b)| |I(a)||I(b)|M = CM</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Modern Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Addison Wesley</publisher>
			<pubPlace>Reading, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Empirical analysis of predictive algorithms for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Breese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 14th Conference on Uncertainty in Artificial Intelligence<address><addrLine>Madison, Wisconsin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meeting times of random walks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nader</forename><surname>Hanna Bshouty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Higham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jolanta</forename><surname>Warpechowska-Gruca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="259" to="265" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced hypertext categorization using hyperlinks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD International Conference on Management of Data</title>
		<meeting>ACM SIGMOD International Conference on Management of Data<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploiting hierarchical domain structure to compute similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
		<ptr target="http://dbpubs.stanford.edu/pub/2001-26" />
		<imprint>
			<date type="published" when="2001-06" />
		</imprint>
		<respStmt>
			<orgName>Stanford University Database Group</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using collaborative filtering to weave an information tapestry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Oki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="61" to="70" />
			<date type="published" when="1992-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Scaling personalized web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
		<ptr target="http://www-db.stanford.edu/˜glenj/spws.pdf" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Stanford University Database Group</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bibliographic coupling between scientific papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kessler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
			<publisher>American Documentation</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="10" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Authoritative sources in a hyperlinked environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Ninth Annual ACM-SIAM Symposium on Discrete Algorithms<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Grou-pLens: Applying collaborative filtering to Usenet news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><forename type="middle">N</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><forename type="middle">R</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="1997-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bibliometrics of the World-Wide Web: An exploratory analysis of the intellectual structure of cyberspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the American Society for Information Science</title>
		<meeting>the Annual Meeting of the American Society for Information Science<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Indexing and retrieval of scientific literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Information and Knowledge Management</title>
		<meeting>the Eighth International Conference on Information and Knowledge Management<address><addrLine>Kansas City, Missouri</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Walks on Graphs: A Survey</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="1993" />
			<publisher>Bolyai Society Mathematical Studies</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Similarity flooding: A versatile graph matching algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Melnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Rahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighteenth International Conference on Data Engineering</title>
		<meeting>Eighteenth International Conference on Data Engineering<address><addrLine>San Jose, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<ptr target="http://citeseer.nj.nec.com/368196.html" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Stanford University Database Group</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Life, death, and lawfulness on the electronic frontier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Pitkow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pirolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Factors in Computing Systems</title>
		<meeting>the Conference on Human Factors in Computing Systems<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clustering and identifying temporal trends in document databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandrin</forename><surname>Popescul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Flake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Advances in Digital Libraries</title>
		<meeting>the IEEE Advances in Digital Libraries<address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Program</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="130" to="137" />
			<date type="published" when="1980-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social information filtering: Algorithms for automating &quot;word of mouth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upendra</forename><surname>Shardanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pattie</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Factors in Computing Systems</title>
		<meeting>the Conference on Human Factors in Computing Systems<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Co-citation in the scientific literature: A new measure of the relationship between two documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="265" to="269" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
