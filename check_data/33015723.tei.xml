<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning To Count Objects in Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Visual Geometry Group University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning To Count Objects in Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object). Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efficiently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization. The proposed framework is very flexible as it can accept any domain-specific visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The counting problem is the estimation of the number of objects in a still image or video frame. It arises in many real-world applications including cell counting in microscopic images, monitoring crowds in surveillance systems, and performing wildlife census or counting the number of trees in an aerial image of a forest.</p><p>We take a supervised learning approach to this problem, and so require a set of training images with annotation. The question is what level of annotation is required? Arguably, the bare minimum of annotation is to provide the overall count of objects in each training image. This paper focusses on the next level of annotation which is to specify the object position by putting a single dot on each object instance in each image. <ref type="figure" target="#fig_0">Figure 1</ref> gives examples of the counting problems and the dotted annotation we consider. Dotting (pointing) is the natural way to count objects for humans, at least when the number of objects is large. It may be argued therefore that providing dotted annotations for the training images is no harder for a human than giving just the raw counts. On the other hand, a spatial arrangement of the dots provides a wealth of additional information, and this paper is, in part, about how to exploit this "free lunch" (in the context of the counting problem). Overall, it should be noted that dotted annotation is less labourintensive than the bounding-box annotation, let alone pixel-accurate annotation, traditionally used by the supervised methods in the computer vision community <ref type="bibr" target="#b13">[15]</ref>. Therefore, the dotted annotation represents an interesting and, perhaps, under-investigated case.</p><p>This paper develops a simple and general discriminative learning-based framework for counting objects in images. Similar to global regression methods (see below), it also evades the hard problem of detecting all object instances in the images. However, unlike such methods, the approach also takes full and extensive use of the spatial information contained in the dotted supervision.</p><p>The high-level idea of our approach is extremely simple: given an image I, our goal is to recover a density function F as a real function of pixels in this image. Our notion of density function loosely  <ref type="bibr" target="#b27">[29]</ref>), right -counting people in a surveillance video frame (from <ref type="bibr" target="#b8">[10]</ref>). Close-ups are shown alongside the images. The bottom close-ups show examples of the dotted annotations (crosses). Our framework learns to estimate the number of objects in the previously unseen images based on a set of training images of the same kind augmented with dotted annotations.</p><p>corresponds to the physical notion of density as well as to the mathematical notion of measure. Given the estimate F of the density function and the query about the number of objects in the entire image I, the number of objects in the image is estimated by integrating F over the entire I. Furthermore, integrating the density over an image subregion S ⊂ I gives an estimate of the count of objects in that subregion.</p><p>Our approach assumes that each pixel p in an image is represented by a feature vector x p and models the density function as a linear transformation of x p : F (p) = w T x p . Given a set of training images, the parameter vector w is learnt in the regularized risk framework, so that the density function estimates for the training images matches the ground truth densities inferred from the user annotations (under regularization on w).</p><p>The key conceptual difficulty with the density function is the discrete nature of both image observations (pixel grid) and, in particular, the user training annotation (sparse set of dots). As a result, while it is easy to reason about average densities over the extended image regions (e.g. the whole image), the notion of density is not well-defined at a pixel level. Thus, given a set of dotted annotation there is no trivial answer to the question: what should be the ground truth density for this training example. Consequently, this local ambiguity also renders standard pixel-based distances between density functions inappropriate for the regularized risk framework.</p><p>Our main contribution, addressing this conceptual difficulty, is a specific distance metric D between density functions used as a loss in our framework, which we call the MESA distance (where MESA stands for Maximum Excess over SubArrays, as well as for the geological term for the elevated plateau). This distance possess two highly desirable properties:</p><p>1. Robustness. The MESA distance is robust to the additive local perturbations of its arguments such as independent noise or high-frequency signal as long as the integrals (counts) of these perturbations over larger region are close to zero. Thus, it does not matter much how exactly we define the ground truth density locally, as long as the integrals of the ground truth density over the larger regions reflect the counts correctly. We can then naturally define the "ground truth" density for a dotted annotation to be a sum of normalized gaussians centered at the dots.</p><p>2. Computability. The MESA distance can be computed exactly via an efficient combinatorial algorithm (maximum sub-array <ref type="bibr" target="#b6">[8]</ref>). Plugging it into the regularized risk framework then leads to a convex quadratic program for estimating w. While this program has a combinatorial number of linear constraints, the cutting-plane procedure finds the close approximation to the globally optimal w after a small number of iterations.</p><p>The proposed approach is highly versatile. As virtually no assumptions is made about the features x p , our framework can benefit from much of the research on good features for object detection. Thus, the confidence maps produced by object detectors or the scene explanations resulting from fitting the generative models can be turned into features and used by our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work.</head><p>A number of approaches tackle counting problems in an unsupervised way, performing grouping based on self-similarities <ref type="bibr" target="#b1">[3]</ref> or motion similarities <ref type="bibr" target="#b25">[27]</ref>. However, the counting accuracy of such fully unsupervised methods is limited, and therefore others considered approaches based on supervised learning. Those fall into two categories:  Counting by detection: This assumes the use of a visual object detector, that localizes individual object instances in the image. Given the localizations of all instances, counting becomes trivial. However, object detection is very far from being solved <ref type="bibr" target="#b13">[15]</ref>, especially for overlapping instances. In particular, most current object detectors operate in two stages: first producing a real-valued confidence map; and second, given such a map, a further thresholding and non-maximum suppression steps are needed to locate peaks correspoinding to individual instances <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b24">26]</ref>. More generative approaches avoid nonmaximum suppression by reasoning about relations between object parts and instances <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b32">34]</ref>, but they are still geared towards a situation with a small number of objects in images and require time-consuming inference. Alternatively, several methods assume that objects tend to be uniform and disconnected from each other by the distinct background color, so that it is possible to localize individual instances via a Monte-Carlo process <ref type="bibr" target="#b11">[13]</ref>, morphological analysis <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b27">29]</ref> or variational optimization <ref type="bibr" target="#b23">[25]</ref>. Methods in these groups deliver accurate counts when their underlying assumptions are met but are not applicable in more challenging situations.</p><p>Counting by regression: These methods avoid solving the hard detection problem. Instead, a direct mapping from some global image characteristics (mainly histograms of various features) to the number of objects is learned. Such a standard regression problem can be addressed by a multitude of machine learning tools (e.g. neural networks <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b20">22]</ref>). This approach however has to discard any available information about the location of the objects (dots), using only its 1-dimensional statistics (total number) for learning. As a result, a large number of training images with the supplied counts needs to be provided during training. Finally, counting by segmentation methods <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b26">28]</ref> can be regarded as hybrids of counting-by-detection and counting-by-regression approaches. They segment the objects into separate clusters and then regress from the global properties of each cluster to the overall number of objects in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Framework</head><p>We now provide the detailed description of our framework starting with the description of the learning setting and notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning to Count</head><p>We assume that a set of N training images (pixel grids) I 1 , I 2 , . . . I N is given. It is also assumed that each pixel p in each image I i is associated with a real-valued feature vector x i p ∈ R K . We give the examples of the particular choices of the feature vectors in the experimental section. It is finally assumed that each training image I i is annotated with a set of 2D points P i = {P 1 , . . . , P C(i) }, where C(i) is the total number of objects annotated by the user.</p><p>The density functions in our approaches are real-valued functions over pixel grids, whose integrals over image regions should match the object counts. For a training image I i , we define the ground truth density function to be a kernel density estimate based on the provided points:</p><formula xml:id="formula_0">∀p ∈ I i , F 0 i (p) = P ∈Pi N (p; P, σ 2 1 2×2 ) .<label>(1)</label></formula><p>Here, p denotes a pixel, N (p; P, σ 2 1 2×2 ) denotes a normalized 2D Gaussian kernel evaluated at p, with the mean at the user-placed dot P , and an isotropic covariance matrix with σ being a small value (typically, a few pixels). With this definition, the sum of the ground truth density p∈Ii F 0 i (p) over the entire image will not match the dot count C i exactly, as dots that lie very close to the image boundary result in their Gaussian probability mass being partly outside the image. This is a natural and desirable behaviour for most applications, as in many cases an object that lies partly outside the image boundary should not be counted as a full object, but rather as a fraction of an object.</p><p>Given a set of training images together with their ground truth densities, we aim to learn the linear transformation of the feature representation that approximates the density function at each pixel:</p><formula xml:id="formula_1">∀p ∈ I i , F i (p|w) = w T x i p ,<label>(2)</label></formula><p>where w ∈ R K is the parameter vector of the linear transform that we aim to learn from the training data, and F i (•|w) is the estimate of the density function for a particular value of w. The regularized risk framework then suggests choosing w so that it minimizes the sum of the mismatches between the ground truth and the estimated density functions (the loss function) under regularization:</p><formula xml:id="formula_2">w = argmin w w T w + λ N i=1 D F i (•), F i (•|w) ,<label>(3)</label></formula><p>Here, λ is a standard scalar hyperparameter, controlling the regularization strength. It is the only hyperparameter in our framework (in addition to those that might be used during feature extraction).</p><p>After the optimal weight vector has been learned from the training data, the system can produce a density estimate for an unseen image I by a simple linear weighting of the feature vector computed in each pixel as suggested by <ref type="bibr" target="#b0">(2)</ref>. The problem is thus reduced to choosing the right loss function D and computing the optimal w in (3) under that loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The MESA distance</head><p>The distance D in (3) measures the mismatch between the ground truth and the estimated densities (the loss) and has a significant impact on the performance of the entire learning framework. There are two natural choices for D:</p><p>• One can choose D to be some function of an L P metric, e.g. the L 1 metric (sum of absolute per-pixel differences) or a square of the L 2 metric (sum of squared per-pixel differences). Such choices turns (3) into standard regression problems (i.e. support vector regression and ridge regression for L 1 and L 2 2 cases respectively), where each pixel in each training image effectively provides a sample in the training set. The problem with such loss is that it is not directly related to the real quantity that we care about, i.e. the overall counts of objects in images. E.g. strong zero-mean noise would affect such metric a lot, while the overall counts would be unaffected.</p><p>• As the overall counts is what we ultimately care about, one may choose D to be an absolute or squared difference between the overall sums over the entire images for the two arguments, e.g.</p><formula xml:id="formula_3">D (F 1 (•), F 2 (•)) = | p∈I F 1 (p) − p∈I F 2 (p)|.</formula><p>The use of such a pseudometric as a loss turns (3) into the counting-by-regression framework discussed in Section 1.1. Once again, we get either the support vector regression (for the absolute differences) or ridge regression (for the squared differences), but now each training sample corresponds to the entire training image. Thus, although this choice of the loss matches our ultimate goal of learning to count very well, it requires many annotated images for training as spatial information in the annotation is discarded.</p><p>Given the significant drawbacks of both baseline distance measures, we suggest an alternative, which we call the MESA distance. Given an image I, the MESA distance D MESA between two functions F 1 (p) and F 2 (p) on the pixel grid is defined as the largest absolute difference between sums of F 1 (p) and F 2 (p) over all box subarrays in I:</p><formula xml:id="formula_4">D MESA (F 1 , F 2 ) = max B∈B p∈B F 1 (p) − p∈B F 2 (p)<label>(4)</label></formula><p>Here, B is the set of all box subarrays of I.</p><p>The MESA distance (in fact, a metric) can be regarded as an L ∞ distance between combinatorially-long vectors of subarray sums. In the 1D case, it is related to the Kolmogorov-Smirnov distance between probability distributions <ref type="bibr" target="#b21">[23]</ref> (in our terminology, the Kolmogorov-Smirnov distance is the maximum of absolute differences over the subarrays with one corner fixed at top-left; thus the strict subset of B is considered in the Kolmogorov-Smirnov case). In the bottom row, we compare side-by-side the per-pixel L1 distance, the absolute difference of overall counts, and the MESA distance between the original and the perturbed densities (the distances are normalized across the 5 examples). The MESA distance has a unique property that it tolerates the local modifications (noise, jitter, change of Gaussian kernel), but reacts strongly to the change in the number of objects or their positions. In the middle row we give per-pixel plots of the differences between the respective densities and show the boxes on which the maxima in the definition of the MESA distance are achieved.</p><p>The MESA distance has a number of desirable properties in our framework. Firstly, it is directly related to the counting objective we want to optimize. Since the set of all subarrays include the full image,</p><formula xml:id="formula_5">D MESA (F 1 , F 2 )</formula><p>is an upper bound on the absolute difference of the overall count estimates given by the two densities F 1 and F 2 . Secondly, when the two density functions differ by a zero-mean high-frequency signal or an independent zero-mean noise, the D MESA distance between them is small, because positive and negative deviations of F 1 from F 2 pixels tend to cancel each other over the large regions. Thirdly, D MESA is sensitive to the overall spatial layout of the denisities. Thus, if the difference between F 1 and F 2 is a low-frequency signal, e.g. F 1 and F 2 are the ground truth densities corresponding to the two point sets leaning towards two different corners of the image, then the D MESA distance between F 1 and F 2 is large, even if F 1 and F 2 sum to the same counts over the entire image. These properties are illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>The final property of D MESA is that it can be computed efficiently. This is because it can be rewritten as:</p><formula xml:id="formula_6">D MESA (F 1 , F 2 ) = max   max B∈B p∈B F 1 (p) − F 2 (p) , max B∈B p∈B F 2 (p) − F 1 (p)   .<label>(5)</label></formula><p>Computing both inner maxima in (5) then constitutes a 2D maximum subarray problem, which is finding the box subarray of a given 2D array with the largest sum. This problem has a number of efficient solutions. Perhaps, the simplest of the efficient ones (from <ref type="bibr" target="#b6">[8]</ref>) is an exhaustive search over one image dimension (e.g. for the top and bottom dimensions of the optimal subarray) combined with the dynamic programming (Kadane's algorithm <ref type="bibr" target="#b5">[7]</ref>) to solve the 1D maximum subarray problem along the other dimension in the inner loop. This approach has complexity O(|I| 1.5 ), where |I| is the number of pixels in the image grid. It can be further improved in practice by replacing the exhaustive search over the first dimension with branch-and-bound <ref type="bibr" target="#b2">[4]</ref>. More extensive algorithms that guarantee even better worst-case complexity are known <ref type="bibr" target="#b29">[31]</ref>. In our experiments, the algorithm <ref type="bibr" target="#b6">[8]</ref> was sufficient, as the time bottleneck lied in the QP solver (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Optimization</head><p>We finally discuss how the optimization problem in (3) can be solved in the case when the D MESA distance is employed. The learning problem (3) can then be rewritten as a convex quadratic program:</p><formula xml:id="formula_7">min w,ξ1,...ξ N w T w + λ N i=1 ξ i , subject to (6) ∀i, ∀B ∈ B i : ξ i ≥ p∈B F 0 i (p) − w T x i p , ξ i ≥ p∈B w T x i p − F 0 i (p)<label>(7)</label></formula><p>Here, ξ i are the auxiliary slack variables (one for each training image) and B i is the set of all subarrays in image i. At the optimum of (6)- <ref type="formula" target="#formula_7">7</ref>, the optimal vectorŵ is the solution of (3) while the slack variables equal the MESA distances:</p><formula xml:id="formula_8">ξ i = D MESA F 0 i (•), F i (•|ŵ)</formula><p>. The number of linear constraints in (7) is combinatorial, so that a custom QP-solver cannot be applied directly. A standard iterative cutting-plane procedure, however, overcomes this problem: one starts with only a small subset of constraints activated (we choose 20 boxes with random dimensions in random subset of images to initialize the process). At each iteration, the QP (6)-(7) is solved with an active subset of constraints. Given the solution j w, j ξ 1 , . . . j ξ N after the iteration j, one can find the box subarrays corresponding to the most violated constraints among <ref type="bibr" target="#b5">(7)</ref>. To do that, for each image we find the subarrays that maximize the right hand sides of (7), which are exactly the 2D maximum subarrays of</p><formula xml:id="formula_9">F 0 i (•) − F i (•| j w) and F i (•| j w) − F 0 i (•)</formula><p>respectively. The boxes j B 1 i and j B 2 i corresponding to these maximum subarrays are then found for each image i. If the respective sums</p><formula xml:id="formula_10">p∈ j B 1 i F 0 i (p) − j w T x i p and p∈ j B 2 i j w T x i p − F 0 i (p) exceed j ξ i • (1 + )</formula><p>, the corresponding constraints are activated, and the next iteration is performed. The iterations terminate when for all images the sums corresponding to maximum subarrays are within (1 + ) factor from j ξ i and hence no constraints are activated. In the derivation here, &lt;&lt; 1 is a constant that promotes convergence in a small number of iterations to the approximation of the global minimum. Setting to 0 solves the program (6)-(7) exactly, while it has been shown in similar circumstances <ref type="bibr" target="#b14">[16]</ref> that setting to a small finite value does not affect the generalization of the learning algorithm and brings the guarantees of convergence in small number of steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Our framework and several baselines were evaluated on counting tasks for two types of imagery shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We now discuss the experiments and the quantitative results. The test datasets and the densities computed with our method can be further assessed qualitatively at the project webpage [1].</p><p>Bacterial cells in fluorescence-light microscopy images. Our first experiment is concerned with synthetic images, emulating microscopic views of the colonies of bacterial cell, generated with <ref type="bibr" target="#b17">[19]</ref>  <ref type="figure" target="#fig_0">(Figure 1-left)</ref>. Such synthetic images <ref type="figure" target="#fig_0">(Figure 1-left)</ref> are highly realistic and simulate such effects as cell overlaps, shape variability, strong out-of-focus blur, vignetting, etc. For the experiments, we generated a dataset of images (available at [1]), with the overall number of cells varying between 74 and 317. Few annotated datasets with real cell microscopy images also exist. While it is tempting to use real rather than synthetic imagery, all the real image datasets to the best of our knowledge are small (only few images have annotations), and, most importantly, there always are very big discrepancies between the annotations of different human experts. The latter effectively invalidates the use of such real datasets for quantitative comparison of different counting approaches.</p><p>Below we discuss the comparison of the counting accuracy achieved by our approach and baseline approaches. The features used in all approaches were based on the dense SIFT descriptor <ref type="bibr" target="#b19">[21]</ref> computing using <ref type="bibr" target="#b30">[32]</ref> software at each pixel of each image with the fixed SIFT frame radius (about the size of the cell) and fixed orientation. Each algorithm was trained on N training images, while another N images were used for the validation of metaparameters. The following approaches were considered:</p><p>1. The proposed density-based approach. A very simple feature representation was chosen: a codebook of K entries was constructed via k-means on SIFT descriptors extracted from the hold-out 20 images. Then each pixel is represented by a vector of length K, which is 1 at the dimension corresponding to the entry of the SIFT descriptor at that pixel and for all other dimensions. We used training images to learn the vector w as discussed in Section 2.1. Counting is then performed by summing the values w t assigned to the codebook entries t for all pixels in the test image.   <ref type="table">Table 2</ref>: Mean absolute errors for people counting in the surveillance video <ref type="bibr" target="#b8">[10]</ref>. The columns correspond to the four scenarios (splits) reproduced from <ref type="bibr" target="#b26">[28]</ref> ('maximal','downscale','upscale','minimal') and for the two new sets of splits ('dense' and 'sparse'). Our method outperforms counting-by-regression methods and is competitive with the hybrid method in <ref type="bibr" target="#b26">[28]</ref>, which uses more detailed annotation.</p><p>2. The counting-by-regression baseline. Each of the training images was described by a global histogram of the entries occurrences for the same codebook as above. We then learned two types of regression (ridge regression with linear and Gaussian kernels) to the number of cells in the image.</p><p>3. The counting-by-detection baseline. We trained a detector based on a linear SVM classifier. The SIFT descriptors corresponding to the dotted pixels were considered positive examples. To sample negative examples, we built a Delaunay triangulation on the dots and took SIFT descriptors corresponding to the pixels at the middle of Delaunay edges. At detection time, we applied the SVM at each pixel, and then found peaks in the resulting confidence map (e.g. <ref type="figure" target="#fig_2">Figure 2</ref>-middle) via non-maximum suppression with the threshold τ and radius ρ using the code <ref type="bibr" target="#b16">[18]</ref>. We also considered a variant with the linear correction of the obtained number to account for systematic biases (detection+correction). The slope and the intercept of the correction for each combination of τ , ρ, and regularization strength were estimated via robust regression on the union of the training and validation sets.</p><p>4. Application-specific method <ref type="bibr" target="#b27">[29]</ref>. We also evaluated the software specifically designed for analyzing cells in fluorescence-light images <ref type="bibr" target="#b27">[29]</ref>. The counting algorithm here is based on adaptive thresholding and morphological analysis. For this baseline, we tuned the free parameter (cell division threshold) on the test set, and computed the mean absolute error, which was 16.2.</p><p>The meta-parameters (K, regularization strengths, Gaussian kernel width for ridge regression, τ and ρ for non-maximum suppression) were learned in each case on the validation set. The objective minimized during the validation was counting accuracy. For counting-by-detection, we also considered optimizing detection accuracy (computed via Hungarian matching with the ground truth), and, for our approach, we also considered minimizing the MESA distance with the ground truth density on the validation set.</p><p>The results for a different number N of training and validation images are given in <ref type="table" target="#tab_0">Table 1</ref>, based on 5 random draws of training and validation sets. A hold out set of 100 images was used for testing. The proposed method outperforms the baseline approaches for all sizes of the training set.</p><p>Pedestrians in surveillance video. Here we focus on a 2000-frames video dataset <ref type="bibr" target="#b8">[10]</ref> from a camera overviewing a busy pedestrian street <ref type="figure" target="#fig_0">(Figure 1-right)</ref>. The authors of <ref type="bibr" target="#b8">[10]</ref> also provided the dotted ground truth for these frames, the position of the ground plane, and the region of interest, where the counts should be performed. Recently, <ref type="bibr" target="#b26">[28]</ref> performed extensive experiments on the dataset and reported the performance of three approaches (two counting-by-regression including <ref type="bibr" target="#b15">[17]</ref> and the hybrid approach: split into blobs, and regress the number for each blob). The hybrid approach in <ref type="bibr" target="#b26">[28]</ref> required more detailed annotations than dotting (see <ref type="bibr" target="#b26">[28]</ref> for details). For the sake of comparison, we adhered to the experimental protocols described in <ref type="bibr" target="#b26">[28]</ref>, so that the performance of our method is directly comparable.</p><p>In particular, 4 train/test splits were suggested in <ref type="bibr" target="#b26">[28]</ref>: 1) 'maximal': train on frames 600:5:1400 (in Matlab notation) 2) 'downscale': train on frames 1205:5:1600 (the most crowded) 3) 'upscale': train on frames 805:5:1100 (the least crowded) 4) 'minimal': train on frames 640:80:1360 (10 frames). Testing is performed on the frames outside the training range. For future reference, we also included two additional scenarios ('dense' and 'sparse') with multiple similar splits in each (permitting variance estimation). Both scenarios are based on splitting the 2000 frames into 5 contiguous chunks of 400 frames. In each of the two scenarios, we then performed training on one chunk and testing on the other 4. In the "dense" scenario we trained on 80 frames sampled from the training split with uniform spacing, while in the 'sparse' scenario, we took just 10 frames.</p><p>Extracting features in this case is more involved as several modalities, namely the image itself, the difference image with the previous frame, and the background subtracted image have to be combined to achieve the best performance (a simple median filtering was used to estimate the static background image). We used a randomized tree approach similar to <ref type="bibr" target="#b22">[24]</ref> to get features combining these modalities. Thus, we first extracted the primary features in each pixel including the absolute differences with the previous frame and the background, the image intensity, and the absolute values xand y-derivatives. On the training subset of the smallest 'minimal' split, we then trained a random forest <ref type="bibr" target="#b7">[9]</ref> with 5 randomized trees. The training objective was the regression from the appearance of each pixel and its neighborhood to the ground truth density. For each pixel at testtime, the random forest performs a series of simple tests comparing the value of in the particular primary channel at location defined by a particular offset with the particular threshold, while during forest pretraining the number of the channel, the offset and the threshold are randomized. Given the pretrained forest, each pixel p gets assigned a vector x p of dimension equal to the total number of leaves in all trees, with ones corresponding to the leaves in each of the five trees the pixel falls into and zeros otherwise. Finally, to account for the perspective distortion, we multiplied x p by the square of the depth of the ground plane at p (provided with the sequence).</p><p>Within each scenario, we allocated one-fifth of the training frames to pick λ and the tree depth through validation via the MESA distance.</p><p>The quantitative comparison in <ref type="table">Table 2</ref>, demonstrates the competitiveness of our method.</p><p>Overall comments. In both sets of experiments, we tried two strategies for setting σ (kernel width in the definition of the ground truth densities): setting σ = 0 (effectively, the ground truth is then a sum of delta-functions), and setting σ = 4 (roughly comparable with object half-size in both experiments). In the first case (cells) both strategies gave almost the same results for all N , highlighting the insensitivity of our approach to the choice of σ (see also <ref type="figure" target="#fig_3">Figure 3</ref> on that). The results in <ref type="table" target="#tab_0">Table 1</ref> is for σ = 0. In the second case (pedestrians), σ = 4 had an edge over σ = 0, and the results in <ref type="table">Table 2</ref> are for that value.</p><p>At train time, we observed that the cutting plane algorithm converged in a few dozen iterations (less than 100 for our choice = 0.01). The use of a general-purpose quadratic solver <ref type="bibr" target="#b0">[2]</ref> meant that the training times were considerable (from several seconds to few hours depending on the value of λ and the size of the training set). We anticipate a big reduction in training time for the purpose-built solver. At test time, our approach introduces virtually no time overhead over feature extraction. E.g. in the case of pedestrians, one can store the value w t computed during learning at each leaf t in each tree, so that counting would require simply "pushing" each pixel down the forest, and summing the resulting w t from the obtained leaves. This can be done in real-time <ref type="bibr" target="#b28">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have presented the general framework for learning to count objects in images. While our ultimate goal is the counting accuracy over the entire image, during the learning our approach is optimizing the loss based on the MESA-distance. This loss involves counting accuracy over multiple subarrays of the entire image (and not only the entire image itself). We demonstrate that given limited amount of training data, such an approach achieves much higher accuracy than optimizing the counting accuracy over the entire image directly (counting-by-regression). At the same time, the fact that we avoid the hard problem of detecting and discerning individual object instances, gives our approach an edge over the countingby-detection method in our experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of counting problems. Left -counting bacterial cells in a fluorescence-light microscopy image (from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Input: 6 and 10 Detection: 6</head><label>106</label><figDesc>and unclear Density: 6.52 and 9.37</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Processing results for a previously unseen image. Left -a fragment of the microscopy image. Emphasized are the two rectangles containing 6 and 10 cells respectively. Middle -the confidence map produced by an SVM-based detector, 6 peaks are clearly discernible for the 1st rectangle, but the number of peaks in the 2nd rectangle is unclear. Right -the density map, that our approach produces. The integrals over the rectangles (6.52 and 9.37) are close to the correct number of cells. (MATLAB jet colormap is used)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of distances for matching density functions. Here, the top-left image shows one of the densities, computed as the ground truth density for a set of dots. The densities in the top row are obtained through some perturbations of the original one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 -</head><label>2</label><figDesc>right gives an example of the respective density (see also [1]). Validation N = 1 N = N = N = 8 N = 16 N = 32 linear ridge regression counting 67.3±25.2 37.7±14.0 16.7±3.1 8.8±1.5 6.4±0.7 5.9±0.5 kernel ridge regression counting 60.4±16.5 38.7±17.0 18.6±5.0 10.4±2.5 6.0±0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>8 5.2±0.3 detection counting 28.0±20.6 20.8±5.8 13.6±1.5 10.2±1.9 10.4±1.2 8.5±0.5 detection detection 20.8±3.8 20.1±5.5 15.7±2.0 15.0±4.1 11.8±3.1 12.0±0.8 Mean absolute errors for cell counting on the test set of 100 fluorescent microscopy images. The rows correspond to the methods described in the text. The second column corresponds to the error measure used for learning meta-parameters on the validation set. The last 6 columns correspond to the numbers of images in the training and validation sets. The average number of cells is 171±64 per image. Standard deviations in the table correspond to 5 different draws of training and validation image sets. The proposed method (density learning) outperforms considerably the baseline approaches (including the application-specific baseline with the error rate = 16.2) for all sizes of the training set.</figDesc><table><row><cell cols="2">detection+correction counting</cell><cell>-</cell><cell cols="5">22.6±5.3 16.8±6.5 6.8±1.2 6.1±1.6 4.9±0.5</cell></row><row><cell>density learning</cell><cell cols="7">counting 12.7±7.3 7.8±3.7 5.0±0.5 4.6±0.6 4.2±0.4 3.6±0.2</cell></row><row><cell>density learning</cell><cell>MESA</cell><cell cols="6">9.5±6.1 6.3±1.2 4.9±0.6 4.9±0.7 3.8±0.2 3.5±0.2</cell></row><row><cell></cell><cell cols="7">'maximal' 'downscale' 'upscale' 'minimal' 'dense' 'sparse'</cell></row><row><cell cols="2">Counting-by-Regression [17]</cell><cell>2.07</cell><cell>2.66</cell><cell>2.78</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">Counting-by-Regression [28]</cell><cell>1.80</cell><cell>2.34</cell><cell>2.52</cell><cell>4.46</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">Counting-by-Segmentation [28]</cell><cell>1.53</cell><cell>1.64</cell><cell>1.84</cell><cell>1.31</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Density learning</cell><cell></cell><cell>1.70</cell><cell>1.28</cell><cell>1.59</cell><cell>2.02</cell><cell cols="2">1.78±0.39 2.06±0.59</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is suppoted by EU ERC grant VisRec no. 228180. V. Lempitsky is also supported by Microsoft Research projects in Russia. We thank Prof. Jiri Matas (CTU Prague) for suggesting the detection+correction baseline.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.mosek.com/" />
		<title level="m">The MOSEK optimization software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Extracting texels in 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>1d natural textures. ICCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient algorithms for subwindow search in object detection and localization. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peursum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cell segmentation with median filter and mathematical morphology operation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anoraganingrum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Processing, International Conference on</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">1043</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the detection of multiple object instances using Hough transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Programming pearls: Algorithm design techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="865" to="871" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Programming pearls: Perspective on performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1087" to="1092" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-S</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A neural-based crowd estimation by hybrid global learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W S</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="535" to="541" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Discriminative models for multi-class object layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object extraction using a stochastic birth-and-death dynamics in continuum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Descombes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhizhina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast crowd segmentation using shape indexing. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zoghlami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Zisserman. The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>VOC2009) Results</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="27" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A viewpoint invariant approach for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1187" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MATLAB and Octave functions for computer vision and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Kovesi</surname></persName>
		</author>
		<ptr target="http://www.csse.uwa.edu.au/∼pk/research/matlabfns/" />
		<imprint/>
		<respStmt>
			<orgName>School of Computer Science &amp; Software Engineering, The University of Western Australia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computational framework for simulating fluorescence microscope images with cell populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lehmussola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ruusuvuori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Selinummi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huttunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yli-Harja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1010" to="1016" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="259" to="289" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Estimation of crowd density using image processing. Image Processing for Security Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Marana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Lotufo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The kolmogorov-smirnov test for goodness of fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Massey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">253</biblScope>
			<biblScope unit="page" from="68" to="78" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fast discriminative visual codebooks using randomized clustering forests. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cell segmentation using coupled level sets and graph-vertex coloring. MICCAI (1)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bunyak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A neural network architecture for automatic segmentation of fluorescence micrographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Nattkemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wersing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="357" to="367" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Counting crowded moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="705" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crowd counting using multiple local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DICTA &apos;09: Proceedings of the 2009 Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Software for quantification of labeled bacteria from digital microscope images by automated image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Selinummi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yli-Harja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Puhakka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biotechniques</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="859" to="63" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Implementing decision trees and forests on a GPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Algorithms for the maxium subarray problem based on matrix multiplication. SODA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tokuyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="446" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Segmentation of multiple, partially occluded objects by grouping, merging, assigning part detection responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Bayesian human segmentation in crowded situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
