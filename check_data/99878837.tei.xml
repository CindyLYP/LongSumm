<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-01-03">3 Jan 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
							<email>chia-wei.liu@mail.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
							<email>ryan.lowe@mail.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><forename type="middle">V</forename><surname>Serban</surname></persName>
							<email>iulian.vlad.serban@umontreal.ca</email>
							<affiliation key="aff1">
								<orgName type="laboratory">DIRO</orgName>
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Noseworthy</surname></persName>
							<email>michael.noseworthy@mail.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
							<email>lcharlin@cs.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
							<email>jpineau@cs.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-01-03">3 Jan 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1603.08023v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model&apos;s generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An important aspect of dialogue response generation systems, which are trained to produce a reasonable utterance given a conversational context, is how to evaluate the quality of the generated response. Typically, evaluation is done using human-generated supervised signals, such as a task completion test or a user satisfaction score <ref type="bibr" target="#b32">(Walker et al., 1997;</ref><ref type="bibr" target="#b26">Möller et al., 2006;</ref><ref type="bibr" target="#b17">Kamm, 1995)</ref>, which are relevant when the dialogue is task-focused. We call models optimized for such supervised objectives supervised dialogue models, while those that do not are unsupervised dialogue models.</p><p>This paper focuses on unsupervised dialogue response generation models, such as chatbots. These * Denotes equal contribution. models are receiving increased attention, particularly using end-to-end training with neural networks <ref type="bibr" target="#b31">(Serban et al., 2016;</ref><ref type="bibr">Vinyals and Le, 2015)</ref>. This avoids the need to collect supervised labels on a large scale, which can be prohibitively expensive. However, automatically evaluating the quality of these models remains an open question. Automatic evaluation metrics would help accelerate the deployment of unsupervised response generation systems.</p><p>Faced with similar challenges, other natural language tasks have successfully developed automatic evaluation metrics. For example, BLEU <ref type="bibr">(Papineni et al., 2002a)</ref> and <ref type="bibr">METEOR (Banerjee and Lavie, 2005)</ref> are now standard for evaluating machine translation models, and ROUGE <ref type="bibr" target="#b23">(Lin, 2004)</ref> is often used for automatic summarization. These metrics have recently been adopted by dialogue researchers <ref type="bibr" target="#b29">(Ritter et al., 2011;</ref><ref type="bibr" target="#b21">Li et al., 2015;</ref><ref type="bibr" target="#b12">Galley et al., 2015b;</ref><ref type="bibr">Wen et al., 2015;</ref><ref type="bibr" target="#b22">Li et al., 2016)</ref>. However these metrics assume that valid responses have significant word overlap with the ground truth responses. This is a strong assumption for dialogue systems, where there is significant diversity in the space of valid responses to a given context. This is illustrated in <ref type="table" target="#tab_0">Table 1</ref>, where two reasonable responses are proposed to the context, but these responses do not share any words in common and do not have the same semantic meaning.</p><p>In this paper, we investigate the correlation between the scores from several automatic evaluation metrics and human judgements of dialogue response quality, for a variety of response generation models. We consider both statistical word-overlap similar- ity metrics such as BLEU, METEOR, and ROUGE, and word embedding metrics derived from word embedding models such as Word2Vec <ref type="bibr" target="#b24">(Mikolov et al., 2013)</ref>. We find that all metrics show either weak or no correlation with human judgements, despite the fact that word overlap metrics have been used extensively in the literature for evaluating dialogue response models (see above, and <ref type="bibr" target="#b20">Lasguido et al. (2014)</ref>). In particular, we show that these metrics have only a small positive correlation on the chitchat oriented Twitter dataset, and no correlation at all on the technical Ubuntu Dialogue Corpus. For the word embedding metrics, we show that this is true even though all metrics are able to significantly distinguish between baseline and state-of-the-art models across multiple datasets. We further highlight the shortcomings of these metrics using: a) a statistical analysis of our survey's results; b) a qualitative analysis of examples from our data; and c) an exploration of the sensitivity of the metrics.</p><p>Our results indicate that a shift must be made in the research community away from these metrics, and highlight the need for a new metric that correlates more strongly with human judgement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>We focus on metrics that are model-independent, i.e. where the model generating the response does not also evaluate its quality; thus, we do not consider word perplexity, although it has been used to evaluate unsupervised dialogue models . This is because it is not computed on a per-response basis, and cannot be computed for retrieval models. Further, we only consider metrics that can be used to evaluate proposed responses against ground-truth responses, so we do not consider retrieval-based metrics such as recall, which has been used to evaluate dialogue models <ref type="bibr" target="#b29">(Schatzmann et al., 2005;</ref>. We also do not consider evaluation methods for supervised evaluation methods. <ref type="bibr">1</ref> Several recent works on unsupervised dialogue systems adopt the BLEU score for evaluation. <ref type="bibr" target="#b29">Ritter et al. (2011)</ref> formulate the unsupervised learning problem as one of translating a context into a candidate response. They use a statistical machine translation (SMT) model to generate responses to various contexts using Twitter data, and show that it outperforms information retrieval baselines according to both BLEU and human evaluations. Sordoni et al. (2015) extend this idea using a recurrent language model to generate responses in a context-sensitive manner. They also evaluate using BLEU, however they produce multiple ground truth responses by retrieving 15 responses from elsewhere in the corpus, using a simple bag-of-words model. <ref type="bibr" target="#b21">Li et al. (2015)</ref> evaluate their proposed diversity-promoting objective function for neural network models using BLEU score with only a single ground truth response. A modified version of BLEU, deltaBLEU <ref type="bibr" target="#b12">(Galley et al., 2015b)</ref>, which takes into account several humanevaluated ground truth responses, is shown to have a weak to moderate correlation to human judgements using Twitter dialogues. However, such human annotation is often infeasible to obtain in practice. <ref type="bibr" target="#b12">Galley et al. (2015b)</ref> also show that, even with several ground truth responses available, the standard BLEU metric does not correlate strongly with human judgements.</p><p>There has been significant previous work that evaluates how well automatic metrics correlate with human judgements in in both machine translation <ref type="bibr" target="#b5">(Callison-Burch et al., 2010;</ref><ref type="bibr" target="#b6">Callison-Burch et al., 2011;</ref><ref type="bibr" target="#b2">Bojar et al., 2014;</ref><ref type="bibr" target="#b13">Graham et al., 2015)</ref> and natural language generation (NLG) <ref type="bibr" target="#b32">(Stent et al., 2005;</ref><ref type="bibr" target="#b3">Cahill, 2009;</ref><ref type="bibr" target="#b28">Reiter and Belz, 2009;</ref><ref type="bibr" target="#b9">Espinosa et al., 2010)</ref>. There has also been work criticizing the usefulness of BLEU in particular for machine translation <ref type="bibr" target="#b4">(Callison-Burch et al., 2006)</ref>. While many of the criticisms in these works apply to dialogue generation, we note that generating dialogue responses conditioned on the conversational context is in fact a more difficult problem. This is because most of the difficulty in automatically evaluating language generation models lies in the large set of correct answers. Dialogue response generation given solely the context intuitively has a higher diversity (or entropy) than translation given text in a source language, or surface realization given some intermediate form <ref type="bibr" target="#b0">(Artstein et al., 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Metrics</head><p>Given a dialogue context and a proposed response, our goal is to automatically evaluate how appropriate the proposed response is to the conversation. We focus on metrics that compare it to the ground truth response of the conversation. In particular, we investigate two approaches: word based similarity metrics and word-embedding based similarity metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Overlap-based Metrics</head><p>We first consider metrics that evaluate the amount of word-overlap between the proposed response and the ground-truth response. We examine the BLEU and METEOR scores that have been used for machine translation, and the ROUGE score that has been used for automatic summarization. While these metrics have been shown to correlate with human judgements in their target domains <ref type="bibr">(Papineni et al., 2002a;</ref><ref type="bibr" target="#b23">Lin, 2004)</ref>, they have not been thoroughly investigated for dialogue systems. <ref type="bibr">2</ref> We denote the ground truth response as r (thus we assume that there is a single candidate ground truth response), and the proposed response asr. The j'th token in the ground truth response r is denoted by w j , withŵ j denoting the j'th token in the proposed responser.</p><p>BLEU. BLEU <ref type="bibr">(Papineni et al., 2002a)</ref> analyzes the co-occurrences of n-grams in the ground truth and the proposed responses. It first computes an n-gram precision for the whole dataset (we assume that there is a single candidate ground truth response 2 To the best of our knowledge, only BLEU has been evaluated in the dialogue system setting quantitatively by <ref type="bibr">Galley et al. (2015a)</ref> on the Twitter domain. However, they carried out their experiments in a very different setting with multiple ground truth responses, which are rarely available in practice, and without providing any qualitative analysis of their results. per context):</p><formula xml:id="formula_0">P n (r,r) = k min(h(k, r), h(k,r i )) k h(k, r i )</formula><p>where k indexes all possible n-grams of length n and h(k, r) is the number of n-grams k in r. <ref type="bibr">3</ref> To avoid the drawbacks of using a precision score, namely that it favours shorter (candidate) sentences, the authors introduce a brevity penalty. BLEU-N, where N is the maximum length of n-grams considered, is defined as:</p><formula xml:id="formula_1">BLEU-N := b(r,r) exp( N n=1 β n log P n (r,r))</formula><p>β n is a weighting that is usually uniform, and b(•) is the brevity penalty. The most commonly used version of BLEU uses N = 4. Modern versions of BLEU also use sentence-level smoothing, as the geometric mean often results in scores of 0 if there is no 4-gram overlap <ref type="bibr" target="#b7">(Chen and Cherry, 2014)</ref>. Note that BLEU is usually calculated at the corpus-level, and was originally designed for use with multiple reference sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METEOR.</head><p>The METEOR metric (Banerjee and Lavie, 2005) was introduced to address several weaknesses in BLEU. It creates an explicit alignment between the candidate and target responses. The alignment is based on exact token matching, followed by WordNet synonyms, stemmed tokens, and then paraphrases. Given a set of alignments, the METEOR score is the harmonic mean of precision and recall between the proposed and ground truth sentence.</p><p>ROUGE. ROUGE <ref type="bibr" target="#b23">(Lin, 2004</ref>) is a set of evaluation metrics used for automatic summarization. We consider ROUGE-L, which is a F-measure based on the Longest Common Subsequence (LCS) between a candidate and target sentence. The LCS is a set of words which occur in two sentences in the same order; however, unlike n-grams the words do not have to be contiguous, i.e. there can be other words in between the words of the LCS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embedding-based Metrics</head><p>An alternative to using word-overlap based metrics is to consider the meaning of each word as defined by a word embedding, which assigns a vector to each word. Methods such as Word2Vec (Mikolov et al., 2013) calculate these embeddings using distributional semantics; that is, they approximate the meaning of a word by considering how often it co-occurs with other words in the corpus. 4 These embeddingbased metrics usually approximate sentence-level embeddings using some heuristic to combine the vectors of the individual words in the sentence. The sentence-level embeddings between the candidate and target response are compared using a measure such as cosine distance.</p><p>Greedy Matching. Greedy matching is the one embedding-based metric that does not compute sentence-level embeddings. Instead, given two sequences r andr, each token w ∈ r is greedily matched with a tokenŵ ∈r based on the cosine similarity of their word embeddings (e w ), and the total score is then averaged across all words:</p><p>G(r,r) = w∈r; maxŵ ∈r cos sim(e w , eŵ) |r|</p><formula xml:id="formula_2">GM (r,r) = G(r,r) + G(r, r) 2</formula><p>This formula is asymmetric, thus we must average the greedy matching scores G in each direction. This was originally introduced for intelligent tutoring systems (Rus and Lintean, 2012). The greedy approach favours responses with key words that are semantically similar to those in the ground truth response.</p><p>Embedding Average. The embedding average metric calculates sentence-level embeddings using additive composition, a method for computing the meanings of phrases by averaging the vector representations of their constituent words <ref type="bibr">(Foltz et al., 1998;</ref><ref type="bibr">Landauer and Dumais, 1997;</ref><ref type="bibr" target="#b25">Mitchell and Lapata, 2008)</ref>. This method has been widely used in other domains, for example in textual similarity tasks <ref type="bibr">(Wieting et al., 2015)</ref>. The embedding average,ē, is defined as the mean of the word embeddings of each token in a sentence r:</p><formula xml:id="formula_3">e r = w∈r e w | w ∈r e w | .</formula><p>To compare a ground truth response r and retrieved responser, we compute the cosine similarity between their respective sentence level embeddings: EA := cos(ē r ,ēr).</p><p>Vector Extrema. Another way to calculate sentence-level embeddings is using vector extrema <ref type="bibr" target="#b10">(Forgues et al., 2014)</ref>. For each dimension of the word vectors, take the most extreme value amongst all word vectors in the sentence, and use that value in the sentence-level embedding:</p><formula xml:id="formula_4">e rd =</formula><p>max w∈r e wd if e wd &gt; | min w ∈r e w d | min w∈r e wd otherwise where d indexes the dimensions of a vector; e wd is the d'th dimensions of e w (w's embedding). The min in this equation refers to the selection of the largest negative value, if it has a greater magnitude than the largest positive value. Similarity between response vectors is again computed using cosine distance. Intuitively, this approach prioritizes informative words over common ones; words that appear in similar contexts will be close together in the vector space. Thus, common words are pulled towards the origin because they occur in various contexts, while words carrying important semantic information will lie further away. By taking the extrema along each dimension, we are thus more likely to ignore common words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dialogue Response Generation Models</head><p>In order to determine the correlation between automatic metrics and human judgements of response quality, we obtain response from a diverse range of response generation models in the recent literature, including both retrieval and generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Retrieval Models</head><p>Ranking or retrieval models for dialogue systems are typically evaluated based on whether they can retrieve the correct response from a corpus of predefined responses, which includes the ground truth  response to the conversation <ref type="bibr" target="#b29">(Schatzmann et al., 2005)</ref>. Such systems can be evaluated using recall or precision metrics. However, when deployed in a real setting these models will not have access to the correct response given an unseen conversation. Thus, in the results presented below we remove one occurrence of the ground-truth response from the corpus and ask the model to retrieve the most appropriate response from the remaining utterances. Note that this does not mean the correct response will not appear in the corpus at all; in particular, if there exists another context in the dataset with an identical ground-truth response, this will be available for selection by the model. We then evaluate each model by comparing the retrieved response to the ground truth response of the conversation. This closely imitates real-life deployment of these models, as it tests the ability of the model to generalize to unseen contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TF-IDF.</head><p>We consider a simple Term Frequency -Inverse Document Frequency (TF-IDF) retrieval model . TF-IDF is a statistic that intends to capture how important a given word is to some document, which is calculated as:</p><formula xml:id="formula_5">tfidf(w, c, C) = f (w, c) × log N |{c∈C:w∈c}| ,</formula><p>where C is the set of all contexts in the corpus, f (w, c) indicates the number of times word w appeared in context c, N is the total number of dialogues, and the denominator represents the number of dialogues in which the word w appears.</p><p>In order to apply TF-IDF as a retrieval model for dialogue, we first compute the TF-IDF vectors for each context and response in the corpus. We then return the response with the largest cosine similarity in the corpus, either between the input context and corpus contexts (C-TFIDF), or between the input context and corpus responses (R-TFIDF).</p><p>Dual Encoder. Next we consider the recurrent neural network (RNN) based architecture called the Dual Encoder (DE) model ). The DE model consists of two RNNs which respectively compute the vector representation of an input context and response, c, r ∈ R n . The model then calculates the probability that the given response is the ground truth response given the context, by taking a weighted dot product: p(r is correct|c, r, M ) = σ(c T M r + b) where M is a matrix of learned parameters and b is a bias. The model is trained using negative sampling to minimize the cross-entropy error of all (context, response) pairs. To our knowledge, our application of neural network models to large-scale retrieval in dialogue systems is novel.    LSTM language model. The baseline model is an LSTM language model (Hochreiter and Schmidhuber, 1997) trained to predict the next word in the (context, response) pair. During test time, the model is given a context, encodes it with the LSTM and generates a response using a greedy beam search procedure <ref type="bibr" target="#b14">(Graves, 2013)</ref>.</p><p>HRED. Finally we consider the Hierarchical Recurrent Encoder-Decoder (HRED) <ref type="bibr" target="#b31">(Serban et al., 2015)</ref>. In the traditional Encoder-Decoder framework, all utterances in the context are concatenated together before encoding. Thus, information from previous utterances is far outweighed by the most recent utterance. The HRED model uses a hierarchy of encoders; each utterance in the context passes through an 'utterance-level' encoder, and the output of these encoders is passed through another 'context-level' encoder, which enables the handling of longer-term dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Conclusions from an Incomplete Analysis</head><p>When evaluation metrics are not explicitly correlated to human judgement, it is possible to draw misleading conclusions by examining how the metrics rate different models. To illustrate this point, we compare the performance of selected models according to the embedding metrics on two different domains: the Ubuntu Dialogue Corpus <ref type="bibr" target="#b31">(Lowe et al., 2015)</ref>, which contains technical vocabulary and where conversations are often oriented towards solv-ing a particular problem, and a non-technical Twitter corpus collected following the procedure of <ref type="bibr" target="#b28">Ritter et al. (2010)</ref>. We consider these two datasets since they cover contrasting dialogue domains, i.e. technical help vs casual chit-chat, and because they are amongst the largest publicly available corpora, making them good candidates for building data-driven dialogue systems.</p><p>Results on the proposed embedding metrics are shown in <ref type="table" target="#tab_2">Table 2</ref>. For the retrieval models, we observe that the DE model significantly outperforms both TFIDF baselines on all metrics across both datasets. Further, the HRED model significantly outperforms the basic LSTM generative model in both domains, and appears to be of similar strength as the DE model. Based on these results, one might be tempted to conclude that there is some information being captured by these metrics, that significantly differentiates models of different quality. However, as we show in the next section, the embedding-based metrics correlate only weakly with human judgements on the Twitter corpus, and not at all on the Ubuntu Dialogue Corpus. This demonstrates that metrics that have not been specifically correlated with human judgements on a new task should not be used to evaluate that task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Human Correlation Analysis</head><p>Data Collection. We conducted a human survey to determine the correlation between human judgements on the quality of responses, and the score assigned by each metric. We aimed to follow the procedure for the evaluation of BLEU <ref type="bibr">(Papineni et al.,</ref>   2002a). 25 volunteers from the Computer Science department at the author's institution were given a context and one proposed response, and were asked to judge the response quality on a scale of 1 to 5. ; a 1 indicates that the response is not appropriate or sensible given the context, and a 5 indicates that the response is very reasonable. Out of the 25 respondents, 23 had Cohen's kappa scores κ &gt; 0.2 w.r.t. the other respondents, which is a standard measure for inter-rater agreement <ref type="bibr" target="#b8">(Cohen, 1968)</ref>. The 2 respondents with κ &lt; 0.2, indicating slight agreement, were excluded from the analysis below. The median κ score was approximately 0.55, roughly indicating moderate to strong annotator agreement.</p><p>Each volunteer was given 100 questions per dataset. These questions correspond to 20 unique contexts, with 5 different responses: one utterance <ref type="bibr">5</ref> Studies asking humans to evaluate text often rate different aspects separately, such as 'adequacy', 'fluency' and 'informativeness' of the text <ref type="bibr" target="#b15">(Hovy, 1999;</ref><ref type="bibr" target="#b27">Papineni et al., 2002b)</ref> Our evaluation focuses on adequacy. We did not consider fluency because 4 out of the 5 proposed responses to each context were generated by a human. We did not consider informativeness because in the domains considered, it is not necessarily important (in Twitter), or else it seems to correlate highly with adequacy (in Ubuntu). randomly drawn from elsewhere in the test set, the response selected from each of the TF-IDF, DE, and HRED models, and a response written by a human annotator. These were chosen as they cover the range of qualities almost uniformly (see <ref type="figure" target="#fig_2">Figure 1</ref>).</p><p>Survey Results. We present correlation results between the human judgements and each metric in Table 3. We compute the Pearson correlation, which estimates linear correlation, and Spearman correlation, which estimates any monotonic correlation.</p><p>The first observation is that in both domains the BLEU-4 score, which has previously been used to evaluate unsupervised dialogue systems, shows very weak if any correlation with human judgement. In fact we found that the BLEU-3 and BLEU-4 scores were near-zero for a majority of response pairs; for BLEU-4, only four examples had a score &gt; 10 −9 . Despite this, they still correlate with human judgements on the Twitter Corpus at a rate similar to BLEU-2. This is because of the smoothing constant, which gives a tiny weight to unigrams and bigrams despite the absence of higher-order n-grams. BLEU-3 and BLEU-4 behave as a scaled, noisy version of BLEU-2; thus, if one is to evaluate dialogue</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context of Conversation</head><p>A: dearest! question. how many thousands of people can panaad occupy? B: @user panaad has &lt;number&gt; k seat capacity while rizal has &lt;number&gt; k thats why they choose rizal i think .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth Response</head><p>A: now i know about the siting capacity . thanks for the info @user great evening. Proposed Response A: @user makes sense. thanks!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context of Conversation</head><p>A: never felt more sad than i am now B: @user aww why ? A: @user @user its a long story ! sure you wanna know it ? bahaha and thanks for caring btw &lt;heart&gt; Ground Truth Response A: @user i don 't mind to hear it i 've got all day and youre welcome &lt;number&gt; Proposed Response A: @user i know , i 'm just so happy for you ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! <ref type="figure">Figure 2</ref>: Examples where the metrics rated the response poorly and humans rated it highly (left), and the converse (right). Both responses are given near-zero score by BLEU-N for N&gt; 1. While no metric will perform perfectly on all examples, we present these examples to provide intuition on how example-level errors become aggregated into poor correlation to human judgements at the corpus-level.</p><p>responses with BLEU, we recommend the choice of N = 2 over N = 3 or 4. Note that using a test corpus larger than the size reported in this paper may lead to stronger correlations for BLEU-3 and BLEU-4, due to a higher number of non-zero scores.</p><p>It is interesting to note that, while some of the embedding metrics and BLEU show small positive correlation in the non-technical Twitter domain, there is no metric that significantly correlates with humans on the Ubuntu Dialogue Corpus. This is likely because the correct Ubuntu responses contain specific technical words that are less likely to be produced by our models. Further, it is possible that responses in the Ubuntu Dialogue Corpus have intrinsically higher variability (or entropy) than Twitter when conditioned on the context, making the evaluation problem significantly more difficult. <ref type="figure" target="#fig_2">Figure 1</ref> illustrates the relationship between metrics and human judgements. We include only the best performing metric using word-overlaps, i.e. the BLEU-2 score (left), and the best performing metric using word embeddings, i.e. the vector average (center). These plots show how weak the correlation is: in both cases, they appear to be random noise. It seems as though the BLEU score obtains a positive correlation because of the large number of responses that are given a score of 0 (bottom left corner of the first plot). This is in stark contrast to the inter-rater agreement, which is plotted between two randomly sampled halves of the raters (right-most plots). We also calculated the BLEU scores after removing stopwords and punctuation from the responses. As shown in <ref type="table" target="#tab_3">Table 4</ref>, this weakens the cor-relation with human judgements for BLEU-2 compared to the values in <ref type="table" target="#tab_5">Table 3</ref>, and suggests that BLEU is sensitive to factors that do not change the semantics of the response.</p><p>Finally, we examined the effect of response length on the metrics, by considering changes in scores when the ground truth and proposed response had a large difference in word counts. <ref type="table" target="#tab_3">Table 4</ref> shows that BLEU and METEOR are particularly sensitive to this aspect, compared to the Embedding Average metric and human judgement.</p><p>Qualitative Analysis. In order to determine specifically why the metrics fail, we examine qualitative samples where there is a disagreement between the metrics and human rating. Although these only show inconsistencies at the example-level, they provide some intuition as to why the metrics don't correlate with human judgements at the corpuslevel. We present in <ref type="figure">Figure 2</ref> two examples where all of the embedding-based metrics and BLEU-1 score the proposed response significantly differently than the humans.</p><p>The left of <ref type="figure">Figure 2</ref> shows an example where the embedding-based metrics score the proposed response lowly, while humans rate it highly. It is clear from the context that the proposed response is reasonable -indeed both responses intend to express gratitude. However, the proposed response has a different wording than the ground truth response, and therefore the metrics are unable to separate the salient words from the rest. This suggests that the embedding-based metrics would ben-efit from a weighting of word saliency.</p><p>The right of the figure shows the reverse scenario: the embedding-based metrics score the proposed response highly, while humans do not. This is most likely due to the frequently occurring 'i' token, and the fact that 'happy' and 'welcome' may be close together in the embedding space. However, from a human perspective there is a significant semantic difference between the responses as they pertain to the context. Metrics that take into account the context may be required in order to differentiate these responses. Note that in both responses in <ref type="figure">Figure 2</ref>, there are no overlapping n-grams greater than unigrams between the ground truth and proposed responses; thus, all of BLEU-2,3,4 would assign a score near 0 to the response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have shown that many metrics commonly used in the literature for evaluating unsupervised dialogue systems do not correlate strongly with human judgement. Here we elaborate on important issues arising from our analysis.</p><p>Constrained tasks. Our analysis focuses on relatively unconstrained domains. Other work, which separates the dialogue system into a dialogue planner and a natural language generation component for applications in constrained domains, may find stronger correlations with the BLEU metric. For example, Wen et al. (2015) propose a model to map from dialogue acts to natural language sentences and use BLEU to evaluate the quality of the generated sentences. Since the mapping from dialogue acts to natural language sentences has lower diversity and is more similar to the machine translation task, it seems likely that BLEU will correlate better with human judgements. However, an empirical investigation is still necessary to justify this.</p><p>Incorporating multiple responses. Our correlation results assume that only one ground truth response is available given each context. Indeed, this is the common setting in most of the recent literature on training end-to-end conversation models. There has been some work on using a larger set of automatically retrieved plausible responses when evaluating with BLEU <ref type="bibr" target="#b12">(Galley et al., 2015b)</ref>. However, there is no standard method for doing this in the literature. Future work should examine how retrieving additional responses affects the correlation with word-overlap metrics.</p><p>Searching for suitable metrics. While we provide evidence against existing metrics, we do not yet provide good alternatives for unsupervised evaluation. Despite the poor performance of the word embedding-based metrics in this survey, we believe that metrics based on distributed sentence representations hold the most promise for the future. This is because word-overlap metrics will simply require too many ground-truth responses to find a significant match for a reasonable response, due to the high diversity of dialogue responses. As a simple example, the skip-thought vectors of <ref type="bibr" target="#b18">Kiros et al. (2015)</ref> could be considered. Since the embedding-based metrics in this paper only consist of basic averages of vectors obtained through distributional semantics, they are insufficiently complex for modeling sentence-level compositionality in dialogue. Instead, these metrics can be interpreted as calculating the topicality of a proposed response (i.e. how on-topic the proposed response is, compared to the ground-truth).</p><p>All of the metrics considered in this paper directly compare a proposed response to the ground-truth, without considering the context of the conversation. However, metrics that take into account the context could also be considered. Such metrics could come in the form of an evaluation model that is learned from data. This model could be either a discriminative model that attempts to distinguish between model and human responses, or a model that uses data collected from the human survey in order to provide human-like scores to proposed responses. Finally, we must consider the hypothesis that learning such models from data is no easier than solving the problem of dialogue response generation. If this hypothesis is true, we must concede and always use human evaluations together with metrics that only roughly approximate human judgements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Scatter plots showing the correlation between metrics and human judgements on the Twitter corpus (a) and Ubuntu Dialogue Corpus (b). The plots represent BLEU-2 (left), embedding average (center), and correlation between two randomly selected halves of human respondents (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Example showing the intrinsic diversity of valid responses in a dialogue. The (reasonable) model response would receive a BLEU score of 0.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TFIDF 0.536 ± 0.003 0.370 ± 0.002 0.342 ± 0.002 0.483 ± 0.002 0.356 ± 0.001 0.340 ± 0.001 C-TFIDF 0.571 ± 0.003 0.373 ± 0.002 0.353 ± 0.002 0.531 ± 0.002 0.362 ± 0.001 0.353 ± 0.001 DE 0.650 ± 0.003 0.413 ± 0.002 0.376 ± 0.001 0.597 ± 0.002 0.384 ± 0.001 0.365 ± 0.001 LSTM 0.130 ± 0.003 0.097 ± 0.003 0.089 ± 0.002 0.593 ± 0.002 0.439 ± 0.002 0.420 ± 0.002 HRED 0.580 ± 0.003 0.418 ± 0.003 0.384 ± 0.002 0.599 ± 0.002 0.439 ± 0.002 0.422 ± 0.002</figDesc><table><row><cell cols="2">Ubuntu Dialogue Corpus</cell><cell></cell><cell></cell><cell>Twitter Corpus</cell><cell></cell></row><row><cell>Embedding</cell><cell>Greedy</cell><cell>Vector</cell><cell>Embedding</cell><cell>Greedy</cell><cell>Vector</cell></row><row><cell>Averaging</cell><cell>Matching</cell><cell>Extrema</cell><cell>Averaging</cell><cell>Matching</cell><cell>Extrema</cell></row><row><cell>R-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Models evaluated using the vector-based evaluation metrics, with 95% confidence intervals.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Correlation between BLEU metric and human judgements after removing stopwords and punctuation for the Twitter dataset.</figDesc><table><row><cell></cell><cell cols="2">Mean score</cell><cell></cell></row><row><cell></cell><cell cols="3">∆w &lt;= 6 ∆w &gt;= 6 p-value</cell></row><row><cell></cell><cell>(n=47)</cell><cell>(n=53)</cell><cell></cell></row><row><cell>BLEU-1</cell><cell>0.1724</cell><cell>0.1009</cell><cell>&lt; 0.01</cell></row><row><cell>BLEU-2</cell><cell>0.0744</cell><cell>0.04176</cell><cell>&lt; 0.01</cell></row><row><cell>Average</cell><cell>0.6587</cell><cell>0.6246</cell><cell>0.25</cell></row><row><cell>METEOR</cell><cell>0.2386</cell><cell>0.2073</cell><cell>&lt; 0.01</cell></row><row><cell>Human</cell><cell>2.66</cell><cell>2.57</cell><cell>0.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Effect of differences in response length</cell></row><row><cell>for the Twitter dataset, ∆w = absolute difference in</cell></row><row><cell>#words between a ground truth response and pro-</cell></row><row><cell>posed response</cell></row><row><cell>4.2 Generative Models</cell></row></table><note>In addition to retrieval models, we also consider gen- erative models. In this context, we refer to a model</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Correlation between each metric and human judgements for each response. Correlations shown in the human row result from randomly dividing human judges into two groups.</figDesc><table /><note>as generative if it is able to generate entirely new sentences that are unseen in the training set.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Evaluation methods in the supervised setting have been well studied, see<ref type="bibr" target="#b32">(Walker et al., 1997;</ref><ref type="bibr" target="#b26">Möller et al., 2006;</ref><ref type="bibr" target="#b16">Jokinen and McTear, 2009)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Note that the min in this equation is calculating the number of co-occurrences of n-gram k between the ground truth response r and the proposed responser, as it computes the fewest appearances of k in either response.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To maintain statistical independence between the task and each performance metric, it is important that the word embeddings used are trained on corpora which do not overlap with the task corpus.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Distribution of kappa scores <ref type="table">Table 6</ref> shows the full distribution over κ scores for each pair of human annotators. It is apparent that most of the scores (88.9%) are over 0.4, indicating a moderate agreement. This suggests that the task was reasonable and well understood by the annotators.  <ref type="table">Table 6</ref>: Distribution of pairwise κ scores between each pair of human annotators, other than the annotators that were discarded due to low scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full scatter plots</head><p>We present the scatterplots for all of the metrics consider and their correlation with human judgement, in Figures 3-7 below. As previously emphasized, there is very little correlation for any of the metrics, and the BLEU-3 and BLEU-4 scores are often close to zero.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-formal evaluation of conversational characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Languages: From Formal to Natural</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="22" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">S</forename><surname>Lavie2005</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Correlating human and automatic evaluation of a german surface realiser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cahill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP 2009 Conference Short Papers</title>
		<meeting>the ACL-IJCNLP 2009 Conference Short Papers</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="97" to="100" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Re-evaluation the role of bleu in machine translation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="17" to="53" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Findings of the 2011 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="22" to="64" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A systematic comparison of smoothing techniques for sentence-level bleu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">362</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">213</biblScope>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The measurement of textual coherence with latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Espinosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>P. W. Foltz, W. Kintsch, and T. K. Landauer</editor>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="285" to="307" />
		</imprint>
	</monogr>
	<note>Further metaevaluation of broad-coverage surface realization</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forgues</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Larcheveque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Galley et al.2015a] M. Galley,. Short Papers</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Galley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06863</idno>
		<title level="m">deltableu: A discriminative metric for generation tasks with intrinsically diverse targets</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate evaluation of segment-level machine translation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1183" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
	</analytic>
	<monogr>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Toward finely differentiated evaluation metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eagles Workshop on Standards and Evaluation</title>
		<meeting>the Eagles Workshop on Standards and Evaluation</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Spoken Dialogue Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">K</forename><surname>Mctear2009</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jokinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mctear</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">User interfaces for voice applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="10031" to="10037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3276" to="3284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dumais. 1997. A solution to plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">211</biblScope>
		</imprint>
	</monogr>
	<note>Landauer and Dumais1997</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Utilizing humanto-human conversation examples for a multi domain chat-oriented dialog system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lasguido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE TRANSACTIONS on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1497" to="1505" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03055</idno>
		<title level="m">A diversity-promoting objective function for neural conversation models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06155</idno>
		<title level="m">A persona-based neural conversation model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out: Proceedings of the ACL-04 workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>SIGDIAL</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">J</forename><surname>Lapata2008</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MeMo: towards automatic usability evaluation of spoken dialogue services by user error simulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Möller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on Association for Computational Linguistics (ACL)</title>
		<editor>INTERSPEECH. [Papineni et al.2002a] K. Papineni, S. Roukos, T. Ward, and W. Zhu</editor>
		<meeting>the 40th annual meeting on Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>BLEU: a method for automatic evaluation of machine translation</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Corpus-based comprehensive and diagnostic MT evaluation: Initial Arabic, Chinese, French, and Spanish results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Papineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international conference on Human Language Technology Research</title>
		<meeting>the second international conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="132" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An investigation into the validity of some metrics for automatically evaluating natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; A</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="529" to="558" />
		</imprint>
	</monogr>
	<note>Unsupervised modeling of twitter conversations</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="157" to="162" />
		</imprint>
	</monogr>
	<note>Quantitative evaluation of user simulation techniques for spoken dialogue systems. In 6th Special Interest Group on Discourse and Dialogue (SIGDIAL</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A neural network approach to contextsensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06069</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A hierarchical latent variable encoder-decoder model for generating dialogues</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Stent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05869</idno>
		<idno>arXiv:1508.01745</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics</title>
		<editor>T.-H. Wen, M. Gasic, N. Mrksic, P.-H. Su, D. Vandyke, and S. Young</editor>
		<meeting>the eighth conference on European chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Evaluating evaluation methods for generation in the presence of variation. Wieting et al.2015] J. Wieting, M. Bansal, K. Gimpel, and K. Livescu. 2015. Towards universal paraphrastic sentence embeddings. CoRR, abs/1511.08198</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
