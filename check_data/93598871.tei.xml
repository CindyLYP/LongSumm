<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LSTM: A Search Space Odyssey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Istituto Dalle Molle di studi sull&apos;Intelligenza Artificiale (IDSIA)</orgName>
								<orgName type="institution" key="instit2">the Scuola universitaria professionale della Svizzera italiana (SUPSI)</orgName>
								<orgName type="institution" key="instit3">Università della Svizzera italiana (USI)</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Istituto Dalle Molle di studi sull&apos;Intelligenza Artificiale (IDSIA)</orgName>
								<orgName type="institution" key="instit2">the Scuola universitaria professionale della Svizzera italiana (SUPSI)</orgName>
								<orgName type="institution" key="instit3">Università della Svizzera italiana (USI)</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
							<email>rupesh@idsia.ch</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Istituto Dalle Molle di studi sull&apos;Intelligenza Artificiale (IDSIA)</orgName>
								<orgName type="institution" key="instit2">the Scuola universitaria professionale della Svizzera italiana (SUPSI)</orgName>
								<orgName type="institution" key="instit3">Università della Svizzera italiana (USI)</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Istituto Dalle Molle di studi sull&apos;Intelligenza Artificiale (IDSIA)</orgName>
								<orgName type="institution" key="instit2">the Scuola universitaria professionale della Svizzera italiana (SUPSI)</orgName>
								<orgName type="institution" key="instit3">Università della Svizzera italiana (USI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Istituto Dalle Molle di studi sull&apos;Intelligenza Artificiale (IDSIA)</orgName>
								<orgName type="institution" key="instit2">the Scuola universitaria professionale della Svizzera italiana (SUPSI)</orgName>
								<orgName type="institution" key="instit3">Università della Svizzera italiana (USI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bas</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Istituto Dalle Molle di studi sull&apos;Intelligenza Artificiale (IDSIA)</orgName>
								<orgName type="institution" key="instit2">the Scuola universitaria professionale della Svizzera italiana (SUPSI)</orgName>
								<orgName type="institution" key="instit3">Università della Svizzera italiana (USI)</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Istituto Dalle Molle di studi sull&apos;Intelligenza Artificiale (IDSIA)</orgName>
								<orgName type="institution" key="instit2">the Scuola universitaria professionale della Svizzera italiana (SUPSI)</orgName>
								<orgName type="institution" key="instit3">Università della Svizzera italiana (USI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Istituto Dalle Molle di studi sull&apos;Intelligenza Artificiale (IDSIA)</orgName>
								<orgName type="institution" key="instit2">the Scuola universitaria professionale della Svizzera italiana (SUPSI)</orgName>
								<orgName type="institution" key="instit3">Università della Svizzera italiana (USI)</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Istituto Dalle Molle di studi sull&apos;Intelligenza Artificiale (IDSIA)</orgName>
								<orgName type="institution" key="instit2">the Scuola universitaria professionale della Svizzera italiana (SUPSI)</orgName>
								<orgName type="institution" key="instit3">Università della Svizzera italiana (USI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutík</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Istituto Dalle Molle di studi sull&apos;Intelligenza Artificiale (IDSIA)</orgName>
								<orgName type="institution" key="instit2">the Scuola universitaria professionale della Svizzera italiana (SUPSI)</orgName>
								<orgName type="institution" key="instit3">Università della Svizzera italiana (USI)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LSTM: A Search Space Odyssey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TNNLS.2016.2582924</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recurrent neural networks</term>
					<term>Long Short-Term Memory</term>
					<term>LSTM</term>
					<term>sequence learning</term>
					<term>random search</term>
					<term>fANOVA</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (≈ 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recurrent neural networks with Long Short-Term Memory (which we will concisely refer to as LSTMs) have emerged as an effective and scalable model for several learning problems related to sequential data. Earlier methods for attacking these problems have either been tailored towards a specific problem or did not scale to long time dependencies. LSTMs on the other hand are both general and effective at capturing longterm temporal dependencies. They do not suffer from the optimization hurdles that plague simple recurrent networks (SRNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and have been used to advance the state-ofthe-art for many difficult problems. This includes handwriting recognition <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> and generation <ref type="bibr" target="#b5">[6]</ref>, language modeling <ref type="bibr" target="#b6">[7]</ref> and translation <ref type="bibr" target="#b7">[8]</ref>, acoustic modeling of speech <ref type="bibr" target="#b8">[9]</ref>, speech The central idea behind the LSTM architecture is a memory cell which can maintain its state over time, and non-linear gating units which regulate the information flow into and out of the cell. Most modern studies incorporate many improvements that have been made to the LSTM architecture since its original formulation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. However, LSTMs are now applied to many learning problems which differ significantly in scale and nature from the problems that these improvements were initially tested on. A systematic study of the utility of various computational components which comprise LSTMs (see <ref type="figure" target="#fig_0">Figure 1</ref>) was missing. This paper fills that gap and systematically addresses the open question of improving the LSTM architecture.</p><p>We evaluate the most popular LSTM architecture (vanilla LSTM; Section II) and eight different variants thereof on three benchmark problems: acoustic modeling, handwriting recognition, and polyphonic music modeling. Each variant differs from the vanilla LSTM by a single change. This allows us to isolate the effect of each of these changes on the performance of the architecture. Random search <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> is used to find the best-performing hyperparameters for each variant on each problem, enabling a reliable comparison of the performance of different variants. We also provide insights gained about hyperparameters and their interaction using fANOVA <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. VANILLA LSTM</head><p>The LSTM setup most commonly used in literature was originally described by Graves and Schmidhuber <ref type="bibr" target="#b19">[20]</ref>. We refer to it as vanilla LSTM and use it as a reference for comparison of all the variants. The vanilla LSTM incorporates changes by Gers et al. <ref type="bibr" target="#b20">[21]</ref> and Gers and Schmidhuber <ref type="bibr" target="#b21">[22]</ref> into the original LSTM <ref type="bibr" target="#b14">[15]</ref> and uses full gradient training. Section III provides descriptions of these major LSTM changes.</p><p>A schematic of the vanilla LSTM block can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>. It features three gates (input, forget, output), block input, a single cell (the Constant Error Carousel), an output activation function, and peephole connections <ref type="bibr" target="#b0">1</ref> . The output of the block is recurrently connected back to the block input and all of the gates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Forward Pass</head><p>Let x t be the input vector at time t, N be the number of LSTM blocks and M the number of inputs. Then we get the following weights for an LSTM layer:</p><formula xml:id="formula_0">• Input weights: W z , W i , W f , W o ∈ R N ×M • Recurrent weights: R z , R i , R f , R o ∈ R N ×N • Peephole weights: p i , p f , p o ∈ R N • Bias weights: b z , b i , b f , b o ∈ R N</formula><p>Then the vector formulas for a vanilla LSTM layer forward pass can be written as:</p><formula xml:id="formula_1">z t = W z x t + R z y t−1 + b z z t = g(z t )</formula><p>block input</p><formula xml:id="formula_2">i t = W i x t + R i y t−1 + p i c t−1 + b i i t = σ(ī t ) input gatē f t = W f x t + R f y t−1 + p f c t−1 + b f f t = σ(f t ) forget gate c t = z t i t + c t−1 f t cell o t = W o x t + R o y t−1 + p o c t + b o o t = σ(ō t ) output gate y t = h(c t ) o t block output</formula><p>Where σ, g and h are point-wise non-linear activation functions. The logistic sigmoid (σ(x) = 1 1+e −x ) is used as gate activation function and the hyperbolic tangent (g(x) = h(x) = tanh(x)) is usually used as the block input and output activation function. Point-wise multiplication of two vectors is denoted by .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Backpropagation Through Time</head><p>The deltas inside the LSTM block are then calculated as:</p><formula xml:id="formula_3">δy t = ∆ t + R T z δz t+1 + R T i δi t+1 + R T f δf t+1 + R T o δo t+1 δō t = δy t h(c t ) σ (ō t ) δc t = δy t o t h (c t ) + p o δō t + p i δī t+1 + p f δf t+1 + δc t+1 f t+1 δf t = δc t c t−1 σ (f t ) δī t = δc t z t σ (ī t ) δz t = δc t i t g (z t )</formula><p>Here ∆ t is the vector of deltas passed down from the layer above. If E is the loss function it formally corresponds to ∂E ∂y t , but not including the recurrent dependencies. The deltas for the inputs are only needed if there is a layer below that needs training, and can be computed as follows:</p><formula xml:id="formula_4">δx t = W T z δz t + W T i δī t + W T f δf t + W T o δō t</formula><p>Finally, the gradients for the weights are calculated as follows, where can be any of {z,ī,f ,ō}, and 1 , 2 denotes the outer product of two vectors:</p><formula xml:id="formula_5">δW = T t=0 δ t , x t δp i = T −1 t=0 c t δī t+1 δR = T −1 t=0 δ t+1 , y t δp f = T −1 t=0 c t δf t+1 δb = T t=0 δ t δp o = T t=0 c t δō t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HISTORY OF LSTM</head><p>The initial version of the LSTM block <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> included (possibly multiple) cells, input and output gates, but no forget gate and no peephole connections. The output gate, unit biases, or input activation function were omitted for certain experiments. Training was done using a mixture of Real Time Recurrent Learning (RTRL) <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> and Backpropagation Through Time (BPTT) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Only the gradient of the cell was propagated back through time, and the gradient for the other recurrent connections was truncated. Thus, that study did not use the exact gradient for training. Another feature of that version was the use of full gate recurrence, which means that all the gates received recurrent inputs from all gates at the previous time-step in addition to the recurrent inputs from the block outputs. This feature did not appear in any of the later papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Forget Gate</head><p>The first paper to suggest a modification of the LSTM architecture introduced the forget gate <ref type="bibr" target="#b20">[21]</ref>, enabling the LSTM to reset its own state. This allowed learning of continual tasks such as embedded Reber grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Peephole Connections</head><p>Gers and Schmidhuber <ref type="bibr" target="#b21">[22]</ref> argued that in order to learn precise timings, the cell needs to control the gates. So far this was only possible through an open output gate. Peephole connections (connections from the cell to the gates, blue in <ref type="figure" target="#fig_0">Figure 1</ref>) were added to the architecture in order to make precise timings easier to learn. Additionally, the output activation function was omitted, as there was no evidence that it was essential for solving the problems that LSTM had been tested on so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Full Gradient</head><p>The final modification towards the vanilla LSTM was done by Graves and Schmidhuber <ref type="bibr" target="#b19">[20]</ref>. This study presented the full backpropagation through time (BPTT) training for LSTM networks with the architecture described in Section II, and presented results on the TIMIT <ref type="bibr" target="#b25">[26]</ref> benchmark. Using full BPTT had the added advantage that LSTM gradients could be checked using finite differences, making practical implementations more reliable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Other Variants</head><p>Since its introduction the vanilla LSTM has been the most commonly used architecture, but other variants have been suggested too. Before the introduction of full BPTT training, Gers et al. <ref type="bibr" target="#b26">[27]</ref> utilized a training method based on Extended Kalman Filtering which enabled the LSTM to be trained on some pathological cases at the cost of high computational complexity. Schmidhuber et al. <ref type="bibr" target="#b27">[28]</ref> proposed using a hybrid evolution-based method instead of BPTT for training but retained the vanilla LSTM architecture.</p><p>Bayer et al. <ref type="bibr" target="#b28">[29]</ref> evolved different LSTM block architectures that maximize fitness on context-sensitive grammars. A larger study of this kind was later done by Jozefowicz et al. <ref type="bibr" target="#b29">[30]</ref>. Sak et al. <ref type="bibr" target="#b8">[9]</ref> introduced a linear projection layer that projects the output of the LSTM layer down before recurrent and forward connections in order to reduce the amount of parameters for LSTM networks with many blocks. By introducing a trainable scaling parameter for the slope of the gate activation functions, Doetsch et al. <ref type="bibr" target="#b4">[5]</ref> were able to improve the performance of LSTM on an offline handwriting recognition dataset. In what they call Dynamic Cortex Memory, Otte et al. <ref type="bibr" target="#b30">[31]</ref> improved convergence speed of LSTM by adding recurrent connections between the gates of a single block (but not between the blocks).</p><p>Cho et al. <ref type="bibr" target="#b31">[32]</ref> proposed a simplified variant of the LSTM architecture called Gated Recurrent Unit (GRU). They used neither peephole connections nor output activation functions, and coupled the input and the forget gate into an update gate. Finally, their output gate (called reset gate) only gates the recurrent connections to the block input (W z ). Chung et al. <ref type="bibr" target="#b32">[33]</ref> performed an initial comparison between GRU and Vanilla LSTM and reported mixed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION SETUP</head><p>The focus of our study is to empirically compare different LSTM variants, and not to achieve state-of-the-art results. Therefore, our experiments are designed to keep the setup simple and the comparisons fair. The vanilla LSTM is used as a baseline and evaluated together with eight of its variants. Each variant adds, removes, or modifies the baseline in exactly one aspect, which allows to isolate their effect. They are evaluated on three different datasets from different domains to account for cross-domain variations.</p><p>For fair comparison, the setup needs to be similar for each variant. Different variants might require different settings of hyperparameters to give good performance, and we are interested in the best performance that can be achieved with each variant. For this reason we chose to tune the hyperparameters like learning rate or amount of input noise individually for each variant. Since hyperparameter space is large and impossible to traverse completely, random search was used in order to obtain good-performing hyperparameters <ref type="bibr" target="#b17">[18]</ref> for every combination of variant and dataset. Random search was also chosen for the added benefit of providing enough data for analyzing the general effect of various hyperparameters on the performance of each LSTM variant (Section V-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Each dataset is split into three parts: a training set, a validation set used for early stopping and for optimizing the hyperparameters, and a test set for the final evaluation.</p><p>TIMIT: The TIMIT Speech corpus <ref type="bibr" target="#b25">[26]</ref> is large enough to be a reasonable acoustic modeling benchmark for speech recognition, yet it is small enough to keep a large study such as ours manageable. Our experiments focus on the frame-wise classification task for this dataset, where the objective is to classify each audio-frame as one of 61 phones. <ref type="bibr" target="#b1">2</ref> From the raw audio we extract 12 Mel Frequency Cepstrum Coefficients (MFCCs) <ref type="bibr" target="#b34">[35]</ref> + energy over 25ms hamming-windows with stride of 10ms and a pre-emphasis coefficient of 0.97. This preprocessing is standard in speech recognition and was chosen in order to stay comparable with earlier LSTM-based results (e.g. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>). The 13 coefficients along with their first and second derivatives comprise the 39 inputs to the network and were normalized to have zero mean and unit variance.</p><p>The performance is measured as classification error percentage. The training, testing, and validation sets are split in line with Halberstadt <ref type="bibr" target="#b36">[37]</ref> into 3696, 400, and 192 sequences, having 304 frames on average.</p><p>We restrict our study to the core test set, which is an established subset of the full TIMIT corpus, and use the splits into training, testing, and validation sets as detailed by Halberstadt <ref type="bibr" target="#b36">[37]</ref>. In short, that means we only use the core test set and drop the SA samples 3 from the training set. The validation set is built from some of the discarded samples from the full test set.</p><p>IAM Online: The IAM Online Handwriting Database <ref type="bibr" target="#b37">[38]</ref> 4 consists of English sentences as time series of pen movements that have to be mapped to characters. The IAM-OnDB dataset splits into one training set, two validation sets, and one test set, having 775, 192, 216, and 544 boards each. Each board, see <ref type="figure">Figure 2</ref>(a), contains multiple hand-written lines, which in turn consist of several strokes. We use one line per sequence, and joined the two validation sets together, so the final training, validation, and testing sets contain 5 355, 2 956 and 3 859 sequences respectively.</p><p>Each handwriting line is accompanied with a target character sequence, see <ref type="figure">Figure 2</ref>(b), assembled from the following 81 ASCII characters:</p><formula xml:id="formula_6">abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ 0123456789 !"#&amp;\'() * +,-./[]:;?</formula><p>The board labeled as a08-551z (in the training set) contains a sequence of eleven percent (%) characters that does not have an image in the strokes, and the percent character does not occur in any other board. That board was removed from the experiments.</p><p>We subsampled each sequence to half its length, which speeds up the training and does not harm performance. Each frame of the sequence is a 4-dimensional vector containing ∆x, ∆y (the change in pen position), t (time since the beginning of the stroke), and a fourth dimension that contains value of one at the time of the pen lifting (a transition to the next stroke) and zeroes at all other time steps. Possible starts and ends of characters within each stroke are not explicitly marked. No additional preprocessing (like base-line straightening, cursive correction, etc.) was used.</p><p>The networks were trained using the Connectionist Temporal Classification (CTC) error function by Graves et al. <ref type="bibr" target="#b38">[39]</ref> with 82 outputs (81 characters plus the special empty label). We measure performance in terms of the Character Error Rate (CER) after decoding using best-path decoding <ref type="bibr" target="#b38">[39]</ref>.</p><p>JSB Chorales: JSB Chorales is a collection of 382 fourpart harmonized chorales by J. S. Bach <ref type="bibr" target="#b39">[40]</ref>, consisting of 202 chorales in major keys and 180 chorals in minor keys. We used the preprocessed piano-rolls provided by Boulanger-Lewandowski et al. <ref type="bibr" target="#b40">[41]</ref>. <ref type="bibr" target="#b4">5</ref> These piano-rolls were generated by transposing each MIDI sequence in C major or C minor and sampling frames every quarter note. The networks where trained to do next-step prediction by minimizing the negative log-likelihood. The complete dataset consists of 229, 76, and 77 sequences (training, validation, and test sets respectively) with an average length of 61.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Architectures &amp; Training</head><p>A network with a single LSTM hidden layer and a sigmoid output layer was used for the JSB Chorales task. Bidirectional LSTM <ref type="bibr" target="#b19">[20]</ref> was used for TIMIT and IAM Online tasks, consisting of two hidden layers, one processing the input forwards and the other one backwards in time, both connected to a single softmax output layer. As loss function we employed Cross-Entropy Error for TIMIT and JSB Chorales, while for the IAM Online task the Connectionist Temporal Classification (CTC) loss by Graves et al. <ref type="bibr" target="#b38">[39]</ref> was used. The initial weights for all networks were drawn from a normal distribution with standard deviation of 0.1. Training was done using Stochastic Gradient Descent with Nesterov-style momentum <ref type="bibr" target="#b41">[42]</ref> with updates after each sequence. The learning rate was rescaled by a factor of (1 − momentum). Gradients were computed using full BPTT for LSTMs <ref type="bibr" target="#b19">[20]</ref>. Training stopped after 150 epochs or once there was no improvement on the validation set for more than fifteen epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. LSTM Variants</head><p>The vanilla LSTM from Section II is referred as Vanilla (V). For activation functions we follow the standard and use the logistic sigmoid for σ, and the hyperbolic tangent for both g and h. The derived eight variants of the V architecture are the following. We only report differences to the forward pass formulas presented in Section II-A:</p><p>NIG: No Input Gate: </p><formula xml:id="formula_7">i t =</formula><formula xml:id="formula_8">f t = 1 − i t NP:</formula><p>No Peepholes:</p><formula xml:id="formula_9">i t = W i x t + R i y t−1 + b ī f t = W f x t + R f y t−1 + b f o t = W o x t + R o y t−1 + b o</formula><p>FGR: Full Gate Recurrence:</p><formula xml:id="formula_10">i t = W i x t + R i y t−1 + p i c t−1 + b i + R ii i t−1 + R f i f t−1 + R oi o t−1 f t = W f x t + R f y t−1 + p f c t−1 + b f + R if i t−1 + R f f f t−1 + R of o t−1 o t = W o x t + R o y t−1 + p o c t−1 + b o + R io i t−1 + R f o f t−1 + R oo o t−1</formula><p>The first six variants are self-explanatory. The CIFG variant uses only one gate for gating both the input and the cell recurrent self-connection -a modification of LSTM referred to as Gated Recurrent Units (GRU) <ref type="bibr" target="#b31">[32]</ref>. This is equivalent to setting f t = 1 − i t instead of learning the forget gate weights independently. The FGR variant adds recurrent connections between all the gates as in the original formulation of the LSTM <ref type="bibr" target="#b14">[15]</ref>. It adds nine additional recurrent weight matrices, thus significantly increasing the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyperparameter Search</head><p>While there are other methods to efficiently search for good hyperparameters (cf. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>), random search has several advantages for our setting: it is easy to implement, trivial to parallelize, and covers the search space more uniformly, thereby improving the follow-up analysis of hyperparameter importance.</p><p>We performed 27 random searches (one for each combination of the nine variants and three datasets). Each random search encompasses 200 trials for a total of 5400 trials of randomly sampling the following hyperparameters:</p><p>• number of LSTM blocks per hidden layer: log-uniform samples from <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">200]</ref>;</p><p>• learning rate: log-uniform samples from [10 −6 , 10 −2 ];</p><p>• momentum: 1 − log-uniform samples from [0.01, 1.0];</p><p>• standard deviation of Gaussian input noise: uniform samples from [0, 1].</p><p>In the case of the TIMIT dataset, two additional (boolean) hyperparameters were considered (not tuned for the other two datasets). The first one was the choice between traditional momentum and Nesterov-style momentum <ref type="bibr" target="#b41">[42]</ref>. Our analysis showed that this had no measurable effect on performance so the latter was arbitrarily chosen for all further experiments. The second one was whether to clip the gradients to the range [−1, 1]. This turned out to hurt overall performance, <ref type="bibr" target="#b5">6</ref> therefore the gradients were never clipped in the case of the other two datasets.</p><p>Note that, unlike an earlier small-scale study <ref type="bibr" target="#b32">[33]</ref>, the number of parameters was not kept fixed for all variants. Since different variants can utilize their parameters differently, fixing this number can bias comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS &amp; DISCUSSION</head><p>Each of the 5400 experiments was run on one of 128 AMD Opteron CPUs at 2.5 GHz and took 24.3 h on average to complete. This sums up to a total single-CPU computation time of just below 15 years.</p><p>For TIMIT the test set performance of the best trial were 29.6% classification error (CIFG) which is close to the best reported result of 26.9% <ref type="bibr" target="#b19">[20]</ref>. Our best result of -8.38 loglikelihood (NIAF) on the JSB Chorales dataset on the other hand is well below the -5.56 from Boulanger-Lewandowski et al. <ref type="bibr" target="#b40">[41]</ref>. Best LSTM result is 26.9% For the IAM Online dataset our best result was a Character Error Rate of 9.26% (NP) on the test set. The best previously published result is 11.5% CER by Graves et al.</p><p>[45] using a different and much more extensive preprocessing. <ref type="bibr" target="#b6">7</ref> Note though, that the goal of this study is not to provide state-of-the-art results, but to do a fair comparison of different LSTM variants. So these numbers are only meant as a rough orientation for the reader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison of the Variants</head><p>A summary of the random search results is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Welch's t-test at a significance level of p = 0.05 was used <ref type="bibr" target="#b7">8</ref> to determine whether the mean test set performance of each variant was significantly different from that of the baseline. The box for a variant is highlighted in blue if its mean performance differs significantly from the mean performance of the vanilla LSTM.</p><p>The results in the top half of <ref type="figure" target="#fig_3">Figure 3</ref>   specific to our choice of search ranges. We have tried to chose reasonable ranges for the hyperparameters that include the best settings for each variant and are still small enough to allow for an effective search. The means and variances tend to be rather similar for the different variants and datasets, but even here some significant differences can be found.</p><p>In order to draw some more interesting conclusions we restrict our further analysis to the top 10% performing trials for each combination of dataset and variant (see bottom half of <ref type="figure" target="#fig_3">Figure 3</ref>). This way our findings will be less dependent on the chosen search space and will be representative for the case of "reasonable hyperparameter tuning efforts." <ref type="bibr" target="#b8">9</ref> The first important observation based on <ref type="figure" target="#fig_3">Figure 3</ref> is that removing the output activation function (NOAF) or the forget gate (NFG) significantly hurt performance on all three datasets. Apart from the CEC, the ability to forget old information and the squashing of the cell state appear to be critical for the LSTM architecture. Indeed, without the output activation function, the block output can in principle grow unbounded. Coupling the input and the forget gate avoids this problem and might render the use of an output non-linearity less important, which could explain why GRU performs well without it.</p><p>Input and forget gate coupling (CIFG) did not significantly change mean performance on any of the datasets, although the best performance improved slightly on music modeling. Similarly, removing peephole connections (NP) also did not lead to significant changes, but the best performance improved slightly for handwriting recognition. Both of these variants simplify LSTMs and reduce the computational complexity, so it might be worthwhile to incorporate these changes into the architecture.</p><p>Adding full gate recurrence (FGR) did not significantly change performance on TIMIT or IAM Online, but led to worse results on the JSB Chorales dataset. Given that this variant greatly increases the number of parameters, we generally advise against using it. Note that this feature was present in the original proposal of LSTM <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, but has been absent in all following studies.</p><p>Removing the input gate (NIG), the output gate (NOG), and the input activation function (NIAF) led to a significant reduction in performance on speech and handwriting recognition. However, there was no significant effect on music modeling performance. A small (but statistically insignificant) average performance improvement was observed for the NIG and NIAF architectures on music modeling. We hypothesize that these behaviors will generalize to similar problems such as language modeling. For supervised learning on continuous real-valued data (such as speech and handwriting recognition), the input gate, output gate, and input activation function are all crucial for obtaining good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Impact of Hyperparameters</head><p>The fANOVA framework for assessing hyperparameter importance by Hutter et al. <ref type="bibr" target="#b18">[19]</ref> is based on the observation that marginalizing over dimensions can be done efficiently in regression trees. This allows predicting the marginal error for one hyperparameter while averaging over all the others. Traditionally this would require a full hyperparameter grid search, whereas here the hyperparameter space can be sampled at random.</p><p>Average performance for any slice of the hyperparameter space is obtained by first training a regression tree and then summing over its predictions along the corresponding subset of dimensions. To be precise, a random regression forest of 100 trees is trained and their prediction performance is averaged. This improves the generalization and allows for an estimation of uncertainty of those predictions. The obtained marginals can then be used to decompose the variance into additive components using the functional ANalysis Of VAriance (fANOVA) method [46] which provides an insight into the overall importance of hyperparameters and their interactions.</p><p>Learning rate: Learning rate is the most important hyperparameter, therefore it is very important to understand how to set it correctly in order to achieve good performance. <ref type="figure" target="#fig_5">Figure 4</ref> shows (in blue) how setting the learning rate value affects the predicted average performance on the test set. It is important to note that this is an average over all other hyperparameters and over all the trees in the regression forest. The shaded area around the curve indicates the standard deviation over tree predictions (not over other hyperparameters), thus quantifying the reliability of the average. The same is shown in green with the predicted average training time.</p><p>The plots in <ref type="figure" target="#fig_5">Figure 4</ref> show that the optimal value for the learning rate is dependent on the dataset. For each dataset, there is a large basin (up to two orders of magnitude) of good learning rates inside of which the performance does not vary much. A related but unsurprising observation is that there is a sweet-spot for the learning rate at the high end of the basin. <ref type="bibr" target="#b9">10</ref> In this region, the performance is good and the training time is small. So while searching for a good learning rate for the LSTM, it is sufficient to do a coarse search by starting with a high value (e.g. 1.0) and dividing it by ten until performance stops increasing. <ref type="figure">Figure 5</ref> also shows that the fraction of variance caused by the learning rate is much bigger than the fraction due to interaction between learning rate and hidden layer size (some part of the "higher order" piece, for more see below at Interaction of Hyperparameters). This suggests that the learning rate can be quickly tuned on a small network and then used to train a large one.</p><p>Hidden Layer Size: Not surprisingly the hidden layer size is an important hyperparameter affecting the LSTM network performance. As expected, larger networks perform better, but with diminishing returns. It can also be seen in <ref type="figure" target="#fig_5">Figure 4</ref> (middle, green) that the required training time increases with the network size. Note that the scale here is wall-time and thus factors in both the increased computation time for each epoch as well as the convergence speed.</p><p>Input Noise: Additive Gaussian noise on the inputs, a traditional regularizer for neural networks, has been used for LSTM as well. However, we find that not only does it almost always hurt performance, it also slightly increases training times. The only exception is TIMIT, where a small dip in error for the range of [0.2, 0.5] is observed.</p><p>Momentum: One unexpected result of this study is that momentum affects neither performance nor training time in any significant way. This follows from the observation that for none of the datasets, momentum accounted for more than 1% of the variance of test set performance. It should be noted that for TIMIT the interaction between learning rate and momentum accounts for 2.5% of the total variance, but as with learning rate × hidden size (cf. Interaction of Hyperparameters below) it does not reveal any interpretable structure. This may be the result of our choice to scale learning rates dependent on momentum (Section IV-B). These observations suggest that momentum does not offer substantial benefits when training LSTMs with online stochastic gradient descent.</p><p>Analysis of Variance: <ref type="figure">Figure 5</ref> shows what fraction of the test set performance variance can be attributed to different hyperparameters. It is obvious that the learning rate is by far the most important hyperparameter, always accounting for more than two thirds of the variance. The next most important hyperparameter is the hidden layer size, followed by the input noise, leaving the momentum with less than one percent of the variance. Higher order interactions play an important role in the case of TIMIT, but are much less important for the other two data sets.</p><p>Interaction of Hyperparameters: Some hyperparameters interact with each other resulting in different performance from what could be expected by looking at them individually. As shown in <ref type="figure">Figure 5</ref> all these interactions together explain between 5% and 20% of the variance in test set performance. Understanding these interactions might allow us to speed up the search for good combinations of hyperparameters. To that end we visualize the interaction between all pairs of hyperparameters in <ref type="figure">Figure 6</ref>. Each heat map in the left part shows marginal performance for different values of the respective two hyperparameters. This is the average performance predicted by the decision forest when marginalizing over all other hyperparameters. So each one is the 2D version of the performance plots from <ref type="figure" target="#fig_5">Figure 4</ref> in the paper.</p><p>The right side employs the idea of ANOVA to better illustrate the interaction between the hyperparameters. This means that variance of performance that can be explained by varying a single hyperparameter has been removed. In case two hyperparameters do not interact at all (are perfectly independent), that residual would thus be all zero (grey).  . Predicted marginal error (blue) and marginal time for different values of the learning rate, hidden size, and the input noise (columns) for the test set of all three datasets (rows). The shaded area indicates the standard deviation between the tree-predicted marginals and thus the reliability of the predicted mean performance. Note that each plot is for the vanilla LSTM but curves for all variants that are not significantly worse look very similar. <ref type="figure">Figure 5</ref>. Pie charts showing which fraction of variance of the test set performance can be attributed to each of the hyperparameters. The percentage of variance that is due to interactions between multiple parameters is indicated as "higher order."</p><p>For example, looking at the pair hidden size and learning rate on the left side for the TIMIT dataset, we can see that performance varies strongly along the x-axis (learning rate), first decreasing and then increasing again. This is what we would expect knowing the valley-shape of the learning rate from <ref type="figure" target="#fig_5">Figure 4</ref>. Along the y-axis (hidden size) performance seems to decrease slightly from top to bottom. Again this is roughly what we would expect from the hidden size plot in <ref type="figure" target="#fig_5">Figure 4</ref>.</p><p>On the right side of <ref type="figure">Figure 6</ref> we can see for the same pair of hyperparameters how their interaction differs from the case of them being completely independent. This heat map exhibits less structure, and it may in fact be the case that we would need more samples to properly analyze the interplay between them. However, given our observations so far this might not be worth the effort. In any case, it is clear from the plot on the left that varying the hidden size does not change the region of optimal learning rate.</p><p>One clear interaction pattern can be observed in the IAM Online and JSB datasets between learning rate and input noise. Here it can be seen that for high learning rates ( 10 −4 ) lower input noise ( .5) is better like also observed in the marginals from <ref type="figure" target="#fig_5">Figure 4</ref>. But this trend reverses for lower learning rates, where higher values of input noise are beneficial. Though interesting this is not of any practical relevance because performance is generally bad in that region of low learning rates. Apart from this, however, it is difficult to discern any regularities in the analyzed hyperparameter interactions. We conclude that there is little practical value in attending to the interplay between hyperparameters. So for practical purposes hyperparameters can be treated as approximately independent and thus optimized separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This paper reports the results of a large scale study on variants of the LSTM architecture. We conclude that the most commonly used LSTM architecture (vanilla LSTM) performs reasonably well on various datasets. None of the eight investigated modifications significantly improves performance. However, certain modifications such as coupling the input and  <ref type="figure">Figure 6</ref>. Total marginal predicted performance for all pairs of hyperparameters (left) and the variation only due to their interaction (right). The plot is divided vertically into three subplots, one for every dataset (TIMIT, IAM Online, and JSB Chorales). The subplots itself are divided horizontally into two parts, each containing a lower triangular matrix of heat maps. The rows and columns of these matrices represent the different hyperparameters (learning rate, momentum, hidden size, and input noise) and there is one heat map for every combination. The color encodes the performance as measured by the Classification Error for TIMIT, Character Error Rate for IAM Online and Negative Log-Likelihood for the JSB Chorales Dataset. For all datasets low (blue) is better than high (red).</p><p>forget gates (CIFG) or removing peephole connections (NP) simplified LSTMs in our experiments without significantly decreasing performance. These two variants are also attractive because they reduce the number of parameters and the computational cost of the LSTM. The forget gate and the output activation function are the most critical components of the LSTM block. Removing any of them significantly impairs performance. We hypothesize that the output activation function is needed to prevent the unbounded cell state to propagate through the network and destabilize learning. This would explain why the LSTM variant GRU can perform reasonably well without it: its cell state is bounded because of the coupling of input and forget gate.</p><p>As expected, the learning rate is the most crucial hyperparameter, followed by the network size. Surprisingly though, the use of momentum was found to be unimportant in our setting of online gradient descent. Gaussian noise on the inputs was found to be moderately helpful for TIMIT, but harmful for the other datasets.</p><p>The analysis of hyperparameter interactions revealed no apparent structure. Furthermore, even the highest measured interaction (between learning rate and network size) is quite small. This implies that for practical purposes the hyperparameters can be treated as approximately independent. In particular, the learning rate can be tuned first using a fairly small network, thus saving a lot of experimentation time.</p><p>Neural networks can be tricky to use for many practitioners compared to other methods whose properties are already well understood. This has remained a hurdle for newcomers to the field since a lot of practical choices are based on the intuitions of experts, as well as experiences gained over time. With this study, we have attempted to back some of these intuitions with experimental results. We have also presented new insights, both on architecture selection and hyperparameter tuning for LSTM networks which have emerged as the method of choice for solving complex sequence learning problems. In future work, we plan to explore more complex modifications of the LSTM architecture.</p><p>Rupesh Srivastava is a PhD student at IDSIA &amp; USI in Switzerland, supervised by Prof. Jürgen Schmidhuber. He currently works on understanding and improving neural network architectures. In particular, he has worked on understanding the beneficial properties of local competition in neural networks, and new architectures which allow gradientbased training of extremely deep networks. In the past, Rupesh worked on reliability based design optimization using evolutionary algorithms at the Kanpur Genetic Algorithms Laboratory, supervised by Prof. Kalyanmoy Deb for his Masters thesis.</p><p>Jan Koutník received his Ph.D. in computer science from the Czech Technical University in Prague in 2008. He works as machine learning researcher at The Swiss AI Lab IDSIA. His research is mainly focused on artificial neural networks, recurrent neural networks, evolutionary algorithms and deep-learning applied to reinforcement learning, control problems, image classification, handwriting and speech recognition. to revolutionize connected handwriting recognition, speech recognition, machine translation, optical character recognition, image caption generation, and are now in use at Google, Apple, Microsoft, IBM, Baidu, and many other companies. IDSIA's Deep Learners were also the first to win object detection and image segmentation contests, and achieved the world's first superhuman visual classification results, winning nine international competitions in machine learning &amp; pattern recognition (more than any other team). They also were the first to learn control policies directly from high-dimensional sensory input using reinforcement learning. His research group also established the field of mathematically rigorous universal AI and optimal universal problem solvers. His formal theory of creativity &amp; curiosity &amp; fun explains art, science, music, and humor. He also generalized algorithmic information theory and the manyworlds theory of physics, and introduced the concept of Low-Complexity Art, the information age's extreme form of minimal art. Since 2009 he has been member of the European Academy of Sciences and Arts. He has published 333 peer-reviewed papers, earned seven best paper/best video awards, and is recipient of the 2013 Helmholtz Award of the International Neural Networks Society and the 2016 IEEE Neural Networks Pioneer Award.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Detailed schematic of the Simple Recurrent Network (SRN) unit (left) and a Long Short-Term Memory block (right) as used in the hidden layers of a recurrent neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Ben Zoma said: "The days of 1thy life means in the day-time; all the days of 1thy life means even at night-time ." (Berochoth .) And the Rabbis thought it important that when we read the (b) Figure 2. (a) Example board (a08-551z, training set) from the IAM-OnDB dataset and (b) its transcription into character label sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>represent the distribution of all 200 test set performances over the whole search space. Any conclusions drawn from them are therefore V CIFG FGR NP NOG NIAF NIG NFG NOAF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Test set performance for all 200 trials (top) and for the best 10% (bottom) trials (according to the validation set) for each dataset and variant. Boxes show the range between the 25 th and the 75 th percentile of the data, while the whiskers indicate the whole range. The red dot represents the mean and the red line the median of the data. The boxes of variants that differ significantly from the vanilla LSTM are shown in blue with thick lines. The grey histogram in the background presents the average number of parameters for the top 10% performers of every variant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Predicted marginal error (blue) and marginal time for different values of the learning rate, hidden size, and the input noise (columns) for the test set of all three datasets (rows). The shaded area indicates the standard deviation between the tree-predicted marginals and thus the reliability of the predicted mean performance. Note that each plot is for the vanilla LSTM but curves for all variants that are not significantly worse look very similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0 -1.5 -1.0 -0.5 -0.0 momentum 1.0 1.2 1.5 1.8 20 -1.5 -1.0 -0.5 -0.0 momentum 1.0 1.2 1.5 1.8 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Configuration. In Proc. of LION-5, pages 507-523, 2011. [45] Alex Graves, Marcus Liwicki, Horst Bunke, Jürgen Schmidhuber, and Santiago Fernández. Unconstrained on-line handwriting recognition with recurrent neural networks. In Advances in Neural Information Processing Systems, pages 577-584, 2008. [46] Giles Hooker. Generalized Functional ANOVA Diagnostics for High-Dimensional Functions of Dependent Variables. Journal of Computational and Graphical Statistics, 16(3):709-732, September 2007. ISSN 1061-8600, 1537-2715. doi: 10.1198/106186007X237892. URL http://www. tandfonline.com/doi/abs/10.1198/106186007X237892. R. Steunebrink is a postdoctoral researcher at the Swiss AI lab IDSIA. He received his PhD in 2010 from Utrecht University, the Netherlands. Bas's interests and expertise include Artificial General Intelligence (AGI), cognitive robotics, machine learning, resource-constrained control, and affective computing. Schmidhuber is Professor of Artificial Intelligence (AI) at USI in Switzerland. He has pioneered self-improving general problem solvers since 1987, and Deep Learning Neural Networks (NNs) since 1991. The recurrent NNs (RNNs) developed by his research groups at the Swiss AI Lab IDSIA &amp; USI &amp; SUPSI &amp; TU Munich were the first RNNs to win official international contests. They have helped</figDesc><table><row><cell>Bas Jürgen</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Some studies omit peephole connections, described in Section III-B. arXiv:1503.04069v2 [cs.NE] 4 Oct 2017</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that in linguistics a phone represents a distinct speech sound independent of the language. In contrast, a phoneme refers to a sound that distinguishes two words in a given language<ref type="bibr" target="#b33">[34]</ref>. These terms are often confused in the machine learning literature.<ref type="bibr" target="#b2">3</ref> The dialect sentences (the SA samples) were meant to expose the dialectal variants of the speakers and were read by all 630 speakers. We follow<ref type="bibr" target="#b36">[37]</ref> and remove them because they bias the distribution of phones.<ref type="bibr" target="#b3">4</ref> The IAM-OnDB was obtained from http://www.iam.unibe.ch/fki/databases/ iam-on-line-handwriting-database</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Available at http://www-etud.iro.umontreal.ca/ ∼ boulanni/icml2012 at the time of writing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Although this may very well be the result of the range having been chosen too tightly.<ref type="bibr" target="#b6">7</ref> Note that these numbers differ from the best test set performances that can be found inFigure 3. This is the case because here we only report the single best performing trial as determined on the validation set. InFigure 3, on the other hand, we show the test set performance of the 20 best trials for each variant.<ref type="bibr" target="#b7">8</ref> We applied the Bonferroni adjustment to correct for performing eight different tests (one for each variant).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">How much effort is "reasonable" will still depend on the search space. If the ranges are chosen much larger, the search will take much longer to find good hyperparameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">Note that it is unfortunately outside the investigated range for IAM Online and JSB Chorales. This means that ideally we should have chosen the range of learning rates to include higher values as well.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen Netzen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<pubPlace>München</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Masters Thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Field Guide to Dynamical Recurrent Neural Networks</title>
		<editor>S. C. Kremer and J. F. Kolen</editor>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Novel Connectionist System for Improved Unconstrained Handwriting Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théodore</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4569</idno>
		<ptr target="http://arxiv.org/abs/1312.4569" />
		<title level="m">Dropout improves Recurrent Neural Networks for Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and robust training of recurrent neural networks for offline handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Kozielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<ptr target="http://people.sabanciuniv.edu/berrin/cs581/Papers/icfhr2014/data/4334a279.pdf" />
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Frontiers in Handwriting Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<ptr target="http://arxiv.org/abs/1308.0850" />
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Recurrent Neural Network Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<ptr target="http://arxiv.org/abs/1409.2329" />
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Addressing the Rare Word Problem in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8206</idno>
		<ptr target="http://arxiv.org/abs/1410.8206" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Françoise</forename><surname>Beaufays</surname></persName>
		</author>
		<ptr target="http://193.6.4.39/∼czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS141304.PDF" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH)</title>
		<meeting>the Annual Conference of International Speech Communication Association (INTERSPEECH)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TTS synthesis with bidirectional LSTM based recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenglong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Protein Secondary Structure Prediction with Long Short Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaae</forename><surname>Søren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7828</idno>
		<idno>arXiv: 1412.7828</idno>
		<ptr target="http://arxiv.org/abs/1412.7828" />
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
	<note>cs, q-bio</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-resolution linear prediction based features for audio onset detection with bidirectional LSTM neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gabrielli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Squartini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2014.6853982</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="2164" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<idno>arXiv: 1411.4389</idno>
		<ptr target="http://arxiv.org/abs/1411.4389" />
		<title level="m">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<imprint>
			<date type="published" when="2014-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Long Short Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>FKI-207-95</idno>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.3117" />
		<imprint>
			<date type="published" when="1995-08" />
			<pubPlace>München</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="http://www.bioinf.jku.at/publications/older/2604.pdf" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recent Advances in Finding Best Operating Conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.2307/2281072</idno>
		<ptr target="http://www.jstor.org/stable/2281072" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">264</biblScope>
			<biblScope unit="page" from="789" to="798" />
			<date type="published" when="1953-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger J.-B</forename><surname>Solis</surname></persName>
		</author>
		<idno type="DOI">http://pubsonline.informs.org/doi/abs/10.1287/moor.6.1.19</idno>
		<ptr target="http://pubsonline.informs.org/doi/abs/10.1287/moor.6.1.19" />
	</analytic>
	<monogr>
		<title level="j">Wets. Minimization by Random Search Techniques. Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="1981-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2188395" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An Efficient Approach for Assessing Hyperparameter Importance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v32/hutter14.html" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="754" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>0893-6080. doi: 10.1016/j. neunet.2005.06.042</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0893608005001206" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks, 1999. ICANN 99. Ninth International Conference on</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE-INNS-ENNS International Joint Conference on</title>
		<meeting>the IEEE-INNS-ENNS International Joint Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
	<note>Neural Networks. ISBN 0769506194</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The utility driven dynamic error propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Fallside</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge Department of Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Complexity of exact gradient computation algorithms for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno>NU-CCS-89-27</idno>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>Boston</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Northeastern University, College of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalization of backpropagation with application to a recurrent gas market model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CD-ROM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Js Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NTIS Order</title>
		<imprint>
			<biblScope unit="page" from="91" to="505065" />
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Antonio</forename><surname>Pérez-Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Defk-Lstm</surname></persName>
		</author>
		<title level="m">ESANN 2002, Proceedings of the 10th Eurorean Symposium on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training Recurrent Networks by EVOLINO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F J</forename><surname>Gagliolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="757" to="779" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evolving memory cell structures for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">http://link.springer.com/chapter/10.1007/978-3-642-04277-5_76</idno>
		<ptr target="http://link.springer.com/chapter/10.1007/978-3-642-04277-576" />
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks-ICANN 2009</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="755" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic Cortex Memory: Enhancing Recurrent Neural Networks for Gradient-Based Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Otte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zell</surname></persName>
		</author>
		<idno type="DOI">http://link.springer.com/chapter/10.1007/978-3-319-11179-7_1</idno>
		<idno>978-3-319-11178-0, 978-3-319-11179-7</idno>
		<ptr target="http://link.springer.com/chapter/10.1007/978-3-319-11179-71" />
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2014</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014-09" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<ptr target="http://arxiv.org/abs/1406.1078" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<ptr target="http://arxiv.org/abs/1412.3555" />
		<title level="m">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dictionary of linguistics and phonetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crystal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distance measures for speech recognition: Psychological and instrumental</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition and Artificial Intelligence</title>
		<editor>C. H. Chen</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1976" />
			<biblScope unit="page" from="374" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">Ph.d.</note>
	<note>The Technical University of Munich</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Heterogeneous acoustic measurements and multiple classifiers for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Halberstadt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">IAM-OnDB-an on-line English sentence database acquired from handwritten text on a whiteboard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Eighth International Conference on</title>
		<meeting>Eighth International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="956" to="961" />
		</imprint>
	</monogr>
	<note>Document Analysis and Recognition</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1143891" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Harmonising chorales by probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moray</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<ptr target="http://icml.cc/discuss/2012/590.html" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1159" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v28/sutskever13.html" />
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Practical Bayesian Optimization of Machine Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequential Model-Based Optimization for General Algorithm Klaus Greff received his Diploma in Computer Science from the University of Kaiserslautern, Germany in 2011. Currently he is pursuing his PhD at IDSIA in Lugano, Switzerland, under the supervision of Prof</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Jürgen Schmidhuber in the field of Machine Learning. His research interests include Sequence Learning and Recurrent Neural Networks</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
