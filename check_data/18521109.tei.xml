<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dialog-based Language Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
							<email>jase@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dialog-based Language Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of [30] and large-scale question answering from [4]. We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher&apos;s response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all. Task 1: Imitating an Expert Student Task 2: Positive and Negative Feedback Mary went to the hallway. Mary went to the hallway. John moved to the bathroom. John moved to the bathroom. Mary travelled to the kitchen. Mary travelled to the kitchen. Where is Mary? A:kitchen Where is Mary? A:playground Where is John? A:bathroom No, that&apos;s incorrect. Where is John? A:bathroom Yes, that&apos;s right! (+) Task 3: Answers Supplied by Teacher Task 4: Hints Supplied by Teacher Mary went to the hallway. Mary went to the hallway. John moved to the bathroom. John moved to the bathroom. Mary travelled to the kitchen. Mary travelled to the kitchen. Where is Mary? A:bedroom Where is Mary? A:bathroom No, the answer is kitchen. No, they are downstairs. Where is John? A:bathroom Where is John? A:kitchen Correct! (+) No, they are upstairs. Task 5: Supporting Facts Supplied by Teacher Task 6: Partial Feedback Mary went to the hallway. Mary went to the hallway. John moved to the bathroom. John moved to the bathroom. Mary travelled to the kitchen. Mary travelled to the kitchen. Where is Mary? A:kitchen Where is Mary? A:kitchen Yes, that&apos;s right! (+) Yes, that&apos;s right! Where is John? A:hallway Where is John? A:bathroom No, because John moved to the bathroom. Yes, that&apos;s correct! (+) Task 7: No Feedback Task 8: Imitation and Feedback Mixture Mary went to the hallway. Mary went to the hallway. John moved to the bathroom. John moved to the bathroom. Mary travelled to the kitchen. Mary travelled to the kitchen. Where is Mary? A:kitchen Where is Mary? A:kitchen Yes, that&apos;s right! Where is John? A:bathroom Where is John? A:bathroom That&apos;s right! (+) Yes, that&apos;s correct! Task 9: Asking For Corrections Task 10: Asking For Supporting Facts Mary went to the hallway. Mary went to the hallway. John moved to the bathroom. John moved to the bathroom. Mary travelled to the kitchen. Mary travelled to the kitchen.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many of machine learning's successes have come from supervised learning, which typically involves employing annotators to label large quantities of data per task. However, humans can learn by acting and learning from the consequences of (i.e, the feedback from) their actions. When humans act in dialogs (i.e., make speech utterances) the feedback is from other human's responses, which hence contain very rich information. This is perhaps most pronounced in a student/teacher scenario where the teacher provides positive feedback for successful communication and corrections for unsuccessful ones <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28]</ref>. However, in general any reply from a dialog partner, teacher or not, is likely to contain an informative training signal for learning how to use language in subsequent conversations.</p><p>In this paper we explore whether we can train machine learning models to learn from dialogs. The ultimate goal is to be able to develop an intelligent dialog agent that can learn while conducting conversations. To do that it needs to learn from feedback that is supplied as natural language. However, most machine learning tasks in the natural language processing literature are not of this form: they are either hand labeled at the word level (part of speech tagging, named entity recognition), segment (chunking) or sentence level (question answering) by labelers. Subsequently, learning algorithms have been developed to learn from that kind of supervision. We therefore need to develop evaluation datasets for the dialog-based language learning setting, as well as developing models and algorithms able to learn in such a regime.</p><p>The contribution of the present work is thus:</p><p>• We introduce a set of tasks that model natural feedback from a teacher and hence assess the feasibility of dialog-based language learning. • We evaluate some baseline models on this data, comparing to standard supervised learning.</p><p>• We introduce a novel forward prediction model, whereby the learner tries to predict the teacher's replies to its actions, yielding promising results, even with no reward signal at all. In human language learning the usefulness of social interaction and natural infant directed conversations is emphasized, see e.g. the review paper <ref type="bibr" target="#b8">[9]</ref>, although the usefulness of feedback for learning grammar is disputed <ref type="bibr" target="#b12">[13]</ref>. Support for the usefulness of feedback is found however in second language learning <ref type="bibr" target="#b0">[1]</ref> and learning by students <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>In machine learning, one line of research has focused on supervised learning from dialogs using neural models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4]</ref>. Question answering given either a database of knowledge <ref type="bibr" target="#b1">[2]</ref> or short stories <ref type="bibr" target="#b29">[30]</ref> can be considered as a simple case of dialog which is easy to evaluate. Those tasks typically do not consider feedback. There is work on the the use of feedback and dialog for learning, notably for collecting knowledge to answer questions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>, the use of natural language instruction for learning symbolic rules <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5]</ref> and the use of binary feedback (rewards) for learning parsers <ref type="bibr" target="#b2">[3]</ref>.</p><p>Another setting which uses feedback is the setting of reinforcement learning, see e.g. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> for a summary of its use in dialog. However, those approaches often consider reward as the feedback model rather than exploiting the dialog feedback per se. Nevertheless, reinforcement learning ideas have been used to good effect for other tasks as well, such as understanding text adventure games <ref type="bibr" target="#b14">[15]</ref>, image captioning <ref type="bibr" target="#b31">[32]</ref>, machine translation and summarization <ref type="bibr" target="#b17">[18]</ref>. Recently, <ref type="bibr" target="#b13">[14]</ref> also proposed a reward-based learning framework for learning how to learn.</p><p>Finally, forward prediction models, which we make use of in this work, have been used for learning eye tracking <ref type="bibr" target="#b20">[21]</ref>, controlling robot arms <ref type="bibr" target="#b11">[12]</ref> and vehicles <ref type="bibr" target="#b26">[27]</ref>, and action-conditional video prediction in atari games <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>. We are not aware of their use thus far for dialog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dialog-Based Supervision Tasks</head><p>Dialog-based supervision comes in many forms. As far as we are aware it is a currently unsolved problem which type of learning strategy will work in which setting. In this section we therefore identify different modes of dialog-based supervision, and build a learning problem for each. The goal is to then evaluate learners on each type of supervision.</p><p>We thus begin by selecting two existing datasets: (i) the single supporting fact problem from the bAbI datasets <ref type="bibr" target="#b29">[30]</ref> which consists of short stories from a simulated world followed by questions; and (ii) the MovieQA dataset <ref type="bibr" target="#b3">[4]</ref> which is a large-scale dataset (∼ 100k questions over ∼ 75k entities) based on questions with answers in the open movie database (OMDb). For each dataset we then consider ten modes of dialog-based supervision. The supervision modes are summarized in <ref type="figure">Fig. 1</ref> using a snippet of the bAbI dataset as an example. The same setups are also used for MovieQA, some examples of which are given in <ref type="bibr">Fig 2.</ref> We now describe each supervision setup in turn.</p><p>Imitating an Expert Student In Task 1 the dialogs take place between a teacher and an expert student who gives semantically coherent answers. Hence, the task is for the learner to imitate that expert student, and become an expert themselves. For example, imagine the real-world scenario where a child observes their two parents talking to each other, it can learn but it is not actually taking part in the conversation. Note that our main goal in this paper is to examine how a non-expert can learn to improve its dialog skills while conversing. The rest of our tasks will hence concentrate on that goal. This task can be seen as a natural baseline for the rest of our tasks given the same input dialogs and questions.</p><p>Positive and Negative Feedback In Task 2, when the learner answers a question the teacher then replies with either positive or negative feedback. In our experiments the subsequent responses are variants of "No, that's incorrect" or "Yes, that's right". In the datasets we build there are 6 templates for positive feedback and 6 templates for negative feedback, e.g. "Sorry, that's not it.", "Wrong", etc. To separate the notion of positive from negative (otherwise the signal is just words with no notion that yes is better than no) we assume an additional external reward signal that is not part of the text. As shown in <ref type="figure">Fig. 1</ref> Task 2, (+) denotes positive reward external to the dialog (e.g. feedback provided by another medium, such as a nod of the head from the teacher). This is provided with every positive response. Note the difference in supervision compared to Task 1: there every answer is right and provides positive supervision. Here, only the answers the learner got correct <ref type="figure">Figure 1</ref>: Sample dialogs with differing supervision signals (tasks 1 to 10). In each case the same example is given for simplicity. Black text is spoken by the teacher, red text denotes responses by the learner, blue text is provided by an expert student (which the learner can imitate), (+) denotes positive reward external to the dialog (e.g. feedback provided by another medium, such as a nod of the head from the teacher).</p><p>have positive supervision. This could clearly be a problem when the learner is unskilled: it will supply incorrect answers and never (or hardly ever) receive positive responses.</p><p>Answers Supplied by Teacher In Task 3 the teacher gives positive and negative feedback as in Task 2, however when the learner's answer is incorrect, the teacher also responds with the correction. For example if "where is Mary?" is answered with the incorrect answer "bedroom" the teacher responds "No, the answer is kitchen"', see <ref type="figure">Fig. 1</ref> Task 3. If the learner knows how to use this extra information, it effectively has as much supervision signal as with Task 1, and much more than for Task 2.</p><p>Hints Supplied by Teacher In Task 4, the corrections provided by the teacher do not provide the exact answer as in Task 3, but only a useful hint. This setting is meant to mimic the real life <ref type="figure">Figure 2</ref>: Samples from the MovieQA dataset <ref type="bibr" target="#b3">[4]</ref>. In our experiments we consider 10 different language learning setups as described in <ref type="figure">Figure 1</ref> and Sec. 3. The examples given here are for tasks and 3, questions are in black and answers in red, and (+) indicates receiving positive reward. occurrence of being provided only partial information about what you did wrong. In our datasets we do this by providing the class of the correct answer, e.g. "No, they are downstairs" if the answer should be kitchen, or "No, it is a director" for the question "Who directed Monsters, Inc.?" (using OMDB metadata). The supervision signal here is hence somewhere in between Task 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supporting Facts Supplied by Teacher In Task 5, another way of providing partial supervision</head><p>for an incorrect answer is explored. Here, the teacher gives a reason (explanation) why the answer is wrong by referring to a known fact that supports the true answer that the incorrect answer may contradict. For example "No, because John moved to the bathroom" for an incorrect answer to "Where is John?", see <ref type="figure">Fig. 1</ref> Task 5. This is related to what is termed strong supervision in <ref type="bibr" target="#b29">[30]</ref> where supporting facts and answers are given for question answering tasks.</p><p>Partial Feedback Task 6 considers the case where external rewards are only given some of (50% of) the time for correct answers, the setting is otherwise identical to Task 3. This attempts to mimic the realistic situation of some learning being more closely supervised (a teacher rewarding you for getting some answers right) whereas other dialogs have less supervision (no external rewards). The task attempts to assess the impact of such partial supervision.</p><p>No Feedback In Task 7 external rewards are not given at all, only text, but is otherwise identical to Tasks 3 and 6. This task explores whether it is actually possible to learn how to answer at all in such a setting. We find in our experiments the answer is surprisingly yes, at least in some conditions.</p><p>Imitation and Feedback Mixture Task 8 combines Tasks 1 and 2. The goal is to see if a learner can learn successfully from both forms of supervision at once. This mimics a child both observing pairs of experts talking (Task 1) while also trying to talk (Task 2).</p><p>Asking For Corrections Another natural way of collecting supervision is for the learner to ask questions of the teacher about what it has done wrong. Task 9 tests one of the most simple instances, where asking "Can you help me?" when wrong obtains from the teacher the correct answer. This is thus related to the supervision in Task 3 except the learner must first ask for help in the dialog. This is potentially harder for a model as the relevant information is spread over a larger context.</p><p>Asking for Supporting Facts Finally, in Task 10, a second less direct form of supervision for the learner after asking for help is to receive a hint rather than the correct answer, such as "A relevant fact is John moved to the bathroom" when asking "Can you help me?", see <ref type="figure">Fig. 1</ref> Task 10. This is thus related to the supervision in Task 5 except the learner must request help.</p><p>In our experiments we constructed the ten supervision tasks for the two datasets which are all available for download at http://fb.ai/babi. They were built in the following way: for each task we consider a fixed policy 1 for performing actions (answering questions) which gets questions correct with probability π acc (i.e. the chance of getting the red text correct in Figs. 1 and 2). We thus can compare different learning algorithms for each task over different values of π acc (0.5, 0.1 and 0.01). In all cases a training, validation and test set is provided. For the bAbI dataset this consists of</p><p>Since the policy is fixed and actually does not depend on the model being learnt, one could also think of it as coming from another agent (or the same agent in the past) which in either case is an imperfect expert. 1000, 100 and 1000 questions respectively per task, and for movieQA there are ∼ 96k, ∼ 10k and ∼ 10k respectively. MovieQA also includes a knowledge base (KB) of ∼ 85k facts from OMDB, the memory network model we employ uses inverted index retrieval based on the question to form relevant memories from this set, see <ref type="bibr" target="#b3">[4]</ref> for more details. Note that because the policies are fixed the experiments in this paper are not in a reinforcement learning setting.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Models</head><p>Our main goal is to explore training strategies that can execute dialog-based language learning. To this end we evaluate four possible strategies: imitation learning, reward-based imitation, forward prediction, and a combination of reward-based imitation and forward prediction. We will subsequently describe each in turn.</p><p>We test all of these approaches with the same model architecture: an end-to-end memory network (MemN2N) <ref type="bibr" target="#b25">[26]</ref>. Memory networks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref> are a recently introduced model that have been shown to do well on a number of text understanding tasks, including question answering and dialog <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2]</ref>, language modeling <ref type="bibr" target="#b25">[26]</ref> and sentence completion <ref type="bibr" target="#b6">[7]</ref>. In particular, they outperform LSTMs and other baselines on the bAbI datasets <ref type="bibr" target="#b29">[30]</ref> which we employ with dialog-based learning modifications in Sec. 3. They are hence a natural baseline model for us to use in order to explore differing modes of learning in our setup. In the following we will first review memory networks, detailing the explicit choices of architecture we made, and then show how they can be modified and applied to our setting of dialog-based language learning.</p><p>Memory Networks A high-level description of the memory network architecture we use is given in <ref type="figure" target="#fig_0">Fig. 3 (a)</ref>. The input is the last utterance of the dialog, x, as well as a set of memories (context) (c 1 , . . . , c N ) which can encode both short-term memory, e.g. recent previous utterances and replies, and long-term memories, e.g. facts that could be useful for answering questions. The context inputs c i are converted into vectors m i via embeddings and are stored in the memory. The goal is to produce an outputâ by processing the input x and using that to address and read from the memory, m, possibly multiple times, in order to form a coherent reply. In the figure the memory is read twice, which is termed multiple "hops" of attention.</p><p>In the first step, the input x is embedded using a matrix A of size d × V where d is the embedding dimension and V is the size of the vocabulary, giving q = Ax, where the input x is as a bag-ofwords vector. Each memory c i is embedded using the same matrix, giving m i = Ac i . The output of addressing and then reading from memory in the first hop is:</p><formula xml:id="formula_0">o 1 = i p 1 i m i , p 1 i = Softmax(q m i ).</formula><p>Here, the match between the input and the memories is computed by taking the inner product followed by a softmax, yielding p 1 , giving a probability vector over the memories. The goal is to select memories relevant to the last utterance x, i.e. the most relevant have large values of p 1 i . The output memory representation o 1 is then constructed using the weighted sum of memories, i.e. weighted by p 1 . The memory output is then added to the original input, u 1 = R 1 (o 1 + q), to form the new state of the controller, where R 1 is a d × d rotation matrix <ref type="bibr" target="#b1">2</ref> . The attention over the memory can then be repeated using u 1 instead of q as the addressing vector, yielding:</p><formula xml:id="formula_1">o 2 = i p 2 i m i , p 2 i = Softmax(u 1 m i ),</formula><p>The controller state is updated again with u 2 = R 2 (o 2 + u 1 ), where R 2 is another d × d matrix to be learnt. In a two-hop model the final output is then defined as:</p><formula xml:id="formula_2">a = Softmax(u 2 Ay 1 , . . . , u 2 Ay C )<label>(1)</label></formula><p>where there are C candidate answers in y. In our experiments C is the set of actions that occur in the training set for the bAbI tasks, and for MovieQA it is the set of words retrieved from the KB.</p><p>Having described the basic architecture, we now detail the possible training strategies we can employ for our tasks.</p><p>Imitation Learning This approach involves simply imitating one of the speakers in observed dialogs, which is essentially a supervised learning objective <ref type="bibr" target="#b2">3</ref> . This is the setting that most existing dialog learning, as well as question answer systems, employ for learning. Examples arrive as (x, c, a) triples, where a is (assumed to be) a good response to the last utterance x given context c. In our case, the whole memory network model defined above is trained using stochastic gradient descent by minimizing a standard cross-entropy loss betweenâ and the label a.</p><p>Reward-based Imitation If some actions are poor choices, then one does not want to repeat them, that is we shouldn't treat them as a supervised objective. In our setting positive reward is only obtained immediately after (some of) the correct actions, or else is zero. A simple strategy is thus to only apply imitation learning on the rewarded actions. The rest of the actions are simply discarded from the training set. This strategy is derived naturally as the degenerate case one obtains by applying policy gradient <ref type="bibr" target="#b30">[31]</ref> in our setting where the policy is fixed (see end of Sec. 3). In more complex settings (i.e. where actions that are made lead to long-term changes in the environment and delayed rewards) applying reinforcement learning algorithms would be necessary, e.g. one could still use policy gradient to train the MemN2N but applied to the model's own policy, as used in <ref type="bibr" target="#b24">[25]</ref>.</p><p>Forward Prediction An alternative method of training is to perform forward prediction: the aim is, given an utterance x from speaker 1 and an answer a by speaker 2 (i.e., the learner), to predict x, the response to the answer from speaker 1. That is, in general to predict the changed state of the world after action a, which in this case involves the new utterancex.</p><p>To learn from such data we propose the following modification to memory networks, also shown in <ref type="figure" target="#fig_0">Fig. 3 (b)</ref>: essentially we chop off the final output from the original network of <ref type="figure" target="#fig_0">Fig. 3 (a)</ref> and replace it with some additional layers that compute the forward prediction. The first part of the network remains exactly the same and only has access to input x and context c, just as before. The computation up to u 2 = R 2 (o 2 + u 1 ) is thus exactly the same as before.</p><p>At this point we observe that the computation of the output in the original network, by scoring candidate answers in eq. (1) looks similar to the addressing of memory. Our key idea is thus to perform another "hop" of attention but over the candidate answers rather than the memories. Crucially, we also incorporate the information of which action (candidate) was actually selected in the dialog (i.e. which one is a). After this "hop", the resulting state of the controller is then used to do the forward prediction.</p><p>Concretely, we compute:</p><formula xml:id="formula_3">o 3 = i p 3 i (Ay i + β * [a = y i ]), p 3 i = Softmax(u 2 Ay i ),<label>(2)</label></formula><p>Optionally, different dictionaries can be used for inputs, memories and outputs instead of being shared. Imitation learning algorithms are not always strictly supervised algorithms, they can also depend on the agent's actions. That is not the setting we use here, where the task is to imitate one of the speakers in a dialog. where β * is a d-dimensional vector, that is also learnt, that represents in the output o 3 the action that was actually selected. After obtaining o 3 , the forward prediction is then computed as:</p><formula xml:id="formula_4">x = Softmax(u 3 Ax 1 , . . . , u 3 AxC)</formula><p>where</p><formula xml:id="formula_5">u 3 = R 3 (o 3 + u 2 )</formula><p>. That is, it computes the scores of the possible responses to the answer a overC possible candidates. The mechanism in eq. <ref type="formula" target="#formula_3">2</ref>gives the model a way to compare the most likely answers to x with the given answer a, which in terms of supervision we believe is critical. For example in question answering if the given answer a is incorrect and the model can assign high p i to the correct answer then the output o 3 will contain a small amount of β * ; conversely, o 3 has a large amount of β * if a is correct. Thus, o 3 informs the model of the likely responsex from the teacher.</p><p>Training can then be performed using the cross-entropy loss betweenx and the labelx, similar to before. In the event of a large number of candidatesC we subsample the negatives, always keepinḡ x in the set. The set of answers y can also be similarly sampled, making the method highly scalable.</p><p>A major benefit of this particular architectural design for forward prediction is that after training with the forward prediction criterion, at test time one can "chop off" the top again of the model to retrieve the original memory network model of <ref type="figure" target="#fig_0">Fig. 3 (a)</ref>. One can thus use it to predict answersâ given only x and c. We can thus evaluate its performance directly for that goal as well.</p><p>Finally, and importantly, if the answer to the responsex carries pertinent supervision information for choosingâ, as for example in many of the settings of Sec. 3 (and <ref type="figure">Fig. 1)</ref>, then this will be backpropagated through the model. This is simply not the case in the imitation, reward-shaping <ref type="bibr" target="#b23">[24]</ref> or reward-based imitation learning strategies which concentrate on the x, a pairs.</p><p>Reward-based Imitation + Forward Prediction As our reward-based imitation learning uses the architecture of <ref type="figure" target="#fig_0">Fig. 3 (a)</ref>, and forward prediction uses the same architecture but with the additional layers of <ref type="figure" target="#fig_0">Fig 3 (b)</ref>, we can learn jointly with both strategies. One simply shares the weights across the two networks, and performs gradient steps for both criteria, one of each type per action. The former makes use of the reward signal -which when available is a very useful signal -but fails to use potential supervision feedback in the subsequent utterances, as described above. It also effectively ignores dialogs carrying no reward. Forward prediction in contrast makes use of dialog-based feedback and can train without any reward. On the other hand not using rewards when available is a serious handicap. Hence, the mixture of both strategies is a potentially powerful combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conducted experiments on the datasets described in Section 3. As described before, for each task we consider a fixed policy for performing actions (answering questions) which gets questions correct with probability π acc . We can thus compare the different training strategies described in Sec. 4 over each task for different values of π acc . Hyperparameters for all methods are optimized on the validation sets. A summary of the results is reported in <ref type="table" target="#tab_3">Table 1</ref> for the bAbI dataset and <ref type="table" target="#tab_4">Table 2</ref> for MovieQA. We observed the following results: • Imitation learning, ignoring rewards, is a poor learning strategy when imitating inaccurate answers, e.g. for π acc &lt; 0.5. For imitating an expert however (Task 1) it is hard to beat.</p><p>• Reward-based imitation (RBI) performs better when rewards are available, particularly in <ref type="table" target="#tab_3">Table 1</ref>, but also degrades when they are too sparse e.g. for π acc = 0.01.</p><p>• Forward prediction (FP) is more robust and has stable performance at different levels of π acc . However as it only predicts answers implicitly and does not make use of rewards it is outperformed by RBI on several tasks, notably Tasks 1 and 8 (because it cannot do supervised learning) and Task 2 (because it does not take advantage of positive rewards).</p><p>• FP makes use of dialog feedback in Tasks 3-5 whereas RBI does not. This explains why FP does better with useful feedback (Tasks 3-5) than without (Task 2), whereas RBI cannot.</p><p>• Supplying full answers (Task 3) is more useful than hints (Task 4) but hints still help FP more than just yes/no answers without extra information (Task 2).</p><p>• When positive feedback is sometimes missing (Task 6) RBI suffers especially in <ref type="table" target="#tab_3">Table 1</ref>. FP does not as it does not use this feedback.</p><p>• One of the most surprising results of our experiments is that FP performs well overall, given that it does not use feedback, which we will attempt to explain subsequently. This is particularly evident on Task 7 (no feedback) where RBI has no hope of succeeding as it has no positive examples. FP on the other hand learns adequately.</p><p>• Tasks 9 and 10 are harder for FP as the question is not immediately before the feedback.</p><p>• Combining RBI and FP ameliorates the failings of each, yielding the best overall results.</p><p>One of the most interesting aspects of our results is that FP works at all without any rewards. In Task 2 it does not even "know" the difference between words like "yes" or "'correct" vs. words like "wrong" or "incorrect", so why should it tend to predict actions that lead to a response like "yes, that's right"? This is because there is a natural coherence to predicting true answers that leads to greater accuracy in forward prediction. That is, you cannot predict a "right" or "wrong" response from the teacher if you don't know what the right answer is. In our experiments our policies π acc sample negative answers equally, which may make learning simpler. We thus conducted an experiment on Task (positive and negative feedback) of the bAbI dataset with a much more biased policy: it is the same as π acc = 0.5 except when the policy predicts incorrectly there is probability 0.5 of choosing a random guess as before, and 0.5 of choosing the fixed answer bathroom. In this case the FP method obtains 68% accuracy showing the method still works in this regime, although not as well as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a set of evaluation datasets and models for dialog-based language learning. The ultimate goal of this line of research is to move towards a learner capable of talking to humans, such that humans are able to effectively teach it during dialog. We believe the dialog-based language learning approach we described is a small step towards that goal.</p><p>This paper only studies some restricted types of feedback, namely positive feedback and corrections of various types. However, potentially any reply in a dialog can be seen as feedback, and should be useful for learning. It should be studied if forward prediction, and the other approaches we tried, work there too. Future work should also develop further evaluation methodologies to test how the models we presented here, and new ones, work in those settings, e.g. in more complex settings where actions that are made lead to long-term changes in the environment and delayed rewards, i.e. extending to the reinforcement learning setting. Finally, dialog-based feedback could also be used as a medium to learn non-dialog based skills, e.g. natural language dialog for completing visual or physical tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Architectures for (reward-based) imitation and forward prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Memory</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Model for (reward-based) imitation learning (b) Model for forward prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Task 2: Positive and Negative Feedback Task 3: Answers Supplied by Teacher What movies are about open source? Revolution OS What films are about Hawaii? 50 First Dates</figDesc><table><row><cell>That's right! (+)</cell><cell>Correct! (+)</cell></row><row><cell>What movies did Darren McGavin star in? Carmen</cell><cell>Who acted in Licence to Kill? Billy Madison</cell></row><row><cell>Sorry, that's not it.</cell><cell>No, the answer is Timothy Dalton.</cell></row><row><cell>Who directed the film White Elephant? M. Curtiz</cell><cell>What genre is Saratoga Trunk in? Drama</cell></row><row><cell>No, that is incorrect.</cell><cell>Yes! (+)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) on the Single Supporting Fact bAbI dataset for various supervision approachess (training with 1000 examples on each) and different policies π acc . A task is successfully passed if ≥ 95% accuracy is obtained (shown in blue).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>MemN2N</cell><cell></cell><cell></cell><cell>MemN2N</cell><cell></cell><cell></cell><cell cols="2">MemN2N</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>imitation</cell><cell></cell><cell cols="3">reward-based</cell><cell></cell><cell>forward</cell><cell></cell><cell></cell><cell cols="2">MemN2N</cell></row><row><cell></cell><cell></cell><cell></cell><cell>learning</cell><cell></cell><cell cols="3">imitation (RBI)</cell><cell cols="3">prediction (FP)</cell><cell></cell><cell cols="2">RBI + FP</cell></row><row><cell>Supervision Type</cell><cell>π acc =</cell><cell cols="3">0.5 0.1 0.01</cell><cell cols="3">0.5 0.1 0.01</cell><cell cols="3">0.5 0.1 0.01</cell><cell cols="3">0.5 0.1 0.01</cell></row><row><cell>1 -Imitating an Expert Student</cell><cell></cell><cell cols="3">100 100 100</cell><cell></cell><cell cols="2">100 100</cell><cell>23</cell><cell>30</cell><cell>29</cell><cell>99</cell><cell>99</cell><cell>100</cell></row><row><cell cols="2">2 -Positive and Negative Feedback</cell><cell>79</cell><cell>28</cell><cell>21</cell><cell>99</cell><cell>92</cell><cell>91</cell><cell>93</cell><cell>54</cell><cell>30</cell><cell>99</cell><cell>92</cell><cell>96</cell></row><row><cell>3 -Answers Supplied by Teacher</cell><cell></cell><cell>83</cell><cell>37</cell><cell>25</cell><cell></cell><cell>96</cell><cell>92</cell><cell></cell><cell>96</cell><cell>99</cell><cell></cell><cell>100</cell><cell>98</cell></row><row><cell>4 -Hints Supplied by Teacher</cell><cell></cell><cell>85</cell><cell>23</cell><cell>22</cell><cell></cell><cell>91</cell><cell>90</cell><cell></cell><cell>99</cell><cell>66</cell><cell></cell><cell cols="2">100 100</cell></row><row><cell cols="3">5 -Supporting Facts Supplied by Teacher 84</cell><cell>24</cell><cell>27</cell><cell></cell><cell>96</cell><cell>83</cell><cell></cell><cell>99</cell><cell>100</cell><cell></cell><cell>99</cell><cell>100</cell></row><row><cell>6 -Partial Feedback</cell><cell></cell><cell>90</cell><cell>22</cell><cell>22</cell><cell></cell><cell>81</cell><cell>59</cell><cell></cell><cell>100</cell><cell>99</cell><cell></cell><cell>100</cell><cell>99</cell></row><row><cell>7 -No Feedback</cell><cell></cell><cell>90</cell><cell>34</cell><cell>19</cell><cell></cell><cell>22</cell><cell>29</cell><cell></cell><cell>98</cell><cell>99</cell><cell></cell><cell>99</cell><cell>99</cell></row><row><cell>8 -Imitation + Feedback Mixture</cell><cell></cell><cell>90</cell><cell>89</cell><cell>82</cell><cell></cell><cell>98</cell><cell>98</cell><cell></cell><cell>64</cell><cell>67</cell><cell></cell><cell>98</cell><cell>97</cell></row><row><cell>9 -Asking For Corrections</cell><cell></cell><cell>85</cell><cell>30</cell><cell>22</cell><cell></cell><cell>89</cell><cell>83</cell><cell></cell><cell>15</cell><cell>21</cell><cell></cell><cell>90</cell><cell>84</cell></row><row><cell>10 -Asking For Supporting Facts</cell><cell></cell><cell>86</cell><cell>25</cell><cell>26</cell><cell></cell><cell>96</cell><cell>84</cell><cell></cell><cell>30</cell><cell>48</cell><cell></cell><cell>95</cell><cell>91</cell></row><row><cell cols="2">Number of completed tasks (≥ 95%)</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell></cell><cell>5</cell><cell>2</cell><cell></cell><cell>5</cell><cell>4</cell><cell></cell><cell>8</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) on the MovieQA dataset dataset for various supervision approaches. Numbers in bold are the winners for that task and choice of π acc .</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MemN2N</cell><cell cols="2">MemN2N</cell><cell cols="2">MemN2N</cell><cell></cell></row><row><cell></cell><cell></cell><cell>imitation</cell><cell></cell><cell cols="2">reward-based</cell><cell>forward</cell><cell></cell><cell cols="2">MemN2N</cell></row><row><cell></cell><cell></cell><cell>learning</cell><cell></cell><cell cols="2">imitation (RBI)</cell><cell cols="2">prediction (FP)</cell><cell cols="2">RBI + FP</cell></row><row><cell>Supervision Type</cell><cell>π acc =</cell><cell cols="8">0.5 0.1 0.01 0.5 0.1 0.01 0.5 0.1 0.01 0.5 0.1 0.01</cell></row><row><cell>-Imitating an Expert Student</cell><cell></cell><cell>80 80</cell><cell>80</cell><cell>80</cell><cell>80</cell><cell>23</cell><cell>24</cell><cell>77</cell><cell>77</cell></row><row><cell cols="2">2 -Positive and Negative Feedback</cell><cell>46 29</cell><cell>27</cell><cell>32</cell><cell>26</cell><cell>34</cell><cell>24</cell><cell>53</cell><cell>34</cell></row><row><cell>3 -Answers Supplied by Teacher</cell><cell></cell><cell>48 29</cell><cell>26</cell><cell>52 32</cell><cell>27</cell><cell>60 57</cell><cell>58</cell><cell>69 65</cell><cell>62</cell></row><row><cell>4 -Hints Supplied by Teacher</cell><cell></cell><cell>47 29</cell><cell>26</cell><cell>51 32</cell><cell>28</cell><cell>58 58</cell><cell>42</cell><cell>70 54</cell><cell>32</cell></row><row><cell cols="3">5 -Supporting Facts Supplied by Teacher 47 28</cell><cell>26</cell><cell>51 32</cell><cell>26</cell><cell>43 44</cell><cell>33</cell><cell>66 53</cell><cell>40</cell></row><row><cell>6 -Partial Feedback</cell><cell></cell><cell>48 29</cell><cell>27</cell><cell>49 32</cell><cell>24</cell><cell>60 58</cell><cell>58</cell><cell>70 63</cell><cell>62</cell></row><row><cell>7 -No Feedback</cell><cell></cell><cell>51 29</cell><cell>27</cell><cell>22 21</cell><cell>21</cell><cell>60 53</cell><cell>58</cell><cell>61 56</cell><cell>50</cell></row><row><cell>8 -Imitation + Feedback Mixture</cell><cell></cell><cell>60 50</cell><cell>47</cell><cell>63 53</cell><cell>51</cell><cell>46 31</cell><cell>23</cell><cell>72 69</cell><cell>69</cell></row><row><cell>9 -Asking For Corrections</cell><cell></cell><cell>48 29</cell><cell>27</cell><cell>52 34</cell><cell>26</cell><cell>67 52</cell><cell>44</cell><cell>68 52</cell><cell>39</cell></row><row><cell>10 -Asking For Supporting Facts</cell><cell></cell><cell>49 29</cell><cell>27</cell><cell>52 34</cell><cell>27</cell><cell>51 44</cell><cell>35</cell><cell>69 53</cell><cell>36</cell></row><row><cell>Mean Accuracy</cell><cell></cell><cell>52 36</cell><cell>34</cell><cell>52 38</cell><cell>34</cell><cell>52 45</cell><cell>40</cell><cell>69 60</cell><cell>50</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Arthur Szlam, Y-Lan Boureau, Marc'Aurelio Ranzato, Ronan Collobert, Michael Auli, David Grangier, Alexander Miller, Sumit Chopra, Antoine Bordes and Leon Bottou for helpful discussions and feedback, and the Facebook AI Research team in general for supporting this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactional feedback and the impact of attitude and motivation on noticing l2 form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bassiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">English Language and Literature Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth conference on computational natural language learning</title>
		<meeting>the fourteenth conference on computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Evaluating prerequisite qualities for learning end-to-end dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06931</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning from natural instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="232" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The conscientious consumer: Reconsidering the role of assessment feedback in student learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Skelton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in higher education</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning knowledge graphs for question answering through conversational dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Early language acquisition: cracking the speech code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Kuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="831" to="843" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Guiding a reinforcement learner with natural language advice: Initial results in robocup soccer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI-2004 workshop on supervisory control of learning and adaptive systems</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning through feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Latham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Leadership</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="86" to="87" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepmpc: Learning deep latent features for model predictive control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Negative evidence in language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="85" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08130</idno>
		<title level="m">A roadmap towards machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Language understanding for text-based games using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2845" to="2853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predicting tasks in goal-oriented spoken dialog systems using semantic knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL</title>
		<meeting>the SIGDIAL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="242" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reinforcement learning for adaptive dialogue systems: a data-driven methodology for dialogue management and natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lemon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. The knowledge engineering review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuttle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="97" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to generate artificial fovea trajectories for target detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">01n02</biblScope>
			<biblScope unit="page" from="125" to="134" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Incentivizing exploration in reinforcement learning with deep predictive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00814</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reward shaping with recurrent neural networks for speeding up on-line policy learning in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03391</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mazebase: A sandbox for learning from games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1511.07401</idno>
		<ptr target="http://arxiv.org/abs/1511.07401" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2431" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hierarchical control using networks trained with higher-level forward models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Abbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Instructive feedback: Review of parameters and effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Werts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wolery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Gast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Education</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="75" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: a set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
