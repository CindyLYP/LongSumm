{"1010": "relation detection is a core component of many nlp applications including knowledge base question answering ( kbqa). although general relation detection1 methods are well studied in the nlp community, such studies usually do not take the end task of kbqa into consideration. as a result, there is a significant gap between general relation detection studies and kb-specific relation detection. first, in most general relation detection tasks, the number of target relations is limited, normally smaller than in contrast, in kbqa even a small kb, like freebase2m, contains more than 6,000 relation types. second, noticing 1in the information extraction field such tasks are usually called relation extraction or relation classification. that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations. third, we use deep bidirectional lstms to learn different levels of question representations in order to match the different levels of relation information. finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract ( deeper) question representations, thus improves hierarchical matching. relation extraction relation extraction recent research benefits a lot from the advancement of deep learning: from word embeddings to deep networks like cnns and lstms and attention models the above research assumes there is a fixed ( closed) set of relation types, thus no zero-shot learning capability is required. the number of relations is usually not large: the widely used ace2005 has 11/32 coarse/fine-grained relations; semeval2010 task8 has relations; tac- kbp2015 has relations although it considers open-domain wikipedia relations. all are much fewer than thousands of relations in kbqa. as a result, few work in this field focuses on dealing with large number of relations or unseen relations. deep bilstms have been widely used for prediction of word embeddings of question words (e.g. q q1, qn, qn) and word embeddings of relation words (e.g. episode, written, and relation names, series, when the target is a chain like in figure 1 ( b) ) . in this paper, we propose to use deep bilstms for prediction of word embeddings of question words (e.g. in this paper, we propose a new approach to the kbqa end task called hierarchical residual bilstm ( hr-bilstm) model. hr-bilstm is a hierarchical residual matching model ( hr -bilstm) that is based on the hierarchical residual bilstm ( hr-bilstm) model. to a relation detector to score all the relations r re that are associated to the entity e in the kb.4 because we have a single topic entity input in this step, we do reformating following question: we replace the candidate e, the entity mention in 4note that the number of entities and the number of relation candidates will be much smaller than those in the previous step. this helps the model better distinguish the relative position of each word compared to the entity. we use the hr-bilstm model to predict the score of each relation r re: srel ( r; e, q) hr-bilstm is a combination of two bilstms (both on words) with residual connections between their hidden states. the main idea is to use the hierarchical architecture to learn different levels of abstraction. we hypothesize that hr-bilstm is more than combination of two bilstms with residual connections, because it encourages the hierarchical architecture to learn different levels of abstraction. to verify this, we replace the deep bilstm with two single-layer bilstms with shortcut connections between their hidden states. this decreases test accuracy to it gives similar training accuracy compared to hr-bilstm, indicating a more serious over-fitting problem. in the experiments a two-layer bilstm converges to 94.99, even lower than the achieved by a single-layer bilstm. under our setting the twolayer model captures the single-layer model as a special case ( so it could potentially better fit the training data), this result suggests that the deep bilstm without shortcut connections might suffers more from training difficulty. kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks. we propose a novel kb relation detection model, hr-bilstm, that performs hierarchical matching between questions and kb relations. our model outperforms the previous methods on kb relation detection tasks and allows our kbqa system to achieve state-of-the -arts. for future work, we will investigate the integration of our hr-bilstm into end-to-end systems. in this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects kb relations given an input question. our method uses deep residual bidirectional lstms to compare questions and relation names via different levels of abstraction. additionally, we propose a simple kbqa system that integrates entity linking and our proposed relation detector to make the two components enhance each other. our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our kbqa system achieve state-of-theart accuracy for both single-relation and multi-relation qa benchmarks. this paper proposes a hierarchical bilstm ( hr-bilstm) model for the kbqa end task. the hr-bilstm model is used to score all relations in the kb that are associated to the entities in elk ( q) by taking into account the maximum overlapping sequence of characters between them. if the matching score is larger than a threshold ( tuned on training set), we will add the constraint entity c ( and rc) to the query by attaching it to the corresponding node v on the core-chain. the hr-bilstm model is used to predict the score of each relation r re : srel ( r; e; q) srel ( r; e; q) where is a constant parameter. for each question q, after generating a score srel ( r; e; q) for each relation using hr-bilstm, we use the top l best scoring relations ( rlq) to re-rank the original entity candidates.", "1000": "###", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1007": "###", "1008": "###", "1009": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1018": "###", "1019": "###", "1020": "###", "1021": "###"}