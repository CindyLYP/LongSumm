<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speed learning on the fly</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-11-08">8 Nov 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Yves</forename><surname>Massé</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
						</author>
						<title level="a" type="main">Speed learning on the fly</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-11-08">8 Nov 2015</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1511.02540v1[math.OC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>The practical performance of online stochastic gradient descent algorithms is highly dependent on the chosen step size, which must be tediously hand-tuned in many applications. The same is true for more advanced variants of stochastic gradients, such as SAGA, SVRG, or AdaGrad. Here we propose to adapt the step size by performing a gradient descent on the step size itself, viewing the whole performance of the learning trajectory as a function of step size. Importantly, this adaptation can be computed online at little cost, without having to iterate backward passes over the full data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>This work aims at improving gradient ascent procedures for use in machine learning contexts, by adapting the step size of the descent as it goes along.</p><p>Let ℓ 0 , ℓ 1 , . . . , ℓ t , . . . be functions to be maximised over some parameter space Θ. At each time t, we wish to compute or approximate the parameter θ * t ∈ Θ that maximizes the sum</p><formula xml:id="formula_0">L t (θ) := s≤t ℓ s (θ).<label>(1)</label></formula><p>In the experiments below, as in many applications, ℓ t (θ) writes ℓ(x t , θ) for some data x 0 , x 1 , . . . , x t , . . . A common strategy, especially with large data size or dimensionality <ref type="bibr" target="#b1">[Bot10]</ref>, is the online stochastic gradient ascent (SG)</p><formula xml:id="formula_1">θ t+1 = θ t + η ∂ θ ℓ t (θ t )<label>(2)</label></formula><p>with step size η, where ∂ θ ℓ t stands for the Euclidean gradient of ℓ t with respect to θ. Such an approach has become a mainstay of both the optimisation and machine learning communities <ref type="bibr" target="#b1">[Bot10]</ref>. Various conditions for convergence exist, starting with the celebrated article of Robbins and Monro <ref type="bibr" target="#b7">[RM51]</ref>, or later <ref type="bibr" target="#b5">[KC78]</ref>. Other types of results are proved in convex settings, Several variants have since been introduced, in part to improve the convergence of the algorithm, which is much slower in stochastic than than in deterministic settings. For instance, algorithms such as SAGA, Stochastic Variance Reduced Gradient (SVRG) or Stochastic Average Gradient (SAG) <ref type="bibr" target="#b2">[DBLJ14,</ref><ref type="bibr" target="#b4">JZ13,</ref><ref type="bibr" target="#b8">SLRB13]</ref>, perform iterations using a comparison between the latest gradient and an average of past gradients. This reduces the variance of the resulting estimates and allows for nice convergence theorems <ref type="bibr" target="#b2">[DBLJ14,</ref><ref type="bibr" target="#b8">SLRB13]</ref>, provided a reasonable step size η is used.</p><p>Influence of the step size. The ascent requires a parameter, the step size η, usually called "learning rate" in the machine learning community. Empirical evidence highligting the sensitivity of the ascent to its actual numerical value exists aplenty; see for instance the graphs in Section 3.2.1. Slow and tedious hand-tuning is therefore mandatory in most applications. Moreover, admittable values of η depend on the parameterisation retained-except for descents described in terms of Riemannian metrics <ref type="bibr" target="#b0">[Ama98]</ref>, which provide some degree of parameterisation-invariance.</p><p>Automated procedures for setting reasonable value of η are therefore of much value. For instance, AdaGrad <ref type="bibr" target="#b3">[DHS11]</ref> divides the derivative ∂ θ ℓ t by a root mean square average of the magnitude of its recent values, so that the steps are of size approximately 1; but this still requires a "master step size" η.</p><p>Shaul, Zhang and LeCun in <ref type="bibr" target="#b9">[SZL13]</ref> study a simple separable quadratic loss model and compute the value of η which minimises the expected loss after each parameter update. This value can be expressed in terms of computable quantities depending on the trajectory of the descent. These quantities still make sense for non-quadratic models, making this idea amenable to practical use.</p><p>More recently, Maclaurin, Douglas and Duvenaud <ref type="bibr" target="#b6">[MDA15]</ref> propose to directly conduct a gradient ascent on the hyperparameters (such as the learning rate η) of any algorithm. The gradients with respect to the hyperparameters are computed exactly by "chaining derivatives backwards through the entire training procedure" <ref type="bibr" target="#b6">[MDA15]</ref>. Consequently, this approach is extremely impractical in an online setting, as it optimizes the learning rate by performing several passes, each of which goes backwards from time t to time 0.</p><p>Finding the best step size. The ideal value of the step size η would be the one that maximizes the cumulated objective function (1). Write θ t (η) for the parameter value obtained after t iterations of the gradient step (2) using a given value η, and consider the sum s≤t ℓ s (θ s (η)).</p><p>(3)</p><p>Our goal is to find an online way to approximate the value of η that provides the best value of this sum. This can be viewed as an ascent on the space of stochastic ascent algorithms. We suggest to update η through a stochastic gradient ascent on this sum:</p><formula xml:id="formula_2">η ← η + α ∂ ∂η ℓ t (θ t (η))<label>(4)</label></formula><p>and then to use, at each time, the resulting value of η for the next gradient step (2). The ascent (4) on η depends, in turn, on a step size α. Hopefully, the dependance on α of the whole procedure is somewhat lower than that of the original stochastic gradient scheme on its step size η.</p><p>This approach immediately extends to other stochastic gradient algorithms; in what follows we apply it both to the standard SG ascent and to the SVRG algorithm.</p><p>The main point in this approach is to find efficient ways to compute or approximate the derivatives ∂ ∂η ℓ t (θ t (η)). Indeed, the value θ t (η) after t steps depends on the whole trajectory of the algorithm, and so does its derivative with respect to η.</p><p>After reviewing the setting for gradient ascents in Section 1, in Section 2.1 we provide an exact but impractical way of computing the derivatives ∂ ∂η ℓ t (θ t (η)). Sections 2.2-2.3 contain the main contribution: SG/SG and SG/AG, practical algorithms to adjust η based on two approximations with respect to these exact derivatives.</p><p>Section 2.4 extends this to other base algorithms such as SVRG. In Section 4 one of the approximations is justified by showing that it computes a derivative, not with respect to a fixed value of η as in (4), but with respect to the sequences of values of η effectively used along the way. This also suggests improved algorithms.</p><p>Section 3 provides experimental comparisons of gradient ascents using traditional algorithms with various values of η, and the same algorithms where η is self-adjusted according to our scheme. The comparisons are done on three sets of synthetic data: a one-dimensional Gaussian model, a one-dimensional Bernoulli model and a 50-dimensional linear regression model: these simple models already exemplify the strong dependence of the traditional algorithms on the value of η.</p><p>Terminology. We say that an algorithm is of type "LLR" for "Learning the Learning Rate" when it updates its step size hyperparameter η as it unfolds. We refer to LLR algorithms by a compound abbreviation: "SVRG/SG", for instance, for an algorithm which updates its parameter θ through SVRG and its hyperparameter η through an SG algorithm on η.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Stochastic Gradient algorithm</head><p>To fix ideas, we define the Stochastic Gradient (SG) algorithm as follows. In all that follows, Θ = R n for some n. 1 The functions ℓ t are assumed to be smooth. In all our algorithms, the index t starts at 0.</p><p>Algorithm 1 (Stochastic Gradient). We maintain θ t ∈ Θ (current parameter), initialised at some arbitrary θ 0 ∈ Θ. We fix η ∈ R. At each time t, we fix a rate f (t) ∈ R. The update equation reads:</p><formula xml:id="formula_3">θ t+1 = θ t + η f (t) ∂ θ ℓ t (θ t ).<label>(5)</label></formula><p>The chosen rate f (t) usually satisfies the well-known Robbins-Monro conditions <ref type="bibr" target="#b7">[RM51]</ref>:</p><formula xml:id="formula_4">t≥0 f (t) −1 = ∞, t≥0 f (t) −2 &lt; ∞.<label>(6)</label></formula><p>The divergence of the sum of the rates allows the ascent to go anywhere in parameter space, while the convergence of the sum of the squares ensures that variance remains finite. Though custom had it that small such rates should be chosen, such as f (t) = 1/t, recently the trend bucked towards the use of large ones, to allow for quick exploration of the parameter space. Throughout the article and experiments we use one such rate:</p><formula xml:id="formula_5">f (t) = √ t + 2 log(t + 3).<label>(7)</label></formula><p>2 Learning the learning rate on a stochastic gradient algorithm</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The loss as a function of step size</head><p>To formalise what we said in the introduction, let us define, for each η ∈ R, the sequence</p><formula xml:id="formula_6">(θ 0 , θ 1 , θ 2 , . . .)<label>(8)</label></formula><p>obtained by iterating (5) from some initial value θ 0 . Since they depend on η, we introduce, for each t &gt; 0, the operator</p><formula xml:id="formula_7">T t : η ∈ R → T t (η) ∈ Θ,<label>(9)</label></formula><p>which maps any η ∈ R to the parameter θ t obtained after t iterations of (5). T 0 maps every η to θ 0 . For each t ≥ 0, the map T t is a regular function of Θ may also be any Riemannian manifold, a natural setting when dealing with gradients. Most of the text is written in this spirit.</p><p>η. As explained in the introduction, we want to optimise η according to the function:</p><formula xml:id="formula_8">L t (η) := s≤t ℓ s (T s (η)),<label>(10)</label></formula><p>by conducting an online stochastic gradient ascent on it. We therefore need to compute the derivative in (4):</p><formula xml:id="formula_9">∂ ∂η ℓ t (T t (η)).<label>(11)</label></formula><p>To act more decisively on the order of magnitude of η, we perform an ascent on its logarithm, so that we actually need to compute :</p><formula xml:id="formula_10">∂ ∂ log η ℓ t (T t (η)).<label>(12)</label></formula><p>Now, the derivative of the loss at time t with respect to η can be computed as the product of the derivative of ℓ t with respect to θ (the usual input of SG) and the derivative of θ t with respect to η:</p><formula xml:id="formula_11">∂ ∂ log η ℓ t (T t (η)) = ∂ θ ℓ t (T t (η)) • A t (η)<label>(13)</label></formula><p>where</p><formula xml:id="formula_12">A t (η) := ∂T t (η) ∂ log η .<label>(14)</label></formula><p>Computation of the quantity A t and its approximation h t to be introduced later, are the main focus of this text.</p><p>Lemma 1. The derivative A t (η) may be computed through the following recursion equation. A 0 (η) = 0 and, for t ≥ 0,</p><formula xml:id="formula_13">A t+1 (η) = A t (η) + η f (t) ∂ θ ℓ t (T t (η)) + η f (t) ∂ 2 θ ℓ t (T t (η)) • A t (η).<label>(15)</label></formula><p>The proof lies in Section C.1. This update of A involves the Hessian of the loss function with respect to θ, evaluated in the direction of A t . Often this quantity is unavailable or too costly. Therefore we will use a finite difference approximation instead:</p><formula xml:id="formula_14">∂ 2 θ ℓ t (T t (η)) • A t (η) ≈ ∂ θ ℓ t (T t (η) + A t (η)) − ∂ θ ℓ t (T t (η)).<label>(16)</label></formula><p>This design ensures that the resulting update on A t (η) uses the gradient of ℓ t only once:</p><formula xml:id="formula_15">A t+1 (η) ≈ A t (η) + η f (t) ∂ θ ℓ t (T t (η) + A t (η)) .<label>(17)</label></formula><p>An alternative approach would be to compute the Hessian in the direction A t by numerical differentiation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LLR on SG: preliminary version with simplified expressions (SG/SG)</head><p>Even with the approximation above, computing the quantities A t would have a quadratic cost in t: each time we update η thanks to (4), we would need to compute anew all the A s (η), s ≤ t, as well as the whole trajectory θ t = T t (η), at each iteration t. We therefore replace the A t (η)'s by online approximations, the quantities h t , which implement the same evolution equation <ref type="formula" target="#formula_15">17</ref>as A t , disregarding the fact that η may have changed in the meantime. These quantities will be interpreted more properly in Section 4 as derivatives taken along the effective trajectory of the ascent. This yields the SG/SG algorithm.</p><p>Algorithm 2 (SG/SG). We maintain θ t ∈ Θ (current parameter), η t ∈ R (current step size) and h t ∈ T θt Θ (approximation of the derivative of θ t with respect to log(η)).</p><p>The first two are initialised arbitrarily, and h 0 is set to 0. The update equations read:</p><formula xml:id="formula_16">             log η t+1 = log η t + 1 µ t ∂ θ ℓ t (θ t ) • h t h t+1 = h t + η t+1 f (t) ∂ θ ℓ t (θ t + h t ) θ t+1 = θ t + η t+1 f (t) ∂ θ ℓ t (θ t ) ,<label>(18)</label></formula><p>where µ t is some learning rate on log η, such as µ t = √ t + 2 log(t + 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">LLR on SG: efficient version (SG/AG)</head><p>To obtain better performances, we actually use an adagrad-inspired scheme to update the logarithm of the step size.</p><p>Algorithm 3 (SG/AG). We maintain θ t ∈ Θ (current parameter), η t ∈ R (current step size), h t ∈ T θt Θ (approximation of the derivative of θ t with respect to log(η)), n t ∈ R (average of the squared norms of ∂ℓ t • T t /∂ log η), and d t ∈ R (renormalising factor for the computation of n t ). θ et η are initially set to θ 0 and η 0 , the other variables are set to 0. At each time t, we compute µ t ∈ R (a rate used in several updates), and λ t ∈ R (the approximate derivative of ℓ t • θ t with respect to log(η) at η t ).</p><p>The update equations read:</p><formula xml:id="formula_17">                                           µ t = √ t + 2 log(t + 3) λ t = l(θ t ) • h t d t+1 = 1 − 1 µ t d t + 1 µ t n 2 t+1 = 1 − 1 µ t n 2 t + 1 µ t λ 2 t d −1 t+1 log η t+1 = log η t + 1 µ t λ t n t+1 h t+1 = h t + η t+1 f (t) ∂ θ ℓ t (θ t + h t ) θ t+1 = θ t + η t+1 f (t) ∂ θ ℓ t (θ t ).<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">LLR on other Stochastic Gradient algorithms</head><p>The LLR procedure may be applied to any stochastic gradient algorithm of the form</p><formula xml:id="formula_18">θ t+1 = F (θ t , η t )<label>(20)</label></formula><p>where θ t may store all the information maintained by the algorithm, not necessarily just a parameter value. Appendix B presents the algorithm in this case. Appendix A presents SVRG/AG, which is the particular case of this procedure applied to SVRG with an AdaGrad scheme for the update of η t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments on SG and SVRG</head><p>We now present the experiments conducted to test our procedure. We first describe the experimental set up, then discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Presentation of the experiments</head><p>We conducted ascents on synthetic data generated by three different probabilistic models: a one-dimensional Gaussian model, a Bernoulli model and a 50-dimensional linear regression model. Each model has two components: a generative distribution, and a set of distributions used to approximate the former.</p><p>One Dimensional Gaussian Model. The mean and value of the Gaussian generative distribution were set to 5 and 2 respectively. Let us note p θ the density of a standard Gaussian random variable. The function to maximise we used is:</p><formula xml:id="formula_19">ℓ t (θ) = log p θ (x t ) = − 1 2 (x t − θ) 2 .<label>(21)</label></formula><p>Bernoulli model. The parameter in the standard parameterisation for the Bernoulli model was set to p = 0.3, but we worked with a logit parameterisation θ = log(p/(1 − p)) for both the generative distribution and the discriminative function. The latter is then:</p><formula xml:id="formula_20">ℓ t (θ) = θ • x t − log 1 + e θ .<label>(22)</label></formula><p>Fifty-dimensional Linear Regression model. In the last model, we compute a fixed random matrix M . We then draw samples Z from a standard 50-dimensional Gaussian distribution. We then use M to make random linear combinations X = M Z of the coordinates of the Z vectors. Then we observe X and try to recover first coordinate of the sample Z. The solution θ * is the first row of the inverse of M . Note Y the first coordinate of Z so that the regression pair is (X, Y ). We want to maximise:</p><formula xml:id="formula_21">ℓ t (θ) = − 1 2 (y t − θ • x t ) 2 ,<label>(23)</label></formula><p>For each model, we drew 2500 samples from the data (7500 for the 50dimensional model), then conducted ascents on those with on the one hand the SG and SVRG algorithms, and on the other hand their LLR counterparts, SG/SG and SVRG/SG, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Description and analysis of the results</head><p>For each model, we present four different types of results. We start with the trajectories of the ascents for several initial values of η (in the 50-dimensional case, we plot the first entry of θ T • M ). Then we present the cumulated regrets. Next we show the evolution of the logarithm of η t along the ascents for the LLR algorithms. Finally, we compare this to trajectories of the nonadaptive algorithms with good initial values of η. Each time, we present three figures, one for each model. Each figure for the ascent looks the same: there are several well distinguishable trajectories in the graphs of the standard algorithms, the upper ones, while trajectories are much closer to each other in those of the LLR algorithms, the lower ones. Indeed, for many values of η, the standard algorithms will perform poorly. For instance, low values of η will result in dramatically low convergence towards the ML, as may be seen in some trajectories of the SG graphs. The SVRG algorithm performs noticeably better, but may start to oscillate, as in <ref type="figure" target="#fig_1">Figures 2 and 3</ref>.</p><p>These inconveniences are significantly improved by the use of LLR procedures. Indeed, in each model, almost every trajectory gets close to that of the ML in the SG/AG graphs. In the SVRG/AG graphs, the oscillations are overwhelmingly damped. Improvements for SG, though significant, are not as decisive in the linear regression model as in the other two, probably due to its greater complexity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cumulated regrets</head><p>Each curve of Figures 4 to 6 represents the difference between the cumulated regret of the algorithm used and that of the ML, for the η 0 chosen. The curves of SG and SVRG all go upwards, which means that the difference increases with time, whereas those of SG/AG and SVRG/SG tend to stagnate strikingly quickly. Actually, the trajectories for the linear regression model do not stagnate, but they are still significantly better for the LLR algorithms than for the original ones. The stagnation means that the values of the parameter found by these algorithms are very quickly as good as the Maximum Likelihood for the prediction task. Arguably, the fluctuations of the ascents around the later are therefore not a defect of the model: the cumulated regret graphs show that they are irrelevant for the minimisation at hand. and 8, log(η t ) tends to stagnate quite quickly. This may seem a desirable behaviour : the algorithms have reached good values for η t , and the ascent may accordingly proceed with those. However, this analysis may seem somewhat unsatisfactory due to the 1/f (t) dampening term in the parameter update, which remains unaltered by our procedure. For the linear regression model, in <ref type="figure" target="#fig_9">Figure 9</ref>, the convergence takes longer in the SG/SG case, and even in the SVRG/SG one, which may be explained again by the complexity of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Evolution of the step size of the LLR algorithms during the ascents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">LLR versus hand-crafted learning rates</head><p>Figures 10 to 12 show the trajectories of the ascents for LLR algorithms with poor initial values of the step size, compared to the trajectories of the original algorithms with hand-crafted optimal values of η. The trajectories of the original algorithms appear in red. They possess only two graphs each, where all the trajectories are pretty much undistinguishable from another. This shows that the LLR algorithms show acceptable behaviour even with poor initial values of η, proving the procedure is able to rescue very badly initialised algorithms. However, one caveat is that the LLR procedure encounters difficulties dealing with too large values of η , and is much more efficient at dealing with small values of η 0 . We have no satisfying explana-  <ref type="figure">Figure 4</ref>: Difference between the cumulated regrets of the algorithm and of the ML for a Gaussian model in one dimension for several algorithms and several η 0 's tion of this phenomenon yet. We thus suggest, in practice, to underestimate rather than overestimate the initial value η .</p><formula xml:id="formula_22">η0=10 −4 η0=10 −3 η0=10 −2 η0=10 −1 η0=1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">η t in a quadratic model</head><p>In a quadratic deterministic one-dimensional model, where we want to maximise:</p><formula xml:id="formula_23">f (θ) = −α x 2 2 ,<label>(24)</label></formula><p>SG is numerically stable if, and only if,</p><formula xml:id="formula_24">− αη f (t) &lt; 1,<label>(25)</label></formula><p>that is η 2f (t) &lt; α −1 .</p><p>Each graph of <ref type="figure" target="#fig_0">Figure 13</ref> has two curves, one for the original algorithm, the other for its LLR version. The curve of the LLR version goes down quickly, then much more slowly, while the other curve goes down slowly all the time. This shows that, for α = 10 8 , the ratio above converges quickly towards  Then, the algorithm has converged, and η t stays nearly constant, so much so that the LLR curve behaves like the other one. However, the convergence of η t happens too slowly: θ t takes very large values before η t reaches this value, and even though it eventually converges to 0, such behaviour is unacceptable in practise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A pathwise interpretation of the derivatives</head><p>Until now, we have tried to optimise the step size for a stochastic gradient ascent. This may be interpreted as conducting a gradient ascent on the subspace of the ascent algorithms which gathers the stochastic gradient algorithms, parametrised by η ∈ R. However, we had to replace the A t (η)'s by the h t 's because computing the former gave our algorithm a quadratic complexity in time. Indeed, adhesion to Equation 1 entails using A 0 (η 1 ) to compute A 1 (η 1 ), for instance. Likewise, A 0 (η 2 ) and A 1 (η 2 ) would be necessary to compute A 2 (η 2 ), and this scheme would repeat itself for every iteration.</p><p>We now introduce a formalism which shows the approximations we use are actually derivatives taken alongside the effective trajectory of the ascent. Difference of cumulated regret between the ML and SVRG/AG <ref type="figure">Figure 6</ref>: Difference between the cumulated regrets of the algorithm and of the ML for a 50-dimensional linear regression model for several algorithms and several η 0 's It will also allow us to devise a new algorithm. It will, however, not account for the approximation of the Hessian. To this avail, let us parameterise stochastic gradient algorithms by a sequence of step sizes</p><formula xml:id="formula_26">η0=10 −6 η0=10 −5 η0=10 −4 η0=10 −3 η0=10 −2</formula><formula xml:id="formula_27">η = (η 0 , η 1 , . . .)<label>(27)</label></formula><p>such that at iteration t, the update equation for θ t becomes:</p><formula xml:id="formula_28">θ t+1 = θ t + η t+1 f (t) ∂ θ ℓ t (θ t ).<label>(28)</label></formula><p>4.1 The loss as a function of step size: extension of the formalism</p><p>Consider the space S of infinite real sequences</p><formula xml:id="formula_29">η = (η 0 , η 1 , η 2 , . . .)<label>(29)</label></formula><p>We expand the T t operators defined in Section 2 to similar ones defined on S, with the same notation. Namely, define T 0 (η) = θ 0 and, for t &gt; 0, </p><formula xml:id="formula_30">T t : η ∈ S → T t (η) ∈ R<label>(30)</label></formula><formula xml:id="formula_31">η0=10 −4 η0=10 −3 η0=10 −2 η0=10 −1 η0=1 ML</formula><p>Figure 7: Evolution of log(η t ) in regard of the corresponding ascents for a Gaussian model in one dimension for SG and SV RG with LLR and several η 0 's where θ t has been obtained thanks to t iterations of (28). T t is a regular function of η, as the computations only involve</p><formula xml:id="formula_32">η 0 , η 1 , . . . , η t ,<label>(31)</label></formula><p>and so take place in finite-dimensional spaces. This will apply in all the computations below. As before, we work on a space we call log(S), the image of S by the mapping</p><formula xml:id="formula_33">η = (η t ) t≥0 ∈ S → log(η) = (log η t ) t≥0 ,<label>(32)</label></formula><p>but we do not change notation for the functions η → T t (η), as in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The update of the step size in the SG/SG algorithm as a gradient ascent</head><p>We now prove that in SG/SG, when the Hessian is used without approximations, the step size η t indeed follows a gradient ascent scheme. </p><formula xml:id="formula_34">Proposition 1. Let (θ t ) t≥0 , η = (η t ) t≥0<label>(33)</label></formula><formula xml:id="formula_35">h t+1 = h t + η µ t ∂ θ ℓ t (θ t ) + η µ t ∂ 2 θ ℓ t (θ t ) • h t .<label>(34)</label></formula><p>Define e in the tangent plane of log(S) at log(η) by e t = 1, t ≥ 0.</p><p>Then, for all t ≥ 0,</p><formula xml:id="formula_37">log η t+1 = log η t + 1 µ t ∂ ∂e ℓ t (T t (η)).<label>(36)</label></formula><p>The proof lies in Appendix C.2.1.</p><p>4.3 A new algorithm, using a notion of "memory" borrowed from <ref type="bibr" target="#b9">[SZL13]</ref> We would now like to compute the change in η implied by a small modification of all previous coordinates η s for s less than the current time t, but to  <ref type="figure" target="#fig_0">Figure 10</ref>: Trajectories of the ascents for a Gaussian model in one dimension for LLR algorithms with poor η 0 's and original algorithms with empirically optimal η's compute the modification differently according to whether the coordinate s is "outdated" or not. To do it, we use the quantity τ t defined in Section 4.2 of <ref type="bibr" target="#b9">[SZL13]</ref> as the "number of samples in recent memory". We want to discard the old η's and keep the recent ones. Therefore, at each time t, we   for a quadratic deterministic one-dimensional model for SG, SVRG and their LLR versions compute γ t = exp(−1/τ t ).</p><formula xml:id="formula_38">η0=10 −4 η0=10 −3 η0=10 −2 η0=1 SVRG, η=10 −1</formula><formula xml:id="formula_39">η0=10 −7 η0=10 −6 η0=10 −3 η0=5×10 −3 SVRG, η=10 −4</formula><p>(37)</p><p>Choose η ∈ log(S), and consider the vector in the tangent plane to log(S) at η:</p><formula xml:id="formula_40">e j t =        t k=j γ j , j ≤ t 0, j ≥ t + 1.<label>(38)</label></formula><p>To run an algorithm using the e t 's instead of e as before, all we need to compute again is the formula for the update of the derivative below:</p><formula xml:id="formula_41">H t := ∂ ∂e t T t (η).<label>(39)</label></formula><p>H t may indeed be computed, thanks to the following result.</p><p>Proposition 2. The update equation of H t is:</p><formula xml:id="formula_42">H t+1 = γ t+1 H t + γ t+1 η t+1 f (t) ∂ θ ℓ t (T t (η t )) + γ t+1 η t+1 f (t) ∂ 2 T ℓ t (T t (η)) • H t . (40)</formula><p>The proof lies in Section C.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A LLR applied to the Stochastic Variance Reduced Gradient</head><p>The Stochastic Variance Reduced Gradient (SVRG) was introduced by Johnson and Zhang in <ref type="bibr" target="#b4">[JZ13]</ref>. We define here a version intended for online use.</p><p>Algorithm 4 (SVRG online). We maintain θ t , θ b ∈ Θ (current parameter and base parameter) and s b t ∈ T ≃θt Θ (sum of the gradients of the ℓ s computed at θ s up to time t).</p><p>θ is set to θ 0 and θ b along s b to 0. The update equations read:</p><formula xml:id="formula_43">       s b t+1 = s b t + ∂ θ ℓ t (θ b ) θ t+1 = θ t + η ∂ θ ℓ t (θ t ) − ∂ θ ℓ t (θ b ) + s b t+1 t + 1 .<label>(41)</label></formula><p>We now present the LLR version, obtained by updating the η of SVRG thanks to an SG ascent. We call this algorithm "SVRG/SG".</p><p>Algorithm 5 (SVRG/AG). We maintain θ t , θ b ∈ Θ (current parameter and base parameter), η t ∈ R (current step size), s b t ∈ T ≃θt Θ (sum of the gradients of the ℓ s computed at θ s up to time t), h t ∈ T θt Θ (approximation of the derivative of T t with respect to log(η) at η t ) and the real numbers n t (average of the squared norms of the λ s defined below) and d t (renormalising factor for the computation of n t ).</p><p>θ is set to θ 0 , the other variables are set to 0. At each time t, we compute µ t ∈ R (a rate used in several updates), and λ t ∈ R (the approximate derivative of ℓ t • θ t with respect to log(η) at η t ).</p><p>The </p><formula xml:id="formula_44">λ t = ∂ θ ℓ t (θ t ) • h t d t+1 = 1 − 1 µ t d t + 1 µ t n 2 t+1 = − 1 µ t n t + 1 µ t λ 2 t d −1 t+1 η t+1 = η t exp 1 µ t λ t n t+1 s b t+1 = s b t + ∂ θ ℓ t (θ b ) h t+1 = h t + η t+1 ∂ θ ℓ t (θ t + h t ) − ∂ θ ℓ t (θ b ) + s b t+1 t + 1 θ t+1 = θ t + η t+1 ∂ θ ℓ t (θ t ) − ∂ θ ℓ t (θ b ) + s b t+1 t + 1 .<label>(42)</label></formula><p>B LLR applied to a general stochastic gradient algorithm</p><p>Let Θ and H be two spaces. Θ is the space of parameters, H is that of hyperparameters. In this section, a parameter potentially means a tuple of parameters in the sense of other sections. For instance, in SVRG/SG online, we would call a parameter the couple</p><formula xml:id="formula_45">θ t , θ b .<label>(43)</label></formula><p>Likewise, in the same algorithm, we would call a hyperparameter the couple (η t , h t ) .</p><p>Let</p><formula xml:id="formula_47">F : Θ × H → Θ (θ, η) → F (θ, η).<label>(45)</label></formula><p>be differentiable with respect to both variables. We consider the algorithm:</p><formula xml:id="formula_48">θ t+1 = F (θ t , η t ).<label>(46)</label></formula><p>Let us present its LLR version. We call it GEN/SG, GEN standing for "general".</p><p>Algorithm 6 (GEN/SG). We maintain θ t ∈ Θ (current parameter), η t ∈ H (current hyperparameter), h t ∈ T θt Θ (approximation of the derivative of T t in the direction of e ∈ T ηt H). θ and η are set to user-defined values. The update equations read:</p><formula xml:id="formula_49">         η t+1 = η t + α∂ θ ℓ t (θ t ) • h t h t+1 = ∂ θ F (θ t , η t ) • h t + ∂ η F (θ t , η t ) • ∂ ∂e η t θ t+1 = F (θ t , η t+1 ) .<label>(47)</label></formula><p>C Computations C.1 Computations for Section 2: proof of Fact 1</p><p>Proof. θ 0 is fixed, so A 0 (η) = 0. Let t ≥ 0. We differentiate (5) with respect to log(η), to obtain: To prove Proposition 1, we use the following three lemmas. The first two are technical, and are used in the proof of the third one, which provides an update formula for the derivative appearing in the statement of the proposition. We may have proceeded without these, as in the proof of Fact 1, but they allow the approach to be more generic.</p><formula xml:id="formula_50">∂ ∂log η T t+1 (η) = ∂ ∂log η T t (η) + η f (t) ∂ θ ℓ t (θ t ) + η f (t) ∂ 2 θ ℓ t (θ t (η)) • ∂ ∂log η T t (η),<label>(48)</label></formula><formula xml:id="formula_51">Lemma 2. Let F t : Θ × R → Θ (θ, η) → F t (θ, η) = θ + η f (t) ∂ θ ℓ t (θ).<label>(49)</label></formula><p>Then, ∂ ∂θ F t (θ, η) = Id + η f (t) ∂ 2 θ ℓ t (θ)</p><p>and ∂ ∂η F t (θ, η) = 1 f (t) ∂ θ ℓ t (θ).</p><p>Id is the identity on the tangent plane to Θ in θ.</p><formula xml:id="formula_54">Lemma 3. Let V t : S → Θ × R η → V t (η) = (θ t (η), η t+1 ).<label>(52)</label></formula><p>Consider log(η) ∈ log(S), and any vector e tangent to log(S) at this point. Then the directional derivative of</p><formula xml:id="formula_55">F t • V t : S → Θ η → F t (V t (η)) = T t (η) + η t+1 f (t) ∂ θ ℓ t (T t (η))<label>(53)</label></formula><p>at the point log(η) and in the direction e is</p><formula xml:id="formula_56">∂ ∂e F t • V t (η) = ∂ ∂e T t (η) + ∂ ∂e η t+1 1 f (t) ∂ θ ℓ t (T t (η)) + η t+1 f (t) ∂ 2 θ ℓ t (T t (η)) • ∂ ∂e T t (η).<label>(54)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Each figure of Figures 1 to 3 is made of four graphs: the upper ones are those of SG and SVRG, the lower ones are those of SG/SG and SVRG/SG. Figures 1 to 3 present the trajectories of the ascents for several orders of magnitude of η 0 , while Figures 4 to 6 present the cumulated regrets for the same η 0 's. The trajectory of the running maximum likelihood (ML) is displayed in red in each plot. Trajectories of the ascents for a Gaussian model in one dimension for several algorithms and several η 0 's 3.2.1 Trajectories of θ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Trajectories of the ascents for a Bernoulli model for several algorithms and several η 's</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figures 7 to 9 Figure 3 :</head><label>93</label><figDesc>show the evolution of the value of the logarithm of η t in the LLR procedures for the three models, in regard of the trajectories of the corresponding ascents. For the Gaussian and Bernoulli models, in Figures 7 Trajectories of the ascents for a 50-dimensional linear regression model for several algorithms and several η 's</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>regret between the ML and SVRG/AG</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>regret between the ML and SVRG/AGη0=10 −3 η0=10 −2 η0=10 −1 η0=1 η0=10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Difference between the cumulated regrets of the algorithm and of the ML for a Bernoulli model for several algorithms and several η 0 's α −1 for SG/AG and SVRG/AG, showing the ascent on η is indeed efficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Evolution of log(η t ) in regard of the corresponding ascents for a Bernoulli model in one dimension for SG and SVRG with LLR and several η 0 's be the sequences of parameters and step-sizes obtained with the SG/SG algorithm, where the Hessian is not approximated: this is Algorithm 2 where the update on h t is replaced with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Evolution of log(η t ) in regard of the corresponding ascents for a 50-dimensional linear regression model for SG and SVRG with LLR and several η LLR on SG/AG with bad values vs SG with optimal ones LLR on SVRG/AG with bad values vs SVRG with optimal ones</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>LLR on SG/AG with bad values vs SG with optimal ones LLR on SVRG/AG with bad values vs SVRG with optimal ones Trajectories of the ascents for a Bernoulli model for LLR algorithms with poor η 0 's and original algorithms with empirically optimal ηLLR on SG/AG with bad values vs SG with optimal ones LLR on SVRG/AG with bad values vs SVRG with optimal ones</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Trajectories of the ascents for a 50-dimensional linear regression model for LLR algorithms with poor η 0 's and original algorithms with empirically optimal η's Convergence towards the inverse of the eigenvalue for SG/AG SG, η=10 −1 SG/AG, η0=10 −1 Convergence towards the inverse of the eigenvalue for SVRG/AG SVRG, η=10 −1 SVRG/AG, η0=10 −1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Evolution of log ηt 2 √ t+2 log(t+3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>which concludes the proof. C.2 Computations for Section 4 C.2.1 Computations for Section 4.2: proof of Proposition 1</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is an abuse of notation as Tt is not a function of log η but of η. Formally, we would need to replace Tt with Tt • exp, which we refrain from doing to avoid burdensome notation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The first author would like to thank Gaetan Marceau-Caron for his advice on programming, and Jérémy Bensadon for crucial help with L A T E X.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We may then prove the following lemma. Then for all t ≥ 0,</p><p>Proof. The update equation of T t (η), (28), is such that:</p><p>From the above and Lemma 3,</p><p>which concludes the proof.</p><p>Finally, we prove Proposition 1.</p><p>Proof of Proposition 1. It is sufficient to prove that, for all t ≥ 0,</p><p>that is,</p><p>Therefore, it is sufficient to prove that, for all t ≥ 0, T t (η) = θ t and H t = h t . T 0 (η) = θ by construction and, since θ 0 does not depend on η, H 0 = 0 = h 0 . Assuming the results hold up to iteration t, it is straighforward that T t+1 (η) = θ t+1 , since for all s ≤ t, T s (η) = θ s . Therefore, thanks to Lemma 4, H t and h t have the same update, so that H t+1 = h t+1 , which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 Computations for Section 4.3: proof of Proposition 2</head><p>Proof. Thanks to (58) in Lemma 3,</p><p>that is:</p><p>T t (η).</p><p>(63) We first prove:</p><p>Define (f j ) j≥0 the canonical basis of the tangent plane to log(S) at η. Then, e t+1 = γ t+1 (e t + f t+1 ).</p><p>because the last term is 0. Therefore,</p><p>Then, thanks to (58), ∂ ∂e t+1 η t+1 = γ t+1 η t+1 ,</p><p>which is true since</p><p>and concludes the proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural gradient works efficiently in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shun-Ichi Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS 2014)</title>
		<editor>Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Accelerating Stochastic Gradient Descent using Predictive Variance Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS 2013)</title>
		<editor>Christopher J. C. Burges, Léon Bottou, Max Welling, Zoubin Ghahramani, and Kilian Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic Approximation Methods for Constrained and Unconstrained Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">S</forename><surname>Kushner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="1978" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gradient-based hypermarameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Minimizing finite sums with the stochastic average gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno>00860051</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>HAL</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">No More Pesky Learning Rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
	<note>Sanjoy Dasgupta and David McAllester</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
