<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cooperative Inverse Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-07-05">5 Jul 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Hadfield-Menell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California at Berkeley Berkeley</orgName>
								<address>
									<postCode>94709</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anca</forename><surname>Dragan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California at Berkeley Berkeley</orgName>
								<address>
									<postCode>94709</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California at Berkeley Berkeley</orgName>
								<address>
									<postCode>94709</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California at Berkeley Berkeley</orgName>
								<address>
									<postCode>94709</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cooperative Inverse Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-07-05">5 Jul 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1606.03137v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partialinformation game with two agents, human and robot; both are rewarded according to the human&apos;s reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>"If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively . . . we had better be quite sure that the purpose put into the machine is the purpose which we really desire." So wrote Norbert <ref type="bibr" target="#b25">Wiener (1960)</ref> in one of the earliest explanations of the problems that arise when autonomous systems operate with objectives that differ from those of humans. This value alignment problem is not trivial to solve. Humans are prone to mis-stating their objectives, which can lead to unexpected implementations, as King Midas found out. <ref type="bibr" target="#b22">Russell &amp; Norvig (2010)</ref> give the example of specifying a reward function for a vacuum robot: if we reward the action of cleaning up dirt, which seems reasonable, the robot repeatedly dumps and cleans up the same dirt to maximize its reward. A solution to the value alignment problem has long-term implications for the future of AI and its relationship to humanity <ref type="bibr" target="#b3">(Bostrom, 2014</ref>) and short-term utility for the design of usable AI systems. Giving robots the right objectives and enabling them to make the right trade-offs is crucial for self-driving cars, personal assistants, and human-robot interaction more broadly. Value alignment problems are not unique to artificial systems. Economic systems often involve multiple agents with distinct objectives (e.g., employees and employers) and for whom incentive schemes (e.g., wages) must be designed. <ref type="bibr" target="#b14">Kerr (1975)</ref> explains several examples of value misalignment in this context. Our work has strong connections with the principal-agent problem from economics; we give a brief overview in Section 2. The field of inverse reinforcement learning or IRL <ref type="bibr" target="#b23">(Russell, 1998;</ref><ref type="bibr" target="#b19">Ng &amp; Russell, 2000;</ref><ref type="bibr" target="#b0">Abbeel &amp; Ng, 2004)</ref> is certainly relevant to the value alignment problem. An IRL algorithm infers the reward function of an agent from observations of the agent's behavior, which is assumed to be optimal (or approximately so). One might imagine that IRL provides a simple solution to the value alignment problem: the robot observes human behavior, learns the human reward function, and behaves according to that function. This simple idea has two flaws. The first flaw is obvious: we don't want the robot to adopt the human reward function as its own. For example, human behavior (especially in the morning) often conveys a desire for coffee, and the robot can learn this with IRL, but we don't want the robot to want coffee! This flaw is easily fixed: we need to formulate the value alignment problem so that the robot always has the fixed objective of optimizing reward for the human, and becomes better able to do so as it learns what the human reward function is. The second flaw is less obvious, and less easy to fix. IRL assumes that observed behavior is optimal in the sense that it accomplishes a given task efficiently and this precludes a variety of useful teaching behaviors. For example, acting optimally in coffee acquisition, while leaving the robot as a passive observer, is a suboptimal way to teach a robot to get coffee. Instead, the human should perhaps explain the steps in coffee preparation and show the robot where the backup coffee supplies are kept and what do if the coffee pot is left on the heating plate too long, while the robot might ask what the button with the puffy steam symbol is for and try its hand at coffee making with guidance from the human, even if the first results are undrinkable. None of these things fit in with the standard IRL framework. Cooperative inverse reinforcement learning. We propose, therefore, that value alignment should be formulated as a cooperative and interactive reward maximization process. More precisely, we define a cooperative inverse reinforcement learning (CIRL) problem as a two-player game of partial information, in which the "human", H, knows the reward function (represented by a generalized parameter θ), while the "robot", R, does not; the robot's payoff is exactly the human's actual reward. Optimal solutions to this game maximize human reward; we show that solutions may involve active instruction by the human and active learning by the robot. Reduction to POMDP and Sufficient Statistics. As one might expect, the structure of CIRL games is such that they admit more efficient solution algorithms than are possible for general partialinformation games. Let (π H , π R ) be a pair of policies for human and robot, each depending, in general, on the complete history of observations and actions. A policy pair yields an expected sum of rewards for each player and is a Nash equilibrium if neither actor has an incentive to deviate. In CIRL, the reward function is shared so there is well-defined optimal Nash equilibrium that maximizes value. 1 In Section 3 we reduce the problem of computing an optimal policy pair to the solution of a (single-agent) POMDP, and hence show that the robot's posterior over θ is a sufficient statistic, in the sense that there are optimal policy pairs in which the robot's behavior depends only on this statistic. Moreover, the complexity of solving the POMDP is exponentially lower than the NEXP-hard bound that <ref type="bibr" target="#b2">(Bernstein et al., 2000)</ref> obtains for the natural reduction of the problem to a general Dec-POMDP. Apprenticeship Learning and Suboptimality of IRL-Like Solutions. In Section 3.3 we model apprenticeship learning <ref type="bibr" target="#b0">(Abbeel &amp; Ng, 2004)</ref> as a two-phase CIRL game. In the first phase, the learning phase, both H and R can take actions and this lets R learn about θ. In the second phase, the deployment phase, R uses what it learned to maximize reward (without supervision from H). We show that classic IRL falls out as the best-response policy for the robot under the assumption that the human's policy is "demonstration by expert" (DBE), i.e., acting optimally in isolation as if no robot exists. But we show also that this DBE/IRL policy pair is not, in general, a Nash equilibrium: even if the human knows the robot is running an IRL algorithm, demonstrating expert behavior is not the best way to teach that algorithm.</p><p>Approximate Algorithm for CIRL. We introduce a graph search algorithm that approximately computes H's best response when R is running IRL and rewards are linear in θ and state features. Section 4 compares this best-response policy with the DBE policy in a grid-world example and provides empirical confirmation that the best-response policy, which turns out to "teach" R about the value landscape of the problem, is better than DBE. Thus, designers of apprenticeship learning systems should expect that users will violate the assumption of expert demonstrations in order to better communicate reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our proposed model shares aspects with a variety of existing models. We divide the related work into three categories: inverse reinforcement learning, optimal teaching, and principal-agent models.</p><p>Inverse Reinforcement Learning. <ref type="bibr" target="#b19">Ng &amp; Russell (2000)</ref> define inverse reinforcement learning (IRL) as follows: "Given measurements of an [actor]'s behavior over time. . . . Determine the reward function being optimized." The key assumption IRL makes is that the observed behavior is optimal in the sense that the observed trajectory maximizes the sum of rewards. We call this the A coordination problem arises if there are multiple optimal policy pairs; we defer this issue to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Reward Function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expert Demonstration</head><p>Instructive Demonstration <ref type="figure">Figure 1</ref>: The difference between demonstration-by-expert and instructive demonstration in the mobile robot navigation problem from Section 4. Left: The ground truth reward function. Lighter grid cells indicates areas of higher reward. Middle: The demonstration trajectory generated by the expert policy, superimposed on the maximum a-posteriori reward function the robot infers. Notice that the path remains at a single point of high reward for several steps. The robot successfully learns where the maximum reward is, but little else. Right: An instructive demonstration generated by the algorithm in Section 3.4 superimposed on the maximum a-posteriori reward function that the robot infers. This demonstration highlights both points of high reward and so the robot learns a better estimate of the reward.</p><p>demonstration-by-expert (DBE) assumption. One of our contributions is to prove that this may be suboptimal behavior in a CIRL game, as H may choose to accept less reward on a particular action in order to convey more information to R. In CIRL the DBE assumption prescribes a fixed policy for H. As a result, many IRL algorithms can be derived as state estimation for a best response to different π H , where the state includes the unobserved reward parametrization θ.</p><p>Ng &amp; Russell <ref type="formula">2000</ref>, <ref type="bibr" target="#b0">Abbeel &amp; Ng (2004)</ref>, and <ref type="bibr" target="#b21">Ratliff et al. (2006)</ref> compute constraints that characterize the set of reward functions so that the observed behavior maximizes reward. In general, there will be many reward functions consistent with this constraint. They use a max-margin heuristic to select a single reward function from this set as their estimate. In CIRL, the constraints they compute characterize R's belief about θ under the DBE assumption. <ref type="bibr" target="#b20">Ramachandran &amp; Amir (2007)</ref> and <ref type="bibr" target="#b26">Ziebart et al. (2008)</ref> consider the case where π H is "noisily expert," i.e., π H is a Boltzmann distribution where actions or trajectories are selected in proportion to the exponent of their value. <ref type="bibr" target="#b20">Ramachandran &amp; Amir (2007)</ref> adopt a Bayesian approach and place an explicit prior on rewards. <ref type="bibr" target="#b26">Ziebart et al. (2008)</ref> places a prior on reward functions indirectly by assuming a uniform prior over trajectories. In our model, these assumptions are variations of DBE and both implement state estimation for a best response to the appropriate fixed H. <ref type="bibr" target="#b17">Natarajan et al. (2010)</ref> introduce an extension to IRL where R observes multiple actors that cooperate to maximize a common reward function. This is a different type of cooperation than we consider, as the reward function is common knowledge and R is a passive observer. <ref type="bibr" target="#b24">Waugh et al. (2011)</ref> and <ref type="bibr" target="#b15">Kuleshov &amp; Schrijvers (2015)</ref> consider the problem of inferring payoffs from observed behavior in a general (i.e., non-cooperative) game given observed behavior. It would be interesting to consider an analogous extension to CIRL, akin to mechanism design, in which R tries to maximize collective utility for a group of Hs that may have competing objectives. Fern et al. (2014) consider a hidden-goal MDP, a special case of a POMDP where the goal is an unobserved part of the state. This can be considered a special case of CIRL, where θ encodes a particular goal state. The frameworks share the idea that R helps H. The key difference between the models lies in the treatment of the human (the agent in their terminology). Fern et al. (2014) model the human as part of the environment. In contrast, we treat H as an actor in a decision problem that both actors collectively solve. This is crucial to modeling the human's incentive to teach.</p><p>Optimal Teaching. Because CIRL incentivizes the human to teach, as opposed to maximizing reward in isolation, our work is related to optimal teaching: finding examples that optimally train a learner <ref type="bibr" target="#b1">(Balbach &amp; Zeugmann, 2009;</ref><ref type="bibr" target="#b9">Goldman et al., 1993;</ref><ref type="bibr" target="#b8">Goldman &amp; Kearns, 1995)</ref>. The key difference is that efficient learning is the objective of optimal teaching, while it emerges as a property of optimal equilibrium behavior in CIRL. <ref type="bibr" target="#b5">Cakmak &amp; Lopes (2012)</ref> consider an application of optimal teaching where the goal is to teach the learner the reward function for an MDP. The teacher gets to pick initial states from which an expert executes the reward-maximizing trajectory. The learner uses IRL to infer the reward function, and the teacher picks initial states to minimize the learner's uncertainty. In CIRL, this approach can be characterized as an approximate algorithm for a highly restricted H that greedily minimizes the entropy of R's belief. Beyond teaching, several models focus on taking actions that convey some underlying state, not necessarily a reward function. Examples include finding a motion that best communicates an agent's intention <ref type="bibr" target="#b6">(Dragan &amp; Srinivasa, 2013)</ref>, or finding a natural language utterance that best communicates a particular grounding <ref type="bibr" target="#b10">(Golland et al., 2010)</ref>. All of these approaches model the observer's inference process and compute actions (motion or speech) that maximize the probability an observer infers the correct hypothesis or goal. Our approximate solution to CIRL is analogous to these approaches, in that we compute actions that are informative of the correct reward function.</p><p>Principal-agent models. Value alignment problems are not intrinsic to artificial agents. <ref type="bibr" target="#b14">Kerr (1975)</ref> describes a wide variety of misaligned incentives in the aptly titled "On the folly of rewarding A, while hoping for B." In economics, this is known as the principal-agent problem: the principal (e.g., the employer) specifies incentives so that an agent (e.g., the employee) maximizes the principal's profit <ref type="bibr" target="#b13">(Jensen &amp; Meckling, 1976)</ref>. Principal-agent models study the problem of generating appropriate incentives in a non-cooperative setting with asymmetric information. In this setting, misalignment arises because the agents that economists model are people and intrinsically have their own desires. In AI, misalignment arises entirely from the information asymmetry between the principal and the agent; if we could characterize the correct reward function, we could program it into an artificial agent. <ref type="bibr" target="#b7">Gibbons (1998)</ref> provides a useful survey of principal-agent models and their applications. <ref type="bibr" target="#b11">Holmstrom &amp; Milgrom (1987)</ref> gives structural results on optimal incentive schemes in linear principal-agent models. From the perspective of AI research, one of the most interesting lines of research in this literature studies the impacts of distorted incentives. <ref type="bibr" target="#b12">Holmstrom &amp; Milgrom (1991)</ref> develop a multi-task model where some tasks are more easily measured and rewarded than others. The key result shows that incentives for the more precisely measured tasks should be reduced to avoid diverting too much effort from poorly measured tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cooperative Inverse Reinforcement Learning</head><p>This section formulates CIRL as a two-player Markov game with identical payoffs, proves that the problem of computing an optimal equilibrium for a CIRL game is lower complexity than the näive bound from Dec-POMDP's suggests, and characterizes apprenticeship learning as a subclass of CIRL games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CIRL Formulation</head><p>Definition 1. A cooperative inverse reinforcement learning (CIRL) game M is a two-player Markov game with identical payoffs between a human or principal, H, and a robot or agent, R. The game is described by a tuple,</p><formula xml:id="formula_0">M = S, {A H , A R }, T (•|•, •, •), {Θ, R(•, •, •; •)}, P 0 (•, •), γ ,<label>(1)</label></formula><p>with the following definitions: S a set of world states: s ∈ S.</p><p>A H a set of actions for H: a H ∈ A H . A R a set of actions for R: We write the reward for a state-parameter pair as R(s, a H , a R ; θ) to distinguish the static reward parameters θ from the changing world state s. The game proceeds as follows. First, the initial state, a tuple (s, θ), is sampled from P 0 . H observes θ. This parameter represents the human's internal reward function. This observation models that only the human knows the reward function, while both actors know a prior distribution over possible reward functions. At each timestep t, H and R observe the current state s t and select their actions a H t , a R t . Both actors receive reward r t = R(s t , a H t , a R t ; θ) and observe each other's action selection. A state for the next timestep is sampled from the transition distribution, s t+1 ∼ P T (s |s t , a H t , a R t ), and the process repeats. Behavior in a CIRL game is defined by a pair of policies, (π H , π R ), that determine action selection for H and R respectively. In general, these policies can be arbitrary functions of their observation histories; π H :</p><formula xml:id="formula_1">a R ∈ A R . T (•|•, •,</formula><formula xml:id="formula_2">A H × A R × S * × Θ → A H , π R : A H × A R × S * → A R .</formula><p>The optimal joint policy is the policy that maximizes value. The value of a state is the expected sum of discounted rewards under the initial distribution of reward parameters and world states.</p><p>Remark 1. A key property of CIRL is that the human and the robot get rewards determined by the same reward function. This incentivizes the human to teach and the robot to learn without explicitly encoding these as objectives of the actors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structural Results for Optimal Equilibrium Computation</head><p>The analogue in CIRL to computing an optimal policy for an MDP is the problem of computing an optimal policy pair. This is a pair of policies that maximizes the expected sum of discounted rewards. This is not the same as 'solving' a CIRL game, as a real world implementation of a CIRL agent must account for coordination problems and strategic uncertainty <ref type="bibr" target="#b4">(Boutilier, 1999)</ref>. The optimal policy pair represents the best H and R can do if they can coordinate perfectly before H observes θ.</p><p>Computing an optimal joint policy for a cooperative game is the solution to a decentralizedpartially observed Markov decision process (Dec-POMDP). Unfortunately, Dec-POMDPs are NEXPcomplete <ref type="bibr" target="#b2">(Bernstein et al., 2000)</ref> so general Dec-POMDP algorithms have a computational complexity that is doubly exponential. Fortunately, CIRL games have special structure that makes optimal equilibrium computation more efficient. <ref type="bibr" target="#b18">Nayyar et al. (2013)</ref> shows that a Dec-POMDP can be reduced to a coordination-POMDP. The actor in this POMDP is a coordinator that observes all common observations and specifies a policy for each actor. These policies map each actor's private information to an action. The structure of a CIRL game implies that the private information is limited to H's initial observation of θ. This allows the reduction to a coordination-POMDP to preserve the size of the (hidden) state space, making the problem easier.</p><p>Theorem 1. Let M be an arbitrary CIRL game with state space S and reward space Θ. There exists a (single-actor) POMDP M C with (hidden) state space S C such that |S C | = |S| • |Θ| and, for any policy pair in M , there is a policy in M C that achieves the same sum of discounted rewards.</p><p>Proof. See supplementary material.</p><p>This reduction lets us show that R's belief about θ is a sufficient statistic for optimal behavior.</p><p>Corollary 1. Let M be a CIRL game. There exist optimal policies (π H * , π R * ) that only depend on the current state and R's belief.</p><formula xml:id="formula_3">π H * : S × ∆ Θ × Θ → A H , π R * : S × ∆ Θ → A R .</formula><p>Proof. See supplementary material.</p><p>Remark 2. In a general Dec-POMDP, the hidden state for the coordinator-POMDP includes each actor's history of observations. In CIRL, θ is the only private information so we get an exponential decrease in the complexity of the reduced problem. This allows one to apply general POMDP algorithms to compute optimal joint policies in CIRL.</p><p>It is important to note that the reduced problem may still be very challenging. POMDPs are difficult in their own right and the reduced problem still has a much larger action space. That being said, this reduction is still useful in that it characterizes optimal joint policy computation for CIRL as significantly easier than Dec-POMDPs. Furthermore, this theorem can be used to justify approximate methods (e.g., iterated best response) that only depend on R's belief state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A Formal Model of Apprenticeship Learning</head><p>A common paradigm for robot learning from humans is apprenticeship learning. In this paradigm, a human gives demonstrations to a robot of a sample task and the robot is asked to imitate it in a subsequent task. In what follows, we formulate apprenticeship learning as turn-based CIRL with a learning phase and a deployment phase. We characterize IRL as the best response to a demonstrationby-expert policy for H. We also show that this policy is, in general, not an equilibrium policy and so IRL is generally a suboptimal approach to apprenticeship learning. Definition 2. (ACIRL) An apprenticeship cooperative inverse reinforcement learning (ACIRL) game is a turn-based CIRL game with two phases: a learning phase where the human and the robot take turns acting, and a deployment phase, where the robot acts independently.</p><p>Example. Consider an example apprenticeship task where R needs to help H make office supplies. H and R can make paperclips and staples and the unobserved θ describe H's preference for paperclips vs staples. We model the problem as an ACIRL game in which the learning and deployment phase each consist of an individual action. The world state in this problem is a tuple (p s , q s , t) where p s and q s respectively represent the number of paperclips and staples H owns. t is the round number. An action is a tuple (p a , q a ) that produces p a paperclips and q a staples. The human can make 2 items total: A H = {(0, 2), (1, 1), (2, 0)}. The robot has different capabilities. It can make 50 units of each item or it can choose to make 90 of a single item: A R = {(0, 90), (50, 50), (90, 0)}. We let Θ = [0, 1] and define R so that θ indicates the relative preference between paperclips and staples:R(s, (p a , q a ); θ) = θp a + (1 − θ)q a . R's action is ignored when t = 0 and H's is ignored when t = 1. At t = 2, the game is over, so the game transitions to a sink state, (0, 0, 2).</p><p>Deployment phase -maximize mean reward estimate. It is simplest to analyze the deployment phase first. R is the only actor in this phase so it get no more observations of its reward. We have shown that R's belief about θ is a sufficient statistic for the optimal policy. This belief about θ induces a distribution over MDPs. A straightforward extension of a result due to <ref type="bibr" target="#b20">Ramachandran &amp; Amir (2007)</ref> shows that R's optimal deployment policy maximizes reward for the mean reward function. Theorem 2. Let M be an ACIRL game. In the deployment phase, the optimal policy for R maximizes reward in the MDP induced by the mean θ from R's belief.</p><p>Proof. See supplementary material.</p><p>In our example, suppose that π</p><formula xml:id="formula_4">H selects (0, 2) if θ ∈ [0, 1 ), (1, 1) if θ ∈ [ 1 , 2 ]</formula><p>and (2, 0) otherwise. R begins with a uniform prior on θ so observing, e.g., a H = (0, 2) leads to a posterior distribution that is uniform on [0, 1 ). Theorem 2 shows that the optimal action maximizes reward for the mean θ so an optimal R behaves as though θ = 1 6 during the deployment phase. Learning phase -expert demonstrations are not optimal. A wide variety of apprenticeship learning approaches assume that demonstrations are given by an expert. We say that H satisfies the demonstration-by-expert (DBE) assumption in ACIRL if she greedily maximizes immediate reward on her turn. This is an 'expert' demonstration because it demonstrates a reward maximizing action but does not account for that action's impact on R's belief. We use E to represent actors that satisfy this assumption and π E to represent the corresponding policy. Theorem 2 enables us to characterize the best response for R under the DBE assumption in ACIRL: use IRL to compute the posterior over θ during the learning phase and then act to maximize reward under the mean θ in the deployment phase. Note that this does not define R's behavior during learning, just its belief. CIRL also gives us the ability to analyze the DBE assumption itself. In particular, we can show that π E is not a component of an equilibrium joint policy. Theorem 3. Suppose that π R = br(π E ). There exist ACIRL games where the best-response for H to π R violates the expert demonstrator assumption. In other words br(br(π E )) = π E .</p><p>Proof. See supplementary material.</p><p>The supplementary material proves this theorem by computing the optimal equilibrium for our example. In that equilibrium,</p><formula xml:id="formula_5">H selects (1, 1) if θ ∈ [ 41 , 51</formula><p>]. In contrast, π E only chooses (1, 1) if θ = 0.5. The change arises because there are situations (e.g., θ = 0.49) where the immediate loss of reward to H is worth the improvement in R's estimate of θ.</p><p>Remark 3. We should expect experienced users of apprenticeship learning systems to present demonstrations optimized for fast learning rather than demonstrations that maximize reward. Importantly, the demonstrator is incentivized to deviate from R's assumptions. This has implications for the design and analysis of apprenticeship systems in robotics. Inaccurate assumptions about user behavior are notorious for leading to bugs in software systems (see, e.g., <ref type="bibr" target="#b16">Leveson &amp; Turner (1993)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Approximate Best Response to Feature Matching</head><p>Now, we consider the problem of computing H's best response when R uses IRL as a state estimator. For our toy example, we computed equilibria exhaustively, for realistic problems we need a more efficient approach. Section 3.2 shows that this can be reduced to an POMDP where the state is a tuple of world state, reward parameters, and R's belief. While this is easier than solving a general Dec-POMDP, it is a computational challenge. If we restrict our attention to the case of linear reward functions 2 we can develop an efficient approximate algorithm to compute a best response. Specifically, we consider the case where the reward for a state (s, θ) is defined as a linear combination of state features for some feature function φ : R(s, a H , a R ; θ) = φ(s) θ. Standard results from the IRL literature show that policies with the same expected feature counts have the same value <ref type="bibr" target="#b0">(Abbeel &amp; Ng, 2004)</ref>. Combined with Theorem 2, this implies that the optimal π R under the DBE assumption computes a policy that matches the observed feature counts from the learning phase. This suggests a simple approximation scheme. To compute a demonstration trajectory τ H , first compute the feature counts R would observe in expectation from the true θ and then select actions that maximize similarity to these target features. If φ θ are the expected feature counts induced by θ then this scheme amounts to the following decision rule:</p><formula xml:id="formula_6">τ H ← argmax τ φ(τ ) θ − η||φ θ − φ(τ )|| 2 .<label>(2)</label></formula><p>This rule selects a trajectory that trades off between the sum of rewards φ(τ ) θ and the feature dissimilarity ||φ θ − φ(τ )|| 2 . Note that this is generally distinct from the action selected by the demonstration-by-expert policy. The goal is to match the expected sum of features under a distribution of trajectories with the sum of features from a single trajectory. The correct measure of feature similarity is regret: the difference between the reward R would collect if it knew the true θ and the reward R actually collects using the inferred θ. Computing this similarity is expensive, so we use an 2 norm as a proxy measure for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present a learning experiment for 2D mobile robot navigation. We compare the approximation from Section 3.4 to IRL. Specifically, we examine the case where R is implemented with IRL and measure the value of different policies for H: expert, which matches the IRL assumption, and best-responder, which computes the best response to IRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cooperative Learning for Mobile Robot Navigation</head><p>Our experimental domain is a 2D navigation problem on a discrete grid. In the learning phase of the game, H teleoperates a trajectory while R observes. In the deployment phase, R controls the robot and attempts to maximize reward. We use a finite horizon H, and let the first H timesteps be the learning phase. There are N φ state features defined as radial basis functions where the centers are common knowledge. Rewards are linear in these features and θ. The initial world state is in the middle of the map. We use a uniform distribution on [−1, 1] N φ for the prior on θ. Actions move in one of the four cardinal directions {N, S, E, W } and there is an additional no-op ∅ that each actor executes deterministically on the other agent's turn. <ref type="figure">Figure 1</ref> shows an example comparison between demonstration-by-expert and the approximate best response policy in Section 3.4. The leftmost image is the ground truth reward function. Next to it are demonstration trajectories produce by these two policies. Each path is superimposed on the maximum a-posteriori reward function the robot infers from the demonstration. We can see that the demonstration-by-expert policy immediately goes to the highest reward and stays there. In contrast, the best response policy moves to both areas of high reward. The robot reward function the robot  <ref type="table">Table 1</ref>: Comparison of 'expert' demonstration (π E ) with 'instructive' demonstration (br). Lower numbers are better. Using the best response causes R to infer a better distribution over θ so it does a better job of maximizing reward.</p><p>infers from the best response demonstration is much more representative of the true reward function, when compared with the reward function it infers from demonstration-by-expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Demonstration-by-Expert vs Best Responder</head><p>Hypothesis. When R plays an IRL algorithm that matches features, H prefers the best response policy from Section 3.4 to π E : the best response policy will significantly outperform the demonstrationby-expert policy. Manipulated Variables. Our experiment consists of 2 factors: H-policy and num-features. We make the assumption that R uses an IRL algorithm to compute its estimate of θ during learning and maximizes reward under this estimate during deployment. We use Maximum-Entropy IRL <ref type="bibr" target="#b26">(Ziebart et al., 2008)</ref> to implement R's policy. H-policy varies H's strategy π H and has two levels: demonstration-by-expert and best-responder. In the demonstration-by-expert level H adheres to the DBE assumption and maximizes reward during the demonstration. In the best-responder level H uses the approximate algorithm from Section 3.4 to compute an (approximate) best response to π R . The trade-off between reward and communication η is set by cross-validation before the game begins. The num-features factor varies the dimensionality of φ. We consider two levels for this factor: 3 features and 10 features. We do this to test whether and how the difference between experts and best-responders is affected by dimensionality. We use a factorial design across these factors. This leads to 4 distinct conditions. We test each condition against a random sample of N = 500 different reward parameters. We use a within-subjects design with respect to the the H-policy factor so the same reward parameters are tested for both levels. We use a between-subjects design for the num-features factor. Dependent Measures. We use the regret with respect to a fully-observed setting where the robot knows the ground truth θ as a measure of performance. We letθ be the robot's estimate of the reward parameters and let θ GT be the ground truth reward parameters. The primary measure is the sub-optimality of robot's policy reward: the difference between the value of the policy that maximizeŝ θ and the value of the policy that maximizes θ GT . We also use two secondary measures. The first is the KL-divergence between the maximum-entropy trajectory distribution induced byθ and the maximum-entropy trajectory distribution induced by θ. Finally, we use the 2 -norm between the vector or rewards defined byθ and the vector induced by θ. Results. <ref type="table">Table 1</ref> shows the dependent measures from our experiment. We are able to confirm our hypothesis that the demonstration-by-expert level of the H-policy factor results has substantially higher regret than the best-response policy. The size of the gap is especially large when num-features=10. This is because Θ is higher-dimensional so an instructive trajectory is more informative. One particularly interesting observation is that even when R's inferred reward function is inaccurate the best-response policy still results in low regret with respect to the value of the policy pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work, we presented a game-theoretic model for cooperative learning, CIRL. Key to this model is that the robot knows that it is in a shared environment and is attempting to maximize the human's reward (as opposed to estimating the human's reward function and adopting it as its own). This leads to cooperative learning behavior and provides a framework in which to design HRI algorithms and analyze the incentives of both actors in a learning environment. We reduced the problem of computing an optimal policy pair to solving a POMDP. This is a useful theoretical tool and can be used to design new algorithms, but it is clear that optimal policy pairs are only part of the story. In particular, when it performs a centralized computation, the reduction assumes that we can effectively program both actors to follow a set coordination policy. This may not be feasible in reality, although it may nonetheless be helpful in training humans to be better teachers.</p><p>An important avenue for future research will be to consider the problem of equilibrium acquisition: the process by which two independent actors arrive at an equilibrium pair of policies. Returning to Wiener's warning, we believe that the best solution is not to put a specific purpose into the machine at all, but instead to design machines that provably converge to the right purpose as they go along.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>•) a conditional distribution on the next world state, given previous state and action for both agents: T (s |s, a H , a R ).Θ a set of possible static reward parameters, only observed by H: θ ∈ Θ. R(•, •, •; •) a parameterized reward function that maps world states, joint actions, and reward parameters to real numbers. R : S × A H × A R × Θ → R. P 0 (•, •) a distribution over the initial state, represented as tuples: P 0 (s 0 , θ) γ a discount factor: γ ∈ [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Regret KL-divergence ||θ GT −θ|| 2 Regret KL-divergence ||θ GT −θ|| 2 π</figDesc><table><row><cell>H-policy</cell><cell></cell><cell>num-features=3</cell><cell></cell><cell></cell><cell>num-features=10</cell><cell></cell></row><row><cell>E</cell><cell>11.3</cell><cell>5.53</cell><cell>9.7</cell><cell>16.1</cell><cell>6.2</cell><cell>15.3</cell></row><row><cell>br</cell><cell>0.2</cell><cell>0.1</cell><cell>1.0</cell><cell>4.3</cell><cell>2.4</cell><cell>13.3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">where linearity is with respect to θ.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the DARPA Simplifying Complexity in Scientific Discovery (SIMPLEX) program. Dylan Hadfield-Menell is supported by a NSF Graduate Reseach Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recent developments in algorithmic teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zeugmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language and Automata Theory and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The complexity of decentralized control of Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Superintelligence: Paths, dangers, strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bostrom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sequential optimality and coordination in multiagent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithmic and human teaching of sequential decision tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A decision-theoretic model of assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tadepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="71" to="104" />
		</imprint>
	</monogr>
	<note>Generating legible motion</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Incentives in organizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gibbons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>National Bureau of Economic Research</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the complexity of teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="31" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning binary relations and total orders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1006" to="1034" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to generating spatial descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Aggregation and linearity in the provision of intertemporal incentives. Econometrica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holmstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milgrom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="303" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multitask principal-agent analyses: Incentive contracts, asset ownership, and job design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holmstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milgrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economics, &amp; Organization</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="24" to="52" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note>Journal of Law</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Theory of the firm: Managerial behavior, agency costs and ownership structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Meckling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Financial Economics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="305" to="360" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the folly of rewarding A, while hoping for B</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kerr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academy of Management Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="769" to="783" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inverse game theory. Web and Internet Economics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Schrijvers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An investigation of the Therac-25 accidents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leveson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="18" to="41" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-agent inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kunapuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tadepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kand</forename><surname>Shavlik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Conference on Machine Learning and Applications</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decentralized stochastic control with partial history sharing: A common information approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nayyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teneketzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1644" to="1658" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Algorithms for inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum margin planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Pearson</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning agents for uncertain environments (extended abstract)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>In COLT</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Computational rationalization: The inverse equilibrium problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Waugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Some moral and technical consequences of automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
