<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds using Paraphrases in a Neural Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-03-21">21 Mar 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Waterson</surname></persName>
							<email>waterson@google.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department Bar</orgName>
								<orgName type="institution">Ilan University Ramat-Gan</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway Mountain View</addrLine>
									<postCode>94043</postCode>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds using Paraphrases in a Neural Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-03-21">21 Mar 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1803.08073v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Automatic interpretation of the relation between the constituents of a noun compound, e.g. olive oil (source) and baby oil (purpose) is an important task for many NLP applications. Recent approaches are typically based on either noun-compound representations or paraphrases. While the former has initially shown promising results, recent work suggests that the success stems from memorizing single prototypical words for each relation. We explore a neural paraphrasing approach that demonstrates superior performance when such memorization is not possible.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic classification of a noun-compound (NC) to the implicit semantic relation that holds between its constituent words is beneficial for applications that require text understanding. For instance, a personal assistant asked "do I have a morning meeting tomorrow?" should search the calendar for meetings occurring in the morning, while for group meeting it should look for meetings with specific participants. The NC classification task is a challenging one, as the meaning of an NC is often not easily derivable from the meaning of its constituent words <ref type="bibr" target="#b19">(Spärck Jones, 1983)</ref>. Previous work on the task falls into two main approaches. The first maps NCs to paraphrases that express the relation between the constituent words (e.g. <ref type="bibr" target="#b11">Nakov and Hearst, 2006;</ref><ref type="bibr" target="#b13">Nulty and Costello, 2013)</ref>, such as mapping coffee cup and garbage dump to the pattern</p><formula xml:id="formula_0">[w 1 ] CONTAINS [w 2 ].</formula><p>The second approach computes a representation for NCs from the distributional representation of their individual constituents. While this approach * Work done during an internship at Google. yielded promising results, recently, <ref type="bibr" target="#b2">Dima (2016)</ref> showed that similar performance is achieved by representing the NC as a concatenation of its constituent embeddings, and attributed it to the lexical memorization phenomenon <ref type="bibr" target="#b9">(Levy et al., 2015)</ref>.</p><p>In this paper we apply lessons learned from the parallel task of semantic relation classification. We adapt HypeNET <ref type="bibr" target="#b17">(Shwartz et al., 2016)</ref> to the NC classification task, using their path embeddings to represent paraphrases and combining with distributional information. We experiment with various evaluation settings, including settings that make lexical memorization impossible. In these settings, the integrated method performs better than the baselines. Even so, the performance is mediocre for all methods, suggesting that the task is difficult and warrants further investigation. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Various tasks have been suggested to address noun-compound interpretation. NC paraphrasing extracts texts explicitly describing the implicit relation between the constituents, for example student protest is a protest LED BY, BE SPONSORED BY, or BE ORGANIZED BY students (e.g. <ref type="bibr" target="#b11">Nakov and Hearst, 2006;</ref><ref type="bibr" target="#b7">Kim and Nakov, 2011;</ref><ref type="bibr" target="#b5">Hendrickx et al., 2013;</ref><ref type="bibr" target="#b13">Nulty and Costello, 2013)</ref>. Compositionality prediction determines to what extent the meaning of the NC can be expressed in terms of the meaning of its constituents, e.g. spelling bee is non-compositional, as it is not related to bee (e.g. <ref type="bibr" target="#b15">Reddy et al., 2011)</ref>. In this paper we focus on the NC classification task, which is defined as follows: given a pre-defined set of relations, classify nc = w 1 w 2 to the relation that holds between w 1 and w 2 . We review the various features used in the literature for classification. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Compositional Representations</head><p>In this approach, classification is based on a vector representing the NC (w 1 w 2 ), which is obtained by applying a function to its constituents' distributional representations: v w 1 , v w 2 ∈ R n . Various functions have been proposed in the literature. <ref type="bibr" target="#b10">Mitchell and Lapata (2010)</ref> proposed 3 simple combinations of v w 1 and v w 2 (additive, multiplicative, dilation). Others suggested to represent compositions by applying linear functions, encoded as matrices, over word vectors. <ref type="bibr" target="#b1">Baroni and Zamparelli (2010)</ref> focused on adjective-noun compositions (AN) and represented adjectives as matrices, nouns as vectors, and ANs as their multiplication. Matrices were learned with the objective of minimizing the distance between the learned vector and the observed vector (computed from corpus occurrences) of each AN. The full-additive model <ref type="bibr" target="#b24">(Zanzotto et al., 2010;</ref><ref type="bibr" target="#b4">Dinu et al., 2013</ref>) is a similar approach that works on any two-word composition, multiplying each word by a square matrix:</p><formula xml:id="formula_1">nc = A • v w 1 + B • v w 2 .</formula><p>Socher et al. <ref type="formula">2012</ref>suggested a non-linear composition model. A recursive neural network operates bottom-up on the output of a constituency parser to represent variable-length phrases. Each constituent is represented by a vector that captures its meaning and a matrix that captures how it modifies the meaning of constituents that it combines with. For a binary NC,</p><formula xml:id="formula_2">nc = g(W • [ v w 1 ; v w 2 ]),</formula><p>where W ∈ R 2n×n and g is a non-linear function.</p><p>These representations were used as features in NC classification, often achieving promising results (e.g. Van de Cruys et al., 2013; Dima and Hinrichs, 2015). However, <ref type="bibr" target="#b2">Dima (2016)</ref> recently showed that similar performance is achieved by representing the NC as a concatenation of its constituent embeddings, and argued that it stems from memorizing prototypical words for each relation. For example, classifying any NC with the head oil to the SOURCE relation, regardless of the modifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Paraphrasing</head><p>In this approach, the paraphrases of an NC, i.e. the patterns connecting the joint occurrences of the constituents in a corpus, are treated as features. For example, both paper cup and steel knife may share the feature MADE OF. Séaghdha and Copestake (2013) leveraged this "relational similarity" in a kernel-based classification approach. They combined the relational information with the complementary lexical features of each constituent separately. Two NCs labeled to the same relation may consist of similar constituents (paper-steel, cup-knife) and may also appear with similar paraphrases. Combining the two information sources has shown to be beneficial, but it was also noted that the relational information suffered from data sparsity: many NCs had very few paraphrases, and paraphrase similarity was based on ngram overlap.</p><p>Recently, <ref type="bibr" target="#b20">Surtani and Paul (2015)</ref> suggested to represent NCs in a vector space model (VSM) using paraphrases as features. These vectors were used to classify new NCs based on the nearest neighbor in the VSM. However, the model was only tested on a small dataset and performed similarly to previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We similarly investigate the use of paraphrasing for NC relation classification. To generate a signal for the joint occurrences of w 1 and w 2 , we follow the approach used by HypeNET <ref type="bibr" target="#b17">(Shwartz et al., 2016)</ref>. For an w 1 w 2 in the dataset, we collect all the dependency paths that connect w 1 and w 2 in the corpus, and learn path embeddings as detailed in Section 3.2. Section 3.1 describes the classification models with which we experimented. <ref type="figure" target="#fig_0">Figure 1</ref> provides an overview of the models: path-based, integrated, and integrated-NC, each which incrementally adds new features not present in the previous model. In the following sections, x denotes the input vector representing the NC. The network classifies NC to the highest scoring relation: r = argmax i softmax( o) i , where o is the output layer. All networks contain a single hidden layer whose dimension is |x| 2 . k is the number of relations in the dataset. See Appendix A for additional technical details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classification Models</head><p>Path-based. Classifies the NC based only on the paths connecting the joint occurrences of w 1 and w 2 in the corpus, denoted P (w 1 , w 2 ). We define the feature vector as the average of its path embeddings, where the path embedding p of a path p is  weighted by its frequency f p,(w 1 ,w 2 ) :</p><formula xml:id="formula_3">... coffee cup ... coffee cup ... [w 1 ] of [w 2 ] [w 1 ] containing [w 2 ] ... [w 2 ] in [w 1 ] v w2 v w1 v nc paths(w 1 , w 2 ) mean pooling • • • • • • Path</formula><formula xml:id="formula_4">x = v P (w 1 ,w 2 ) = p∈P (w 1 ,w 2 ) f p,(w 1 ,w 2 ) • p p∈P (w 1 ,w 2 ) f p,(w 1 ,w 2 )</formula><p>Integrated. We concatenate w 1 and w 2 's word embeddings to the path vector, to add distributional information:</p><formula xml:id="formula_5">x = [ v w 1 , v w 2 , v P (w 1 ,w 2 ) ].</formula><p>Potentially, this allows the network to utilize the contextual properties of each individual constituent, e.g. assigning high probability to SUBSTANCE-MATERIAL-INGREDIENT for edible w 1 s (e.g. vanilla pudding, apple cake). Integrated-NC. We add the NC's observed vector v nc as additional distributional input, providing the contexts in which w 1 w 2 occur as an NC:</p><formula xml:id="formula_6">v nc = [ v w 1 , v w 2 , v nc , v P (w 1 ,w 2 ) ]</formula><p>. Like Dima (2016), we learn NC vectors using the GloVe algorithm <ref type="bibr" target="#b14">(Pennington et al., 2014)</ref>, by replacing each NC occurrence in the corpus with a single token.</p><p>This information can potentially help clustering NCs that appear in similar contexts despite having low pairwise similarity scores between their constituents. For example, gun violence and abortion rights belong to the TOPIC relation and may appear in similar news-related contexts, while (gun, abortion) and (violence, rights) are dissimilar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Path Embeddings</head><p>Following HypeNET, for a path p composed of edges e 1 , ..., e k , we represent each edge by the concatenation of its lemma, part-of-speech tag, dependency label and direction vectors:</p><formula xml:id="formula_7">v e = [ v l , v pos , v dep , v dir ].</formula><p>The edge vectors v e 1 , ..., v e k are encoded using an LSTM <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997)</ref>, and the last output vector p is used as the path embedding.</p><p>We use the NC labels as distant supervision. While HypeNET predicts a word pair's label from the frequency-weighted average of the path vectors, we differ from it slightly and compute the label from the frequency-weighted average of the predictions obtained from each path separately:</p><formula xml:id="formula_8">o = p∈P (w 1 ,w 2 ) f p,(w 1 ,w 2 ) • softmax( p) p∈P (w 1 ,w 2 ) f p,(w 1 ,w 2 ) r = argmax i o i</formula><p>We conjecture that label distribution averaging allows for more efficient training of path embeddings when a single NC contains multiple paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We follow <ref type="bibr" target="#b2">Dima (2016)</ref> and evaluate on the Tratz (2011) dataset, with 19,158 instances and two levels of labels: fine-grained (Tratz-fine, 37 relations) and coarse-grained (Tratz-coarse, 12 relations). We report results on both versions. See <ref type="bibr" target="#b21">Tratz (2011)</ref> for the list of relations.</p><p>Dataset Splits <ref type="bibr" target="#b2">Dima (2016)</ref> showed that a classifier based only on v w 1 and v w 2 performs on par with compound representations, and that the success comes from lexical memorization <ref type="bibr" target="#b9">(Levy et al., 2015)</ref>: memorizing the majority label of single words in particular slots of the compound (e.g. TOPIC for travel guide, fishing guide, etc.). This memorization paints a skewed picture of the stateof-the-art performance on this difficult task.</p><p>To better test this hypothesis, we evaluate on 4 different splits of the datasets to train, test, and validation sets: (1) random, in a 75:20:5 ratio, (2)   lexical-full, in which the train, test, and validation sets each consists of a distinct vocabulary. The split was suggested by <ref type="bibr" target="#b9">Levy et al. (2015)</ref>, and it randomly assigns words to distinct sets, such that for example, including travel guide in the train set promises that fishing guide would not be included in the test set, and the models do not benefit from memorizing that the head guide is always annotated as TOPIC. Given that the split discards many NCs, we experimented with two additional splits:</p><p>(3) lexical-mod split, in which the w 1 words are unique in each set, and (4) lexical-head split, in which the w 2 words are unique in each set. Table 2 displays the sizes of each split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>Frequency Baselines. mod freq classifies w 1 w 2 to the most common relation in the train set for NCs with the same modifier (w 1 w ′ 2 ), while head freq considers NCs with the same head (w ′ 1 w 2 ). 5</p><p>Distributional Baselines. Ablation of the pathbased component from our models: Dist uses only w 1 and w 2 's word embeddings:</p><formula xml:id="formula_9">x = [ v w 1 , v w 2 ],</formula><p>while Dist-NC includes also the NC embedding:</p><formula xml:id="formula_10">x = [ v w 1 , v w 2 , v nc ].</formula><p>The network architecture is defined similarly to our models (Section 3.1).</p><p>Compositional Baselines. We re-train <ref type="bibr" target="#b2">Dima's (2016</ref><ref type="bibr">) models, various combinations of NC rep-resentations (Zanzotto et al., 2010</ref><ref type="bibr" target="#b18">Socher et al., 2012)</ref> and single word embeddings in a fully connected network. 6 <ref type="table" target="#tab_2">Table 1</ref> shows the performance of various methods on the datasets. <ref type="bibr" target="#b2">Dima's (2016)</ref> compositional models perform best among the baselines, and on the random split, better than all the methods. On the lexical splits, however, the baselines exhibit a dramatic drop in performance, and are outperformed by our methods. The gap is larger in the lexical-full split. Finally, there is usually no gain from the added NC vector in Dist-NC and Integrated-NC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Path Embeddings. To focus on the changes from previous work, we analyze the performance of the path-based model on the Tratz-fine random split. This dataset contains 37 relations and the model performance varies across them. Some relations, such as MEASURE and PER-SONAL TITLE yield reasonable performance (F 1 score of 0.87 and 0.68). <ref type="table" target="#tab_5">Table 3</ref> focuses on these relations and illustrates the indicative paths that the model has learned for each relation. We compute these by performing the analysis in <ref type="bibr" target="#b17">Shwartz et al. (2016)</ref>, where each path is fed into the pathbased model, and is assigned to its best-scoring relation. For each relation, we consider paths with a score ≥ 0.8. Other relations achieve very low F 1 scores, indicating that the model is unable to learn them at all. Interestingly, the four relations with the lowest performance in our model 7 are also those   with the highest error rate in <ref type="bibr" target="#b2">Dima (2016)</ref>, very likely since they express complex relations. For example, the LEXICALIZED relation contains noncompositional NCs (soap opera) or lexical items whose meanings departed from the combination of the constituent meanings. It is expected that there are no paths that indicate lexicalization. In PAR-TIAL ATTRIBUTE TRANSFER (bullet train), w 1 transfers an attribute to w 2 (e.g. bullet transfers speed to train). These relations are not expected to be expressed in text, unless the text aims to explain them (e.g. train as fast as a bullet).</p><p>Looking closer at the model confusions shows that it often defaulted to general relations like OB-JECTIVE (recovery plan) or RELATIONAL-NOUN-COMPLEMENT (eye shape). The latter is described as "indicating the complement of a relational noun (e.g., son of, price of)", and the indicative paths for this relation indeed contain many variants of "[w 2 ] of [w 1 ]", which potentially can occur with NCs in other relations. The model also confused between relations with subtle differences, such as the different topic relations. Given that these relations were conflated to a single relation in the inter-annotator agreement computation in <ref type="bibr" target="#b22">Tratz and Hovy (2010)</ref>, we can conjecture that even humans find it difficult to distinguish between them.</p><p>NC Embeddings. To understand why the NC embeddings did not contribute to the classification, we looked into the embeddings of the Tratz-fine test NCs; 3091/3831 (81%) of them had embeddings. For each NC, we looked for the 10 most similar NC vectors (in terms of cosine similarity), and compared their labels. We have found that only 27.61% of the NCs were mostly similar to NCs with the same label. The problem seems to be inconsistency of annotations rather than low embeddings quality. <ref type="table" target="#tab_6">Table 4</ref> displays some examples of NCs from the test set, along with their most similar NC in the embeddings, where the two NCs have different labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We used an existing neural dependency path representation to represent noun-compound paraphrases, and along with distributional information applied it to the NC classification task. Following previous work, that suggested that distributional methods succeed due to lexical memorization, we show that when lexical memorization is not possible, the performance of all methods is much worse. Adding the path-based component helps mitigate this issue and increase performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of the classification models for the NC coffee cup. The model consists of two parts: (1) the distributional representations of the NC (left, orange) and each word (middle, green). (2) the corpus occurrences of coffee and cup, in the form of dependency path embeddings (right, purple).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>All methods' performance (F 1 ) on the various splits: best freq: best performing frequency baseline (head / modifier), 4 best comp: best model from<ref type="bibr" target="#b2">Dima (2016)</ref>.</figDesc><table><row><cell>Dataset</cell><cell>Split</cell><cell>Train</cell><cell>Validation</cell><cell>Test</cell></row><row><cell></cell><cell>Lex f ull</cell><cell>4,730</cell><cell>1,614</cell><cell>869</cell></row><row><cell>TRATZ-FINE</cell><cell>Lex head Lex mod</cell><cell>9,185 9,783</cell><cell>5,819 5,400</cell><cell>4,154 3,975</cell></row><row><cell></cell><cell>Rand</cell><cell>14,369</cell><cell>958</cell><cell>3,831</cell></row><row><cell></cell><cell>Lex f ull</cell><cell>4,746</cell><cell>1,619</cell><cell>779</cell></row><row><cell>TRATZ-COARSE</cell><cell>Lex head Lex mod</cell><cell>9,214 9,732</cell><cell>5,613 5,402</cell><cell>3,964 3,657</cell></row><row><cell></cell><cell>Rand</cell><cell>14,093</cell><cell>940</cell><cell>3,758</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Number of instances in each dataset split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Indicative paths for selected relations, along with NC examples.</figDesc><table><row><cell></cell><cell>Test NC</cell><cell></cell><cell>Most Similar NC</cell></row><row><cell>NC</cell><cell>Label</cell><cell>NC</cell><cell>Label</cell></row><row><cell>majority party</cell><cell>EQUATIVE</cell><cell>minority party</cell><cell>WHOLE+PART OR MEMBER OF</cell></row><row><cell cols="2">enforcement director OBJECTIVE</cell><cell>enforcement chief</cell><cell>PERFORM&amp;ENGAGE IN</cell></row><row><cell>fire investigator</cell><cell>OBJECTIVE</cell><cell>fire marshal</cell><cell>ORGANIZE&amp;SUPERVISE&amp;AUTHORITY</cell></row><row><cell>stabilization plan</cell><cell>OBJECTIVE</cell><cell cols="2">stabilization program PERFORM&amp;ENGAGE IN</cell></row><row><cell>investor sentiment</cell><cell>EXPERIENCER-OF-EXPERIENCE</cell><cell>market sentiment</cell><cell>TOPIC OF COGNITION&amp;EMOTION</cell></row><row><cell>alliance member</cell><cell>WHOLE+PART OR MEMBER OF</cell><cell>alliance leader</cell><cell>OBJECTIVE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Example of NCs from the Tratz-fine random split test set, along with the most similar NC in the embeddings, where the two NCs have different labels.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is available at https://github.com/tensorflow/ models/tree/master/research/lexnet nc.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Leaving out features derived from lexical resources (e.g.<ref type="bibr" target="#b12">Nastase and Szpakowicz, 2003;</ref><ref type="bibr" target="#b22">Tratz and Hovy, 2010)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In practice, in lexical-full this is a random baseline, in lexical-head it is the modifier frequency baseline, and in lexical-mod it is the head frequency baseline.5 Unseen heads/modifiers are assigned a random relation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We only include the compositional models, and omit the "basic" setting which is similar to our Dist model. For the full details of the compositional models, see<ref type="bibr" target="#b2">Dima (2016</ref>). 7 LEXICALIZED, TOPIC OF COGNITION&amp;EMOTION, WHOLE+ATTRIBUTE&amp;FEAT, PARTIAL ATTR TRANSFER</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://catalog.ldc.upenn.edu/ldc2003t05</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Marius Pasca, Susanne Riehemann, Colin Evans, Octavian Ganea, and Xiang Li for the fruitful conversations, and Corina Dima for her help in running the compositional baselines.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Technical Details</head><p>To extract paths, we use a concatenation of English Wikipedia and the Gigaword corpus. <ref type="bibr">8</ref> We consider sentences with up to 32 words and dependency paths with up to 8 edges, including satellites, and keep only 1,000 paths for each nouncompound. We compute the path embeddings in advance for all the paths connecting NCs in the dataset ( §3.2), and then treat them as fixed embeddings during classification ( §3.1).</p><p>We use TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref> to train the models, fixing the values of the hyperparameters after performing preliminary experiments on the validation set. We set the mini-batch size to 10, use Adam optimizer <ref type="bibr" target="#b8">(Kingma and Ba, 2014)</ref> with the default learning rate, and apply word dropout with probability 0.1. We train up to 30 epochs with early stopping, stopping the training when the F 1 score on the validation set drops 8 points below the best performing score.</p><p>We initialize the distributional embeddings with the 300-dimensional pre-trained GloVe embeddings <ref type="bibr" target="#b14">(Pennington et al., 2014)</ref> and the lemma embeddings (for the path-based component) with the 50-dimensional ones. Unlike HypeNET, we do not update the embeddings during training. The lemma, POS, and direction embeddings are initialized randomly and updated during training. NC embeddings are learned using a concatenation of Wikipedia and Gigaword. Similarly to the original GloVe implementation, we only keep the most frequent 400,000 vocabulary terms, which means that roughly 20% of the noun-compounds do not have vectors and are initialized randomly in the model.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Dima</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-1604</idno>
		<ptr target="https://doi.org/10.18653/v1/W16-1604" />
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP, Association for Computational Linguistics, chapter On the Compositionality and Semantic Interpretation of English Noun Compounds</title>
		<meeting>the 1st Workshop on Representation Learning for NLP, Association for Computational Linguistics, chapter On the Compositionality and Semantic Interpretation of English Noun Compounds</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic noun compound interpretation using deep neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Dima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IWCS</title>
		<imprint>
			<biblScope unit="page">173</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">General estimation and evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-3206" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality. Association for Computational Linguistics</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality. Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 4: Free paraphrases of noun compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di-Armuidó</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/S13-2025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large-scale noun compound interpretation using bootstrapping and the web as a corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Su</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D11-1060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="648" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1098" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using verbs to characterize noun-noun relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence: Methodology, Systems, and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring noun-modifier semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth international workshop on computational semantics (IWCS-5)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="285" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">General and specific paraphrases of semantic relations between nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Nulty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fintan</forename><surname>Costello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="357" to="384" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An empirical study on compositionality in compound nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/I11-1024" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Copestake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">terpreting compound nouns with kernel methods</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="331" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1226" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2389" to="2398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D12-1110" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Compound noun interpretation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen Spärck</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Computer Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A vsm-based statistical model for the semantic relation interpretation of noun-modifier pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><surname>Surtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soma</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="636" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semantically-enriched parsing for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>University of Southern California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A taxonomy, dataset, and classifier for automatic noun compound interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P10-1070" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Melodi: A supervised distributional approach for free paraphrasing of noun compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stergos</forename><surname>Afantenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/S13-2026" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation (Se-mEval 2013). Association for Computational Linguistics</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation (Se-mEval 2013). Association for Computational Linguistics<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="144" to="147" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimating linear models for compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Massimo Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Korkontzelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Fallucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1263" to="1271" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
