{"1007": "we present an analysis into the inner workings of convolutional neural networks ( cnns) for processing text. cnns used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs cnns remain a mystery. we aim to understand the method by which the networks process and classify text. we examine common hypotheses to this problem: that filters, accompanied by global max-pooling serve as ngram detectors. we show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. finally, we show practical use cases derived from our findings in the form of model interpretability ( explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and nlp) and prediction interpretability ( explaining predictions) . this paper proposes a new method for text classification based on convolutional neural networks (cnns ) and softmax ( wp ) . the method is based on max-pooling across the ngram dimension , where each ngram is represented by a max-pooled vector (m ) , where m is the dimension of the convolutional network (cnn) and n is the dimension of the max-pooled vector (p ) , where p is the number of ngrams in the max-pooled vector , and m is the dimension of the max-pooled vector (p ) , where m is the dimension of the max-pooled vector (p ) , where m is the dimension of the max-pooled vector (p ) , where m is the dimension of the max-pooled vector (p ) , where m is the dimension of the max-pooled vector (p ) , where m is the dimension of the max-pooled vector (p ) , where m is the dimension of the max-pooled vector (p ) , where m is the dimension of the max-pooled vector (p )  this paper proposes a new method for training a network classifier on a given set of filters. in the paper, the paper proposes a method for training a network classifier on a given set of filters. this paper presents an adversarial model that can be used to find adversarial examples that cause a trained model to misclassify. the model can be used to find adversarial examples that cause a trained model to misclassify. this paper proposes a clustering algorithm that identifies the activations of ngrams in the cnn model. the clustering algorithm identifies two clusters: one primarily containing ngrams of the pattern det intensity-adverb-word, while the second contains ngrams that begin with phrases like go wrong.7 the centroids for these clusters capture the activation patterns well: low-medium-high and high-highlow for clusters and respectively. to summarize, by discarding noisy ngrams which do not pass the filter s threshold and then clustering those that remain according to their slot activation patterns, we arrived at a clearer image 6intuitively, we arrived at a clearer image 6intuitively, we can think of the sampling noise as the ngram embeddings, and the probability distribution as defined by the filter coverage as defined by the filter activation patterns. each cluster corresponds to a slot activation pattern. cnns have been shown to perform well on text classification tasks. in this paper, we show that cnns are not homogeneous. we show that cnns are not homogeneous. we also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words. finally, we use these findings to suggest improvements to model-based and predictionbased interpretability of cnns for text. we decompose the ngram score into word-level scores by treating the convolution of a filter as a sum of word-level convolutions, allowing us to examine the word-level composition of the activation. specifically, by maximizing the word-level activations by iterating over the vocabulary, we observed that filters do not maximize activations at the word-level, but instead form slot activation patterns that give different types of ngrams similar activation strengths. this provides empirical evidence that filters are not homogeneous. this paper presents an analysis into the inner workings of convolutional neural networks ( cnns) for processing text. this paper addresses the question of whether a given filter is homogeneous and specializes in detecting a specific class of ngrams. we challenge this view and show that filters often specialize in multiple distinctly different semantic classes by utilizing activation patterns which are not necessarily maximized. we also show that filters may not only identify good ngrams, but may also actively supress bad ones.", "1000": "###", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1008": "###", "1009": "###", "1010": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1018": "###", "1019": "###", "1020": "###", "1021": "###"}