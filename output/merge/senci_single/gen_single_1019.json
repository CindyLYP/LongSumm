{"1019": "the vast amounts of textual data end users need to consume motivates the need for automatic summarization an automatic summarizer gets as an input one or more documents and possibly also a limit on summary length ( e.g., maximum number of words). the summarizer then needs to produce a textual summary that captures the most salient ( general and informative) content parts within input documents. the summarizer may also be required to satisfy a specific user information need, expressed by one or more queries. therefore, the summarizer will need to produce a focused summary which includes the most relevant information to that need. overall, dual-ces provides a significantly better summarization quality compared to other alternative unsupervised summarizers; and in many cases, it even outperforms that of state-of-the art supervised summarizers. the cross entropy ( ce) method is a monte-carlo optimization framework for solving hard combinatorial problems previously, it was utilized for solving the sentence subset selection problem to this end, the ce-method gets as an input q ( q, d), a constraint on maximum summary length l and an optional pseudo-reference summary sl, whose usage is explained later on. the result of such an invocation is a single length-feasible summary s which contains a subset of sentences selected from d which maximizes q ( q, d). the distilled saliency-based pseudo-feedback to improve the summarization policy search between such switched ( dual) goals serves as a novel aspect of our work. to the best of our knowledge, this on its own, serves as a novel aspect of our work. this paper introduces a novel twostep dual-cascade optimization approach, which utilizes two ces-like invocations. both invocations consider the same sentences powerset solution space. yet, each such invocation utilizes a bit different set of summary predictors, depending on whether the summarizer is quality s goal lay towards higher summary saliency or focus. in the first step, dual-ces relaxes the summary length constraint, aiming at producing a longer and more salient summary. this summary then treated as a pseudoeffective reference summary from which saliency-based pseudo-feedback is distilled. such pseudo-feedback is then utilized in the second step of the cascade for setting an additional auxiliary saliency-driven goal. the ce-method is an extension to dualces that adaptively adjusts the value of hyperparameter l. to this end, we introduce a new learning parameter lt which defines the maximum length limit for summary production ( sampling) that is allowed at iteration t of the ce-method. given a topic statement, which is expressed by one or more questions, and a set of english documents, the main task is to produce a 250-word ( i.e., lmax 250) topic-focused summary. in this paper, we compare the summary quality of dualces to the results that were previously reported for several competitive summarization baselines. these baselines include both supervised and unsupervised methods and apply various strategies for handling the 4r-1.5.5.pl -a -c -m -n -2 -u -p -l saliency versus focus tradeoff. to distinguish between both types of works, we mark supervised method names with a superscript the first line of baselines utilize various surface and graph level features, namely bi-plsa 22, ctsum 24, hiersum 8, hysum 3, multimr 23, qode 16, ra-mds 11, spopt 26, and vaes-a the third line of baselines incorporate various attention models, namely: docrebuild 16, ra-mds 11, spopt 26, and vaes-a the first line of baselines utilize various surface and graph level features, namely bi-plsa 22, ctsum 24, hiersum 8, hysum 3, multimr 23, qode 16, ra-mds 11, spopt 26, and vaes-a in this paper, we propose an unsupervised , query-focused , extractive multi document summarizer called dualces. dualces is an unsupervised , query-focused , extractive multi document summarizer that is based on the dualcascade optimization approach. the pseudo-feedback distillation approach employed between the two steps of dualces has some resemblance to attention models that are used by state-of-the-art deep learning summarization methods 1, 12, first we note that , dual-ces significantly improves over these attentive baselines on rouge-1. on rouge-2 , dual-ces is significantly better than c-attention and similar to crsumsf, while it provides ( more or less) similar quality to crsumsf. closer analysis of the various attention strategies that are employed within these baselines, reveals that, while attsum only attends on a sentence representation level, c-attention and crsumsf further attend on a word level. such a more fine-granular attendance results in an improved saliency for the two latter. yet, while c-attention first attends on sentences then on words, crsumsf performs its attentions reversely. this paper proposes dual-ces a novel unsupervised, query-focused , multi-document extractive summarizer. to this end, dual-ces employs a twostep dual-cascade optimization approach with saliency-based pseudo-feedback distillation. to this end, dual-ces tries to handle the tradeoff by gradually shifting from generating a long summary that is more salient in the first step to generating a short summary that is more focused in the second step. moreover, dualces utilizes the long summary that was generated in the first step for saliency-based pseudo-feedback distillation, which allows to generate a final focused summary with better saliency. in this paper, we propose a novel approach to focus summary production using the ce-method. the ce-method consists of two steps. the first step consists of the ce-method with cem ( qfoc ( q, d) def lmax ( q, d) t. the second step is simply implemented by invoking the ce-method with cem ( qfoc ( q, d) def wq p ( wq p ( wq p ( wq p ( wq p) ) ) . here, the target measure qfoc ( q, d) guides the optimization towards the production of a focused summary, while still keeping high saliency as much as possible. to achieve that, we use an additional focusdriven predictor which bias summary production towards the value of hyperparameter l. moreover, using the pseudo-reference summary sl ( sl) we introduce an additional auxiliary saliency-based predictor, whose goal is to enhance the saliency of produced focused summary. this predictor utilizes pseudo-feedback that is distilled from unique unigram words in sl.", "1000": "###", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1007": "###", "1008": "###", "1009": "###", "1010": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1018": "###", "1020": "###", "1021": "###"}