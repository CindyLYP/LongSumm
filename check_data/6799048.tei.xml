<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-04-21">21 Apr 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><forename type="middle">Huu</forename><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal AdeptMind Scholar Element AI</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université de Montréal Michael Noukhovitch Mila</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Oregon</orgName>
								<address>
									<addrLine>Harm de Vries Mila</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Université de Montréal CIFAR Fellow</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-04-21">21 Apr 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1811.12889v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that endto-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, neural network based models have become the workhorse of natural language understanding and generation. They empower industrial machine translation <ref type="bibr" target="#b34">(Wu et al., 2016</ref>) and text generation <ref type="bibr" target="#b20">(Kannan et al., 2016)</ref> systems and show state-of-the-art performance on numerous benchmarks including Recognizing Textual Entailment <ref type="bibr" target="#b8">(Gong et al., 2017)</ref>, Visual Question Answering <ref type="bibr" target="#b17">(Jiang et al., 2018)</ref>, and Reading Comprehension <ref type="bibr" target="#b33">(Wang et al., 2018)</ref>. Despite these successes, a growing body of literature suggests that these approaches do not generalize outside of the specific distributions on which they are trained, something that is necessary for a language understanding system to be widely deployed in the real world. Investigations on the three aforementioned tasks have shown that neural models easily latch onto statistical regularities which are omnipresent in existing datasets <ref type="bibr" target="#b0">(Agrawal et al., 2016;</ref><ref type="bibr" target="#b10">Gururangan et al., 2018;</ref><ref type="bibr" target="#b16">Jia &amp; Liang, 2017)</ref> and extremely hard to avoid in large scale data collection. Having learned such dataset-specific solutions, neural networks fail to make correct predictions for examples that are even slightly out of domain, yet are trivial for humans. These findings have been corroborated by a recent investigation on a synthetic instruction-following task (Lake &amp; , in which seq2seq models <ref type="bibr" target="#b32">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref> have shown little systematicity <ref type="bibr" target="#b6">(Fodor &amp; Pylyshyn, 1988)</ref> in how they generalize, that is they do not learn general rules on how to compose words and fail spectacularly when for example asked to interpret "jump twice" after training on "jump", "run twice" and "walk twice".</p><p>An appealing direction to improve the generalization capabilities of neural models is to add modularity and structure to their design to make them structurally resemble the kind of rules they are supposed to learn <ref type="bibr" target="#b1">(Andreas et al., 2016;</ref><ref type="bibr" target="#b7">Gaunt et al., 2016)</ref>. For example, in the Neural Module Network paradigm <ref type="bibr">(NMN, Andreas et al. (2016)</ref>), a neural network is assembled from several neural modules, where each module is meant to perform a particular subtask of the input processing, much like a computer program composed of functions. The NMN approach is intuitively appealing but its widespread adoption has been hindered by the large amount of domain knowledge that is required to decide <ref type="bibr" target="#b1">(Andreas et al., 2016)</ref> or predict <ref type="bibr" target="#b19">(Johnson et al., 2017;</ref><ref type="bibr" target="#b12">Hu et al., 2017)</ref> how the modules should be created (parametrization) and how they should be connected (layout) based on a natural language utterance. Besides, their performance has often been matched by more traditional neural models, such as FiLM <ref type="bibr" target="#b28">(Perez et al., 2017)</ref>, Relations Networks <ref type="bibr" target="#b29">(Santoro et al., 2017)</ref>, and MAC networks <ref type="bibr" target="#b14">(Hudson &amp; Manning, 2018)</ref>. Lastly, generalization properties of NMNs, to the best of our knowledge, have not been rigorously studied prior to this work.</p><p>Here, we investigate the impact of explicit modularity and structure on systematic generalization of NMNs and contrast their generalization abilities to those of generic models. For this case study, we focus on the task of visual question answering (VQA), in particular its simplest binary form, when the answer is either "yes" or "no". Such a binary VQA task can be seen as a fundamental task of language understanding, as it requires one to evaluate the truth value of the utterance with respect to the state of the world. Among many systematic generalization requirements that are desirable for a VQA model, we choose the following basic one: a good model should be able to reason about all possible object combinations despite being trained on a very small subset of them. We believe that this is a key prerequisite to using VQA models in the real world, because they should be robust at handling unlikely combinations of objects. We implement our generalization demands in the form of a new synthetic dataset, called Spatial Queries On Object Pairs (SQOOP), in which a model has to perform spatial relational reasoning about pairs of randomly scattered letters and digits in the image (e.g. answering the question "Is there a letter A left of a letter B?"). The main challenge in SQOOP is that models are evaluated on all possible object pairs, but trained on only a subset of them.</p><p>Our first finding is that NMNs do generalize better than other neural models when layout and parametrization are chosen appropriately. We then investigate which factors contribute to improved generalization performance and find that using a layout that matches the task (i.e. a tree layout, as opposed to a chain layout), is crucial for solving the hardest version of our dataset. Lastly, and perhaps most importantly, we experiment with existing methods for making NMNs more end-to-end by inducing the module layout <ref type="bibr" target="#b19">(Johnson et al., 2017)</ref> or learning module parametrization through soft-attention over the question <ref type="bibr" target="#b12">(Hu et al., 2017)</ref>. Our experiments show that such end-to-end approaches often fail by not converging to tree layouts or by learning a blurred parameterization for modules, which results in poor generalization on the hardest version of our dataset. We believe that our findings challenge the intuition of researchers in the field and provide a foundation for improving systematic generalization of neural approaches to language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE SQOOP DATASET FOR TESTING SYSTEMATIC GENERALIZATION</head><p>We perform all experiments of this study on the SQOOP dataset. SQOOP is a minimalistic VQA task that is designed to test the model's ability to interpret unseen combinations of known relation and object words. Clearly, given known objects X, Y and a known relation R, a human can easily verify whether or not the objects X and Y are in relation R. Some instances of such queries are common in daily life (is there a cup on the table), some are extremely rare (is there a violin under the car), and some are unlikely but have similar, more likely counter-parts (is there grass on the frisbee vs is there a frisbee on the grass). Still, a person can easily answer these questions by understanding them as just the composition of the three separate concepts. Such compositional reasoning skills are clearly required for language understanding models, and SQOOP is explicitly designed to test for them.</p><p>Concretely speaking, SQOOP requires observing a 64 × 64 RGB image x and answering a yes-no question q = X R Y about whether objects X and Y are in a spatial relation R. The questions are represented in a redundancy-free X R Y form; we did not aim to make the questions look like natural language. Each image contains 5 randomly chosen and randomly positioned objects. There are 36 objects: the latin letters A-Z and digits 0-9, and there are 4 relations: LEFT OF, RIGHT OF, ABOVE, and BELOW. This results in 36 • 35 • 4 = 5040 possible unique questions (we do not allow questions about identical objects). To make negative examples challenging, we ensure that both X and Y of a question are always present in the associated image and that there are distractor objects Y = Y  and X = X such that X R Y and X R Y are both true for the image. These extra precautions guarantee that answering a question requires the model to locate all possible X and Y then check if any pair of them are in the relation R. Two SQOOP examples are shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Our goal is to discover which models can correctly answer questions about all 36 • 35 possible object pairs in SQOOP after having been trained on only a subset. For this purpose we build training sets containing 36 • 4 • k unique questions by sampling k different right-hand-side (RHS) objects Y 1 , Y 2 , ..., Y k for each left-hand-side (LHS) object X. We use this procedure instead of just uniformly sampling object pairs in order to ensure that each object appears in at least one training question, thereby keeping the all versions of the dataset solvable. We will refer to k as the #rhs/lhs parameter of the dataset. Our test set is composed from the remaining 36 • 4 • (35 − k) questions. We generate training and test sets for rhs/lhs values of 1,2,4,8 and 18, as well as a control version of the dataset, #rhs/lhs=35, in which both the training and the test set contain all the questions (with different images). Note that lower #rhs/lhs versions are harder for generalization due to the presence of spurious dependencies between the words X and Y to which the models may adapt. In order to exclude a possible compounding factor of overfitting on the training images, all our training sets contain 1 million examples, so for a dataset with #rhs/lhs = k we generate approximately 10 6 /(36 • 4•k) different images per unique question. Appendix D contains pseudocode for SQOOP generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODELS</head><p>A great variety of VQA models have been recently proposed in the literature, among which we can distinguish two trends. Some of the recently proposed models, such as FiLM <ref type="bibr" target="#b28">(Perez et al., 2017)</ref> and Relation Networks (RelNet, Santoro et al. (2017)) are highly generic and do not require any taskspecific knowledge to be applied on a new dataset. On the opposite end of the spectrum are modular and structured models, typically flavours of Neural Module Networks <ref type="bibr" target="#b1">(Andreas et al., 2016)</ref>, that do require some knowledge about the task at hand to be instantiated. Here, we evaluate systematic generalization of several state-of-the-art models in both families. In all models, the image x is first fed through a CNN based network, that we refer to as the stem, to produce a feature-level 3D tensor h x . This is passed through a model-specific computation conditioned on the question q, to produce a joint representation h q x . Lastly, this representation is fed into a fully-connected classifier network to produce logits for prediction. Therefore, the main difference between the models we consider is how the computation h q x = model(h x , q) is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GENERIC MODELS</head><p>We consider four generic models in this paper: CNN+LSTM, FiLM, Relation Network (RelNet), and Memory-Attention-Control (MAC) network. For CNN+LSTM, FiLM, and RelNet models, the question q is first encoded into a fixed-size representation h q using a unidirectional LSTM network. CNN+LSTM flattens the 3D tensor h x to a vector and concatenates it with h q to produce h q x :</p><formula xml:id="formula_0">h q x = [f latten(h x ); h q ].<label>(1)</label></formula><p>RelNet <ref type="bibr" target="#b29">(Santoro et al., 2017)</ref> uses a network g which is applied to all pairs of feature columns of h x concatenated with the question representation h q , all of which is then pooled to obtain h q x :</p><formula xml:id="formula_1">h q x = i,j g(h x (i), h x (j), h q ) (2)</formula><p>where h x (i) is the i-th feature column of h x . FiLM networks <ref type="bibr" target="#b28">(Perez et al., 2017)</ref> use N convolutional FiLM blocks applied to h x . A FiLM block is a residual block <ref type="bibr" target="#b11">(He et al., 2016)</ref> in which a feature-wise affine transformation (FiLM layer) is inserted after the 2 nd convolutional layer. The FiLM layer is conditioned on the question at hand via prediction of the scaling and shifting parameters γ n and β n :</p><formula xml:id="formula_2">[γ n ; β n ] = W n q h q + b n q (3) h n q x = BN (W n 2 * ReLU (W n 1 * h n−1 q x + b n )) (4) h n q x = h n−1 q x + ReLU (γ n h n q x ⊕ β n )<label>(5)</label></formula><p>where BN stands for batch normalization <ref type="bibr" target="#b15">(Ioffe &amp; Szegedy, 2015)</ref>, * stands for convolution and stands for element-wise multiplications. h n q x is the output of the n-th FiLM block and h 0 q x = h x . The output of the last FiLM block h N q x undergoes an extra 1 × 1 convolution and max-pooling to produce h q x . MAC network of Hudson &amp; Manning (2018) produces h q x by repeatedly applying a Memory-Attention-Composition (MAC) cell that is conditioned on the question through an attention mechanism. The MAC model is too complex to be fully described here and we refer the reader to the original paper for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NEURAL MODULE NETWORKS</head><p>Neural Module Networks (NMN) <ref type="bibr" target="#b1">(Andreas et al., 2016)</ref> are an elegant approach to question answering that constructs a question-specific network by composing together trainable neural modules, drawing inspiration from symbolic approaches to question answering <ref type="bibr" target="#b25">(Malinowski &amp; Fritz, 2014</ref>).</p><p>To answer a question with an NMN, one first constructs the computation graph by making the following decisions: (a) how many modules and of which types will be used, (b) how will the modules be connected to each other, and (c) how are these modules parametrized based on the question. We refer to the aspects (a) and (b) of the computation graph as the layout and the aspect (c) as the parametrization. In the original NMN and in many follow-up works, different module types are used to perform very different computations, e.g. the Find module from Hu et al. (2017) performs trainable convolutions on the input attention map, whereas the And module from the same paper computes an element-wise maximum for two input attention maps. In this work, we follow the trend of using more homogeneous modules started by <ref type="bibr" target="#b19">Johnson et al. (2017)</ref>, who use only two types of modules: unary and binary, both performing similar computations. We restrict our study to NMNs with homogeneous modules because they require less prior knowledge to be instantiated and because they performed well in our preliminary experiments despite their relative simplicity. We go one step further than <ref type="bibr" target="#b19">Johnson et al. (2017)</ref> and retain a single binary module type, using a zero tensor for the second input when only one input is available. Additionally, we choose to use exactly three modules, which simplifies the layout decision to just determining how the modules are connected. Our preliminary experiments have shown that, even after these simplifications, NMNs are far ahead of other models in terms of generalization.</p><p>In the original NMN, the layout and parametrization were set in an ad-hoc manner for each question by analyzing a dependency parse. In the follow-up works <ref type="bibr" target="#b19">(Johnson et al., 2017;</ref><ref type="bibr" target="#b12">Hu et al., 2017)</ref>, these aspects of the computation are predicted by learnable mechanisms with the goal of reducing the amount of background knowledge required to apply the NMN approach to a new task. We experiment with the End-to-End NMN (N2NMN) <ref type="bibr" target="#b12">(Hu et al., 2017)</ref> paradigm from this family, which predicts the layout with a seq2seq model <ref type="bibr" target="#b32">(Sutskever et al., 2014)</ref> and computes the parametrization of the modules using a soft attention mechanism. Since all the questions in SQOOP have the same structure, we do not employ a seq2seq model but instead have a trainable layout variable and trainable attention variables for each module.</p><p>Formally, our NMN is constructed by repeatedly applying a generic neural module f (θ, γ, s 0 , s 1 ), which takes as inputs the shared parameters θ, the question-specific parametrization γ and the lefthand side and right-hand side inputs s 0 and s 1 . Three such modules are connected and conditioned on a question q = (q 1 , q 2 , q 3 ) as follows:</p><formula xml:id="formula_3">γ k = 3 i=1 α k,i e(q i ) (6) s m k = k−1 j=−1 τ k,j m s j (7) s k = f (θ, γ k , s 0 k , s 1 k ) (8) h qx = s 3<label>(9)</label></formula><p>In the equations above, s −1 = 0 is the zero tensor input, s 0 = h x are the image features outputted by the stem, e is the embedding table for question words. k ∈ {1, 2, 3} is the module number, s k is the output of the k-th module and s m k are its left (m = 0) and right (m = 1) inputs. We refer to A = (α k,i ) and T = (τ k,j m ) as the parametrization attention matrix and the layout tensor respectively.</p><p>We experiment with two choices for the NMN's generic neural module: the Find module from <ref type="bibr" target="#b12">Hu et al. (2017)</ref> and the Residual module from <ref type="bibr" target="#b19">Johnson et al. (2017)</ref>. The equations for the Residual module are as follows:</p><formula xml:id="formula_4">[W k 1 ; b k 1 ; W k 2 ; b k 2 ; W k 3 ; b k 3 ] = γ k (10) s k = ReLU (W k 3 * [s 0 k ; s 1 k ] + b k 3 ),<label>(11)</label></formula><formula xml:id="formula_5">f Residual (γ k , s 0 k , s 1 k ) = ReLU (s k + W k 1 * ReLU (W k 2 * s k + b k 2 )) + b k 1 ),<label>(12)</label></formula><p>and for Find module as follows:</p><formula xml:id="formula_6">[W 1 ; b 1 ; W 2 ; b 2 ] = θ,<label>(13)</label></formula><formula xml:id="formula_7">f F ind (θ, γ k , s 0 k , s 1 k ) = ReLU (W 1 * γ k ReLU (W 2 * s 0 k ; s 1 k + b 2 ) + b 1 ).<label>(14)</label></formula><p>In the formulas above all W 's stand for convolution weights, and all b's are biases. Equations 10 and 13 should be understood as taking vectors γ k and θ respectively and chunking them into weights and biases. The main difference between Residual and Find is that in Residual all parameters depend on the questions words (hence θ is omitted from the signature of f Residual ), where as in Find convolutional weights are the same for all questions, and only the element-wise multipliers γ k vary based on the question. We note that the specific Find module we use in this work is slightly different from the one used in <ref type="bibr" target="#b12">(Hu et al., 2017)</ref> in that it outputs a feature tensor, not just an attention map. This change was required in order to connect multiple Find modules in the same way as we connect multiple residual ones.</p><p>Based on the generic NMN model described above, we experiment with several specific architectures that differ in the way the modules are connected and parametrized (see <ref type="figure" target="#fig_0">Figure 1)</ref>. In NMN-Chain the modules form a sequential chain. Modules 1, 2 and 3 are parametrized based on the first object word, second object word and the relation word respectively, which is achieved by setting the attention maps α 1 , α 2 , α 3 to the corresponding one-hot vectors. We also experiment with giving the image features h x as the right-hand side input to all 3 modules and call the resulting model NMN-Chain-Shortcut. NMN-Tree is similar to NMN-Chain in that the attention vectors are similarly hardcoded, but we change the connectivity between the modules to be tree-like. Stochastic N2NMN follows the N2NMN approach by <ref type="bibr" target="#b12">Hu et al. (2017)</ref> for inducing layout. We treat the layout T as a stochastic latent variable. T is allowed to take two values: T tree as in NMN-Tree, and T chain as in NMN-Chain. We calculate the output probabilities by marginalizing out the layout i.e. probability of answer being "yes" is computed as p(yes|x, q) = T ∈{Ttree,T chain } p(yes|T, x, q)p(T ). Lastly, Attention N2NMN uses the N2NMN method for learning parametrization <ref type="bibr" target="#b12">(Hu et al., 2017)</ref>. It is structured just like NMN-Tree but has α k computed as softmax(α k ), whereα k is a trainable vector. We use Attention N2NMN only with the Find module because using it with the Residual module would involve a highly non-standard interpolation between convolutional weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In our experiments we aimed to: (a) understand which models are capable of exhibiting systematic generalization as required by SQOOP, and (b) understand whether it is possible to induce, in an end-to-end way, the successful architectural decisions that lead to systematic generalization.</p><p>All models share the same stem architecture which consists of 6 layers of convolution (8 for Relation Networks), batch normalization and max pooling. The input to the stem is a × 64 × 3 image, and the feature dimension used throughout the stem is 64. Further details can be found in Appendix A. The code for all experiments is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">WHICH MODELS GENERALIZE BETTER?</head><p>We report the performance for all models on datasets of varying difficulty in <ref type="figure">Figure 3</ref>. Our first observation is that the modular and tree-structured NMN-Tree model exhibits strong systematic generalization. Both versions of this model, with Residual and Find modules, robustly solve all versions of our dataset, including the most challenging #rhs/lhs=1 split.</p><p>The results of NMN-Tree should be contrasted with those of generic models. 2 out of 4 models (Conv+LSTM and RelNet) are not able to learn to answer all SQOOP questions, no matter how easy the split was (for high #rhs/lhs Conv+LSTM overfitted and RelNet did not train). The results of other two models, MAC and FiLM, are similar. Both models are clearly able to solve the SQOOP task, as suggested by their almost perfect &lt; 1% error rate on the control #rhs/lhs=35 split, yet they struggle to generalize on splits with lower #rhs/lhs. In particular, we observe 13.67 ± 9.97% errors for MAC and a 34.73 ± 4.61% errors for FiLM on the hardest #rhs/lhs=1 split. For the splits of intermediate difficulty we saw the error rates of both models decreasing as we increased the #rhs/lhs ratio from 2 to 18. Interestingly, even with 18 #rhs/lhs some MAC and FiLM runs result in a test error rate of ∼ 2%. Given the simplicity and minimalism of SQOOP questions, we believe that these results should be considered a failure to pass the SQOOP test for both MAC and FiLM. That said, we note a difference in how exactly FiLM and MAC fail on #rhs/lhs=1: in several runs (3 out of 15) MAC exhibits a strong generalization performance (∼ 0.5% error rate), whereas in all runs of FiLM the error rate is about 30%. We examine the successful MAC models and find that they converge to a successful setting of the control attention weights, where specific MAC units consistently attend to the right questions words. In particular, MAC models that generalize strongly for each question seem to have a unit focusing strongly on X and a unit focusing strongly on Y (see Appendix B for more details). As MAC was the strongest competitor of NMN-Tree across generic models, we perform an ablation study for this model, in which we vary the number of modules and hidden units, as well as experiment with weight decay. These modifications do not result in any significant reduction of the gap between MAC and NMN-Tree. Interestingly, we find that using the default high number of MAC units, namely 12, is helpful, possibly because it increases the likelihood that at least one unit converges to focus on X and Y words (see Appendix B for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">WHAT IS ESSENTIAL TO STRONG GENERALIZATION OF NMN?</head><p>The superior generalization of NMN-Tree raises the following question: what is the key architectural difference between NMN-Tree and generic models that explains the performance gap between them?</p><p>We consider two candidate explanations. First, the NMN-Tree model differs from the generic models in that it does not use a language encoder and is instead built from modules that are parametrized by question words directly. Second, NMN-Tree is structured in a particular way, with the idea that modules 1 and 2 may learn to locate objects and module 3 can learn to reason about object locations independently of their identities. To understand which of the two differences is responsible for the superior generalization, we compare the performance of the NMN-Tree, NMN-Chain and NMN-Chain-Shortcut models (see <ref type="figure" target="#fig_0">Figure 1)</ref>. These 3 versions of NMN are similar in that none of them are using a language encoder, but they differ in how the modules are connected. The results in <ref type="figure">Figure 3</ref> show that for both Find and Residual module architectures, using a tree layout is absolutely crucial (and sufficient) for generalization, meaning that the generalization gap between NMN-Tree and generic models can not be explained merely by the language encoding step in the latter. In particular, NMN-Chain models perform barely above random chance, doing even worse than generic models on https://github.com/rizar/systematic-generalization-sqoop Error rate (%) 3 2 .7 2 ± 1 .6 3 .0 5 ± 0 .4 0 .6 7 ± 9 .9 7 .7 3 ± 4 .6 1 2 .8 7 ± 0 .7 7 1 5 .7 1 ± 1 .1 2 1 9 .8 0 ± 4 .3 2 1 8 .9 6 ± 0 .9 4 2 7 .4 7 ± 2 0 .9 0 .0 1 ± 0 .9 1 .2 2 ± 0 .2 2 .2 9 ± 1 .0 9 Error rate (%) 4 6 .9 1 ± 0 .2 6 6 .6 1 ± 1 .1 2 5 .5 3 ± 3 .1 0 .0 9 ± 1 .1 8 .1 8 ± 1 .2 8</p><p>.9 3 ± 1 .5 8 .8 2 ± 3 .  <ref type="figure">Figure 3</ref>: Top: Comparing the performance of generic models on datasets of varying difficulty (lower #rhs/lhs is more difficult). Note that NMN-Tree generalizes perfectly on the hardest #rhs/lhs=1 version of SQOOP, whereas MAC and FiLM fail to solve completely even the easiest #rhs/lhs=18 version. Bottom: Comparing NMNs with different layouts and modules. We can clearly observe the superior generalization of NMN-Tree, poor generalization of NMN-Chain and mediocre generalization of NMN-Chain-Shortcut. Means and standard deviations after at least 5 runs are reported. the #rhs/lhs=1 version of the dataset and dramatically failing even on the easiest #rhs/lhs=18 split. This is in stark contrast with NMN-Tree models that exhibits nearly perfect performance on the hardest #rhs/lhs=1 split. As a sanity check we train NMN-Chain models on the vanilla #rhs/lhs=35 split. We find that NMN-Chain has little difficulty learning to answer SQOOP questions when it sees all of them at training time, even though it previously shows poor generalization when testing on unseen examples. Interestingly, NMN-Chain-Shortcut performs much better than NMN-Chain and quite similarly to generic models. We find it remarkable that such a slight change in the model layout as adding shortcut connections from image features h x to the modules results in a drastic change in generalization performance. In an attempt to understand why NMN-Chain generalizes so poorly we compare the test set responses of the 5 NMN-Chain models trained on #rhs/lhs=1 split. Notably, there was very little agreement between predictions of these 5 runs (Fleiss κ = 0.05), suggesting that NMN-Chain performs rather randomly outside of the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CAN THE RIGHT KIND OF NMN BE INDUCED?</head><p>The strong generalization of the NMN-Tree is impressive, but a significant amount of prior knowledge about the task was required to come up with the successful layout and parametrization used in this model. We therefore investigate whether the amount of such prior knowledge can be reduced by fixing one of these structural aspects and inducing the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">LAYOUT INDUCTION</head><p>In our layout induction experiments, we use the Stochastic N2NMN model which treats the layout as a stochastic latent variable with two values (T tree and T chain , see Section 3.2 for details). We experiment with N2NMNs using both Find and Residual modules and report results with different 0 100000 200000 Iterations 0.4 0.5 0.6 0.7 0.8 0.9 1.0 p(tree) 1 rhs/lhs 18 rhs/lhs <ref type="figure">Figure 4</ref>: Learning dynamics of layout induction on 1 rhs/lhs and 18 rhs/lhs datasets using the Residual module with p 0 (tree) = 0.5. All 5 runs do not learn to use the tree layout for 1 rhs/lhs, the very setting where the tree layout is necessary for generalization.</p><p>0.4 0.5 0.6 0.7 0.8 Attention quality 0 10 20 30 40</p><p>Error rate (%) 1 rhs/lhs 2 rhs/lhs 18 rhs/lhs <ref type="figure">Figure 5</ref>: Attention quality κ vs accuracy for Attention N2NMN models trained on different #rhs/lhs splits. We can observe that generalization is strongly associated with high κ for #rhs/lhs=1, while for splits with 2 and 18 rhs/lhs blurry attention may be sufficient.  <ref type="figure">Figure 6</ref>: An example of how attention weights of modules 1 (left), 2 (middle), and 3 (right) evolve during training of an Attention N2NMN model on the 18 rhs/lhs version of SQOOP. Modules 1 and 2 learn to focus on different objects words, X and Y respectively in this example, but they also assign high weight to the relation word R. Module 3 learns to focus exclusively on R.</p><p>initial conditions, p 0 (tree) ∈ 0.1, 0.5, 0.9. We believe that the initial probability p 0 (tree) = 0.1 should not be considered small, since in more challenging datasets the space of layouts would be exponentially large, and sampling the right layout in 10% of all cases should be considered a very lucky initialization. We repeat all experiments on #rhs/lhs=1 and on #rhs/lhs=18 splits, the former to study generalization, and the latter to control whether the failures on #rhs/lhs=1 are caused specifically by the difficulty of this split. The results (see <ref type="table">Table 1)</ref> show that the success of layout induction (i.e. converging to a p(tree) close to 0.9) depends in a complex way on all the factors that we considered in our experiments. The initialization has the most influence: models initialized with p 0 (tree) = 0.1 typically do not converge to a tree (exception being experiments with Residual module on #rhs/lhs=18, in which 3 out of 5 runs converged to a solution with a high p(tree)). Likewise, models initialized with p 0 (tree) = 0.9 always stay in a regime with a high p(tree). In the intermediate setting of p 0 (tree) = 0.5 we observe differences in behaviors for Residual and Find modules. In particular, N2NMN based on Residual modules stays spurious with p(tree) = 0.5 ± 0.08 when #rhs/lhs=1, whereas N2NMN based on Find modules always converges to a tree. <ref type="table">Table 1</ref> is that for the Stochastic N2NMNs with Residual modules, trained with p 0 (tree) = 0.5 and #rhs/lhs=1, make just 1.64±1.79% test error despite never resolving the layout uncertainty through training (p 200K (tree) = 0.56 ± 0.06). We offer an investigation of this result in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One counterintuitive result in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">PARAMETRIZATION INDUCTION</head><p>Next, we experiment with the Attention N2NMN model (see Section 3.2) in which the parametrization is learned for each module as an attention-weighted average of word embeddings. In these experiments, we fix the layout to be tree-like and sample the pre-softmax attention weightsα from a uniform distribution U [0; 1]. As in the layout induction investigations, we experiment with several SQOOP splits, namely we try #rhs/lhs ∈ {1, 2, 18}. The results (reported in <ref type="table" target="#tab_1">Table 2)</ref> show that Attention N2NMN fails dramatically on #rhs/lhs=1 but quickly catches up as soon as #rhs/lhs is increased to 2. Notably, 9 out of 10 runs on #rhs/lhs=2 result in almost perfect performance, and 1 run completely fails to generalize (26% error rate), resulting in a high 8.18% variance of the mean error rate. All 10 runs on the split with 18 rhs/lhs generalize flawlessly. Furthermore, we inspect the learned attention weights and find that for typical successful runs, module 3 focuses on the relation word, whereas modules 1 and 2 focus on different object words (see <ref type="figure">Figure 6</ref>) while still focusing on the relation word. To better understand the relationship between successful layout induction and generalization, we define an attention quality metric κ = min w∈{X,Y } max k∈1,2 α k,w /(1 − α k,R ). Intuitively, κ is large when for each word w ∈ X, Y there is a module i that focuses mostly on this word. The renormalization by 1/(1 − α k,R ) is necessary to factor out the amount of attention that modules 1 and 2 assign to the relation word. For the ground-truth parametrization that we use for NMN-Tree κ takes a value of 1, and if both modules 1 and 2 focus on X, completely ignoring Y, κ equals 0. The scatterplot of the test error rate versus κ ( <ref type="figure">Figure 5)</ref> shows that for #rhs/lhs=1 high generalization is strongly associated with higher κ, meaning that it is indeed necessary to have different modules strongly focusing on different object words in order to generalize in this most challenging setting. Interestingly, for #rhs/lhs=2 we see a lot of cases where N2NMN generalizes well despite attention being rather spurious (κ ≈ 0.6).</p><p>In order to put Attention N2NMN results in context we compare them to those of MAC (see <ref type="table" target="#tab_1">Table 2</ref>). Such a comparison can be of interest because both models perform attention over the question. For 1 rhs/lhs MAC seems to be better on average, but as we increase #rhs/lhs to 2 we note that Attention N2NMN succeeds in 9 out of 10 cases on the #rhs/lhs=2 split, much more often than 1 success out of 10 observed for MAC 2 . This result suggests that Attention N2NMNs retains some of the strong generalization potential of NMNs with hard-coded parametrization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>The notion of systematicity was originally introduced by <ref type="bibr" target="#b6">(Fodor &amp; Pylyshyn, 1988)</ref> as the property of human cognition whereby "the ability to entertain a given thought implies the ability to entertain thoughts with semantically related contents". They illustrate this with an example that no English</p><p>If we judge a run successful when the error rate is lower than τ = 1%, these success rates are different with a p-value of 0.001 according to the Fisher exact test. Same holds for any other threshold τ ∈ [1%; 5%]. speaker can understand the phrase "John loves the girl" without being also able to understand the phrase "the girl loves John". The question of whether or not connectionist models of cognition can account for the systematicity phenomenon has been a subject of a long debate in cognitive science <ref type="bibr" target="#b6">(Fodor &amp; Pylyshyn, 1988;</ref><ref type="bibr" target="#b30">Smolensky, 1987;</ref><ref type="bibr" target="#b26">Marcus, 1998;</ref><ref type="bibr" target="#b27">2003;</ref><ref type="bibr" target="#b5">Calvo &amp; Colunga, 2003)</ref>. Recent research has shown that lack of systematicity in the generalization is still a concern for the modern seq2seq models <ref type="bibr" target="#b3">Bastings et al., 2018;</ref><ref type="bibr" target="#b24">Loula et al., 2018)</ref>. Our findings about the weak systematic generalization of generic VQA models corroborate the aforementioned seq2seq results. We also go beyond merely stating negative generalization results and showcase the high systematicity potential of adding explicit modularity and structure to modern deep learning models.</p><p>Besides the theoretical appeal of systematicity, our study is inspired by highly related prior evidence that when trained on downstream language understanding tasks, neural networks often generalize poorly and latch on to dataset-specific regularities. <ref type="bibr" target="#b0">Agrawal et al. (2016)</ref> report how neural models exploit biases in a VQA dataset, e.g. responding "snow" to the question "what covers the ground" regardless of the image because "snow" is the most common answer to this question. <ref type="bibr" target="#b10">Gururangan et al. (2018)</ref> report that many successes in natural language entailment are actually due to exploiting statistical biases as opposed to solving entailment, and that state-of-the-art systems are much less performant when tested on unbiased data. <ref type="bibr" target="#b16">Jia &amp; Liang (2017)</ref> demonstrate that seemingly state-ofthe-art reading comprehension system can be misled by simply appending an unrelated sentence that resembles the question to the document.</p><p>Using synthetic VQA datasets to study grounded language understanding is a recent trend started by the CLEVR dataset <ref type="bibr" target="#b18">(Johnson et al., 2016)</ref>. CLEVR images are 3D-rendered and CLEVR questions are longer and more complex than ours, but in the associated generalization split CLEVR-CoGenT the training and test distributions of images are different. In our design of SQOOP we aimed instead to minimize the difference between training and test images to make sure that we test a model's ability to interpret unknown combinations of known words. The ShapeWorld family of datasets by <ref type="bibr" target="#b22">Kuhnle &amp; Copestake (2017)</ref> is another synthetic VQA platform with a number of generalization tests, but none of them tests SQOOP-style generalization of relational reasoning to unseen object pairs. Most closely related to our work is the recent study of generalization to long-tail questions about rare objects done by <ref type="bibr" target="#b4">Bingham et al. (2017)</ref>. They do not, however, consider as many models as we do and do not study the question of whether the best-performing models can be made end-to-end.</p><p>The key paradigm that we test in our experiments is Neural Module Networks (NMN). <ref type="bibr" target="#b1">Andreas et al. (2016)</ref> introduced NMNs as a modular, structured VQA model where a fixed number of handcrafted neural modules (such as Find, or Compare) are chosen and composed together in a layout determined by the dependency parse of the question. <ref type="bibr" target="#b1">Andreas et al. (2016)</ref> show that the modular structure allows answering questions that are longer than the training ones, a kind of generalization that is complementary to the one we study here. <ref type="bibr" target="#b12">Hu et al. (2017)</ref> and <ref type="bibr" target="#b19">Johnson et al. (2017)</ref> followed up by making NMNs end-to-end, removing the non-differentiable parser. Both <ref type="bibr" target="#b12">Hu et al. (2017)</ref> and <ref type="bibr" target="#b19">Johnson et al. (2017)</ref> reported that several thousands of ground-truth layouts are required to pretrain the layout predictor in order for their approaches to work. In a recent work, <ref type="bibr" target="#b13">Hu et al. (2018)</ref> attempt to soften the layout decisions, but training their models end-to-end from scratch performed substantially lower than best models on the CLEVR task. <ref type="bibr" target="#b9">Gupta &amp; Lewis (2018)</ref> report successful layout induction on CLEVR for a carefully engineered heterogeneous NMN that takes a scene graph as opposed to a raw image as the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND DISCUSSION</head><p>We have conducted a rigorous investigation of an important form of systematic generalization required for grounded language understanding: the ability to reason about all possible pairs of objects despite being trained on a small subset of such pairs. Our results allow one to draw two important conclusions. For one, the intuitive appeal of modularity and structure in designing neural architectures for language understanding is now supported by our results, which show how a modular model consisting of general purpose residual blocks generalizes much better than a number of baselines, including architectures such as MAC, FiLM and RelNet that were designed specifically for visual reasoning. While this may seem unsurprising, to the best of our knowledge, the literature has lacked such a clear empirical evidence in favor of modular and structured networks before this work. Importantly, we have also shown how sensitive the high performance of the modular models is to the layout of modules, and how a tree-like structure generalizes much stronger than a typical chain of layers.</p><p>Our second key conclusion is that coming up with an end-to-end and/or soft version of modular models may be not sufficient for strong generalization. In the very setting where strong generalization is required, end-to-end methods often converge to a different, less compositional solution (e.g. a chain layout or blurred attention). This can be observed especially clearly in our NMN layout and parametrization induction experiments on the #rhs/lhs=1 version of SQOOP, but notably, strong initialization sensitivity of layout induction remains an issue even on the #rhs/lhs=18 split. This conclusion is relevant in the view of recent work in the direction of making NMNs more end-toend <ref type="bibr" target="#b31">(Suarez et al., 2018;</ref><ref type="bibr" target="#b13">Hu et al., 2018;</ref><ref type="bibr" target="#b14">Hudson &amp; Manning, 2018;</ref><ref type="bibr" target="#b9">Gupta &amp; Lewis, 2018)</ref>. Our findings suggest that merely replacing hard-coded components with learnable counterparts can be insufficient, and that research on regularizers or priors that steer the learning towards more systematic solutions can be required. That said, our parametrization induction results on the #rhs/lhs=2 split are encouraging, as they show that compared to generic models, a weaker nudge (in the form of a richer training signal or a prior) towards systematicity may suffice for end-to-end NMNs.</p><p>While our investigation has been performed on a synthetic dataset, we believe that it is the realworld language understanding where our findings may be most relevant. It is possible to construct a synthetic dataset that is bias-free and that can only be solved if the model has understood the entirety of the dataset's language. It is, on the contrary, much harder to collect real-world datasets that do not permit highly dataset-specific solutions, as numerous dataset analysis papers of recent years have shown (see Section 5 for a review). We believe that approaches that can generalize strongly from imperfect and biased data will likely be required, and our experiments can be seen as a simulation of such a scenario. We hope, therefore, that our findings will inform researchers working on language understanding and provide them with a useful intuition about what facilitates strong generalization and what is likely to inhibit it.  <ref type="figure">Figure 7</ref>: Model test accuracy vs κ for the MAC model on different versions of SQOOP. All experiments are run 10 times with different random seeds. We can observe a clear correlation between κ and error rate for 1, 2 and 4 rhs/lhs. Also note that perfect generalization is always associated with κ close to 1.</p><p>Next, we experiment with a hard-coded variation of MAC. In this model, we use hard-coded control scores such that given a SQOOP question X R Y, the first half of all modules focuses on X while the second half focuses on Y. The relationship between MAC and hardcoded MAC is similar to that between NMN-Tree and end-to-end NMN with parameterization induction. However, this model has not performed as well as the successful runs of MAC. We hypothesize that this could be due to the interactions between the control scores and the visual attention part of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C INVESTIGATION OF CORRECT PREDICTIONS WITH SPURIOUS LAYOUTS</head><p>In Section 4.3.1 we observed that an NMN with the Residual module can answer test questions with a relative low error rate of 1.64 ± 1.79%, despite being a mixture of a tree and a chain (see results in <ref type="table">Table 1</ref>, p 0 (tree) = 0.5). Our explanation for this phenomenon is as follows: when connected in a tree, modules of such spurious models generalize well, and when connected as a chain they generalize poorly. The output distribution of the whole model is thus a mixture of the mostly correct p(y|T = T tree , x, q) and mostly random p(y|T = T chain , x, q). We verify our reasoning by explicitly evaluating test accuracies for p(y|T = T tree , x, q) and p(y|T = T chain , x, q), and find them to be around 99% and 60% respectively, confirming our hypothesis. As a result the predictions of the spurious models with p(tree) ≈ 0.5 have lower confidence than those of sharp tree models, as indicated by the high log loss of 0.27 ± 0.04. We visualize the progress of structure induction for the Residual module with p 0 (tree) = 0.5 in <ref type="figure">Figure 4</ref> which shows how p(tree) saturates to 1.0 for #rhs/lhs=18 and remains around 0.5 when #rhs/lhs=1. for all R, Y in AllRhs × Rel do 10:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D SQOOP PSEUDOCODE</head><p>T rainQuestions ← (X, R, Y ) ∪ T rainQuestions T rain ← sample</p><formula xml:id="formula_8">10 6</formula><p>|T rainQuestions| examples for each (X,R,Y) ∈ T rainQuestions from GENERATEEXAMPLE(X, R, Y ) 31:</p><p>T est ← sample 10 examples for each (X,R,Y) ∈ T estQuestions from GENERATEEXAM-PLE(X, R, Y ) 32: end function</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Different NMN layouts: NMN-Chain-Shortcut (left), NMN-Chain (center), NMN-Tree (right). See Section 3.2 for details. a: S above T? Yes b: W left of A? No</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A positive (top) and negative (bottom) example from the SQOOP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1</head><label>1</label><figDesc>Pseudocode for creating SQOOP1: S ← {A,B,C, . . . , Z, 0,1,2,3, . . . , 9} 2: Rel ← {LEFT-OF, RIGHT-OF, ABOVE, BELOW} relations 3: function CREATESQOOP(k) all X in S do 7:AllRhs ← RandomSample(S \ {X}, k) sample without replacement from S \ {X}8:AllQuestions ← {X} × Rel × (S \ {X}) ∪ AllQuestions 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Parameterization induction results for 1,2,18 rhs/lhs datasets for Attention N2NMN. The model does not generalize well in the difficult 1 rhs/lhs setting. Results for MAC are presented for comparison. Means and standard deviations were estimated based on at least 10 runs.</figDesc><table><row><cell>Model</cell><cell cols="3">#rhs/lhs Test error rate (%) Test loss (%)</cell></row><row><cell cols="2">Attention N2NMN 1 Attention N2NMN 2 Attention N2NMN 18 MAC 1 MAC 2 MAC 18</cell><cell>27.19 ± 16.02 2.82 ± 8.18 0.16 ± 0.12 13.67 ± 9.97 9.21 ± 4.31 0.53 ± 0.74</cell><cell>1.22 ± 0.71 0.14 ± 0.41 0.00 ± 0.00 0.41 ± 0.32 0.28 ± 0.15 0.01 ± 0.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>I ← place X and Y objects so that R holds create the image 24: I ← add X and Y objects to I so that R holds 25: I ← sample 1 more object from S and add to I 26:until X and Y are not in relation R in I</figDesc><table><row><cell>11:</cell><cell>end for</cell><cell></cell></row><row><cell>12:</cell><cell>end for</cell><cell></cell></row><row><cell>13: 14:</cell><cell>T estQuestions ← AllQuestions \ T rainQuestions function GENERATEEXAMPLE(X, R, Y )</cell><cell></cell></row><row><cell>15: 16:</cell><cell>a ∼ {Yes, No} if a = Yes then</cell><cell></cell></row><row><cell>17: 18: 19:</cell><cell>I ← place X and Y objects so that R holds I ← sample 3 objects from S and add to I else</cell><cell>create the image</cell></row><row><cell>20:</cell><cell>repeat</cell><cell></cell></row><row><cell>21: 22: 23:</cell><cell>X ← Sample X from S \ {X} Y ← Sample Y from S \ {Y }</cell><cell></cell></row><row><cell>27:</cell><cell>end if</cell><cell></cell></row><row><cell>28:</cell><cell>return I, X, R, Y , a</cell><cell></cell></row><row><cell>29:</cell><cell>end function</cell><cell></cell></row><row><cell>30:</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Maxime Chevalier-Boisvert, Yoshua Bengio and Jacob Andreas for useful discussions.</p><p>This research was enabled in part by support provided by Compute Canada (www.computecanada.ca), NSERC, Canada Research Chairs and Microsoft Research. We also thank Nvidia for donating NVIDIA DGX-1 used for this research.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">Table 1</ref><p>: Tree layout induction results for Stochastic N2NMNs using Residual and Find modules on 1 rhs/lhs and 18 rhs/lhs datasets. For each setting of p 0 (tree) we report results after 5 runs. p 200K (tree) is the probability of using a tree layout after 200K training iterations. 0.17 ± 0.16 0.01 ± 0.01 1.00 ± 0.00 0.9 0.11 ± 0.03 0.00 ± 0.00 1.00 ± 0.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENT DETAILS</head><p>We trained all models by minimizing the cross entropy loss log p(y|x, q) on the training set, where y ∈ {yes, no} is the correct answer, x is the image, q is the question. In all our experiments we used the Adam optimizer (Kingma &amp; Ba, 2015) with hyperparameters α = 0.0001, β 1 = 0.9, β 2 = 0.999, = 10 −10 . We continuously monitored validation set performance of all models during training, selected the best one and reported its performance on the test set. The number of training iterations for each model was selected in preliminary investigations based on our observations of how long it takes for different models to converge. This information, as well as other training details, can be found in <ref type="table">Table 3</ref>. We performed an ablation study in which we varied the number of MAC units, the model dimensionality and the level of weight decay for the MAC model. The results can be found in <ref type="table">Table  4</ref>. We also perform qualitative investigations to understand the high variance in MAC's performance.</p><p>In particular, we focus on control attention weights (c) for each run and aim to understand if runs that generalize have clear differences when compared to runs that failed. Interestingly, we observe that in successful runs each word w ∈ X, Y has a unit that is strongly focused on it. To present our observations in quantitative terms, we plot attention quality κ = min w∈{X,Y } max k∈[1;12] α k,w /(1−α k,R ), where α are control scores vs accuracy in <ref type="figure">Figure 7</ref> for each run (see Section 4.3.2 for an explanation of κ). We can clearly see a positive correlation between κ and error rate, especially for low #rhs/lhs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the Behavior of Visual Question Answering Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Module Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.02799" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations</title>
		<meeting>the 2015 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jump to better conclusions: SCAN both left and right</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/W18-5407" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Characterizing how Visual Question Answering scales with the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Szerlip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Obermeyer</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goodman</forename><surname>Noah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 Visually-Grounded Interaction and Language Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The statistical brain: Reply to Marcus The algebraic mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliana</forename><surname>Colunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Connectionism and cognitive architecture: A critical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenon</forename><forename type="middle">W</forename><surname>Pylyshyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="71" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Differentiable Programs with Neural Libraries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02109</idno>
		<ptr target="http://arxiv.org/abs/1611.02109" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural Language Inference over Interaction Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04348</idno>
		<ptr target="http://arxiv.org/abs/1709.04348" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Learning Representations</title>
		<meeting>the 2018 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Compositional Denotational Semantics for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D18-1239" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Annotation Artifacts in Natural Language Inference Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02324</idno>
		<ptr target="http://arxiv.org/abs/1803.02324" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2018</title>
		<meeting>NAACL-HLT 2018</meeting>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to Reason: End-to-End Module Networks for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05526</idno>
		<ptr target="http://arxiv.org/abs/1704.05526" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE International Conference on Computer Vision</title>
		<meeting>2017 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explainable Neural Computation via Stack Neural Module Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08556</idno>
		<ptr target="http://arxiv.org/abs/1807.08556" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2018 European Conference on Computer Vision</title>
		<meeting>2018 European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compositional Attention Networks for Machine Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1Euwz-Rb" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Learning Representations</title>
		<meeting>the 2018 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial Examples for Evaluating Reading Comprehension Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1215</idno>
		<ptr target="https://aclanthology.coli.uni-saarland.de/papers/D17-1215/d17-1215" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pythia v0.1: The winning entry to the vqa challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/pythia" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06890</idno>
		<ptr target="http://arxiv.org/abs/1612.06890" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inferring and Executing Programs for Visual Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1705.03633" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE International Conference on Computer Vision</title>
		<meeting>2017 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Smart Reply: Automated Response Suggestion for Email</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balint</forename><surname>Miklos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laszlo</forename><surname>Lukacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ramavajjala</surname></persName>
		</author>
		<idno type="DOI">http://doi.acm.org/10.1145/2939672.2939801</idno>
		<ptr target="http://doi.acm.org/10.1145/2939672.2939801" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="955" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations</title>
		<meeting>the 2015 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ShapeWorld -A new test methodology for multimodal language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kuhnle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Copestake</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04517</idno>
		<idno>arXiv: 1704.04517</idno>
		<ptr target="http://arxiv.org/abs/1704.04517" />
		<imprint>
			<date type="published" when="2017-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00350</idno>
		<ptr target="http://arxiv.org/abs/1711.00350" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Loula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1807.07545" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 BlackboxNLP EMNLP Workshop</title>
		<meeting>the 2018 BlackboxNLP EMNLP Workshop</meeting>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Multi-world Approach to Question Answering About Realworld Scenes Based on Uncertain Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2968826.2969014" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking Eliminative Connectionism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
		<idno type="DOI">10.1006/cogp.1998.0694</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0010028598906946" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="282" />
			<date type="published" when="1998-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The algebraic mind: Integrating connectionism and cognitive science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">F</forename><surname>Marcus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FiLM: Visual Reasoning with a General Conditioning Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1709.07871" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 AAAI Conference on Artificial Intelligence</title>
		<meeting>the 2017 AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01427</idno>
		<ptr target="http://arxiv.org/abs/1706.01427" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The constituent structure of connectionist mental states: A reply to Fodor and Pylyshyn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Southern Journal of Philosophy</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="137" to="161" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
	<note>Supplement</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11361</idno>
		<idno>arXiv: 1803.11361</idno>
		<ptr target="http://arxiv.org/abs/1803.11361" />
		<title level="m">DDRprog: A CLEVR Differentiable Dynamic Reasoning Programmer</title>
		<imprint>
			<date type="published" when="2018-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P18-1158" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Klaus Macherey, and others. Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
