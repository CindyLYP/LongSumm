<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Relational Inference for Interacting Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Canadian Institute for Advanced Re-search</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Vector Institute</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Canadian Institute for Advanced Re-search</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Relational Inference for Interacting Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system&apos;s constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A wide range of dynamical systems in physics, biology, sports, and other areas can be seen as groups of interacting components, giving rise to complex dynamics at the level of individual constituents and in the system as a whole. Modeling these type of dynamics is challenging: often, we only have access to individual trajectories, without knowledge of the underlying interactions or dynamical model. As a motivating example, let us take the movement of basketball players on the court. It is clear that the dynamics of a single basketball player are influenced by the other players, and observing these dynamics as a human, we are Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observed dynamics</head><p>Interaction graph <ref type="figure">Figure 1</ref>. Physical simulation of 2D particles coupled by invisible springs (left) according to a latent interaction graph (right). In this example, solid lines between two particle nodes denote connections via springs whereas dashed lines denote the absence of a coupling. In general, multiple, directed edge types -each with a different associated relation -are possible.</p><p>able to reason about the different types of interactions that might arise, e.g. defending a player or setting a screen for a teammate. It might be feasible, though tedious, to manually annotate certain interactions given a task of interest. It is more promising to learn the underlying interactions, perhaps shared across many tasks, in an unsupervised fashion.</p><p>Recently there has been a considerable amount of work on learning the dynamical model of interacting systems using implicit interaction models <ref type="bibr" target="#b40">(Sukhbaatar et al., 2016;</ref><ref type="bibr" target="#b14">Guttenberg et al., 2016;</ref><ref type="bibr" target="#b38">Santoro et al., 2017;</ref><ref type="bibr" target="#b42">Watters et al., 2017;</ref><ref type="bibr" target="#b18">Hoshen, 2017;</ref><ref type="bibr" target="#b40">van Steenkiste et al., 2018)</ref>. These models can be seen as graph neural networks (GNNs) that send messages over the fully-connected graph, where the interactions are modeled implicitly by the message passing function <ref type="bibr" target="#b40">(Sukhbaatar et al., 2016;</ref><ref type="bibr" target="#b14">Guttenberg et al., 2016;</ref><ref type="bibr" target="#b38">Santoro et al., 2017;</ref><ref type="bibr" target="#b42">Watters et al., 2017)</ref> or with the help of an attention mechanism <ref type="bibr" target="#b18">(Hoshen, 2017;</ref><ref type="bibr" target="#b40">van Steenkiste et al., 2018)</ref>.</p><p>In this work, we address the problem of inferring an explicit interaction structure while simultaneously learning the dynamical model of the interacting system in an unsupervised way. Our neural relational inference (NRI) model learns the dynamics with a GNN over a discrete latent graph, and we perform inference over these latent variables. The inferred edge types correspond to a clustering of the interactions. Using a probabilistic model allows us to incorporate prior beliefs about the graph structure, such as sparsity, in a principled manner. arXiv:1802.04687v2 <ref type="bibr">[stat.ML]</ref> 6 Jun 2018</p><p>In a range of experiments on physical simulations, we show that our NRI model possesses a favorable inductive bias that allows it to discover ground-truth physical interactions with high accuracy in a completely unsupervised way. We further show on real motion capture and NBA basketball data that our model can learn a very small number of edge types that enable it to accurately predict the dynamics many time steps into the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background: Graph Neural Networks</head><p>We start by giving a brief introduction to a recent class of neural networks that operate directly on graph-structured data by passing local messages <ref type="bibr" target="#b39">(Scarselli et al., 2009;</ref><ref type="bibr" target="#b28">Li et al., 2016;</ref><ref type="bibr" target="#b12">Gilmer et al., 2017)</ref>. We refer to these models as graph neural networks (GNN). Variants of GNNs have been shown to be highly effective at relational reasoning tasks <ref type="bibr" target="#b38">(Santoro et al., 2017)</ref>, modeling interacting or multi-agent systems <ref type="bibr" target="#b40">(Sukhbaatar et al., 2016;</ref><ref type="bibr" target="#b2">Battaglia et al., 2016)</ref>, classification of graphs <ref type="bibr" target="#b3">(Bruna et al., 2014;</ref><ref type="bibr" target="#b10">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b7">Dai et al., 2016;</ref><ref type="bibr" target="#b35">Niepert et al., 2016;</ref><ref type="bibr" target="#b8">Defferrard et al., 2016;</ref><ref type="bibr" target="#b21">Kearnes et al., 2016)</ref> and classification of nodes in large graphs <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b15">Hamilton et al., 2017)</ref>. The expressive power of GNNs has also been studied theoretically in <ref type="bibr" target="#b43">(Zaheer et al., 2017;</ref><ref type="bibr" target="#b16">Herzig et al., 2018)</ref>.</p><p>Given a graph G = (V, E) with vertices v ∈ V and edges e = (v, v ) ∈ E 1 , we define a single node-to-node message passing operation in a GNN as follows, similar to <ref type="bibr" target="#b12">Gilmer et al. (2017)</ref>:</p><formula xml:id="formula_0">v→e : h l (i,j) = f l e ([h l i , h l j , x (i,j) ])<label>(1)</label></formula><p>e→v :</p><formula xml:id="formula_1">h l+1 j = f l v ([ i∈Nj h l (i,j) , x j ])<label>(2)</label></formula><p>where h l i is the embedding of node v i in layer l, h l (i,j) is an embedding of the edge e (i,j) , and x i and x (i,j) summarize initial (or auxiliary) node and edge features, respectively (e.g. node input and edge type). N j denotes the set of indices of neighbor nodes connected by an incoming edge and <ref type="bibr">[•, •]</ref> denotes concatenation of vectors. The functions f v and f e are node-and edge-specific neural networks (e.g. small MLPs) respectively (see <ref type="figure">Figure 2</ref>). Eqs. (1)-(2) allow for the composition of models that map from edge to node representations or vice-versa via multiple rounds of message passing.</p><p>In the original GNN formulation from <ref type="bibr" target="#b39">Scarselli et al. (2009)</ref> the node embedding h l (i,j) depends only on h l i , the embedding of the sending node, and the edge type, but not on h l j , the embedding of the receiving node. This is of course a special case of this formulation, and more recent works such as interaction networks <ref type="bibr" target="#b2">(Battaglia et al., 2016)</ref> or message passing neural networks <ref type="bibr" target="#b12">(Gilmer et al., 2017)</ref> are in line with our 1 Undirected graphs can be modeled by explicitly assigning two directed edges in opposite direction for each undirected edge. Node-to-edge (v →e ) Edge-to-node (e →v ) <ref type="figure">Figure 2</ref>. Node-to-edge (v→e) and edge-to-node (e→v) operations for moving between node and edge representations in a GNN. v→e represents concatenation of node embeddings connected by an edge, whereas e→v denotes the aggregation of edge embeddings from all incoming edges. In our notation in Eqs. (1)- <ref type="formula" target="#formula_1">2</ref>, every such operation is followed by a small neural network (e.g. a 2-layer MLP), here denoted by a black arrow. For clarity, we highlight which node embeddings are combined to form a specific edge embedding (v→e) and which edge embeddings are aggregated to a specific node embedding (e→v).</p><formula xml:id="formula_2">h 3 l h 4 l h (4,3) l f e f v l l</formula><p>more general formulation. We further note that some recent works factor f l e (•) into a product of two separate functions, one of which acts as a gating or attention mechanism <ref type="bibr" target="#b34">(Monti et al., 2017;</ref><ref type="bibr" target="#b9">Duan et al., 2017;</ref><ref type="bibr" target="#b18">Hoshen, 2017;</ref><ref type="bibr" target="#b41">Veličković et al., 2018;</ref><ref type="bibr" target="#b11">Garcia &amp; Bruna, 2018;</ref><ref type="bibr" target="#b40">van Steenkiste et al., 2018)</ref> which in some cases can have computational benefits or introduce favorable inductive biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Neural Relational Inference Model</head><p>Our NRI model consists of two parts trained jointly: An encoder that predicts the interactions given the trajectories, and a decoder that learns the dynamical model given the interaction graph.</p><p>More formally, our input consists of trajectories of N objects. We denote by x t i the feature vector of object v i at time t, e.g. location and velocity. We denote by x t = {x t 1 , ..., x t N } the set of features of all N objects at time t, and we denote by</p><formula xml:id="formula_3">x i = (x 1 i , ..., x T i )</formula><p>the trajectory of object i, where T is the total number of time steps. Lastly, we mark the whole trajectories by x = (x 1 , ..., x T ). We assume that the dynamics can be modeled by a GNN given an unknown graph z where z ij represents the discrete edge type between objects v i and v j . The task is to simultaneously learn to predict the edge types and learn the dynamical model in an unsupervised way. We formalize our model as a variational autoencoder (VAE) <ref type="bibr" target="#b23">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b37">Rezende et al., 2014)</ref> that maximizes the ELBO:  <ref type="figure">Figure 3</ref>. The NRI model consists of two jointly trained parts: An encoder that predicts a probability distribution q φ (z|x) over the latent interactions given input trajectories; and a decoder that generates trajectory predictions conditioned on both the latent code of the encoder and the previous time step of the trajectory. The encoder takes the form of a GNN with multiple rounds of node-to-edge (v→e) and edge-to-node (e→v) message passing, whereas the decoder runs multiple GNNs in parallel, one for each edge type supplied by the latent code of the encoder q φ (z|x).</p><formula xml:id="formula_4">L = E q φ (z|x) [log p θ (x|z)] − KL[q φ (z|x)||p θ (z)] (3) x x t Δx t … Σ … Encoder Σ Decoder … q φ (z|x) … v</formula><p>The encoder q φ (z|x) returns a factorized distribution of z ij , where z ij is a discrete categorical variable representing the edge type between object v i and v j . We use a one-hot representation of the K interaction types for z ij .</p><p>The decoder</p><formula xml:id="formula_5">p θ (x|z) = T t=1 p θ (x t+1 |x t , ..., x 1 , z)<label>(4)</label></formula><p>models p θ (x t+1 |x t , ..., x 1 , z) with a GNN given the latent graph structure z.</p><p>The prior p θ (z) = i =j p θ (z ij ) is a factorized uniform distribution over edges types. If one edge type is "hard coded" to represent "non-edge" (no messages being passed along this edge type), we can use an alternative prior with higher probability on the "non-edge" label. This will encourage sparser graphs.</p><p>There are some notable differences between our model and the original formulation of the VAE <ref type="bibr" target="#b23">(Kingma &amp; Welling, 2014)</ref>. First, in order to avoid the common issue in VAEs of the decoder ignoring the latent code z (Chen et al., 2017), we train the decoder to predict multiple time steps and not a single step as the VAE formulation requires. This is necessary since interactions often only have a small effect in the time scale of a single time step. Second, the latent distribution is discrete, so we use a continuous relaxation in order to use the reparameterization trick. Lastly, we note that we do not learn the probability p(x 1 ) (i.e. for t = 1) as we are interested in the dynamics and interactions, and this does not have any effect on either (but would be easy to include if there was a need).</p><p>The overall model is schematically depicted in <ref type="figure">Figure 3</ref>. In the following, we describe the encoder and decoder components of the model in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoder</head><p>At a high level, the goal of the encoder is to infer pairwise interaction types z ij given observed trajectories x = (x 1 , ..., x T ). Since we do not know the underlying graph, we can use a GNN on the fully-connected graph to predict the latent graph structure.</p><p>More formally, we model the encoder as</p><formula xml:id="formula_6">q φ (z ij |x) = softmax(f enc,φ (x) ij,1:K ), where f enc,φ (x)</formula><p>is a GNN acting on the fully-connected graph (without self-loops). Given input trajectories x 1 , ..., x K our encoder computes the following message passing operations:</p><formula xml:id="formula_7">h 1 j = f emb (x j ) (5) v→e : h 1 (i,j) = f 1 e ([h 1 i , h 1 j ])<label>(6)</label></formula><p>e→v :</p><formula xml:id="formula_8">h 2 j = f 1 v ( i =j h 1 (i,j) )<label>(7)</label></formula><p>v→e :</p><formula xml:id="formula_9">h 2 (i,j) = f 2 e ([h 2 i , h 2 j ])<label>(8)</label></formula><p>Finally, we model the edge type posterior as q φ (z ij |x) = softmax(h 2 (i,j) ) where φ summarizes the parameters of the neural networks in Eqs. (5)-(8). The use of multiple passes, two in the model presented here, allows the model to "disentangle" multiple interactions while still using only binary terms. In a single pass, Eqs. (5)-(6), the embedding h 1 (i,j) only depends on x i and x j ignoring interactions with other nodes, while h 2 j uses information from the whole graph. The functions f (...) are neural networks that map between the respective representations. In our experiments we used either fully-connected networks (MLPs) or 1D convolutional networks (CNNs) with attentive pooling similar to <ref type="bibr" target="#b29">(Lin et al., 2017)</ref>  are interpreted. Unlike in a typical GNN, the messages h l (i,j) are no longer considered just a transient part of the computation, but an integral part of the model that represents the edge embedding used to perform edge classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sampling</head><p>It is straightforward to sample from q φ (z ij |x), however we cannot use the reparametrization trick to backpropagate though the sampling as our latent variables are discrete. A recently popular approach to handle this difficulty is to sample from a continuous approximation of the discrete distribution <ref type="bibr" target="#b33">(Maddison et al., 2017;</ref><ref type="bibr" target="#b20">Jang et al., 2017)</ref> and use the repramatrization trick to get (biased) gradients from this approximation. We used the concrete distribution <ref type="bibr" target="#b33">(Maddison et al., 2017)</ref> where samples are drawn as:</p><formula xml:id="formula_10">z ij = softmax((h 2 (i,j) + g)/τ )<label>(9)</label></formula><p>where g ∈ R K is a vector of i.i.d. samples drawn from a Gumbel(0, 1) distribution and τ (softmax temperature) is a parameter that controls the "smoothness" of the samples. This distribution converges to one-hot samples from our categorical distribution when τ → 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoder</head><p>The task of the decoder is to predict the future continuation of the interacting system's dynamics</p><formula xml:id="formula_11">p θ (x t+1 |x t , ..., x 1 , z).</formula><p>Since the decoder is conditioned on the graph z we can in general use any GNN algorithm as our decoder.</p><p>For physics simulations the dynamics is Markovian</p><formula xml:id="formula_12">p θ (x t+1 |x t , ..., x 1 , z) = p θ (x t+1 |x t , z)</formula><p>, if the state is location and velocity and z is the ground-truth graph. For this reason we use a GNN similar to interaction networks; unlike interaction networks we have a separate neural network for each edge type. More formally:</p><formula xml:id="formula_13">v→e :h t (i,j) = k z ij,kf k e ([x t i , x t j ])<label>(10)</label></formula><p>e→v :</p><formula xml:id="formula_14">µ t+1 j = x t j +f v ( i =jh t (i,j) ) (11) p(x t+1 j |x t , z) = N (µ t+1 j , σ 2 I)<label>(12)</label></formula><p>Note that z ij,k denotes the k-th element of the vector z ij and σ 2 is a fixed variance. When z ij,k is a discrete one-hot sample the messagesh</p><formula xml:id="formula_15">t (i,j) aref k e ([x t i , x t j ]</formula><p>) for the selected edge type k, and for the continuous relaxation we get a weighted sum. Also note that since in Eq. 11 we add the present state x t j our model only learns the change in state ∆x t j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Avoiding degenerate decoders</head><p>If we look at the ELBO, Eq. 3, the reconstruction loss term has the form</p><formula xml:id="formula_16">T t=1 log[p(x t |x t−1 , z)</formula><p>] which involves only single step predictions. One issue with optimizing this objective is that the interactions can have a small effect on short-term dynamics. For example, in physics simulations a fixed velocity assumption can be a good approximation for a short time period. This leads to a sub-optimal decoder that ignores the latent edges completely and achieves only a marginally worse reconstruction loss.</p><p>We address this issue in two ways: First, we predict multiple steps into the future, where a "degenerate" decoder (which ignores the latent edges) would perform much worse. Second, instead of having one neural network that computes the messages given</p><formula xml:id="formula_17">[x t i , x t j , z ij ]</formula><p>, as was done in <ref type="bibr" target="#b2">(Battaglia et al., 2016)</ref>, we have a separate MLP for each edge type. This makes the dependence on the edge type more explicit and harder to be ignored by the model. Predicting multiple steps is implemented by replacing the correct input x t , with the predicted mean µ t for M steps (we used M = 10 in our experiments), then feed in the correct previous step and reiterate. More formally, if we denote our decoder as µ t+1 j = f dec (x t j ) then we have:</p><formula xml:id="formula_18">µ 2 j = f dec (x 1 j ) µ t+1 j = f dec (µ t j ) t = 2, . . . , M µ M +2 j = f dec (x M +1 j ) µ t+1 j = f dec (µ t j ) t = M + 2, . . . , 2M • • •</formula><p>We are backpropagating through this whole process, and since the errors accumulate for M steps the degenerate decoder is now highly suboptimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Recurrent decoder</head><p>In many applications the Markovian assumption used in Sec. 3.3 does not hold. To handle such applications we use a recurrent decoder that can model p θ (x t+1 |x t , ..., x 1 , z).</p><p>Our recurrent decoder adds a GRU <ref type="bibr" target="#b6">(Cho et al., 2014)</ref> unit to the GNN message passing operation. More formally:</p><formula xml:id="formula_19">v→e :h t (i,j) = k z ij,kf k e ([h t i ,h t j ])<label>(13)</label></formula><p>e→v</p><formula xml:id="formula_20">: MSG t j = i =jh t (i,j) (14) h t+1 j = GRU([MSG t j , x t j ],h t j ) (15) µ t+1 j = x t j + f out (h t+1 j ) (16) p(x t+1 |x t , z) = N (µ t+1 , σ 2 I)<label>(17)</label></formula><p>The input to the message passing operation is the recurrent hidden state at the previous time step. f out denotes an output transformation, modeled by a small MLP. For each node v j the input to the GRU update is the concatenation of the aggregated messages MSG t+1 j , the current input x t+1 j , and the previous hidden stateh t j .</p><p>If we wish to predict multiple time steps in the recurrent setting, the method suggested in Sec. 3.4 will be problematic. Feeding in the predicted (potentially incorrect) path and then periodically jumping back to the true path will generate artifacts in the learned trajectories. In order to avoid this issue we provide the correct input x t j in the first (T − M ) steps, and only utilize our predicted mean µ t j as input at the last M time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training</head><p>Now that we have described all the elements, the training goes as follows: Given training example x we first run the encoder and compute q φ (z ij |x), then we sample z ij from the concrete reparameterizable approximation of q φ (z ij |x). We then run the decoder to compute µ 2 , ..., µ T . The ELBO objective, Eq. 3, has two terms: the recon-</p><formula xml:id="formula_21">struction error E q φ (z|x) [log p θ (x|z)] and KL divergence KL[q φ (z|x)||p θ (z)].</formula><p>The reconstruction error is estimated by:</p><formula xml:id="formula_22">− j T t=2 ||x t j − µ t j || 2 2σ 2 + const (18)</formula><p>while the KL term for a uniform prior is just the sum of entropies (plus a constant):</p><formula xml:id="formula_23">i =j H(q φ (z ij |x)) + const.<label>(19)</label></formula><p>As we use a reparameterizable approximation, we can compute gradients by backpropagation and optimize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Several recent works have studied the problem of learning the dynamics of a physical system from simulated trajectories <ref type="bibr" target="#b2">(Battaglia et al., 2016;</ref><ref type="bibr" target="#b14">Guttenberg et al., 2016;</ref><ref type="bibr" target="#b4">Chang et al., 2017)</ref> and from generated video data <ref type="bibr" target="#b42">(Watters et al., 2017;</ref><ref type="bibr" target="#b40">van Steenkiste et al., 2018)</ref> with a graph neural network. Unlike our work they either assume a known graph structure or infer interactions implicitly.</p><p>Recent related works on graph-based methods for human motion prediction include <ref type="bibr" target="#b0">(Alahi et al., 2016)</ref> where the graph is not learned but is based on proximity and <ref type="bibr" target="#b27">(Le et al., 2017)</ref> tries to cluster agents into roles.</p><p>A number of recent works <ref type="bibr" target="#b34">(Monti et al., 2017;</ref><ref type="bibr" target="#b9">Duan et al., 2017;</ref><ref type="bibr" target="#b18">Hoshen, 2017;</ref><ref type="bibr" target="#b41">Veličković et al., 2018;</ref><ref type="bibr" target="#b11">Garcia &amp; Bruna, 2018;</ref><ref type="bibr" target="#b40">van Steenkiste et al., 2018)</ref> parameterize messages in GNNs with a soft attention mechanism <ref type="bibr" target="#b32">(Luong et al., 2015;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. This equips these models with the ability to focus on specific interactions with neighbors when aggregating messages. Our work is different from this line of research, as we explicitly perform inference over the latent graph structure. This allows for the The problem of inferring interactions or latent graph structure has been investigated in other settings in different fields. For example, in causal reasoning Granger causality <ref type="bibr" target="#b13">(Granger, 1969)</ref> infers causal relations. Another example from computational neuroscience is <ref type="bibr" target="#b31">(Linderman et al., 2016;</ref><ref type="bibr" target="#b30">Linderman &amp; Adams, 2014)</ref> where they infer interactions between neural spike trains.</p><formula xml:id="formula_24">Springs (2D) Kuramoto (1D) Charged (2D)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our encoder implementation uses fully-connected networks (MLPs) or 1D CNNs with attentive pooling as our message passing function. For our decoder we used fully-connected networks or alternatively a recurrent decoder. Optimization was performed using the Adam algorithm <ref type="bibr" target="#b22">(Kingma &amp; Ba, 2015)</ref>. We provide full implementation details in the supplementary material. Our implementation uses PyTorch <ref type="bibr" target="#b36">(Paszke et al., 2017)</ref> and is available online 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Physics simulations</head><p>We experimented with three simulated systems: particles connected by springs, charged particles and phase-coupled oscillators <ref type="bibr">(Kuramoto model)</ref>  <ref type="bibr" target="#b25">(Kuramoto, 1975)</ref>. These settings allow us to attempt to learn the dynamics and interactions when the interactions are known. These systems, controlled by simple rules, can exhibit complex dynamics. For the springs and Kuramoto experiments the objects do or do not interact with equal probability. For the charged particles experiment they attract or repel with equal probability. Example trajectories can be seen in <ref type="figure" target="#fig_1">Fig. 4</ref>. We generate 50k training examples, and 10k validation and test examples for all tasks. Further details on the data generation and implementation are in the supplementary material.</p><p>We note that the simulations are differentiable and so we can use it as a ground-truth decoder to train the encoder. The charged particles simulation, however, suffers from instabil- <ref type="figure">Figure 5</ref>. Trajectory predictions from a trained NRI model (unsupervised). Semi-transparent paths denote the first 49 time steps of ground-truth input to the model, from which the interaction graph is estimated. Solid paths denote self-conditioned model predictions. ity which led to some performance issues when calculating gradients; see supplementary material for further details. We used an external code base <ref type="bibr" target="#b26">(Laszuk, 2017)</ref> for stable integration of the Kuramoto ODE and therefore do not have access to gradient information in this particular simulation.</p><formula xml:id="formula_25">Charged (2D) Springs (2D) Kuramoto (1D) Prediction Truth Prediction Truth Prediction Truth</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We ran our NRI model on all three simulated physical systems and compared our performance, both in future state prediction and in accuracy of estimating the edge type in an unsupervised manner.</p><p>For edge prediction, we compare to the "gold standard" i.e. training our encoder in a supervised way given the ground-truth labels. We also compare to the following baselines: Our NRI model with the ground-truth simulation decoder, NRI (sim.), and two correlation based baselines,</p><p>Corr. (path) and Corr. (LSTM). Corr. (path) estimates the interaction graph by thresholding the matrix of correlations between trajectory feature vectors. Corr. (LSTM) trains an LSTM <ref type="bibr" target="#b17">(Hochreiter &amp; Schmidhuber, 1997)</ref> with shared parameters to model each trajectory individually and calculates correlations between the final hidden states to arrive at an interaction matrix after thresholding. We provide further details on these baselines in the supplementary material.</p><p>Results for the unsupervised interaction recovery task are summarized in <ref type="table" target="#tab_2">Table 1</ref> (average over 5 runs and standard error). As can be seen, the unsupervised NRI model, NRI (learned), greatly surpasses the baselines and recovers the ground-truth interaction graph with high accuracy on most tasks. For the springs model our unsupervised method is comparable to the supervised "gold standard" benchmark. We note that our supervised baseline is similar to the work by <ref type="bibr" target="#b38">(Santoro et al., 2017)</ref>, with the difference that we perform multiple rounds of message passing in the graph. Additional results on experiments with more than two edge types and non-interacting particles are described in the supplementary material.</p><p>For future state prediction we compare to the static baseline, i.e. x t+1 = x t , two LSTM baselines, and a full graph baseline. One LSTM baseline, marked as "single", runs a separate LSTM (with shared weights) for each object. The second, marked as "joint" concatenates all state vectors and feeds it into one LSTM that is trained to predict all future states simultaneously. Note that the latter will only be able to operate on a fixed number of objects (in contrast to the other models).</p><p>In the full graph baseline, we use our message passing decoder on the fully-connected graph without edge types, i.e. without inferring edges. This is similar to the model <ref type="table">Table 2</ref>. Mean squared error (MSE) in predicting future states for simulations with 5 interacting objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Springs Charged Kuramoto</head><p>Prediction steps 1 10 20 10 20 1 10 20 Static 7.93e-5 7.59e-3 2.82e-2 5.09e-3 2.26e-2 5.42e-2 5.75e-2 3.79e-1 3.39e-1 LSTM (single) 2.27e-6 4.69e-4 4.90e-3 2.71e-3 7.05e-3 1.65e-2 7.81e-4 3.80e-2 8.08e-2 LSTM (joint) 4.13e-8 2.19e-5 7.02e-4 1.68e-3 6.45e-3 1.49e-2 3.44e-4 1.29e-2 4.74e-2 NRI (full graph) 1.66e-5 1.64e-3 6.31e-3 1.09e-3 3.78e-3 9.24e-3 2.15e-2 5.19e-2 8.96e-2 NRI (learned)</p><p>3.12e-8 3.29e-6 2.13e-5 1.05e-3 3.21e-3 7.06e-3 1.40e-2 2.01e-2 3.26e-2 NRI (true graph) 1.69e-11 1.32e-9 7.06e-6 1.04e-3 3.03e-3 5.71e-3 1.35e-2 1.54e-2 2.19e-2 <ref type="figure">Figure 6</ref>. Test MSE comparison for motion capture (walking) data (left) and sports tracking (SportVU) data (right).</p><p>used in <ref type="bibr" target="#b42">(Watters et al., 2017)</ref>. We also compare to the "gold standard" model, denoted as NRI (true graph), which is training only a decoder using the ground-truth graph as input. The latter baseline is comparable to previous works such as interaction networks <ref type="bibr" target="#b2">(Battaglia et al., 2016)</ref>.</p><p>In order to have a fair comparison, we generate longer test trajectories and only evaluate on the last part unseen by the encoder. Specifically, we run the encoder on the first 49 time steps (same as in training and validation), then predict with our decoder the following 20 unseen time steps. For the LSTM baselines, we first have a "burn-in" phase where we feed the LSTM the first 49 time steps, and then predict the next 20 time steps. This way both algorithms have access to the first 49 steps when predicting the next 20 steps. We show mean squared error (MSE) results in <ref type="table">Table 2</ref>, and note that our results are better than using LSTM for long term prediction. Example trajectories predicted by our NRI (learned) model for up to 50 time steps are shown in <ref type="figure">Fig. 5</ref>. For the Kuramoto model, we observe that the LSTM baselines excel at smoothly continuing the shape of the waveform for short time frames, but fail to model the long-term dynamics of the interacting system. We provide further qualitative analysis for these results in the supplementary material.</p><p>It is interesting to note that the charged particles experiment achieves an MSE score which is on par with the NRI model given the true graph, while only predicting 82.6% of the edges accurately. This is explained by the fact that far away particles have weak interactions, which have only small effects on future prediction. An example can be seen in <ref type="figure">Fig.  5</ref> in the top row where the blue particle is repelled instead of being attracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Motion capture data</head><p>The CMU Motion Capture Database (CMU, 2003) is a large collection of motion capture recordings for various tasks (such as walking, running, and dancing) performed by human subjects. We here focus on recorded walking motion data of a single subject (subject #35). The data is in the form of 31 3D trajectories, each tracking a single joint. We split the different walking trials into non-overlapping training (11 trials), validation (4 trials) and test sets (7 trials). We provide both position and velocity data. See supplementary material for further details. We train our NRI model with an MLP encoder and RNN decoder on this data using 2 or 4 edge types where one edge type is "hard-coded" as non-edge, i.e. messages are only passed on the other edge types. We found that experiments with 2 and 4 edge types give almost identical results, with two edge types being comparable in capacity to the fully connected graph baseline while four edge types (with sparsity prior) are more interpretable and allow for easier visualization.</p><p>Dynamic graph re-evaluation We find that the learned graph depends on the particular phase of the motion <ref type="figure" target="#fig_3">(Fig. 7)</ref>, which indicates that the ideal underlying graph is dynamic. To account for this, we dynamically re-evaluate the NRI encoder for every time step during testing, effectively resulting in a dynamically changing latent graph that the decoder can utilize for more accurate predictions.</p><p>Results The qualitative results for our method and the same baselines used in Sec. 5.1 can be seen in <ref type="figure">Fig. 6</ref>. As one can see, we outperform the fully-connected graph setting in long-term predictions, and both models outperform the LSTM baselines. Dynamic graph re-evaluation significantly  improves predictive performance for this dataset compared to a static baseline. One interesting observation is that the skeleton graph is quite suboptimal, which is surprising as the skeleton is the "natural" graph. When examining the edges found by our model (trained with 4 edge types and a sparsity prior) we see an edge type that mostly connects a hand to other extremities, especially the opposite hand, as seen in <ref type="figure" target="#fig_3">Fig. 7</ref>. This can seem counter-intuitive as one might assume that the important connections are local, however we note that some leading approaches for modeling motion capture data <ref type="bibr" target="#b19">(Jain et al., 2016)</ref> do indeed include hand to hand interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Pick and Roll NBA data</head><p>The National Basketball Association (NBA) uses the SportVU tracking system to collect player tracking data, where each frame contains the location of all ten players and the ball. Similar to our previous experiments, we test our model on the task of future trajectory prediction. Since the interactions between players are dynamic, and our current formulation assumes fixed interactions during training, we focus on the short Pick and Roll (PnR) instances of the games. PnR is one of the most common offensive tactics in the NBA where an offensive player sets a screen for the ball handler, attempting to create separation between the ball handler and his matchup.</p><p>We extracted 12k segments from the 2016 season and used 10k, 1k, 1k for training, validation, and testing respectively. The segments are 25 frames long (i.e. 4 seconds) and consist of only 5 nodes: the ball, ball hander, screener, and defensive matchup for each of the players.</p><p>The first edge type is "hard-coded" as non-edge and was trained with a prior probability of 0.91. All other edge types received a prior of 0.03 to favor sparse graphs that are easier to visualize. We visualize test data not seen during training. We trained a CNN encoder and a RNN decoder with 2 edge types. For fair comparison, and because the trajectory continuation is not PnR anymore, the encoder is trained on only the first 17 time steps (as deployed in testing). Further details are in the supplementary material. Results for test MSE are shown in <ref type="figure">Figure 6</ref>. Our model outperforms a baseline LSTM model, and is on par with the full graph.</p><p>To understand the latent edge types we show in <ref type="figure" target="#fig_4">Fig. 8</ref> how they are distributed between the players and the ball. As we can see, one edge type mostly connects ball and ball handler (off-ball) to all other players, while the other is mostly inner connections between the other three players. As the ball and ball handler are the key elements in the PnR play, we see that our model does learn an important semantic structure by separating them from the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work we introduced NRI, a method to simultaneously infer relational structure while learning the dynamical model of an interacting system. In a range of experiments with physical simulations we demonstrate that our NRI model is highly effective at unsupervised recovery of ground-truth interaction graphs. We further found that it can model the dynamics of interacting physical systems, of real motion tracking and of sports analytics data at a high precision, while learning reasonably interpretable edge types.</p><p>Many real-world examples, in particular multi-agent systems such as traffic, can be understood as an interacting system where the interactions are dynamic. While our model is trained to discover static interaction graphs, we demonstrate that it is possible to apply a trained NRI model to this evolving case by dynamically re-estimating the latent graph. Nonetheless, our solution is limited to static graphs during training and future work will investigate an extension of the NRI model that can explicitly account for dynamic latent interactions even at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Further experimental analysis</head><p>A.1. <ref type="bibr">Kuramoto LSTM vs. NRI comparison</ref> From the results in our main paper it became evident that a simple LSTM model excels at predicting the dynamics of a network of phase-coupled oscillators (Kuramoto model) for short periods of time, while predictive performance deteriorates for longer sequences. It is interesting to compare the qualitative predictive behavior of this fully recurrent model with our NRI (learned) model that models the state x t+1 at time t + 1 solely based on the state x t at time t and the learned latent interaction graph. In <ref type="figure">Fig. 9</ref> we provide visualizations of model predictions for the LSTM (joint) and the NRI (learned) model, compared to the ground truth continuation of the simulation.</p><p>It can be seen that the LSTM model correctly captures the shape of the sinusoidal waveform but fails to model the phase dynamics that arise due to the interactions between the oscillators. Our NRI (learned) model captures the qualitative behavior of the original coupled model at a very high precision and only in some cases slightly misses the phase dynamics (e.g. in the purple and green curve in the lower right plot). The LSTM model rarely matches the phase of the ground truth trajectory in the last few time steps and often completely goes "out of sync" by up to half a wavelength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Spring simulation variants</head><p>In addition to the experiments presented in the main paper, we analyze the following two variants of the spring simulation experimental setting: i) we test a trained model on completely non-interacting (free-floating) particles, and ii) we add a third edge type with a lower coupling constant.</p><p>To test whether our model can infer an empty graph, we create a test set of 1000 simulations with 5 non-interacting particles and test an unsupervised NRI model which was trained on the spring simulation dataset with 5 particles as before. We find that it achieves an accuracy of 98.4% in identifying "no interaction" edges (i.e. the empty graph).</p><p>The last variant explores a simulation with more than two known edge types. We follow the same procedure for the spring simulation with 5 particles as before with the exception of adding an additional edge type with coupling constant k ij = 0.5 (all three edge types are sampled with equal probability). We fit an unsupervised NRI model to this data (K = 3 in this case, other settings as before) and find that it achieves an accuracy of 99.2% in discovering the correct edge types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Motion capture visualizations</head><p>In <ref type="figure" target="#fig_6">Fig. 10</ref> we visualize predictions of a trained NRI model with learned latent graph for the motion capture dataset. We show 30 predicted time steps of future movement, conditioned on 49 time steps that are provided as ground truth to the model. It can be seen that the model can capture the overall form of the movement with high precision. Mistakes (e.g. the misplaced toe node in frame 30) are possible due to the accumulation of small errors when predicting over long sequences with little chance of recovery. Curriculum learning schemes where noise is gradually added to training sequences can potentially alleviate this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. NBA visualizations</head><p>We show examples of three pick and roll trajectories in <ref type="figure" target="#fig_7">Fig.  11</ref>. In the left column we show the ground truth, in the middle we show our prediction and in the right we show the edges that where sampled by our encoder. As we can see even when our model does not predict the true future path, which is extremely challenging for this data, it still makes semantically reasonable predictions. For example in the middle row it predicts that the player defending the ball handler passes between him and the screener (going over the screen) which is a reasonable outcome even though in reality the defenders switched players.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Simulation data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Springs model</head><p>We simulate N ∈ {5, 10} particles (point masses) in a 2D box with no external forces (besides elastic collisions with the box). We randomly connect, with probability 0.5, each pair of particles with a spring. The particles connected by springs interact via forces given by Hooke's law F ij = −k(r i − r j ) where F ij is the force applied to particle v i by particle v j , k is the spring constant and r i is the 2D location vector of particle v i . The initial location is sampled from a Gaussian N (0, 0.5), and the initial velocity is a random vector of norm 0.5. Given the initial locations and velocity we can simulate the trajectories by solving Newton's equations of motion PDE. We do this by leapfrog integration using a step size of 0.001 and then subsample each 100 steps to get our training and testing trajectories.</p><p>We note that since the leapfrog integration is differentiable, we are able to use it as a ground-truth decoder and backpropagate through it to train the encoder. We implemented the leapfrog integration in PyTorch, which allows us to compare model performance with a learned decoder versus the ground-truth simulation decoder.</p><p>Ground truth NRI (learned) LSTM (joint) <ref type="figure">Figure 9</ref>. Qualitative comparison of model predictions for the LSTM (joint) model (left) and the NRI (learned) model (right). The ground truth trajectories (middle) are shown for reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Charged particles model</head><p>Similar to the springs model, we simulate N ∈ {5, 10} particles in a 2D box, but instead of springs now our particles carry positive or negative charges q i ∈ {±q}, sampled with uniform probability, and interact via Coulomb forces:</p><formula xml:id="formula_26">F ij = C • sign(q i • q j ) ri−rj ||ri−rj || 3</formula><p>where C is some constant. Unlike the springs simulations, here every two particles interact, although the interaction might be weak if they stay far apart, but they can either attract or repel each other.</p><p>Since the forces diverge when the distance between particles goes to zero, this can cause issues when integrating with a fixed step size. The problem might be solved by using a much smaller step size, but this would slow the generation considerably. To circumvent this problem, we clip the forces to some maximum absolute value. While not being exactly physically accurate, the trajectories are indistinguishable to a human observer and the generation process is now stable.</p><p>The force clipping does, however, create a problem for the simulation ground-truth decoder, as gradients become zero when the forces are clipped during the simulation. We attempted to fix this by using "soft" clipping with a softplus(x) = log(1 + e x ) function in the differentiable simulation decoder, but this similarly resulted in vanishing gradients once the model gets stuck in an unfavorable regime with large forces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Phase-coupled oscillators</head><p>The Kuramoto model is a nonlinear system of phase-coupled oscillators that can exhibit a range of complicated dynamics based on the distribution of the oscillators' internal frequencies and their coupling strengths. We use the common form for the Kuramoto model given by the following differential equation:</p><formula xml:id="formula_27">dφ i dt = ω i + j =i k ij sin(φ i − φ j )<label>(20)</label></formula><p>with phases φ i , coupling constants k ij , and intrinsic frequencies ω i . We simulate 1D trajectories by solving Eq. (20) with a fourth-order Runge-Kutta integrator with step size 0.01.</p><p>We simulate N ∈ {5, 10} phase-coupled oscillators in 1D with intrinsic frequencies ω i and initial phases φ t=1 i sampled uniformly from [1, 10) and [0, 2π), respectively. We randomly, with probability of 0.5, connect pairs of oscillators v i and v j (undirected) with a coupling constant k ij = 1. All other coupling constants are set to 0. We subsample the simulated φ i by a factor of 10 and create trajectories x i by concatenating dφi dt , sin φ i , and the intrinsic frequencies ω i (copied for every time step as ω i are static).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>We will describe here the details of our encoder and decoder implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Vectorized implementation</head><p>The message passing operations v→e and v→e can be evaluated in parallel for all nodes (or edges) in the graph and allow for an efficient vectorized implementation. More specifi-   cally, the node-to-edge message passing function f v→e can be vectorized as:</p><formula xml:id="formula_28">H 1 e = f e ([M in v→e H 1 v , M out v→e H 1 v ])<label>(21)</label></formula><p>with</p><formula xml:id="formula_29">H v = [h 1 , h 2 , . . . , h N ] ∈ R N ×F</formula><p>and H e ∈ R E×F defined analogously (layer index omitted), where F and E are the total number of features and edges, respectively. (•) denotes transposition. Both message passing matrices M v→e ∈ R E×N are dependent on the graph structure and can be computed in advance if the underlying graph is static. M in v→e is a sparse binary matrix with M in v→e,ij = 1 when the j-th node is connected to the i-th edge (arbitrary ordering) via an incoming link and 0 otherwise. M out v→e is defined analogously for outgoing edges.</p><p>Similarly, we can vectorize the edge-to-node message passing function f e→v as:</p><formula xml:id="formula_30">H 2 v = f v (M in e→v H 1 e )<label>(22)</label></formula><p>with M in e→v = (M in v→e ) . For large sparse graphs (e.g. by constraining interactions to nearest neighbors), it can be beneficial to make use of sparse-dense matrix multiplications, effectively allowing for an O(E) algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. MLP Encoder</head><p>The basic building block of our MLP encoder is a 2-layer MLP with hidden and output dimension of 256, with batch normalization, dropout, and ELU activations. Given this, the forward model for our encoder is given by the code snippet in <ref type="figure">Fig. 12</ref>. The node2edge module returns for each edge the concatenation of the receiver and sender features. The edge2node module accumulates all incoming edge features via a sum. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. CNN Encoder</head><p>The CNN encoder uses another block which performs 1D convolutions with attention. This allows for encoding with changing trajectory size, and is also appropriate for tasks like the charged particle simulations when the interaction can be strong for a small fraction of time. The forward computation of this module is presented in <ref type="figure">Fig. 13</ref> and the overall decoder in <ref type="figure">Fig.</ref> 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. MLP Decoder</head><p>In <ref type="figure">Fig. 15</ref> we present the code for a single time-step prediction using our MLP decoder for Markovian data. discrete latent edge types. Physical simulation and sports tracking experiments were run for 500 training epochs. For motion capture data we used 200 training epochs, as models tended to converge earlier. We saved model checkpoints after every epoch whenever the validation set performance (measured by path prediction MSE) improved and loaded the best performing model for test set evaluation. We observed that using significantly higher learning rates than 0.0005 often produced suboptimal decoders that ignored the latent graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Physics simulations experiments</head><p>The springs, charged particles and Kuramoto datasets each contain 50k training instances and 10k validation and test instances. Training and validation trajectories where of length 49 while test trajectories continue for another 20 time steps (50 for visualization). We train an MLP encoder for the springs experiment, and CNN encoder for the charged particles and Kuramoto experiments. All experiments used MLP decoders and two edge types. For the Kuramoto model experiments, we explicitly hard-coded the first edge type as a "non-edge", i.e. no messages are passed along edges of this type.</p><p>As noted previously, all of our MLPs have hidden and output dimension of 256. The overall input/output dimension of our model is 4 for the springs and charged particles experiments (2D position and velocity) and 3 for the Kuramoto model experiments (phase-difference, amplitude and intrinsic frequency). During training, we use teacher forcing in every 10-th time step (i.e. every 10th time step, the model receives a ground truth input, otherwise it receives its previous prediction as input). As we always have two edge types in these experiments and their ordering is arbitrary (apart from the Kuramoto model where we assign a special role to edge type 1), we choose the ordering for which the accuracy is highest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.1. BASELINES</head><p>Edge recovery experiments In edge recovery experiments, we report the following baselines along with the performance of our NRI (learned) model:</p><p>• Corr. (path): We calculate a correlation matrix R,</p><p>where</p><formula xml:id="formula_31">R ij = Cij √ CiiCjj</formula><p>with C ij being the covariance between all trajectories x i and x j (for objects v i and v j ) in the training and validation sets. We determine an ideal threshold θ so that A ij = 1 if R ij &gt; θ and A ij = 0 otherwise, based on predictive accuracy on the combined training and validation set. A ij denotes the presence of an interaction edge (arbitrary type) between object v i and v j . We repeat the same procedure for the absolute value of R ij , i.e. A ij = 1 if |R ij | &gt; θ and A ij = 0 otherwise. Lastly, we pick whichever of the two (θ or θ ) produced the best match with the ground truth graph (i.e. highest accuracy score) and report test set accuracy with this setting.</p><p>• Corr. (LSTM): Here, we train a two-layer LSTM with shared parameters and 256 hidden units that models each trajectory individually. It is trained to predict the position and velocity for every time step directly and is conditioned on the previous time steps. The input to the model is passed through a two-layer MLP (256 hidden units and ReLU activations) before it is passed to the LSTM, similarly we pass the LSTM output (last time step) through a two-layer MLP (256 hidden units and ReLU activation on the hidden layer). We provide ground truth trajectory information as input at every time step. We train to minimize MSE between model prediction and ground truth path. We train this model for 10 epochs and finally apply the same correlation matrix procedure as in Corr. (path), but this time calculating correlations between the output of the second LSTM layer at the last time step (instead of using the raw trajectory features). The LSTM is only trained on the training set. The optimal correlation threshold is estimated using the combined training and validation set.</p><p>• NRI (sim.): In this setting, we replace the decoder of the NRI model with the ground-truth simulator (i.e. the integrator of the Newtonian equations of motion). We implement both the charged particle and the springs simulator in PyTorch which gives us access to gradient information. We train the overall model with the same settings as the original NRI (learned) model by backpropagating directly through the simulator. We find that for the springs simulation, a single leap-frog integration step is sufficient to closely approximate the trajectory of the original simulation, which was generated with 100 leap-frog steps per time step. For the charged particle simulation, 100 leap-frog steps per time step are necessary to match the original trajectory when testing the simulation decoder in isolation. We find, however, that due to the force clipping necessary to stabilize the original charged particle simulation, gradients will often become zero, making model training difficult or infeasible.</p><p>• Supervised: For this baseline, we train the encoder in isolation and provide ground-truth interaction graphs as labels. We train using a cross-entropy error and monitor the validation accuracy (edge prediction) for model checkpointing. We train with dropout of p = 0.5 on the hidden layer representation of every MLP in the encoder model, in order to avoid overfitting.</p><p>Path prediction experiments Here, we use the following baselines along with our NRI (learned) model:</p><p>• Static: This baseline simply copies the previous state vector x t+1 = x t .</p><p>• LSTM (single): Same as the LSTM model in Corr. (LSTM), but trained to predict the state vector difference at every time step (as in the NRI model). Instead of providing ground truth input at every time step, we use the same training protocol as for an NRI model with recurrent decoder (see main paper).</p><p>• LSTM (joint): This baseline differs from LSTM (single) in that it concatenates the input representations from all objects after passing them through the input MLP. This concatenated representation is fed into a single LSTM where the hidden unit number is multiplied by the number of objects-otherwise same setting as LSTM (single). The output of the second LSTM layer at the last time step is then divided into vectors of same size, one for each object, and fed through the output MLP to predict the state difference for each object separately. LSTM (joint) is trained with same training protocol as the LSTM (single) model.</p><p>• NRI (full graph): For this model, we keep the latent graph fixed (fully-connected on edge type 2; note that edge types are exclusive, i.e. edges of type 1 are not present in this case) and train the decoder in isolation in the otherwise same setting as the NRI (learned) model.</p><p>• NRI (true graph): Here, we train the decoder in isolation and provide the ground truth interaction graph as latent graph representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Motion capture data experiments</head><p>Our extracted motion capture dataset has a total size of 8,063 frames for 31 tracked points each. We normalize all features (position/velocity) to maximum absolute value of 1. Training and validation set samples are 49 frames long (non-overlapping segments extracted from the respective trials). Test set samples are 99 frames long. In the main paper, we report results on the last 50 frames of this test set data.</p><p>We choose the same hyperparameter settings as in the physical simulation experiments, with the exception that we train models for 200 epochs and with a batch size of 8. Our model here uses an MLP encoder and an RNN decoder (as the dynamics are not Markovian). We further take samples from the discrete distribution during the forward pass in training and calculate gradients via the concrete relaxation. The baselines are identical to before (path prediction experiments for physical simulations) with the following exception: For LSTM (joint) we choose a smaller hidden layer size of 128 units and train with a batch size of 1, as the model did otherwise not fit in GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. NBA experiments</head><p>For the NBA data each example is a 25 step trajectory of a pick and roll (PnR) instance, subsampled from the original 25 frames-per-second SportVU data. Unlike the physical simulation where the dynamics of the interactions do not change over time and the motion capture data where the dynamics are approximately periodic, the dynamics here change considerably over time. The middle of the trajectory is, more or less, the pick and roll itself and the behavior before and after are quite different. This poses a problem for fair comparison, as it is problematic to evaluate on the next time steps, i.e. after the PnR event, since they are quite different from our training data. Therefore in test time we feed in the first 17 time-steps to the encoder and then predict the last 8 steps.</p><p>If we train the model normally as an autoencoder, i.e. feeding in the first N = 17 or 25 time-steps to the encoder and having the decoder predict the same N , then this creates a large difference between training and testing setting, resulting in poor predictive performance. This is expected, as a model trained with N = 17 never sees the post-PnR dynamics and the encoder trained with N = 25 has a much easier task than one trained on N = 17. Therefore in order for our training to be consistent with our testing, we feed during training the first 17 steps to the encoder and predict all 25 with the decoder.</p><p>We used a CNN encoder and RNN decoder with two edge types to have comparable capacity to the full graph model. If we "hard code" one edge type to represent "non-edge" then our model learns the full graph as all players are highly connected. We also experimented with 10 and 20 edge types which did not perform as well on validation data, probably due to over-fitting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Examples of trajectories used in our experiments from simulations of particles connected by springs (left), charged particles (middle), and phase-coupled oscillators (right).incorporation of prior beliefs (such as sparsity) and for an interpretable discrete structure with multiple relation types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Learned latent graphs on motion capture data (4 edge types) 4 . Skeleton shown for reference. Red arrowheads denote directionality of a learned edge. The edge type shown favors a specific hand depending on the state of the movement and gathers information mostly from other extremities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Distribution of learned edges between players (and the ball) in the basketball sports tracking (SportVU) data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Examples of predicted walking motion of an NRI model with learned latent graph compared to ground truth sequences for two different test set trials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Visualization of NBA trajectories. Left: ground truth; middle: model prediction; right: sampled edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>x = self.mlp1(x) # 2−layer ELU net per nodex = self.node2edge(x) x = self.mlp2(x) x skip = x x = self.edge2node(x) x = self.mlp3(x) x = self.node2edge(x) x = torch.cat((x, x skip), dim=2) x = self.mlp4(x) return self.fully connected out(x)Figure 12. PyTorch code snippet of the MLP encoder forward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>→e</figDesc><table><row><cell>Legend:</cell><cell>: Node emb.</cell><cell>: Edge emb.</cell><cell>: MLP</cell><cell cols="2">: Concrete distribution</cell><cell>: Sampling</cell></row><row><cell>e →v</cell><cell>v →e</cell><cell></cell><cell></cell><cell>v →e</cell><cell>e →v</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>for the f (...) functions. See supplementary material for further details.</figDesc><table><row><cell>While this model falls into the general framework presented</cell></row><row><cell>in Sec. 3, there is a conceptual difference in how h l (i,j)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Accuracy (in %) of unsupervised interaction recovery.</figDesc><table><row><cell>Model</cell><cell cols="3">Springs Charged Kuramoto</cell></row><row><cell></cell><cell cols="2">objects</cell><cell></cell></row><row><cell>Corr. (path)</cell><cell>52.4±0.0</cell><cell>55.8±0.0</cell><cell>62.8±0.0</cell></row><row><cell cols="2">Corr. (LSTM) 52.7±0.9</cell><cell>54.2±2.0</cell><cell>54.4±0.5</cell></row><row><cell>NRI (sim.)</cell><cell>99.8±0.0</cell><cell>59.6±0.8</cell><cell>-</cell></row><row><cell cols="3">NRI (learned) 99.9±0.0 82.1±0.6</cell><cell>96.0±0.1</cell></row><row><cell>Supervised</cell><cell>99.9±0.0</cell><cell>95.0±0.3</cell><cell>99.7±0.0</cell></row><row><cell></cell><cell cols="2">objects</cell><cell></cell></row><row><cell>Corr. (path)</cell><cell>50.4±0.0</cell><cell>51.4±0.0</cell><cell>59.3±0.0</cell></row><row><cell cols="2">Corr. (LSTM) 54.9±1.0</cell><cell>52.7±0.2</cell><cell>56.2±0.7</cell></row><row><cell>NRI (sim.)</cell><cell>98.2±0.0</cell><cell>53.7±0.8</cell><cell>-</cell></row><row><cell cols="3">NRI (learned) 98.4±0.0 70.8±0.4</cell><cell>75.7±0.3</cell></row><row><cell>Supervised</cell><cell>98.8±0.0</cell><cell>94.6±0.2</cell><cell>97.1±0.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/ethanfetaya/nri</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank the Toronto Raptors and the NBA for the use of the SportVU data. We would further like to thank Christos Louizos and Elise van der Pol for helpful discussions. This project is supported by the SAP Innovation Center Network.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p># CNN block # inputs is of shape ExFxT, E: number of edges, # T: sequence length, F: num. features x = F.relu(self.conv1(inputs)) x = self.batch norm1(x) x = self.pool(x) x = F.relu(self.conv2(x)) x = self.batch norm2(x) out = self.conv out(x) attention = softmax(self.conv attn(x), axis=2) out = (out * attention).mean(dim=2) return out <ref type="figure">Figure 13</ref>. PyTorch code snippet of the CNN block forward pass, used in the CNN encoder.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5. RNN Decoder</head><p>The RNN decoder adds a GRU style update to the single step prediction, the code snippet for the GRU module is presented in <ref type="figure">Fig. 16</ref> and the overall RNN decoder in <ref type="figure">Fig. 17</ref>.</p><p># GRU block # Takes arguments: inputs, agg msgs, hidden r = F.sigmoid(self.input r(inputs) + self.hidden r(agg msgs)) i = F.sigmoid(self.input i(inputs) + self.hidden i(agg msgs)) n = F.tanh(self.input n(inputs) + r * self.hidden h(agg msgs)) hidden = (1 − i) * n + i * hidden return hidden  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment details</head><p>All experiments were run using the Adam optimizer <ref type="bibr" target="#b22">(Kingma &amp; Ba, 2015</ref>) with a learning rate of 0.0005, decayed by a factor of 0.5 every 200 epochs. Unless otherwise noted, we train with a batch size of 128. The concrete distribution is used with τ = 0.5. During testing, we replace the concrete distribution with a categorical distribution to obtain</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Social LSTM: human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational lossy autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Oneshot imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Investigating causal relations by econometric models and cross-spectral methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Permutation-equivariant neural networks applied to dynamics prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guttenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Virgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Witkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kanai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mapping images to scene graphs with permutation-invariant structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raboh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1802.05451" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vain: Attentional multi-agent predictive modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riley</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auto encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-entrainment of a population of coupled nonlinear oscillators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuramoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Mathematical Problems in Theoretical Physics</title>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="420" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Python implementation of Kuramoto systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laszuk</surname></persName>
		</author>
		<ptr target="http://www.laszukdawid.com/codes" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Coordinated multiagent imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discovering latent network structure in point process data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian latent structure discovery from multi-neuron recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G T</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Relational neural expectation maximization: Unsupervised discovery of objects and their interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems (NIPS)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual interaction networks: Learning a physics simulator from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
