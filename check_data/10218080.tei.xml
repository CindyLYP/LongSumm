<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Inference for Monte Carlo Objectives</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
							<email>amnih@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
							<email>danilor@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">Variational Inference for Monte Carlo Objectives</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2016) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multisample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator (Mnih &amp; Gregor, 2014) proposed for the single-sample variational objective, and is competitive with the currently used biased estimators.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Directed latent variable models parameterized using neural networks have recently enjoyed a surge in popularity due to the recent advances in variational inference methods that made it possible to train such models efficiently. These methods <ref type="bibr" target="#b9">(Kingma &amp; Welling, 2014;</ref> Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48. Copyright 2016 by the author(s). <ref type="bibr">2014</ref>; <ref type="bibr" target="#b10">Mnih &amp; Gregor, 2014)</ref> approximate the intractable posterior of the model with a variational posterior parameterized using a neural network and maximize a lower bound on the intractable marginal log-likelihood, estimating the required gradients using samples from the variational posterior. This approach implements an efficient feedforward approximation to the expensive iterative process required by traditional variational inference methods for each data point.</p><p>One important weakness of variational methods is that training a powerful model using an insufficiently expressive variational posterior can cause the model to use only a small fraction of its capacity. The most direct route to addressing this issue is to develop more expressive but still tractable variational posteriors as was done in <ref type="bibr" target="#b18">(Salimans et al., 2015;</ref><ref type="bibr" target="#b15">Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b4">Gregor et al., 2015)</ref>.</p><p>However, the crippling effect of an excessively simple posterior on the model can alternatively be seen as a consequence of the form of the lower bound optimized by the variational methods <ref type="bibr" target="#b2">(Burda et al., 2016)</ref>. As the bound is based on a single-sample estimate of the marginal likelihood of the observation, it heavily penalizes samples that explain the observation poorly and thus produce low estimates of the likelihood. As result, the variational posterior learns to cover only the high-probability areas of the true posterior, which in turn assumes a simpler shape which is easier to approximate by the variational posterior. A simple way to minimize this effect is to average over multiple samples when computing the marginal likelihood estimate. The resulting lower bound on the log-likelihood gets tighter as the number of samples increases <ref type="bibr" target="#b2">(Burda et al., 2016)</ref>, converging to the true value in the limit of infinitely many samples. We will refer to such objectives derived from likelihood estimates computed by averaging over independent samples as Monte Carlo objectives. When using an objective that averages over multiple samples, the distribution for generating samples no longer explicitly represents the variational posterior and instead is thought of as a proposal distribution due to connections to importance sampling.</p><p>Multi-sample objectives of this type have been used for arXiv:1602.06725v2 <ref type="bibr">[cs.</ref>LG] 1 Jun 2016 generative modelling <ref type="bibr" target="#b1">(Bornschein &amp; Bengio, 2015;</ref><ref type="bibr" target="#b2">Burda et al., 2016)</ref>, structured output prediction <ref type="bibr" target="#b13">(Raiko et al., 2015)</ref>, and models with hard attention <ref type="bibr" target="#b0">(Ba et al., 2015)</ref>. As a multi-sample objective is a better proxy for the loglikelihood than a single-sample one, models trained using multi-sample objectives are likely to achieve better loglikelihoods. This has been empirically demonstrated in the context of generative models by <ref type="bibr" target="#b2">Burda et al. (2016)</ref> and <ref type="bibr" target="#b1">Bornschein &amp; Bengio (2015)</ref>, who also showed that using more samples in the objective increased the number of latent variables used in the deeper layers.</p><p>Unfortunately, unless all the latent variables in the model are continuous, learning the proposal distribution with a multi-sample objective is difficult as the gradient estimator obtained by differentiating the objective has very high variance. As a result, with the exception of <ref type="bibr" target="#b2">Burda et al. (2016)</ref>, who used an alternative estimator available for continuous latent variables, none of the above methods update the parameters of the proposal distribution by following the gradient of the multi-sample objective. Thus, updates for the proposal distribution and the model parameters in these methods are not optimizing the same objective function, which can lead to suboptimal performance and even prevent convergence.</p><p>In this paper we develop a new unbiased gradient estimator for multi-sample objectives that replaces the single learning signal of the naive estimator with much lower variance per-sample learning signals. Unlike the NVIL estimator <ref type="bibr" target="#b10">(Mnih &amp; Gregor, 2014)</ref> designed for single-sample variational objectives, our estimator does not require learning any additional parameters for variance reduction. We expect that the availability of an effective unbiased gradient estimator will make it easier to integrate models with discrete latent variables into larger systems that can be trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multi-sample stochastic lower bounds</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Estimating the likelihood</head><p>Suppose we would like to fit an intractable latent variable model P (x, h) to data. As the intractability of inference rules out using maximum likelihood estimation, we will proceed by maximizing a lower bound on the loglikelihood. One general way to derive such a lower bound is to start with an unbiased estimatorÎ of the marginal likelihood P (x) and then transform it. We will consider estimators of the formÎ(h 1:</p><formula xml:id="formula_0">K ) = 1 K K i=1 f (x, h i ) where h 1 , .</formula><p>.., h K are independent samples from some distribution Q(h|x) which can potentially depend on the observation x. Before showing how to transform such an estimator into a bound, let us consider some possible choices for the likelihood estimator.</p><p>Perhaps the simplest estimator of this form can be constructed by sampling h i 's from the prior P (h) and averaging the resulting conditional likelihoods:</p><formula xml:id="formula_1">I(h 1:K ) = 1 K K i=1 P (x|h i ) with h i ∼ P (h).<label>(1)</label></formula><p>While this estimator is unbiased, it can have very high variance in models where most latent configurations do not explain a given observation well. For such models, the estimator will greatly underestimate the likelihood for most sets of K independent samples and substantially overestimate it for a small number of such sets. This is a consequence of not taking into account the observation we would like the latent variables to explain when sampling them.</p><p>We can incorporate the information about the observation we are estimating the likelihood for by sampling the latents from a proposal distribution Q(h|x) conditional on the observation x and using importance sampling:</p><formula xml:id="formula_2">I(h 1:K ) = 1 K K i=1 P (x, h i ) Q(h i |x)<label>(2)</label></formula><p>with</p><formula xml:id="formula_3">h 1:K ∼ Q(h 1:K |x) ≡ K i=1 Q(h i |x)</formula><p>. In addition to also being unbiased, the variance of this estimator can be much lower than that of the preceding one because it can assign high probability to the latent configurations with high joint probability with the given observation. In fact, if we were able to use the true posterior as the proposal distribution, the estimator would have zero variance. While this is infeasible for the models we are considering, this fact suggests that making the proposal distribution close to the posterior is a sensible strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Lower-bounding the log-likelihood</head><p>Having chosen an estimatorÎ for the likelihood, we can obtain an estimatorL of a lower bound on the log-likelihood simply by taking the logarithm ofÎ. We can justify this by applying Jensen's inequality:</p><formula xml:id="formula_4">E Q(h 1:K |x) logÎ(h 1:K ) ≤ log E Q(h 1:K |x) Î (h 1:K ) = log P (x),</formula><p>where the equality follows from the fact that sinceÎ is unbiased, E Q(h 1:K |x) [Î(h 1:K )] = P (x). Therefore, we can think ofL(h 1:K ) = logÎ(h 1:K ) as a stochastic lower bound on the log-likelihood <ref type="bibr" target="#b2">(Burda et al., 2016)</ref>.</p><p>We note that this approach is not specific to the to estimators from Section 2.1 and can be used with any unbiased likelihood estimator based on random sampling. Thus it might be possible to obtain better lower bounds by using methods from the importance sampling literature such as control variates and adaptive importance sampling. Despite the potential pitfalls described above, estimators involving sampling from the prior have been used successfully for training models for structured output prediction <ref type="bibr" target="#b20">(Tang &amp; Salakhutdinov, 2013;</ref><ref type="bibr" target="#b3">Dauphin &amp; Grangier, 2016)</ref> and models with hard attention <ref type="bibr" target="#b22">Zaremba &amp; Sutskever, 2015)</ref>.</p><p>The multi-sample (K &gt; 1) version of the above estimator has recently been used for variational training of latent variable models <ref type="bibr" target="#b2">(Burda et al., 2016;</ref><ref type="bibr" target="#b1">Bornschein &amp; Bengio, 2015)</ref> as well as models with hard attention <ref type="bibr" target="#b0">(Ba et al., 2015)</ref>. The single-sample version of the estimator yields the classical variational lower bound <ref type="bibr" target="#b7">(Jordan et al., 1999)</ref> </p><formula xml:id="formula_5">L(x) = E Q(h|x) log P (x, h) Q(h|x) ,<label>(3)</label></formula><p>which is used as the objective in much of the recent work on training generative models <ref type="bibr" target="#b9">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b16">Rezende et al., 2014;</ref><ref type="bibr" target="#b10">Mnih &amp; Gregor, 2014)</ref>.</p><p>The advantage of using a multi-sample stochastic lower bound is that increasing the number of samples K is guaranteed to make the bound tighter <ref type="bibr" target="#b2">(Burda et al., 2016)</ref>, thus making it a better proxy for the log-likelihood. Intuitively, averaging over the K samples inside the log, removes the burden of every sample having to explain the observation well, which leads to the proposal distribution being considerably less concentrated than the variational posterior, which is its single-sample counterpart. Training models by optimizing a multi-sample objective can be seen as a generalization of variational training that does not explicitly represent the variational posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Objective</head><p>Thus we will be interested in training models by maximizing objectives of the form</p><formula xml:id="formula_6">L K (x) =E Q(h 1:K |x) log 1 K K i=1 f (x, h i ) ,<label>(4)</label></formula><p>which can be seen as lower bounds on the log-likelihood. This class of objectives is a rich one, including the ones used in variational inference, generative modelling, structured prediction, and hard attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Gradient analysis</head><p>In this section we will analyze the gradient of the objective w.r.t. the parameters of the model and the proposal distribution and explain why developing an effective unbiased estimator for the gradient is difficult in general. In the special case of continuous latent variables an alternative approach to gradient estimation based on reparameterization <ref type="bibr" target="#b9">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b2">Burda et al., 2016</ref>) is likely to be preferable to the more general approach we follow in this paper, which is applicable to all types of latent variables.</p><p>As shown in the supplementary material, differentiating L K (x) w.r.t. the parameters θ of Q and f gives</p><formula xml:id="formula_7">∇ θ L K (x) =E Q(h 1:K |x) jL (h 1:K )∇ θ log Q(h j |x) + E Q(h 1:K |x) jw j ∇ θ log f (x, h j ) ,<label>(5)</label></formula><p>wherew</p><formula xml:id="formula_8">j ≡ f (x,h j ) K i=1 f (x,h i )</formula><p>. As our objective L K (x) is an expectation of the stochastic lower boundL(h 1:K ) w.r.t. to the proposal distribution, it can depend on any given parameter through the proposal distribution, through the value of the stochastic lower bound as a function of a set of K samples, or both. Intuitively, the first and the second terms in Eq. 5 capture the effect of θ on L K (x) though its effect on the proposal distribution and the value of the stochastic lower bound as a function of a set of samples respectively.</p><p>Let us inspect these two terms, both of which are linear combinations of the gradients corresponding to the K samples. The second term is well-behaved and is easy to estimate because weights {w j } are non-negative and sum to 1, ensuring that the norm of the linear combination of the gradients is at most as large as the norm of the largest of the K gradients. In mixture modelling terms, we can think ofw j as the responsibility of sample j for the observation xa measure of how well sample j explains the observation compared to the other K − samples.</p><p>The first term however is considerably more problematic for two reasons. First, the gradients for all K samples are multiplied by the same scalarL(h 1:K ), which can be thought of as the learning signal for the proposal distribution <ref type="bibr" target="#b10">(Mnih &amp; Gregor, 2014)</ref>. As a result, the gradient for a sample that explains the observation well is not given any more weight than the gradient for a sample in the same set of K that explains the observation poorly. This means that the first term does not implement credit assignment within each set of K samples, unlike the second term which achieves that by weighting the gradients using the responsibilities. Thus the learning signal for each sample h i will have high variance, making learning slow.</p><p>Another important source of variance when estimating the first term is the magnitude of the learning signal. Unlike the responsibilities used in the second term, which are between 0 and 1, the learning signal can have potentially unbounded magnitude, which means that the norm of the first term can become much larger than the norm of any of the individual sample gradients. This issue can be especially pronounced early in training, when all samples from the proposal Q explain the data poorly, resulting in a very smallÎ(h 1:K ) and</p><p>Despite not performing credit assignment within sets of K samples, the first term does perform correct credit assignment in expectation over such sets. thus a very negative learning signal. Thus unless special measures are taken, the first term in the gradient will overwhelm the second term and make the overall estimate very noisy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Gradient estimation</head><p>The difficulties described in the previous section affect only the gradient for the parameters the sampling distribution depends on. For all other parameters ψ the first term is identically zero, which leaves only the second, wellbehaved term. As a result, the following naive Monte Carlo estimator based on a single set of K samples works well:</p><formula xml:id="formula_9">∇ ψ L K (x) jw j ∇ ψ log f (x, h j ),<label>(6)</label></formula><p>where h i ∼ Q(h|x). While it is possible to reduce the variance of the estimator by averaging over multiple sets of samples, in this paper we follow the common practice of using a single set and relying on averaging over the training cases in a minibatch to reduce the variance to a reasonable level instead. We will now turn out attention to the more challenging problem of estimating gradients for parameters that affect the proposal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1.">NAIVE</head><p>We will start with the simplest estimator, also based on naive Monte Carlo:</p><formula xml:id="formula_10">∇ θ L K (x) jL (h 1:K )∇ θ log Q(h j |x) + jw j ∇ θ log f (x, h j ),<label>(7)</label></formula><p>with h i ∼ Q(h|x). This estimator does not attempt to eliminate either of the two sources of variance described in Section 2.4 and we include it here for completeness only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2.">WITH BASELINES (NVIL)</head><p>One simple way to reduce the variance due the large magnitude of the learning signal is to reduce its magnitude by subtracting a quantity, called a baseline, correlated with the learning signal but not dependent on the latent variables. This transformation of the learning signal leaves the gradient estimator unbiased because it amounts to subtracting a term which has the expectation of 0 under the proposal distribution. In our use of baselines, we will follow the Neural Variational Inference and Learning (NVIL, Mnih &amp; Gregor, 2014) method for training generative models, which is based on optimizing the classical variational lower bound (Eq. 3). The main idea behind the NVIL estimator is to reduce the magnitude of the learning signal for the parameters of the variational distribution (which is the single-sample counterpart of our proposal distribution) by subtracting two baselines from it: a constant baseline b and an input-dependent one b(x).</p><p>The following estimator is a straightforward adaptation of the same idea to multi-sample objectives:</p><formula xml:id="formula_11">∇ θ L K (x) j (L(h 1:K ) − b(x) − b)∇ θ log Q(h j |x) + jw j ∇ θ log f (x, h j ),<label>(8)</label></formula><p>with h i ∼ Q(h|x). The constant baseline b tracks the mean of the learning signal, while the input-dependent one is fit to minimize the squared residual of the learning signal L(h 1:K ) − b(x) − b, with the goal of capturing the effect of the observation on the magnitude of the learning signal. We implement the input dependent baseline using a one-hidden layer neural network.</p><p>While introducing baselines can addresses the estimator variance due to the large magnitude of the learning signal, it has no effect on the variance resulting from having the same learning signal for all samples in a set of K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3.">PER-SAMPLE LEARNING SIGNALS</head><p>We can reduce the effect of the second source of variance by defining a different local learning signal for each sample in a way that minimizes its dependence on the other samples in the set. This can be accomplished by using a separate baseline for each sample that depends on the value of all other samples and eliminates much of the variance due to them. We will now show that this approach does not bias the resulting estimator.</p><p>Let h −j denote the set of K − 1 samples obtained by leaving out sample j from the original set. Since the samples in a set are independent, evaluating the expectations with respect to them in any order produces the same result. Thus the contribution of sample j to the first term in Eq. 5 can be expressed as</p><formula xml:id="formula_12">E Q(h 1:K |x) L (h 1:K )∇ θ log Q(h j |x) = E Q(h −j |x) E Q(h j |x) L (h 1:K )∇ θ log Q(h j |x) h −j .</formula><p>Since in the inner-most expectation all samples except for h j are conditioned on, adding any function of them to the learning signal for h j has no effect on the value of the expectation. Thus we can define a baseline that depends on h −j in addition to x. We would like this baseline to be as close toL(h 1:K ) as possible without using the value of h j .</p><p>Inspecting the global learning signalL(h 1:K ) (Eq. 4) suggests that we can obtain an effective baseline for the learning signal for sample j by replacing f (x, h j ) in it by some quantity close to it but independent of h j . We could, for example, use some mapping f (x) trained to predict f (x, h i ) from the observation x. This gives rise to the following local learning signal for sample j:</p><formula xml:id="formula_13">L(h j |h −j ) =L(h 1:K ) − log 1 K i =j f (x, h i ) + f (x) .</formula><p>For K = 1, this estimator becomes essentially equivalent to the NVIL estimator, with log f (x) corresponding to the input-dependent baseline b(x).</p><p>We can avoid having to learn an additional mapping by taking advantage of the fact that we have more than one sample in a set. Since the samples in a set are IID, so are the corresponding values f (x, h i ), which means that we can get a reasonable estimatê f (x, h −j ) by combining the f (x, h i ) values for all the other samples in the set using averaging of some sort. We experimented with using the arithmetic mean</p><formula xml:id="formula_14">(f (x, h −j ) = 1 K−1 i =j f (x, h i )) and the geometric mean (f (x, h −j ) = exp 1 K−1 i =j log f (x, h i ) ) and</formula><p>found that the geometric mean worked slightly better. The resulting local learning signals can be written aŝ</p><formula xml:id="formula_15">L(h j |h −j ) = (9) L(h 1:K ) − log 1 K i =j f (x, h i ) +f (x, h −j ) .</formula><p>This approach to variance reduction, unlike the one above or NVIL, does not require learning any additional parameters for performing variance reduction. Moreover, as the total cost of computing the per-sample learning signals in Eq. 9 is of the same order as that of computing of the global learning signal, this approach allows us to implement effective variance reduction in the multi-sample case essentially at no cost. This approach relies on having more than one sample for the same observation, however, and so is not applicable in the single-sample setting.</p><p>The final estimator has the form</p><formula xml:id="formula_16">∇ θ L K (x) jL (h j |h −j )∇ θ log Q(h j |x) + jw j ∇ θ log f (x, h j ).<label>(10)</label></formula><p>We will refer to this estimator as the VIMCO (Variational Inference for Monte Carlo Objectives) estimator. The pseudocode for computing it is provided in the supplementary material. This estimator is a black-box one, in the sense that it can be easily applied to any model for which we can compute the complete log-likelihood log P (x, h) and its parameter gradients exactly. As such, it can be seen as as an alternative to Black Box Variational Inference (Ranganath et al., 2014) and NVIL, specialized for multi-sample objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Structured output prediction</head><p>Structured output prediction (SOP) is a type of supervised learning with high-dimensional outputs with rich structure such as images or text. The particular emphasis of SOP is on capturing the dependencies between the output variables in addition to capturing their dependence on the inputs.</p><p>Here we will take the approach of viewing SOP as conditional probabilistic modelling with latent variables <ref type="bibr" target="#b20">(Tang &amp; Salakhutdinov, 2013;</ref><ref type="bibr" target="#b19">Sohn et al., 2015)</ref>.</p><p>To stay consistent with the terminology for generative models we used so far, we will refer to inputs as contexts and to outputs as observations Thus, given a set of context/observation pairs (c, x), we would like to fit a latent variable model P (x, h|c) to capture the dependencies between the contexts and the observations, as well as those between the observed dimensions. Typically such a model factorizes as P (x, h|c) = P (x|h, c)P (h|c), with both the conditional likelihood and the prior terms being conditional on the context. Thus, this is essentially the same setting as for generative modelling, with the only difference being that every distribution now also conditions on the context c, which makes it straightforward to apply the estimators we presented.</p><p>However, historically such models have been trained using samples from the prior P (h|c), with the gradients computed using either importance sampling <ref type="bibr" target="#b20">(Tang &amp; Salakhutdinov, 2013)</ref> or heuristic rules for backpropagating through binary units <ref type="bibr" target="#b13">(Raiko et al., 2015)</ref>. Since using the prior as the proposal distribution does not allow it to use the information about the observation, such methods tend to require a large number of samples to perform well. Though variational training has been applied recently to SOP models with continuous latent variables <ref type="bibr" target="#b19">(Sohn et al., 2015)</ref>, we are not aware of any work that uses a learned proposal distribution conditional on the observations to train SOP models with multi-sample objectives. We will explore the effectiveness of using this approach in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related work</head><p>Multi-sample objectives: The idea of using a multisample objective for latent variable models was proposed by <ref type="bibr" target="#b13">Raiko et al. (2015)</ref>, who thought of it not as a lower bound on the log-likelihood but an objective in its own right. They evaluated several gradient estimators at optimizing it for training structured prediction models and showed that a simple biased estimator emulating backpropagation performed best. <ref type="bibr" target="#b20">Tang &amp; Salakhutdinov (2013)</ref> proposed an estimator based on importance sampling for an EM-like bound on the log-likelihood using samples from the prior. This is also a biased estimator as it relies on selfnormalized importance sampling to approximate the posterior using a set of weighted samples. <ref type="bibr" target="#b2">Burda et al. (2016)</ref> pointed out that the multi-sample objective of <ref type="bibr" target="#b13">Raiko et al. (2015)</ref> was a tighter lower bound on the log-likelihood than the single-sample variational lower bound and presented a method for training variational autoencoders by optimizing this multi-sample objective. Their method relies on an unbiased gradient estimator which can be used only for mod-els with continuous latent variables.</p><p>Reweighted Wake Sleep: Though the Reweighted Wake Sleep algorithm (RWS, <ref type="bibr" target="#b1">Bornschein &amp; Bengio, 2015)</ref> for training generative models has been derived from the perspective of approximating the log-likelihood gradients using importance sampling, it is closely related to the bound optimization approach we follow in this paper. <ref type="bibr" target="#b2">Burda et al. (2016)</ref> have shown that the RWS gradient estimator for the model parameters is identical to the one given by Eq. 6, which means that the RWS model parameter update aims to maximize the lower bound on the log-likelihood based on the multi-sample importance sampling estimator from Eq. 2. RWS performs two types of updates for the proposal distribution parameters, the first of which, called the wake update, is based on the same weights {w j } as the model parameter update</p><formula xml:id="formula_17">∆θ ∝ jw j ∇ θ log Q(h j |x)<label>(11)</label></formula><p>and is motivated as a (biased) estimator of the gradient KL(P (h|x)||Q(h|x)). Its bias decreases with the increasing number of samples, vanishing in the limit of infinitely many samples.</p><p>The second update, called the sleep update, having the form</p><formula xml:id="formula_18">∆θ ∝ ∇ θ log Q(h|x),<label>(12)</label></formula><p>is based on a sample (x, h) from the model and comes from the original Wake-Sleep algorithm <ref type="bibr" target="#b6">(Hinton et al., 1995)</ref>. The wake update tends to work better than the sleep update, and using the two updates together works even better <ref type="bibr" target="#b1">(Bornschein &amp; Bengio, 2015)</ref>. As neither of these updates appears to be related to the lower bound optimized the model parameter update, RWS does not seem to optimize a well-defined objective, a feature it shares with the original Wake-Sleep algorithm. Despite this theoretical weakness RWS works well in practice, outperforming original Wake-Sleep and NVIL, which are single-sample algorithms, using as few as 5 samples per observation.</p><p>Black Box Methods: As our approach does not assume anything about the structure of the model or the distribution(s) of its latent variables, it can be seen as a black box method for multi-sample objectives. A number of black box methods have been developed for the classical variational objective, usually based around unbiased gradient estimators for the proposal distribution. Black Box Variational Inference <ref type="bibr">(BBVI Ranganath et al., 2014)</ref> and NVIL <ref type="bibr" target="#b10">(Mnih &amp; Gregor, 2014)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We evaluate the effectiveness of the proposed approach at training models for generative modelling and structured output prediction. We chose these two tasks because they involve models with hundreds of latent variables, which poses formidable challenges when estimating the gradients for the proposal distributions. In both cases we compare the performance of the VIMCO estimator to that of the NVIL estimator as well as to an effective biased estimator from the literature. We experiment with varying the number of samples in the objective to see how that affects the performance of the resulting models when using different estimators. The details of the training procedure are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Generative modelling</head><p>We start by applying the proposed estimator to training generative models, concentrating on sigmoid belief networks (SBN) <ref type="bibr" target="#b12">(Neal, 1992)</ref> which consist of layers of binary latent variables. SBNs have been used to evaluate a number of variational training methods for models with discrete latent variables <ref type="bibr" target="#b10">(Mnih &amp; Gregor, 2014;</ref><ref type="bibr" target="#b1">Bornschein &amp; Bengio, 2015;</ref><ref type="bibr" target="#b5">Gu et al., 2016)</ref>.</p><p>Our first comparison is on the MNIST dataset of × 28 images of handwritten digits, using the binarization of <ref type="bibr" target="#b17">Salakhutdinov &amp; Murray (2008)</ref> and the standard 50000/10000/10000 split into the training, validation, and test sets. We use an SBN with three hidden layers of 200 binary latent variables <ref type="bibr">(200-200-200-768)</ref> as the generative model. The proposal distribution is parameterized as an SBN with the same architecture but going in the opposite direction, from the observation to the deepest hidden layer <ref type="bibr">(768-200-200-200)</ref>.</p><p>As our primary goal is here is to see how well the VIMCO estimator performs at optimizing the multisample objective, we train the above model using each of the VIMCO, NVIL, and RWS estimators to optimize the lower bound (Eq. 4) based on 2, 5, 10, and 50 samples (K). To match the computational complexity of the other two estimators, we used only the better-performing wake update for the proposal distribution in RWS. We also trained the model by optimizing the classical variational objective (K = 1) using NVIL to serve as a single-sample baseline. In all cases, the model parameter gradients were estimated using Eq. 6.  <ref type="figure">Figure 1</ref>. Generative modelling: Comparison of multi-sample objective on the validation set for the SBNs trained on MNIST using VIMCO and those trained using (Left) NVIL and (Right) Reweighted Wake Sleep. The number in brackets specifies the number of samples used in the training objective. <ref type="figure">Figure 1</ref> shows the evolution of the training objective on the validation set as training proceeds. From the left plot, which compares the models trained using VIMCO to those trained NVIL, it is apparent that VIMCO is far more effective than NVIL at optimizing the multi-sample objective and benefits much more from using more samples. NVIL performance improves slightly when using a modest number of samples before starting to degrade upon reaching K = 10. The right plot shows the comparison between VIMCO and RWS. The two methods perform similarly, with VIMCO performing better when using 2 samples and RWS learning slightly faster when using more samples.</p><p>Having selected the best model for each method/number of samples combination based on its validation score, we estimated its negative log-likelihood on the test set using 1000 proposal samples for each data point. The results in <ref type="table">Table  show</ref> that VIMCO and NVIL perform slightly better than RWS for 2 samples. However, as the number of samples increases, VIMCO and RWS performance steadily improves while NVIL performance stays virtually the same until reaching K = 50, when it becomes markedly worse. Overall, RWS and VIMCO perform similarly, though VIMCO seems to have a slight edge over RWS for all numbers of samples we considered.</p><p>We also investigated the effectiveness of VIMCO and NVIL variance reduction techniques more directly, by monitoring the magnitude of their learning signals during training. While VIMCO and NVIL performed comparably when using the 2-sample objective, VIMCO benefited much more from using more samples. For the 10-sample objective, the average magnitude of the VIMCO learning signal was 3 times lower than that of NVIL. More details are given in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Structured output prediction</head><p>In the second set of experiments we evaluated the proposed estimator at training structured output prediction models.</p><p>We chose a task that has been used as a benchmark for evaluating gradient estimators for models with binary latent variables by <ref type="bibr" target="#b13">Raiko et al. (2015)</ref> and <ref type="bibr" target="#b5">Gu et al. (2016)</ref>, which involves predicting the lower half of an MNIST digit from its top half. We trained two SBN models, one with two and one with three layers of 200 binary latent variables between the 392-dimensional (14 × 28) input and output layers. We use the same binarized MNIST dataset for this task as for the generative modelling experiments in Section 5.1.</p><p>We consider two different kinds of proposal distributions for training the models. In the first case, we follow the standard practice for training structured output prediction models and use the model prior as the proposal distribution. However, as the prior does not have access to the observation information which is available during training, most of the resulting samples are unlikely to explain the observation well, potentially leading to inefficient use of samples and unnecessarily noisy learning signal. Hence, in the second case we learn a separate proposal distribution that takes both the context and the observation halves of the image as input. We parameterize the proposal distribution using an SBN with the same structure as the prior except that the last  <ref type="figure">Figure 2</ref>. Structured output prediction: Comparison of multi-sample objective on the validation set for a 3-hidden-layer SBN trained with VIMCO against those trained with NVIL using sampling from (Left) the prior and (Right) the learned proposal distribution. The number in brackets specifies the number of samples used in the training objective.</p><p>layer of latent variables in addition to being conditioned on the preceding layer is also conditioned on the observation.</p><p>We train the models with VIMCO and NVIL using 2, 5, 20, and 50 sample objectives. As in the previous experiment, we also train single-sample baseline models using both types of proposals with NVIL (and K = 1). <ref type="figure">Figure 2</ref> shows the resulting multi-sample bound values for the three-layer models on the validation set as a function of the number of parameter updates. The left plot, containing the results for models trained by sampling from the prior, shows that model performance improves dramatically as the number of samples is increased. Though NVIL with 1 or 2 samples, performs better than VIMCO with 2 samples, as the number of samples increases their roles reverse, with VIMCO making much faster progress than NVIL for 20 and 50 samples. The fact that increasing the number of samples has such an effect on model performance strongly suggests that samples generated from the prior rarely explain the observation well.</p><p>The right plot on <ref type="figure">Figure 2</ref> shows the result of training with a learned proposal distribution. It is clear that using a learned proposal leads to drastic improvement for all method / number of samples combinations. In fact, the worst result obtained using a learned proposal distribution is better than the best result obtained by sampling from the prior. In terms of relative performance, the story here is similar to that from the generative modelling experiment: VIMCO performs better than NVIL and benefits much more from increasing the number of samples. The gap between the methods is considerably smaller here, likely due to the task being easier. Inspecting the conditional digit completions sampled from the models shows that the models trained using a learned proposal distribution capture multimodality inherent in the task very well. We show conditional completions from a three-layer model trained using VIMCO with 20 samples in the supplementary material.</p><p>Finally, to compare to the results of <ref type="bibr" target="#b13">Raiko et al. (2015)</ref>, we followed their evaluation protocol and estimated the negative log-likelihoods for the trained models using 100 samples. Their best result on this task was 53.8 nats, obtained using a 2-layer SBN trained using a biased estimator emulating backprop to optimize the 20-sample objective. With VIMCO training, the same model achieves 56.5 nats using the prior as the proposal and 46.1 nats with a learned proposal, which is the first sub-50 nat result on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this paper we introduced VIMCO, the first unbiased general gradient estimator designed specifically for multisample objectives that generalize the classical variational lower bound. By taking advantage of the structure of the objective function, it implements simple and effective variance reduction at no extra computational cost, eliminating the need for the learned baselines relied on by other general unbiased estimators such as NVIL.</p><p>We demonstrated the effectiveness of VIMCO by applying it to variational training of generative and structured output prediction models. It consistently outperformed NVIL and was competitive with the currently used biased estimators.</p><p>While classical variational methods can perform poorly when using an insufficiently expressive variational posterior, multi-sample objectives provide a graceful way of trading computation for quality of fit simply by increasing the number of samples used inside the objective. Combining such objectives with black box variational inference methods could make the latter substantially more effective. We thus hope that the proposed approach will increase the appeal and applicability of black box variational inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Algorithm for computing VIMCO gradients</head><p>Algorithm 1 provides an outline of our implementation of VIMCO gradient computation for a single training case. This version uses the geometric mean to estimate f (x, h j ) from the other K − 1 terms. The computations are performed in the log domain for better numerical stability.</p><p>Algorithm 1 Compute gradient estimates for the model and proposal distribution parameters for a single observation Require:</p><formula xml:id="formula_19">x , K ≥ 2 for i = 1 to K do h i ∼ Q(h|x) l[i] = log f (x, h i ) end for</formula><p>{Compute the multi-sample stochastic bound} L = LogSumExp(l) − log K {Precompute the sum of log f } s = Sum(l) {Compute the baseline for each sample} for i = 1 to K do {Save the current log f for future use and replace it} {with the average of the other K-1 log f terms}</p><formula xml:id="formula_20">temp = l[i] l[i] = (s − l[i])/(K − 1) L −i = LogSumExp(l) − log K l[i]</formula><p>= temp {Restore the saved value} end for w = SoftMax(l) {Compute the importance weights} ∇θ = 0, ∇ψ = 0 {Sum the gradient contributions from the K samples}</p><formula xml:id="formula_21">for i = 1 to K do {Proposal distribution gradient contributions} ∇θ = ∇θ + (L −L −i )∇ θ log Q(h i |x) ∇θ = ∇θ + w[i]∇ θ log f (x, h i ) {Model gradient contribution} ∇ψ = ∇ψ + w[i]∇ ψ log f (x, h i ) end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details of the experimental protocol</head><p>All models were trained using the Adam optimizer <ref type="bibr" target="#b8">(Kingma &amp; Ba, 2015)</ref> with minibatches of size 24. The input to the proposal distribution/inference network was centered by subtracting the mean. For each training method/number of samples combination we trained the model several times using different learning rates, saving the model with the best validation score achieved during each training run. The plots and the scores shown in the paper were obtained from the saved model with the highest validation score. For generative training, we considered the learning rates of {3 × 10 −4 , 1 × 10 −3 , 3 × 10 −3 }. For the structured output prediction experiments, the learning rates were {3×10 −4 , 1×10 −3 , 3×10 −3 } for VIMCO and RWS and {1 × 10 −4 , 3 × 10 −4 , 1 × 10 −3 } for NVIL.</p><p>Our NVIL implementation used both constant and inputdependent baselines as well as variance normalization. The input-dependent baseline for NVIL was a neural network with one hidden layer of 100 tanh units. VIMCO used the geometric mean for computing the per-sample learning signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Effect of variance reduction on the learning signal</head><p>As explained in Sections 2.4 and 2.5, the magnitude of the learning signal used for learning the proposal distribution parameters is closely related to the variance of the resulting gradient estimator. Both VIMCO and NVIL aim to reduce the estimator variance by subtracting a baseline from the original learning signalL(h 1:K ) in order to reduce its magnitude, while keeping the estimator unbiased. We examined the effectiveness of these two approaches by plotting a smoothed estimate of the magnitude of the resulting learning signal (L(h j |h −j ) for VIMCO andL(h 1:K ) − b(x) − b for NVIL) as a function of the number of parameter updates when training the SBN on MNIST in Section 5.1. The magnitude of the learning signal was estimated by taking the square root of the mean of the squared signal values for each minibatch. The results for different numbers of samples shown in <ref type="figure" target="#fig_0">Figure 3</ref> suggest that while VIMCO and NVIL are equally effective at reducing variance when using a 2-sample objective, VIMCO becomes much more effective than NVIL when using more than 2 samples. For 10 samples, the average magnitude of the learning signal for VIMCO is about 3 times lower than for NVIL, which suggests almost an order of magnitude lower variance of the gradient estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Gradient derivation for the multi-sample objective</head><p>In this section we will derive the gradient for the multisample objective</p><formula xml:id="formula_22">L K (x) =E Q(h 1:K |x) L (h 1:K ) =E Q(h 1:K |x) logÎ(h 1:K ) =E Q(h 1:K |x) log 1 K K i=1 f (x, h i ) .</formula><p>We start by using the product rule:</p><formula xml:id="formula_23">∇ θ L K (x) =∇ θ E Q(h 1:K |x) L (h 1:K ) =∇ θ h 1:K Q(h 1:K |x)L(h 1:K ) = h 1:K L (h 1:K )∇ θ Q(h 1:K |x)+ Q(h 1:K |x)∇ θL (h 1:K ) .<label>(13)</label></formula><p>Using the identity ∇ θ g(x) = g(x)∇ θ log g(x), we can express the gradient of Q(h 1:K |x) as ∇ θ Q(h 1:K |x) =Q(h 1:K |x)∇ θ log Q(h 1:K |x)</p><p>=Q(h 1:</p><formula xml:id="formula_24">K |x)∇ θ log K j=1 Q(h j |x) =Q(h 1:K |x) K j=1</formula><p>∇ θ log Q(h j |x). (14)</p><p>We use the chain rule along with the same identity to compute the gradient ofL(h 1:K ):</p><formula xml:id="formula_25">∇ θL (h 1:K ) =∇ θ log 1 K K j=1 f (x, h j ) = 1 K i=1 f (x, h i ) K j=1 ∇ θ f (x, h j ) = 1 K i=1 f (x, h i ) K j=1 f (x, h j )∇ θ log f (x, h j ) = K j=1w j ∇ θ log f (x, h j )<label>(15)</label></formula><p>wherew</p><formula xml:id="formula_26">j ≡ f (x,h j ) K i=1 f (x,h i )</formula><p>. Substituting Eq. 14 and Eq. 15</p><p>into Eq. 13 we obtain</p><formula xml:id="formula_27">∇ θ L K (x) = h 1:K L (h 1:K )Q(h 1:K |x) K j=1 ∇ θ log Q(h j |x)+ Q(h 1:K |x) K j=1w j ∇ θ log f (x, h j ) , = h 1:K Q(h 1:K |x)L(h 1:K ) K j=1</formula><p>∇ θ log Q(h j |x)+</p><formula xml:id="formula_28">h 1:K Q(h 1:K |x) K j=1w j ∇ θ log f (x, h j ), =E Q(h 1:K |x)   jL (h 1:K )∇ θ log Q(h j |x)   + E Q(h 1:K |x)   jw j ∇ θ log f (x, h j )   . (16)</formula><p>E. Structured output prediction: digit completions <ref type="figure">Figure 4</ref> shows multiple completions for the same set of top digit image halves generated using a three-layer <ref type="bibr">(200-200-200</ref>) SBN trained using VIMCO with the 20-sample objective. The completions were obtained by computing observation probabilities based on a single sample from the prior. The variability of the completions shows how the model captured the multimodality of the data distribution. <ref type="figure">Figure 4</ref>. Structured output prediction: Conditional completions generated by sampling from a three-layer SBN trained using VIMCO with the 20-sample objective. The top row shows the original full digit images. The remaining rows combine the top half from the original image with the bottom half generated from the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>The magnitude (root mean square) of the learning signal for VIMCO and NVIL as a function of the number of samples used in the objective and the number of parameter updates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>are two such methods. . each variable in the model. Both methods work well without baselines and involve considering multiple values for latent variables. Unlike VIMCO, the LE method optimizes a single-sample objective and requires computing exact expectations for each variable, which makes it much more computationally expensive.</figDesc><table /><note>VIMCO shares some similarities with the black box method of the local expectations (LE) of Titsias &amp; Lázaro- Gredilla (2015). The LE method provides a relatively low variance unbiased estimator based on local learning signals derived from computing an exact expectation w.r.t</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Estimates of the negative log-likelihood (in nats) for generative modelling on MNIST. The model is an SBN with three latent layers of 200 binary units.</figDesc><table><row><cell>NUMBER OF</cell><cell cols="2">TRAINING ALG.</cell></row><row><cell>SAMPLES</cell><cell cols="3">VIMCO NVIL RWS</cell></row><row><cell></cell><cell>-</cell><cell>95.2</cell><cell>-</cell></row><row><cell></cell><cell>93.5</cell><cell>93.6</cell><cell>94.6</cell></row><row><cell></cell><cell>92.8</cell><cell>93.7</cell><cell>93.4</cell></row><row><cell></cell><cell>92.6</cell><cell>93.4</cell><cell>93.0</cell></row><row><cell></cell><cell>91.9</cell><cell>96.2</cell><cell>92.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Alex Graves, Guillaume Desjardins, Koray Kavukcuoglu, Volodymyr Mnih, Hugo Larochelle, and Mélanie Rey for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning wake-sleep recurrent attention models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Bornschein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Reweighted wakesleep. ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Importance weighted autoencoders. ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<title level="m">Predicting distributions with linearizing belief networks. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Muprop</surname></persName>
		</author>
		<title level="m">Unbiased backpropagation for stochastic neural networks. ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The &quot;wake-sleep&quot; algorithm for unsupervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">5214</biblScope>
			<biblScope unit="page" from="1158" to="1161" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="183" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volodymyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist learning of belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="113" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Techniques for learning binary stochastic feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Black box variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of Deep Belief Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International Conference on Machine Learning</title>
		<meeting>the 25th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1218" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3465" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning stochastic feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="530" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local expectation gradients for black box variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2620" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00521</idno>
		<title level="m">Reinforcement learning neural Turing machines</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
