<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2016 CLUSTERING IS EFFICIENT FOR APPROXIMATE MAXIMUM INNER PRODUCT SEARCH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Auvolat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ecole</forename><forename type="middle">Normale</forename><surname>Supérieure</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">France</forename><surname>Sarath Chandar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Université de Sherbrooke</orgName>
								<address>
									<country>USA, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Under review as a conference paper at ICLR 2016 CLUSTERING IS EFFICIENT FOR APPROXIMATE MAXIMUM INNER PRODUCT SEARCH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical kmeans, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>The Maximum Inner Product Search (MIPS) problem has recently received increased attention, as it arises naturally in many large scale tasks. In recommendation systems <ref type="bibr" target="#b6">(Koenigstein et al., 2012;</ref><ref type="bibr" target="#b0">Bachrach et al., 2014)</ref>, users and items to be recommended are represented as vectors that are learnt at training time based on the user-item rating matrix. At test time, when the model is deployed for suggesting recommendations, given a user vector, the model will perform a dot product of the user vector with all the item vectors and pick top K items with maximum dot product to recommend. With millions of candidate items to recommend, it is usually not possible to do a full linear search within the available time frame of only few milliseconds. This problem amounts to solving a K-MIPS problem.</p><p>Another common instance where the K-MIPS problem arises is in extreme classification tasks <ref type="bibr" target="#b14">(Vijayanarasimhan et al., 2014)</ref>, with a huge number of classes. At inference time, predicting the top-K most likely class labels for a given data point can be cast as a K-MIPS problem. Such extreme (probabilistic) classification problems occur often in Natural Language Processing (NLP) tasks where the classes are words in a predetermined vocabulary. For example in neural probabilistic language models <ref type="bibr" target="#b1">(Bengio et al., 2003)</ref> the probabilities of a next word given the context of the few previous words is computed, in the last layer of the network, as a multiplication of the last hidden layer representation with a very large matrix (an embedding dictionary) that has as many columns as there are words in the vocabulary. Each such column can be seen as corresponding to the embedding of a vocabulary word in the hidden layer space. Thus an inner product is taken between each of these and the hidden representation, to yield an inner product "score" for each vocabulary word. Passed through a softmax nonlinearity, these yield the predicted probabilities for all possible words. The ranking of these probability values is unaffected by the softmax layer, so finding the k most probable words is exactly equivalent to finding the ones with the largest inner product scores, i.e. solving a K-MIPS problem.</p><p>In many cases the retrieved result need not be exact: it may be sufficient to obtain a subset of k vectors whose inner product with the query is very high, and thus highly likely (though not guaranteed) to contain some of the exact K-MIPS vectors. These examples motivate research on approximate K-MIPS algorithms. If we can obtain large speedups over a full linear search without sacrificing too much on precision, it will have a direct impact on such large-scale applications.</p><p>Formally the K-MIPS problem is stated as follows: given a set X = {x 1 , . . . , x n } of points and a query vector q, find argmax</p><formula xml:id="formula_0">(K) i∈X q x i (1)</formula><p>where the argmax (K) notation corresponds to the set of the indices providing the K maximum values. Such a problem can be solved exactly in linear time by calculating all the q x i and selecting the K maximum items, but such a method is too costly to be used on large applications where we typically have hundreds of thousands of entries in the set.</p><p>All the methods discussed in this article are based on the notion of a candidate set, i.e. a subset of the dataset that they return, and on which we will do an exact K-MIPS, making its computation much faster. There is no guarantee that the candidate set contains the target elements, therefore these methods solve approximate K-MIPS. Better algorithms will provide us with candidate sets that are both smaller and have larger intersections with the actual K maximum inner product vectors.</p><p>MIPS is related to nearest neighbor search (NNS), and to maximum similarity search. But it is considered a harder problem because the inner product neither satisfies the triangular inequality as distances usually do, nor does it satisfy a basic property of similarity functions, namely that the similarity of an entry with itself is at least as large as its similarity with anything else: for a vector x, there is no guarantee that x T x ≥ x T y for all y. Thus we cannot directly apply efficient nearest neighbor search or maximum similarity search algorithms to the MIPS problem.</p><p>Given a set X = {x 1 , . . . , x n } of points and a query vector q, the K-NNS problem with Euclidean distance is defined as:</p><formula xml:id="formula_1">argmin (K) i∈X ||q − x i || 2 2 = argmax (K) i∈X q T x i − ||x i || 2 2</formula><p>(2) and the maximum cosine similarity problem (K-MCSS) is defined as:</p><formula xml:id="formula_2">argmax (K) i∈X q T x i ||q|| ||x i || = argmax (K) i∈X q T x i ||x i || (3)</formula><p>K-NNS and K-MCSS are different problems than K-MIPS, but it is easy to see that all three become equivalent provided all data vectors x i have the same Euclidean norm. Several approaches to MIPS make use of this observation and first transform a MIPS problem into a NNS or MCSS problem.</p><p>In this paper, we propose and empirically investigate a very simple approach for the approximate K-MIPS problem. It consists in first reducing the problem to an approximate K-MCSS problem (as has been previously done in (Shrivastava and Li, 2015) ) on top of which we perform a spherical k-means clustering. The few clusters whose centers best match the query yield the candidate set.</p><p>The rest of the paper is organized as follows: In section 2, we review previously proposed approaches for MIPS. Section 3 describes our proposed simple solution k-means MIPS in more details and section 4 discusses ways to further improve the performance by using a hierarchical k-means version. In section 5, we empirically compare our methods to the state-of-the-art in tree-based and hashing-based approaches, on two standard collaborative filtering benchmarks and on a larger word embedding datasets. Section 6 concludes the paper with discussion on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RELATED WORK</head><p>There are two common types of solution for MIPS in the literature: tree-based methods and hashingbased methods. Tree-based methods are data dependent (i.e. first trained to adapt to the specific data set) while hash-based methods are mostly data independent.</p><p>Tree-based approaches: The Maximum Inner Product Search problem was first formalized in <ref type="bibr" target="#b11">(Ram and Gray, 2012)</ref>. <ref type="bibr" target="#b11">Ram and Gray (2012)</ref> provided a tree-based solution for the problem. Specifically, they constructed a ball tree with vectors in the database and bounded the maximum inner product with a ball. Their novel analytical upper bound for maximum inner product of a given point with points in a ball made it possible to design a branch and bound algorithm to solve MIPS using the constructed ball tree. <ref type="bibr" target="#b11">Ram and Gray (2012)</ref> also proposes a dual-tree based search using cone trees when you have a batch of queries. One issue with this ball-tree based approach (IP-Tree) is that it partitions the set of data points based on the Euclidean distance, while the problem hasn't effectively been converted to NNS. In contrast, PCA-Tree <ref type="bibr" target="#b0">(Bachrach et al., 2014)</ref>, the current state-of-the-art tree-based approach to MIPS, first converts MIPS to NNS by appending an additional component to the vector that ensures that all vectors are of constant norm. This is followed by PCA and by a balanced kd-tree style tree construction.</p><p>Hashing based approaches: Shrivastava and Li (2014) is the first work to propose an explicit Asymmetric Locality Sensitive Hashing (ALSH) construction to perform MIPS. They converted MIPS to NNS and used the L2-LSH algorithm <ref type="bibr" target="#b4">(Datar et al., 2004)</ref>. Subsequently, <ref type="bibr" target="#b13">Shrivastava and Li (2015)</ref> proposed another construction to convert MIPS to MCSS and used the Signed Random Projection (SRP) hashing method. Both works were based on the assumption that a symmetric-LSH family does not exist for MIPS problem. Later, <ref type="bibr" target="#b10">Neyshabur and Srebro (2015)</ref> showed an explicit construction of a symmetric-LSH algorithm for MIPS which had better performance than the previous ALSH algorithms. <ref type="bibr">Finally, Vijayanarasimhan et al. (2014)</ref> propose to use Winner-Take-All hashing to pick top-K classes to consider during training and inference in large classification problems.</p><p>Hierarchical softmax: A notable approach to address the problem of scaling classifiers to a huge number of classes is the hierarchical softmax <ref type="bibr" target="#b9">(Morin and Bengio, 2005)</ref>. It is based on prior clustering of the words into a binary, or more generally n-ary tree that serves as a fixed structure for the learning process of the model. The complexity of training is reduced from O(n) to O(log n). Due to its clustering and tree structure, it resembles the MIPS techniques we explore in this paper. However, the approaches differ at a fundamental level. Hierarchical softmax defines the probability of a leaf node as the product of all the probabilities computed by all the intermediate softmaxes on the way to that leaf node. By contrast, an approximate MIPS search imposes no such constraining structure on the probabilistic model, and is better though as efficiently searching for top winners of what amounts to a large ordinary flat softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">k-MEANS CLUSTERING FOR APPROXIMATE MIPS</head><p>In this section, we propose a simple k-means clustering based solution for approximate MIPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MIPS TO MCSS</head><p>We follow the previous work by <ref type="bibr" target="#b13">Shrivastava and Li (2015)</ref> for reducing the MIPS problem to the MCSS problem by ingeniously rescaling the vectors and adding new components, making the norms of all the vectors approximately the same. Let X = {x 1 , . . . , x n } be our dataset. Let U &lt; 1 and m ∈ N * be parameters of the algorithm. The first step is to scale all the vectors in our dataset by the same factor such that max i ||x i || 2 = U . We then apply two mappings P and Q, one on the data points and another on the query vector. These two mappings simply concatenate m new components to the vectors making the norms of the data points all roughly the same. The mappings are defined as follows:</p><formula xml:id="formula_3">P (x) = [x, 1/2 − ||x|| 2 2 , 1/2 − ||x|| 4 2 , . . . , 1/2 − ||x|| 2 m 2 ] (4) Q(x) = [x, 0, 0, . . . , 0]<label>(5)</label></formula><p>As shown in <ref type="bibr" target="#b13">Shrivastava and Li (2015)</ref>, mapping P brings all the vectors to roughly the same norm: we have ||P</p><formula xml:id="formula_4">(x i )|| 2 2 = m/4 + ||x i || 2 m+1 2</formula><p>, with the last term vanishing as m → +∞, since ||x i || 2 ≤ U &lt; 1. We thus have the following approximation of MIPS by MCSS for any query vector q,</p><formula xml:id="formula_5">argmax (K) i q x i argmax (K) i Q(q) P (x i ) ||Q(q)|| 2 • ||P (x i )|| 2<label>(6)</label></formula><p>3.2 MCSS USING SPHERICAL k-MEANS Assuming all data points x 1 , . . . , x n have been transformed as x j ← P (x j ) so as to be scaled to a norm of approximately 1, then the spherical k-means 1 algorithm <ref type="bibr" target="#b15">(Zhong, 2005)</ref> can efficiently be used to do approximate MCSS. Algorithm 1 is a formal specification of the spherical k-means algorithm, where we denote by c i the centroid of cluster i (i ∈ {1, . . . , K}) and a j the index of the cluster assigned to each point x j .</p><p>Algorithm 1 Spherical k-means a j ← rand(k) while c i or a j changed at previous step do</p><formula xml:id="formula_6">c i ← j|a j =i xj || j|a j =i xj || a j ← argmax i∈{1,...,k} x j c i end while</formula><p>The difference between standard k-means clustering and spherical k-means is that in the spherical variant, the data points are clustered not according to their position in the Euclidian space, but according to their direction.</p><p>To find the one vector that has maximum cosine similarity to query point q in a dataset clustered by this method, we first find the cluster whose centroid has the best cosine similarity with the query vector -i.e. the i such that q c i is maximal -and consider all the points belonging to that cluster as the candidate set. We then simply take argmax j|aj =i q x j as an approximation for our maximum cosine similarity vector. This method can be extended for finding the k maximum cosine similarity vectors: we compute the cosine similarity between the query and all the vectors of the candidate set and take the k best matches.</p><p>One issue with constructing a candidate set from a single cluster is that the quality of the set will be poor for points close to the boundaries between clusters. To alleviate this problem, we can increase the size of candidate sets by constructing them instead from the top-p best matching clusters to construct our candidate set.</p><p>We note that other approximate search methods exploit similar ideas. For example, <ref type="bibr" target="#b0">Bachrach et al. (2014)</ref> proposes a so-called neighborhood boosting method for PCA-Tree, by considering the path to each leaf as a binary vector (based on decision to go left or right) and given a target leaf, consider all other leaves which are one hamming distance away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HIERARCHICAL k-MEANS FOR FASTER AND MORE PRECISE SEARCH</head><p>While using a single-level clustering of the data points might yield a sufficiently fast search procedure for moderately large databases, it can be insufficient for much larger collections.</p><p>Indeed, if we have n points, by clustering our dataset into √ n clusters so that each cluster contains approximately √ n points, we reduce the complexity of the search from O(n) to roughly O ( √ n). If we use the single closest cluster as a candidate set, then the candidate set size is of the order of √ n. But as mentioned earlier, we will typically want to consider the two or three closest clusters as a candidate set, in order to limit problems arising from the query points close to the boundary between clusters or when doing approximate K-MIPS with K fairly big (for example 100). A consequence of increasing candidate sets this way is that they can quickly grow wastefully big, containing many unwanted items. To restrict the candidate sets to a smaller count of better targeted items, we would need to have smaller clusters, but then the search for the best matching clusters becomes the most expensive part. To address this situation, we propose an approach where we cluster our dataset into many small clusters, and then cluster the small clusters into bigger clusters, and so on any number of times. Our approach is thus a bottom-up clustering approach.</p><p>Note that we use K to refer to the number of top-K items to retrieve in search and k for the number of clusters in k-means. These two quantities are otherwise not the same.</p><p>For example, we can cluster our datasets in n 2/3 first-level, small clusters, and then cluster the centroids of the first-level clusters into n 1/3 second-level clusters, making our data structure a twolayer hierarchical clustering. This approach can be generalized to as many levels of clustering as necessary.</p><formula xml:id="formula_7">... ... ... ... ... ... ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>Walk down a hierarchical clustering tree: at each level we have a candidate set for the next level. In the first level, the dashed red boxed represent the p best matches, which gives us a candidate set for the second level, etc.</p><p>To search for the small clusters that best match the query point and will constitute a good candidate set, we go down the hierarchy keeping at each level only the p best matching clusters. This process is illustrated in <ref type="figure">Figure 1</ref>. Since at all levels the clusters are of much smaller size, we can take much larger values for p, for example p = 8 or p = 16. i to the clusters of layer l − 1. The candidate set is found using the method described in Algorithm 2. Our candidate set is the set C L obtained at the end of the algorithm. In our approach, Algorithm 2 Search in hierarchical spherical k-means</p><formula xml:id="formula_8">C 0 = I 0 for l = 0, . . . , L − 1 do A l = argmax (p) i∈C l q c (l) i C l+1 = i|a (l+1) i ∈ A l end for return C L</formula><p>we do a bottom-up clustering, i.e. we first cluster the dataset into small clusters, then we cluster the small cluster into bigger clusters, and so on until we get to the top level which is only one cluster. Other approaches have been suggested such as in <ref type="bibr" target="#b8">(Mnih and Hinton, 2009)</ref>, where the method employed is a top-down clustering strategy where at each level the points assigned to the current cluster are divided in smaller clusters. The approach of <ref type="bibr" target="#b8">(Mnih and Hinton, 2009)</ref> also addresses the problem that using a single lowest-level cluster as a candidate set is an inaccurate solution by having the data points be in multiple clusters. We use an alternative solution that consists in exploring several branches of the clustering hierarchy in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIMENTS</head><p>In this section, we will evaluate the proposed algorithm for approximate MIPS. Specifically, we analyze the following characteristics: speedup, compared to the exact full linear search, of retrieving top-K items with largest inner product, and robustness of retrieved results to noise in the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DATASETS</head><p>We have used 2 collaborative filtering datasets and 1 word embedding dataset, which are descibed below:</p><p>Movielens-10M: A collaborative filtering dataset with 10,677 movies (items) and 69,888 users. Given the user-item matrix Z, we follow the pureSVD procedure described in <ref type="bibr" target="#b3">(Cremonesi et al., 2010)</ref> to generate user and movie vectors. Specifically, we subtracted the average rating of each user from his individual ratings and considered unobserved entries as zeros. Then we compute an SVD approximation of Z with its top 150 singular components, Z W ΣR T . Each row in W Σ is used as the vector representation of the user and each row in R is the vector representation of the movie. We construct a database of all 10,677 movies and consider 60,000 randomly selected users as queries.</p><p>Netflix: Another standard collaborative filtering dataset with 17,770 movies (items) and 480,189 users. We follow the same procedure as described for movielens but construct 300 dimensional vector representations, as is standard in the literature <ref type="bibr" target="#b10">(Neyshabur and Srebro, 2015)</ref>. We consider 60,000 randomly selected users as queries.</p><p>Word2vec embeddings: We use the 300-dimensional word2vec embeddings released by <ref type="bibr" target="#b7">Mikolov et al. (2013)</ref>. We construct a database composed of the first 100,000 word embedding vectors. We consider two types of queries: 2,000 randomly selected word vectors from that database, and 2,000 randomly selected word vectors from the database corrupted with Gaussian noise. This acts as a test bench to evaluate the performance of different algorithms based on the characteristics of the queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">BASELINES</head><p>We consider the following baselines to compare with.</p><p>PCA-Tree: PCA-Tree <ref type="bibr" target="#b0">(Bachrach et al., 2014)</ref> is the state-of-the-art tree-based method which was shown to be superior to IP-Tree <ref type="bibr" target="#b6">(Koenigstein et al., 2012)</ref>. This method first converts MIPS to NNS by appending an additional component to the vectors to make them of constant norm. Then the principal directions are learnt and the data is projected using these principal directions. Finally, a balanced tree is constructed using as splitting criteria at each level the median of component values along the corresponding principal direction. Each level uses a different principal direction, in decreasing order of variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SRP-Hash:</head><p>This is the signed random projection hashing method for MIPS proposed in <ref type="bibr" target="#b13">Shrivastava and Li (2015)</ref>. SRP-Hash converts MIPS to MCSS by vector augmentation. We consider n hash functions and each hash function considers p random projections of the vector to compute the hash.</p><p>WTA-Hash: Winner Takes All hashing <ref type="bibr" target="#b14">(Vijayanarasimhan et al., 2014)</ref> is another hashing-based baseline which also converts MIPS to MCSS by vector augmentation. We consider n hash functions and each hash function does p different random permutations of the vector. Then the prefix constituted by the first k elements of each permuted vector is used to construct the hash for the vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SPEEDUP RESULTS</head><p>In these first experiments, we consider the two collaborative filtering tasks and evaluate the speedup provided by the different approximate K-MIPS algorithms (for K ∈ {1, 10, 100}) compared to the exact full search. Note that this section does not include the hierarchical version of k-means in the experiments, as the databases were small enough (less than 20,000) for flat k-means to perform well.</p><p>Specifically, speedup is defined as speedup A0 (A) = Time taken by Algorithm A Time taken by Algorithm A</p><p>where A 0 is the exact linear search algorithm that consists in computing the inner product with all training items. Because we want to compare the preformance of algorithms, rather than of specifically optimized implementations, we approximate the time with the number of dot product operations computed by the algorithm 2 . In other words, our unit of time is the time taken by a dot product.</p><p>For example, k-means algorithm was run using GPU while PCA-Tree was run using CPU.</p><p>All algorithms return a set of candidates for which we do exact linear seacrh. This induces a number of dot products at least as large as the size of the identified candidate set. In addition to the candidate set size, the following operations count towards the count of dot products: k-means: dot products done with all cluster centroids involved in finding the top-p clusters of the (hierarchical) search.</p><p>PCA-Tree: dot product done to project the query to the PCA space. Note that if the tree is of depth d, then we need to do d dot products to project the query.</p><p>SRP-Hash: total number of random projections of the data (each random projection is considered a single dot product). If we have n hashes with p random projections each, then the cost is p * n.</p><p>WTA-Hash: a full random permutation of the vector involves the same number of query element access operations as a single dot product. However, we consider only k prefixes in the permutations, which means we only need to do a fraction of dot product. While a dot product involves accessing all d components of the vector, each permutation in WTA-Hash only needs to access k elements of the vector. So we consider its cost to be a fraction k/d of the cost of a dot product. Specifically, if we have n hash functions each with p random permutations and consider prefixes of length k, then the total cost would be n * p * k/d where d is the dimension of the vector.</p><p>Let us call true top-K the actual K elements from the database that have the largest inner products with the query. Let us call retrieved top-K the K elements, among the candidate set retrieved by a specific approximate MIPS, that have the largest inner products with the query. We define precision for K-MIPS as the number of elements in the intersection of true top-K and retrived top-K vectors, divided by K.</p><formula xml:id="formula_10">precision at K = |retrieved top K ∩ true top K| K<label>(8)</label></formula><p>We varied hyper-parameters of each algorithm (k in k-means, depth in PCA-Tree, number of hash functions in SRP-Hash and WTA-Hash), and computed the precision and speedup in each case. Resulting precision v.s. speedup curves obtained for the Movielens-10M and Netflix datasets are reported in <ref type="figure">Figure 2</ref>. We make the following observations from these results:</p><p>• Hashing-based methods perform better with lower speedups. But their performance decrease rapidly after 10x speedup. • PCA-Tree performs better than SRP-Hash.</p><p>• WTA-Hash performs better than PCA-Tree with lower speedups. However, their performance degrades faster as the speedup increases and PCA-Tree outperforms WTA-Hash with higer speedups. • k-means is a clear winner as the speed up increases. Also, performance of k-means degrades very slowly with increase in speedup as compared to rapid decrease in performance of other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">NEIGHBORHOOD PRESERVING AND ROBUSTNESS RESULTS</head><p>In this experiment, we consider a word embedding retrieval task. As a first experiment, we consider using a query set of 2,000 embeddings, corresponding to a subset of a large database of pretrained embeddings. Note that while a query is thus present in the database, it is not guaranteed to correspond to the top-1 MIPS result. Also, we'll be interested in the top-10 and top-100 MIPS performance. Algorithms which perform better in top-10 and top-100 MIPS for queries which already belong to the database preserve the neighborhood of data points better. <ref type="figure">Figure 3</ref> shows the precision vs. speedup curve for top-1, top-10 and top-100 MIPS. From the results, we can see that data dependent algorithms (k-means and PCA-Tree) better preserve the neighborhood, compared to data independent algorithms (SRP-Hash, WTA-Hash), which is not surprising. However, k-means and hierarchical k-means performs significantly better than PCA-Tree in top-10 and top-100 MIPS suggesting that it is better than PCA-Tree in capturing the neighborhood. One reason might be that k-means has the global view of the vector at every step while PCA-Tree considers one dimension at a time.</p><p>As the next experiment, we would like to study how different algorithms behave with respect to the noise in the query. For a fair comparison, we chose hyper-parameters for each model such that the speedup is the same (we set it to 30x) for all algorithms. We take 2,000 random word embeddings from the database and corrupt them random Gaussian noise. We vary the scale of the noise from to 0.4 and plot the performance. <ref type="figure">Figure 4</ref> shows the performance of various algorithms on the top-1, top-10, top-100 MIPS problems, as the noise increases. We can see that k-means always performs better than other algorithms, even with increase in noise. Also, the performance of kmeans remains reasonable, compared to other algorithms. These results suggest that our approach might be particularly appropriate in a scenario where word embeddings are simultaneously being trained, and are thus not fixed. In such a scenario, having a robust MIPS method would allow us to update the MIPS model less frequently.  <ref type="figure">Figure 4</ref>: Precision in top-K retrieval as the noise in the query increases. We increase the standard deviation of the Gaussian noise and we see that k-means performs better than other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION AND FUTURE WORK</head><p>In this paper, we have proposed a new and efficient way of solving approximate K-MIPS based on a simple clustering strategy, and showed it can be a good alternative to the more popular LSH or tree-based techniques. We regard the simplicity of this approach as one of its strengths. Empirical results on three real-world datasets show that this simple approach clearly outperforms the other families of techniques. It achieves a larger speedup while maintaining precision, and is more robust to input corruption, an important property for generalization, as query test points are expected to not be exactly equal to training data points. Clustering MIPS generalizes better to related, but unseen data than the hashing approaches we evaluated.</p><p>In future work, we plan to research ways to adapt on-the-fly the clustering for our approximate K-MIPS as its input representation evolves during the learning of a model, leverage efficient K-MIPS to speed up extreme classifier training and improve precision and speedup by combining multiple clusterings.</p><p>Finally, we mention that, while putting the final touches to this paper, another very recent and different MIPS approach, based on vector quantization, came to our knowledge <ref type="bibr" target="#b5">(Guo et al., 2015)</ref>. We highlight that the first arXiv post of our work predates their work. Nevertheless, while we did not have time to empirically compare to this approach here, we hope to do so in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Formally, if we</head><label></label><figDesc>have L levels of clustering, let I l be a set of indices for the clusters at level l ∈ {0, . . . , L}. Let c (l) i , i ∈ I l be the centroids of the clusters at level l, with {c (L) i } conveniently defined as being the data points themselves, and let a (l) i ∈ I l−1 , i ∈ I l be the assignment of the centroids c (l)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Speedup results in collaborative filtering. (a-c) correspond to precision in top 1,10,100 MIPS on Movielens-10M dataset, while (d-f) correspond to precision in top 1,10,100 MIPS on Netflix dataset respectively. k-means(3) means k-means algorithm that considers top 3 clusters as candidate set. Speedup results in word embedding retrieval. (a-c) correspond to precision in top 1,10,100 MIPS respectively. k-means(3) means k-means algorithm that considers top 3 clusters as candidate set. and hier-k-means(8)s means a 2 level hierarchical k-means algorithm that considers top 8 clusters as candidate set.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank the developers of Theano <ref type="bibr" target="#b2">(Bergstra et al., 2010)</ref> for developing such a powerful tool. We acknowledge the support of the following organizations for research funding and computing support: Samsung, NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nir Nice, and Ulrich Paquet. Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liran</forename><surname>Katzir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Conference on Recommender Systems, RecSys &apos;14</title>
		<meeting>the 8th ACM Conference on Recommender Systems, RecSys &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
		<idno>1532-4435</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=944919.944966" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Performance of recommender algorithms on top-n recommendation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Turrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth ACM Conference on Recommender Systems, RecSys &apos;10</title>
		<meeting>the Fourth ACM Conference on Recommender Systems, RecSys &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Locality-sensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayur</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahab</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth Annual Symposium on Computational Geometry, SCG &apos;04</title>
		<meeting>the Twentieth Annual Symposium on Computational Geometry, SCG &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Quantization based fast inner product search. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
		<idno>abs/1509.01469</idno>
		<ptr target="http://arxiv.org/abs/1509.01469" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient retrieval of recommendations in a matrix factorization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikshit</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Shavitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<editor>Robert G. Cowell and Zoubin Ghahramani</editor>
		<meeting>the Tenth International Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On symmetric and asymmetric lshs for inner product search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Maximum inner-product search using cone trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikshit</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;12</title>
		<meeting>the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="931" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="2321" to="2329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved asymmetric locality sensitive hashing (alsh) for maximum inner product search (mips)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep networks with large output spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Yagnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7479</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient online spherical k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 2005. IJCNN&apos;05. Proceedings. 2005 IEEE International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3180" to="3185" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
