<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rotation equivariant vector field networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-12-29">29 Dec 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich Ecole des Ponts</orgName>
								<address>
									<settlement>Paris Tech</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Volpi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich Ecole des Ponts</orgName>
								<address>
									<settlement>Paris Tech</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich Ecole des Ponts</orgName>
								<address>
									<settlement>Paris Tech</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich Ecole des Ponts</orgName>
								<address>
									<settlement>Paris Tech</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rotation equivariant vector field networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-12-29">29 Dec 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1612.09346v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We propose a method to encode rotation equivariance or invariance into convolutional neural networks (CNNs). Each convolutional filter is applied with several orientations and returns a vector field that represents the magnitude and angle of the highest scoring rotation at the given spatial location. To propagate information about the main orientation of the different features to each layer in the network, we propose an enriched orientation pooling, i.e. max and argmax operators over the orientation space, allowing to keep the dimensionality of the feature maps low and to propagate only useful information. We name this approach RotEqNet. We apply RotEqNet to three datasets: first, a rotation invariant classification problem, the MNIST-rot benchmark, in which we improve over the state-of-the-art results. Then, a neuron membrane segmentation benchmark, where we show that RotEqNet can be applied successfully to obtain equivariance to rotation with a simple fully convolutional architecture. Finally, we improve significantly the state-of-the-art on the problem of estimating cars&apos; absolute orientation in aerial images, a problem where the output is required to be covariant with respect to the object&apos;s orientation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many Computer Vision problems, such as overhead (aerial or satellite) or biomedical image analysis, there are no dominant up-down or left-right relationships. For example, if the task is to detect cars in aerial images, the absolute orientation of a car is not a discriminant feature. Only the relative orientation to surrounding elements such as roads and buildings can establish a prior on the car presence likelihood. If the absolute orientation of the image image is changed, e.g. by following a different flightpath, we would expect the car detector to score the same values over all the cars, independently from their new orientations in the rotated image. In this case, we say that the problem is rotation equivariant: rotating the input is expected to result in the same output, but rotated. On the other hand, if we were confronted to a classification setting, in which we are only interested in the presence or absence of cars in the scene, the car flag (i.e whether any car is present or not) and the corresponding score should remain constant, no matter the absolute orientation of the input scene. In this case the problem is rotation invariant. The more general case would be rotation covariance, in which the output changes as a function of the rotation of the input with some predefined behavior. Taking again the same example, a rotation covariant problem would be to retrieve the absolute orientation of cars with respect to geographical coordinates: in this case, a rotation of the image should produce a corresponding change of the angle predicted for each car.</p><p>Throughout this article we will make use of the terms equivariance, invariance and covariance of a function f (•) with respect to a transformation g(•) in the following sense: where g (•) is another transformation that is itself function of g(•). With the above definitions, equivariance and invariance are special cases of covariance. We illustrate these properties in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>In this paper, we propose a network that naturally encodes these three properties with respect to rotations. In the following we will recall how they are achieved for translations, before discussing our own proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Dealing with translations in CNNs</head><p>The success of Convolutional Neural Networks (CNNs) is partly due to the translation equivariant nature of the convolution operation. To understand why it is more complex to achieve natural equivariance to rotations by means of convolutions, we will briefly summarize how translation equivariance is obtained by standard CNNs.</p><p>Translation equivariance in CNNs: A convolution of an image x ∈ R M ×N ×d with a filter w ∈ R m×n×d , written y = w * x, is computed by applying the same scalar product operation over all overlapping m × n windows (unit stride) on x. If x undergoes an integer translation in the horizontal and vertical directions by (p, q) pixels, the same pixel neighborhoods in x will exist in the translated x, but again translated by (p, q) pixels. Therefore, any neighborhood-based operation such as the convolution is translation equivariant when applied to images or other data structures characterized by local neighborhoods with identical arrangement. The fact that the operation is local and produces a single scalar per neighborhood has another advantageous effect: the output can be effortlessly re-arranged in useful ways. For instance, one can set the spatial structure of the activations to match the one of the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From equivariance to invariance or covariance:</head><p>Translation equivariance or invariance are highly desirable in problems such as image classification, where a local translation does not change the object class (invariance); and in dense prediction tasks as semantic segmentation and edge extraction, where the output should be subject to the same translation as the input (equivariance). In order to obtain full invariance to small translations, standard CNN architectures apply an additional invariant, non-linear local pooling operator, such as the average or the maximum. This usually comes at the cost of some information loss, typically related to the local distribution of the input values. Such loss is usually controlled by limiting the spatial influence of the pooling operator. If some form of translation covariance is required, some of this information should reach the last layer, for instance via skip connections <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Incorporating rotation equivariance in CNNs</head><p>If we want to account for an additional transformation (as in our case jointly achieving equivariance to 2D translation and rotation), the structure of the layer activations is no longer straightforward. One possibility would be to return a series of values corresponding to the application of rotated versions of the canonical filter w at each location in x. In this case, the activations y would be a 3D tensor where a translation in the 3 rd dimension corresponds to a rotation of w. This would correspond to a rotation covariant operator that keeps all the orientation information in the image. The covariance achieved in this way could easily be transformed into equivariance or invariance by means of pooling across orientations and spatial windows in deeper layers. When processing the 2 nd layer in the network, convolutional filters at different orientations must become 3D and be applied as 3D convolutions across the roto-translation space. This must be done for each orientation of the 3D filters, thus yielding a 4D tensor as output. The 3 rd layer will return a 5D feature map, the 4 th layer a 6D one and so on. This explosion in the dimensionality would result in an enormous increase of the memory requirements, the number of parameters and the computational complexity. Therefore, this naïve approach to rotation covariant CNNs would not be practical for CNNs with more than one convolutional layer. Note that this problem does not arise when using standard convolutions achieving translation    <ref type="figure" target="#fig_3">Fig. 2</ref>, but now the input (a) is the vector field (the output of <ref type="figure" target="#fig_3">Fig. 2)</ref>. Therefore, the filter and its rotated versions (b) are also vector fields.</p><p>equivariance only, since the output feature maps are always of the same dimensionality as the inputs, except for the dimension accounting for the number of features.</p><p>To overcome the exponential increase of the number of parameters, but still propagate useful information about the most important orientations, we propose to perform a single-binned max-pooling operation across the newly added orientation dimension. At each location, it takes the largest activation across orientations and the angle at which it occurred. This way, we are able to keep the 2D arrangement of the image throughout the CNN layers, while achieving inherent rotation equivariance and allowing the network to make use of the information about the orientation of feature activations in previous layers. Similar to spatial max-pooling, this propagates only information about the maximal activation, discarding all information about non-maximal activations. Since the atomic element in the hidden feature maps is no longer a scalar as in conventional CNNs but a 2D vector, each map can be treated as a vector field. We therefore name the proposed method Rotation Equivariant Vector Field Networks (RotEqNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Two families of approaches explicitly account for rotation invariance or equivariance: 1) those that transform the inputs (image or feature maps) and 2) those that rotate the filters. The proposed RotEqNet belongs to the latter.</p><p>1) Rotating the inputs: Jaderberg et al. <ref type="bibr" target="#b12">[13]</ref> propose the Spatial Transformer layer, which learns how to crop and transform a region of the image (or a feature map) that is passed to the next layer. This operator tends to find a relevant region for the task at hand and then transforms it to some canonical form, improving the learning process by reducing pose variations in subsequent layers. Laptev et al. <ref type="bibr" target="#b15">[16]</ref> use several rotated versions of the image as inputs to the same CNN and perform a pooling operation across the different feature vectors at the first fully connected layer. Such scheme allows another subsequent fully connected layer to choose among ro-tated inputs to compute the classification. In the work by Cheng et al. <ref type="bibr" target="#b4">[5]</ref>, several rotated versions of an image are used by the CNN in single minibatch. Their representations after the first fully connected layer are then encouraged to be similar, forcing the CNN to learn rotation invariance. Henriques et al. <ref type="bibr" target="#b9">[10]</ref> warp the images such that the translation equivariance inherent to convolutions is transformed into rotation and scale equivariance.</p><p>On the one hand, these methods have the advantage of using conventional CNN implementations, since they transform only the input images or the feature maps. On the other hand, their main disadvantage is that they can only consider global transformations of the input image. If this is well suited for tasks such as image classification, it limits their applicability to other problems (e.g. semantic segmentation), where relative orientations of some objects with respect to others can help identifying them. It is worth mentioning that this family of approaches also includes standard data augmentation strategies consisting in applying random rotations and flips to training samples <ref type="bibr" target="#b24">[25]</ref>: given enough training samples and model capacity, a CNN might learn that different orientations should score the same by simply learning equivalent filters at different orientations <ref type="bibr" target="#b17">[18]</ref>.</p><p>2) Rotating the filters: Gens and Domingos <ref type="bibr" target="#b8">[9]</ref> tackle the problem of the exploding dimensionality (discussed in Sect. 1.2) by applying learnable kernel-based pooling operations and sampling the symmetry space at each layer. This way, they avoid applying the filters exhaustively across the (high dimensional) feature maps by selectively sampling few rotations. By doing so, only the least important information is lost from layer to layer. Cohen et al. <ref type="bibr" target="#b5">[6]</ref> use a smaller symmetry group, composed of a flipping and four 90 o rotations, and perform pooling within the group (what they call coset pooling). They apply coset pooling only in the deeper layers of the network, since they found that pooling in the early layers discards important information and harms the classification performance. Instead of defining explicitly a symmetry group, Ngiam et al. <ref type="bibr" target="#b20">[21]</ref> pool across several untied filters, thus letting the network learn what kind of invariance is useful. Sifre et al. <ref type="bibr" target="#b23">[24]</ref> use wavelets that are separable in the rototranslational space instead of learned filters, allowing for more efficient computation. Another approach for avoiding the dimensionality explosion in the space of rotations is to limit the network to be shallow: Sohn et al. <ref type="bibr" target="#b25">[26]</ref> and Kivinen et al. <ref type="bibr" target="#b13">[14]</ref> propose such a scheme with unsupervised Restricted Boltzmann Machines (RBM), while Marcos et al. <ref type="bibr" target="#b19">[20]</ref> implement it with supervised CNNs consisting of a single convolutional layer.</p><p>These works find a compromise between the computational resources required and the amount of orientation information kept throughout the layers, by either keeping the model shallow or accounting for a very small amount of orientations. We propose to avoid such compromise by applying a pooling over a large number of orientations and passing forward both the maximum magnitude and the orientation at which it occurred. This modification allows to build deeper rotation equivariant architectures, where the deeper layers become aware of the dominant orientations in the previous ones with minimal memory requirements, since we avoid the above mentioned explosion of the dimensionality of the feature maps and filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Rotation equivariant vector field networks</head><p>In this work we focus on achieving rotation equivariance by performing convolutions with several rotated instances of the same filter (see <ref type="figure" target="#fig_3">Fig. 2</ref>), which we call the canonical filter. The canonical filter w is rotated at R different orientations, evenly spaced in a given interval of angles. In the experiments of Sect. 4 we deal with problems requiring either full invariance, equivariance or covariance, so we use the interval α = [0 o , 360 o ]. However, this interval could be tightened if an adequate range of tilts is known beforehand.</p><p>The output of filter w at a specific image location will consist of the magnitude of the maximal activation across the orientations, plus the corresponding angle. If we convert this polar representation into Cartesian coordinates, each filter w produces a vector field feature map z ∈ R H×W ×2 , where the output of each location consists of two values [u, v] ∈ R 2 representing the maximal activation in both magnitude and direction. Since the feature maps have become vector fields, from this moment on the filters applied to them must also be vector fields, as seen in <ref type="figure" target="#fig_4">Fig. 3</ref>.</p><p>The advantage of representing z ∈ R H×W ×2 in Carte-sian coordinates is that the horizontal and vertical components <ref type="bibr">[u, v]</ref> are orthogonal, and thus a convolution of two vector fields can be computed using standard convolutions separately in each component:</p><formula xml:id="formula_0">(z * w) = (z u * w u ) + (z v * w v ),<label>(1)</label></formula><p>where subscripts u and v denote horizontal and vertical components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RotEqNet building blocks</head><p>RotEqNet requires different CNN building blocks to be able to handle vectors fields as inputs or outputs ( <ref type="figure" target="#fig_3">Fig. 2</ref> and <ref type="figure" target="#fig_4">Fig. 3</ref>). In the following, we present our reformulation of traditional CNN blocks to account for both vector field activations and filters. The implementation is based on the MatConvNet <ref type="bibr" target="#b27">[28]</ref> toolbox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Rotating convolution and its transpose</head><p>Rotating convolution (RotConv): Given an input image with m/2 zero-padding x ∈ R H+m/2×W +m/2×d , we apply the filter w ∈ R m×m×d at R different orientations, corresponding to the rotation angles:</p><formula xml:id="formula_1">α r = 360 R r ∀r = 1, 2 . . . R.<label>(2)</label></formula><p>Each of these rotated versions of the filter <ref type="figure" target="#fig_3">(Fig. 2b</ref> and <ref type="figure" target="#fig_4">Fig. 3b</ref>) is computed by resampling w with bicubic interpolation after rotation of α r degrees around the filter center:</p><formula xml:id="formula_2">w r = rotate(w, α r ).<label>(3)</label></formula><p>Note that interpolation when resampling the filters or the image is always required unless only rotations of multiples of 90 o are considered. In practice, this means that the rotation equivariance will only be approximate (a fact that we will use to our advantage in the experiments, see Sect. 4) . The position [i , j ] after rotation of a specific filter weight, originally located at [i, j] in the canonical form, is</p><formula xml:id="formula_3">[i , j ] = [i, j] cos(α r ) sin(α r ) − sin(α r ) cos(α r ) .<label>(4)</label></formula><p>Coordinates are relative to the center of the filter. Since the rotation can force weights near the corners of the filter to be relocated outside of its spatial support, only the weights within a circle of diameter m pixels are used to compute the convolutions. The output tensor y ∈ R H×W ×R <ref type="figure" target="#fig_3">(Fig. 2c and Fig. 3c</ref>) consists of R feature maps computed as</p><formula xml:id="formula_4">y (r) = (x * w r ) ∀r = 1, 2 . . . R,<label>(5)</label></formula><p>where ( * ) is a standard convolution operator such that</p><formula xml:id="formula_5">(x * w)[i, j] = m n x[i − m, j − n]w[m, n],<label>(6)</label></formula><p>where [m, n] is the neighborhood considered by the filter. The tensor y encodes the roto-translation output space such that rotation in the input corresponds to a translation across the feature maps. Only the canonical, non rotated, version of w is actually stored in the model. During backpropagation, gradients corresponding to each rotated filter, ∇w r , are aligned back to the canonical form and added together:</p><formula xml:id="formula_6">∇w = r rotate(∇w r , −α r ).<label>(7)</label></formula><p>Note that this block can be applied on conventional color images and feature maps ( <ref type="figure" target="#fig_3">Fig. 2a)</ref> or on vector field feature maps <ref type="figure" target="#fig_3">(Fig. 2b</ref>) by applying it independently in each component and summing the resulting 3D tensors (see Eq. (1)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rotating convolution transpose (RotConv ):</head><p>When tackling problems such as semantic segmentation, boundary detection or normal estimation, it has been shown that dense prediction strategies returning outputs of the same size as the inputs offer the best results <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>. The convolution transpose operator, the adjoint of the convolution operator, can be used to learn interpolation filters to upsample activations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref>. As with the convolution, the rotating convolution transpose can also be implemented using a standard convolution transpose module:</p><formula xml:id="formula_7">x = r (y (r) * w r ) .<label>(8)</label></formula><p>where ( * ) is the convolution transpose operator. Note that the output can also be a vector field, just by defining w as a vector field filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Orientation pooling and its un-pooling</head><p>Orientation pooling (OP): Given the 3D tensor y, the role of the orientation pooling is to convert it to a 2D vector field z ∈ R H×W ×2 <ref type="figure" target="#fig_3">(Fig. 2d and Fig. 3d</ref>). It avoids the exploding dimensionality problem, while keeping information about the maximally activating orientation of w. First, we extract a 2D map of the largest activations ρ ∈ R H×W and their corresponding orientations θ ∈ R H×W . Specifically, for activations located at [i, j]:</p><formula xml:id="formula_8">ρ[i, j] = max r y[i, j, r],<label>(9)</label></formula><formula xml:id="formula_9">θ[i, j] = 360 R arg max r y[i, j, r].<label>(10)</label></formula><p>This can be treated as a polar representation of a 2D vector field as long as ρ[i, j] ≥ 0 ∀i, j, condition that is met when using any function on y that returns non-negative values prior to the OP. We employ the common Rectified Linear Unit (ReLu) operation, defined as ReLu(x) = max(x, 0), to ρ, as it provides non-saturating, sparse nonlinear activations offering stable training. This representation can then be transformed into Cartesian coordinates as:</p><formula xml:id="formula_10">u = ReLu(ρ) cos(θ) (11) v = ReLu(ρ) sin(θ)<label>(12)</label></formula><p>with u, v ∈ R H×W . The 2D vector field z is then built as:</p><formula xml:id="formula_11">z = 1 0 u + 0 1 v<label>(13)</label></formula><p>Orientation un-pooling: The adjoint of the orientation pooling is the orientation un-pooling. It takes a vector field z and outputs a stack of scalar maps y, where each individual map y (r) is such that:</p><formula xml:id="formula_12">y (r) = ρ • α r − 360 2R &lt; θ ≥ α r + 360 2R , r ∈ 1 . . . R.<label>(14)</label></formula><p>The resulting y will contain mostly zeros and its values will be such that the result of applying an orientation pooling operator to y will result in z. The orientation un-pooling block is required for the backpropagation of gradients through an OP block. It can also be used for rotation equivariant up-sampling after a RotConv layer, to convert vector field z into roto-translation y feature maps. An alternative up-sampling is to use a RotConv operator directly on z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Spatial pooling (SP) for vector fields</head><p>The max-pooling operator is commonly used in CNNs for obtaining some invariance to small deformations and reducing the size of the feature maps. This is done by taking the input feature map x ∈ R M ×N ×d and downsampling it to </p><formula xml:id="formula_13">x p ∈ R M p × N p ×d .</formula><p>This allows us to define vector field max-pooling as:</p><formula xml:id="formula_15">z p [c] = z[j], where j = arg max i∈c ρ[i],<label>(16)</label></formula><p>where ρ is a standard scalar map containing the magnitudes of the vectors in z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Batch normalization (BN) for vector fields</head><p>BN <ref type="bibr" target="#b11">[12]</ref> normalizes each feature map across a minibatch to zero mean and unit standard deviation. It has been shown to improve the convergence of commonly employed stochastic gradient descent training algorithms. In our case, the feature maps are not scalar fields, but vector fields representing the magnitude of an activation and the orientation of the filter generating it. In this case, BN should limit its effect to normalizing the magnitudes of the vectors to unit standard deviation. It would not make sense to normalize the angles, since their values are already bounded and changing the distribution would alter important information about relative and global orientations. Given a vector field feature map z and its map of magnitudes ρ, we compute batch normalization as:</p><formula xml:id="formula_16">z = z var(ρ) .<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Dropout for vector fields</head><p>Another commonly used layer in CNNs is Dropout <ref type="bibr" target="#b26">[27]</ref>. Dropout makes use of the ideas behind ensemble learning (that an ensemble of independently trained learners tends to produce a better average response than any learner by itself) by randomly switching off some of the neurons at each training iteration. This reduces the risk of overfitting by preventing groups of neurons to co-adapt too tightly. Applying Dropout to vector fields only requires to ensure that both components of the filter are being switched off or on simultaneously. We apply Dropout on both fully connected and convolutional layers, the latter by switching on or off a filter for all locations in each of the samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Rot-O-Block</head><p>It is not always desirable to use the maximum activation angle of a filter. For instance, when the filter shows an approximate radial symmetry, this information would be misleading, since all directions are equivalent, but OP chooses only one. The consequence is that two image locations that are almost identical can produce orthogonal activations. Thus, we propose to use a learning block, that allows each filter to return a learned combination of the vector field and the magnitude field outputs. We name it Rot-O-block. The structure of this block is shown in <ref type="figure" target="#fig_6">Fig. 4</ref>. It takes as input a scalar tensor y in ∈ R M ×N ×R•K , where R is the number of orientations used by the previous RotConv (RC in <ref type="figure" target="#fig_6">Fig. 4</ref>) layer and K is the number of learnable filters. An orientation pooling operation (OP) is applied on y in : this returns simultaneously the corresponding vector field tensor z ∈ R M ×N ×K×2 and its magnitude map ρ ∈ R M ×N ×K . Then, we separate the flows and process them separately by applying a RotConv on each activation. Spatial pooling (SP) and dropout can be optionally applied separately on the independent activations. The outputs are finally concatenated to produce the output y out .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We explore the performance of the proposed method on datasets where the orientation of the patterns of interest is arbitrary. This is very often the case in biomedical imaging and remote sensing, since the orientation of the camera is usually not correlated to the patterns of interest. We will apply RotEqNet to problems from these two fields, as well to a well-known computer vision benchmark dataset, MNIST-rot. Each one of these case studies will allow us to analyze the performances of RotEqNet in problems needing respectively invariance, equivariance and covariance to rotations. Based on a sensitivity analysis, all the results are reported using a number of orientations R = 17 for the RotConv and RotConv layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Invariance: MNIST-rot</head><p>MNIST-rot <ref type="bibr" target="#b16">[17]</ref> is a variant of the original MNIST digit recognition dataset with an additional random rotation between 0 • and 360 • applied to each 28 × 28 image. The training set is also considerably smaller than the standard MNIST, with 12k samples, from which 10k are used for training and 2k for validation and model selection. The test set consists of 50k samples. Since we aim at predicting the same label independently from the rotation that has been applied, the MNIST-rot problem requires rotation invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model:</head><p>We test three CNN models with the same architecture, shown in <ref type="table" target="#tab_1">Table 1</ref>: 1) a cascade of three Rot-O-Blocks, 2) a network with three blocks using only the vector field path of the Rot-O-Block and 3) a network using only the scalar field path. The models are trained for 150 epochs, starting with a large learning rate, weight decay and dropout rate of 0.01, 0.05 and 0.4, respectively. All the three hyper-parameters were gradually reduced to reach 0.0001, 0.005 and 0.0. The hyper-parameters were kept the same across all layers and all networks. We used batch normalization before every convolutional layer ex-  cept the first and last ones.</p><p>Test time data augmentation: We also perform data augmentation at test time, a technique often used with approximately invariant or equivariant CNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>. In particular, we show to the network several rotated versions of the same image using a fixed set of angles between 0 o and 90 o . Even if doing rotation-based data augmentation at test time might seem counter-intuitive in a rotation invariant model, the different rotations coupled to resampling of the images and the filters (cf. Sect. 3.1.1) will produce slightly different activations. After this, we average the scores obtained by all views of the same image to compute the final prediction.</p><p>Results: The results obtained on the test set of the MNIST-rot dataset are reported in <ref type="table" target="#tab_3">Table 2</ref>. The proposed RotEqNet, with the Rot-O-Block architecture, allows to match the results obtained by state-of-the-art TIpooling <ref type="bibr" target="#b15">[16]</ref> and even improve over them when using testtime data augmentation. The latter further improves the accuracy, by allowing the network to score different resamplings for each test image. This is very beneficial, in particular since input images are relatively small and consequently interpolation plays an even more critical role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error rate (in %) SVM <ref type="bibr" target="#b16">[17]</ref> 10.38±0.27 TIRBM <ref type="bibr" target="#b25">[26]</ref> 4.2 TI-pooling <ref type="bibr" target="#b15">[16]</ref> 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Equivariance: ISBI 2012 Challenge</head><p>This dataset <ref type="bibr" target="#b0">[1]</ref> considers the problem of segmenting neuronal structures in electron microscope (EM) stacks <ref type="bibr" target="#b2">[3]</ref>.</p><p>In this semantic segmentation problem we need to locate precisely the neuron membrane boundaries. A rotation in the inputs should lead to the same rotation in the output, making the ISBI 2012 problem a good candidate to study rotation equivariance. The data consist of two EM stacks of drosophila neurons, each with 30 images of size 512 × 512 pixels <ref type="figure" target="#fig_7">(Fig. 5a</ref>). One stack is used for training and the other for testing. The ground truth for the training stack consists of binary images of the same resolution as the data <ref type="figure" target="#fig_7">(Fig. 5b)</ref>. The ground truth for the test stack is kept private and the results are to be submitted to an evaluation server .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model:</head><p>We transform the original binary membrane vs. non-membrane segmentation problem into a three class segmentation problem: 1) non-membrane, 2) central membrane pixels and 3) membrane border pixels. Pixels within the membrane in the original label image but not belonging to either 2) or 3) are consider to belong to no class <ref type="figure" target="#fig_7">(Fig. 5c</ref>). This way we can assign a higher penalization to the non-membrane pixels right next to the membrane that those in the middle of the cells. The central membrane scores are used as the final prediction.</p><p>Since we are dealing with a dense prediction problem in which spatial autorcorrelation at different resolution levels has to be learned, we apply an encoder-decoder CNN   involving either two or three stages in the encoder and decoder blocks. <ref type="table" target="#tab_5">Table 3</ref> shows the architecture for the two-stage CNN. The output from the two-stage and threestage RotEqNets are averaged together to obtain the final result.</p><p>Results: A detailed explanation on the evaluation metrics used in the challenge can be found in the ISBI 2012 challenge website 1 and in <ref type="bibr" target="#b0">[1]</ref>. The winners of the challenge were Chen et al. <ref type="bibr" target="#b3">[4]</ref>, although Beier et al. <ref type="bibr" target="#b1">[2]</ref> had the highest scores at the time of writing. These two works focus on the use of a post-processing pipeline. Our rotation equivariant prediction provides results comparable to the state-of-the-art using raw CNN probabilities <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref> with a linear and relatively simple architecture (see <ref type="table" target="#tab_7">Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Covariance: car orientation estimation</head><p>In the last experiment, we test our method in a rotation covariant setting involving the estimation of car orientations from above-head imagery. We use the dataset provided by the authors of <ref type="bibr" target="#b9">[10]</ref>, which is based on images issued from Google Maps. The dataset is composed by 15 tiles, where cars' bounding boxes and car orientations have been manually annotated. We implement our approach in a way similar to <ref type="bibr" target="#b9">[10]</ref>. We crop a 48×48 square patch around every car, based on the centerpoint of the provided bounding box. We then use these crops for both training and testing of the model. As in <ref type="bibr" target="#b9">[10]</ref>, we use the cars in the first 10 images (388 cars) to train the network and those in the last 5 images (193 cars) to test.</p><p>Model: The function we want to learn is covariant with respect to rotations because a rotation by ∆α o in the input image should result in a numerical increase by ∆α o of the angle predicted. In particular, we try to predict the sine and cosine of α o , because they are continuous with respect to ∆α o . The network's architecture is illustrated in <ref type="table" target="#tab_8">Table 5</ref>. For the output we use a tanh non-linearity, followed by a layer normalizing the output vector to unitnorm.Since the first fully connected layer is not rotation equivariant nor invariant, it is important to train the network with data augmentation based on rotations and flippings. This ensures that there will be no preferred orientations inherited from a biased training set. The weight decay and dropout rate were fixed to 0.1 and 0.3 respectively and the learning rate was gradually decreased from −3 to 10 −5 . All the filters were initialized from a normal distribution with zero mean and 10 −3 standard deviation.</p><p>Results: We report in <ref type="table">Table 6</ref> the average output of the last 30 training epochs out of a total of 330 for each Method Rand Sc. Thin Inf. Sc. Thin CUMedVision <ref type="bibr" target="#b3">[4]</ref> 0.9768 0.9886 IAL MC/LMC <ref type="bibr" target="#b1">[2]</ref> 0.9826 0.9894 DIVE <ref type="bibr" target="#b7">[8]</ref> 0.9685 0.9858 PolyMtl <ref type="bibr" target="#b6">[7]</ref> 0.9689 0.9861 U-Net <ref type="bibr" target="#b22">[23]</ref> 0   model. This allows to overcome the lack of a validation set to select the best model. We compare three similar architectures with different degrees of orientation information passed to the last layer: 1) The two RotConv layers return a vector field, 2) the two RotConv layers return scalar fields and 3) only the second RotConv returns a vector field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Avg. error ( o ) CNN <ref type="bibr" target="#b9">[10]</ref> 28.87 Warped-CNN <ref type="bibr" target="#b9">[10]</ref> 26.44 RotEqNet, both RotConv scalar 26.14 RotEqNet, both RotConv vector 24.62 RotEqNet, 1 st scalar, 2 nd vector 21.37 <ref type="table">Table 6</ref>: Mean error in the prediction of car orientations.</p><p>The use of vector outputs of RotEqNet improves substantially the results, outperforming both state of the art  <ref type="table">Table 7</ref>: Sensitivity to the number of angles used by RotConv, R on the dataset <ref type="bibr" target="#b9">[10]</ref> and a version of RotEqNet using only scalar outputs at each layers. We also observe that alternating a first RotConv layer returning a scalar output and a second returning a vector output improves performances, probably due to the fact that at the original resolution of the images the orientation vector field is noisy, while it becomes more informative after the first spatial pooling. In <ref type="figure" target="#fig_8">Fig. 6</ref> we show the error distribution in the test set for the hybrid model. Note how most samples, 83%, are predicted with less than 15 o of error, while most of the contribution to the total error comes from the 7% of samples with error higher than 150 o , in which the front of the car has been mistaken with the rear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity to R:</head><p>In order to understand the sensitivity of RotEqNet to the number of angles R, we trained the hybrid model using R = 21 and tested it for different values of R (see <ref type="table">Table 7</ref>). We observed relatively small changes in the test error. Since R = 17 gave the best results, we chose it for all the experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Desirable behaviors with respect to rotation of the inputs: (left) equivariance in segmentation; (center) invariance in classification; (right) covariance in orientation estimation. g 45 is an operator that rotates the input image by 45 o .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>-</head><label></label><figDesc>equivariance: f (g(•)) = g(f (•)), -invariance: f (g(•)) = f (•), -covariance: f (g(•)) = g (f (•)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>First block of the proposed RotEqNet. (a) Input image x of size H × W . (b) Nine rotated versions of the canonical filter w.(c) Feature map y with nine orientations, one per r (for better visualization, the activations are juxtaposed in a 2D plane, and the tensor being H × W × 9). (d) Vector map z resulting from orientation pooling to y. Each vector encodes the magnitude and angle of the maximally activating filter orientation. (e) 2 × 2 spatial max-pooling of z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Second block of the proposed RotEqNet. The elements (a) to (e) are equivalent to those in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>This operation is performed by taking the maximum value contained in each one of the C non-overlapping p × p regions of x, indexed by c. It is computed as x p [c] = max i∈c x[i], which can be generalized as: y p [c] = y[j], where j = arg max i∈c y[i].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>The Rot-O-Block architecture diagram is shown within the dashed box. Variables are in squares and operators in circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>An example image from the validation set (#30) of the ISBI 2012 challenge. (a) Detail of the image (190 × 130 pixels). (b) The provided binary membrane ground truth. (c) The pre-processed 3-class ground truth: black is non-membrane, yellow is membrane center, red is membrane border and blue is non-class. (d) Probability map produced by RotEqNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>igure 6 :</head><label>6</label><figDesc>Distribution of the errors in the test set (top). Examples (bottom) of correctly and incorrectly identified orientations. Ground truth arrows in green (thin) and predictions in red (thick). R 12 19 Error 23.9 22.7 22.0 22.0 22.5 21.4 21.8 21.4 23.1 21.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Network architecture used on the MNIST-rot dataset. Layer parameters are in white and variables are shaded in gray.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Error rate on the MNIST-rot dataset. The number is parenthesis is the number of rotated versions of an image that were used for test-time data augmentation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>2-stage network architecture used on the ISBI 2012 Challenge. Parameters are in white and variables are shaded in gray.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Scores on the held out test set for the ISBI 2012 Challenge dataset. The first two<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> apply extended post-processing while the rest, including ours, use raw CNN probabilities.</figDesc><table><row><cell>Type</cell><cell>Size</cell></row><row><cell>Input</cell><cell>48 × 48</cell></row><row><cell>RotConv,</cell><cell>× 11, 6 filt.</cell></row><row><cell>× 2 SP</cell><cell>× 21 × 6</cell></row><row><cell>RotConv</cell><cell>× 9 × 6, 24 filt.</cell></row><row><cell>× 2 SP</cell><cell>× 9 × 24</cell></row><row><cell>Fully</cell><cell>× 9 × 24, 64 filt.</cell></row><row><cell>connected</cell><cell>× 1 × 64</cell></row><row><cell>FC, Normalize</cell><cell>× 1 × 64, 2 filt.</cell></row><row><cell>Output</cell><cell>× 1 × 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Network architecture used with the car orientation dataset. Layer parameters are in white and variables are shaded in gray.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">http://brainiac2.mit.edu/isbi challenge/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a new way of hard-coding rotation equivariance in CNNs by applying each filter at different orientations and extracting a vector field feature map that encodes the maximum activation magnitude and angle.</p><p>Experiments on classification, segmentation and orientation estimation problems show the suitability of this approach for solving problems that are inherently rotation equivariant, invariant or covariant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowdsourcing the creation of image segmentation algorithms for connectomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroanatomy</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An efficient fusion move algorithm for the minimum cost lifted multicut problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>eeding of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An integrated micro-and macroarchitectural analysis of the drosophila brain by computer-assisted serial section electron microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saalfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Preibisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pulokas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tomancak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hartenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1000502</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep contextual networks for neuronal structure segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>eeding of the Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RIFD-CNN: Rotation-Invariant and Fisher Discriminative convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2884" to="2893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07576</idno>
		<title level="m">Group equivariant convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Deep Learning and Data Labeling for Medical Applications Workshop (DLMIA)</title>
		<meeting>eeding of the Deep Learning and Data Labeling for Medical Applications Workshop (DLMIA)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep models for brain em image segmentation: novel insights and improved performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fakhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep symmetry networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2537" to="2545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Warped convolutions: Efficient invariance to spatial transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04382</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">When face recognition meets with deep learning: an evaluation of convolutional neural networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision Workshops (CVPRW)</title>
		<meeting>the IEEE Conf. on Computer Vision Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transformation equivariant boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intl. Conf. on Artificial Neural Networks (ICANN)</title>
		<meeting>the Intl. Conf. on Artificial Neural Networks (ICANN)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Surpassing humans in boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07386</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">TI-pooling: transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06318</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intl. Conf. on Machine Learning (ICML)</title>
		<meeting>the Intl. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="991" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>eeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning rotation invariant convolutional filters for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06720</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tiled convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>eeding of the IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intl. Conf. on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>the Intl. Conf. on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1233" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intl. Conf. on Document Analysis and Recognition (ICDAR)</title>
		<meeting>the Intl. Conf. on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6418</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Intl. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Intl. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>eeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
