<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-03-30">30 Mar 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
							<email>sioffe@google.com</email>
						</author>
						<title level="a" type="main">Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-03-30">30 Mar 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1702.03275v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Batch Normalization ("batchnorm" <ref type="bibr" target="#b5">[6]</ref>) has recently become a part of the standard toolkit for training deep networks. By normalizing activations, batch normalization helps stabilize the distributions of internal activations as the model trains. Batch normalization also makes it possible to use significantly higher learning rates, and reduces the sensitivity to initialization. These effects help accelerate the training, sometimes dramatically so. Batchnorm has been successfully used to enable state-of-the-art architectures such as residual networks <ref type="bibr" target="#b4">[5]</ref>.</p><p>Batchnorm works on minibatches in stochastic gradient training, and uses the mean and variance of the minibatch to normalize the activations. Specifically, consider a particular node in the deep network, producing a scalar value for each input example. Given a minibatch B of m examples, consider the values of this node, x 1 . . . x m . Then batchnorm takes the form:</p><formula xml:id="formula_0">x i ← x i − µ B σ B</formula><p>where µ B is the sample mean of x 1 . . . x m , and σ 2 B is the sample variance (in practice, a small ǫ is added to it for numerical stability). It is clear that the normalized activations corresponding to an input example will depend on the other examples in the minibatch. This is undesirable during inference, and therefore the mean and variance computed over all training data can be used instead.</p><p>In practice, the model usually maintains moving averages of minibatch means and variances, and during inference uses those in place of the minibatch statistics.</p><p>While it appears to make sense to replace the minibatch statistics with whole-data ones during inference, this changes the activations in the network. In particular, this means that the upper layers (whose inputs are normalized using the minibatch) are trained on representations different from those computed in inference (when the inputs are normalized using the population statistics). When the minibatch size is large and its elements are i.i.d. samples from the training distribution, this difference is small, and can in fact aid generalization. However, minibatchwise normalization may have significant drawbacks:</p><p>For small minibatches, the estimates of the mean and variance become less accurate. These inaccuracies are compounded with depth, and reduce the quality of resulting models. Moreover, as each example is used to compute the variance used in its own normalization, the normalization operation is less well approximated by an affine transform, which is what is used in inference.</p><p>Non-i.i.d. minibatches can have a detrimental effect on models with batchnorm. For example, in a metric learning scenario (e.g. <ref type="bibr" target="#b3">[4]</ref>), it is common to bias the minibatch sampling to include sets of examples that are known to be related. For instance, for a minibatch of size 32, we may randomly select 16 labels, then choose 2 examples for each of those labels. Without batchnorm, the loss computed for the minibatch decouples over the examples, and the intra-batch dependence introduced by our sampling mechanism may, at worst, increase the variance of the minibatch gradient. With batchnorm, however, the examples interact at every layer, which may cause the model to overfit to the specific distribution of minibatches and suffer when used on individual examples.</p><p>The dependence of the batch-normalized activations on the entire minibatch makes batchnorm powerful, but it is also the source of its drawbacks. Several approaches have been proposed to alleviate this. However, unlike batchnorm which can be easily applied to an existing model, these methods may require careful analysis of nonlinearities <ref type="bibr" target="#b0">[1]</ref> and may change the class of functions representable by the model <ref type="bibr" target="#b1">[2]</ref>. Weight normalization <ref type="bibr" target="#b9">[10]</ref> presents an alternative, but does not offer guarantees about the activations and gradients when the model contains arbitrary nonlinearities, or contains layers without such normalization. Furthermore, weight normalization has been shown to benefit from mean-only batch normalization, which, like batchnorm, results in different outputs during training and inference. Another alternative <ref type="bibr" target="#b8">[9]</ref> is to use a separate and fixed minibatch to compute the normalization parameters, but this makes the training more expensive, and does not guarantee that the activations outside the fixed minibatch are normalized.</p><p>In this paper we propose Batch Renormalization, a new extension to batchnorm. Our method ensures that the activations computed in the forward pass of the training step depend only on a single example and are identical to the activations computed in inference. This significantly improves the training on non-i.i.d. or small minibatches, compared to batchnorm, without incurring extra cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work: Batch Normalization</head><p>We are interested in stochastic gradient optimization of deep networks. The task is to minimize the loss, which decomposes over training examples:</p><formula xml:id="formula_1">Θ = arg min Θ 1 N N i=1 ℓ i (Θ)</formula><p>where ℓ i is the loss incurred on the ith training example, and Θ is the vector of model weights. At each training step, a minibatch of m examples is used to compute the gradient 1 m ∂ℓ i (Θ) ∂Θ which the optimizer uses to adjust Θ.</p><p>Consider a particular node x in a deep network. We observe that x depends on all the model parameters that are used for its computation, and when those change, the distribution of x also changes. Since x itself affects the loss through all the layers above it, this change in distribution complicates the training of the layers above. This has been referred to as internal covariate shift. Batch Normalization <ref type="bibr" target="#b5">[6]</ref> addresses it by considering the values of x in a minibatch B = {x 1...m }. It then normalizes them as follows:</p><formula xml:id="formula_2">µ B ← 1 m m i=1 x i σ B ← 1 m m i=1 (x i − µ B ) 2 + ǫ x i ← x i − µ B σ B y i ← γ x i + β ≡ BN(x i )</formula><p>Here γ and β are trainable parameters (learned using the same procedure, such as stochastic gradient descent, as all the other model weights), and ǫ is a small constant. Crucially, the computation of the sample mean µ B and sample standard deviation σ B are part of the model architecture, are themselves functions of the model parameters, and as such participate in backpropagation. The backpropagation formulas for batchnorm are easy to derive by chain rule and are given in <ref type="bibr" target="#b5">[6]</ref>.</p><p>When applying batchnorm to a layer of activations x, the normalization takes place independently for each dimension (or, in the convolutional case, for each channel or feature map). When x is itself a result of applying a linear transform W to the previous layer, batchnorm makes the model invariant to the scale of W (ignoring the small ǫ). This invariance makes it possible to not be picky about weight initialization, and to use larger learning rates.</p><p>Besides the reduction of internal covariate shift, an intuition for another effect of batchnorm can be obtained by considering the gradients with respect to different layers. Consider the normalized layer x, whose elements all have zero mean and unit variance. For a thought experiment, let us assume that the dimensions of x are independent. Further, let us approximate the loss ℓ( x) as its first-order Taylor expansion: ℓ ≈ ℓ 0 + g T x, where g = ∂ℓ ∂ x . It then follows that Var[ℓ] ≈ g 2 in which the left-hand side does not depend on the layer we picked. This means that the norm of the gradient w.r.t. a normalized layer ∂ℓ ∂ x is approximately the same for different normalized layers. Therefore the gradients, as they flow through the network, do not explode nor vanish, thus facilitating the training. While the assumptions of independence and linearity do not hold in practice, the gradient flow is in fact significantly improved in batch-normalized models.</p><p>During inference, the standard practice is to normalize the activations using the moving averages µ, σ 2 instead of minibatch mean µ B and variance σ 2 B :</p><formula xml:id="formula_3">y inference = x − µ σ • γ + β</formula><p>which depends only on a single input example rather than requiring a whole minibatch.</p><p>It is natural to ask whether we could simply use the moving averages µ, σ to perform the normalization during training, since this would remove the dependence of the normalized activations on the other example in the minibatch. This, however, has been observed to lead to the model blowing up. As argued in <ref type="bibr" target="#b5">[6]</ref>, such use of moving averages would cause the gradient optimization and the normalization to counteract each other. For example, the gradient step may increase a bias or scale the convolutional weights, in spite of the fact that the normalization would cancel the effect of these changes on the loss. This would result in unbounded growth of model parameters without actually improving the loss. It is thus crucial to use the minibatch moments, and to backpropagate through them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Batch Renormalization</head><p>With batchnorm, the activities in the network differ between training and inference, since the normalization is done differently between the two models. Here, we aim to rectify this, while retaining the benefits of batchnorm.</p><p>Let us observe that if we have a minibatch and normalize a particular node x using either the minibatch statistics or their moving averages, then the results of these two normalizations are related by an affine transform. Specifically, let µ be an estimate of the mean of x, and σ be an estimate of its standard deviation, computed perhaps as a moving average over the last several minibatches. Then, we have: We propose to retain r and d, but treat them as constants for the purposes of gradient computation. In other words, we augment a network, which contains batch normalization layers, with a per-dimension affine transformation applied to the normalized activations. We treat the parameters r and d of this affine transform as fixed, even though they were computed from the minibatch itself. It is important to note that this transform is identity in expectation, as long as σ = E[σ B ] and µ = E[µ B ]. We refer to batch normalization augmented with this affine transform as Batch Renormalization: the fixed (for the given minibatch) r and d correct for the fact that the minibatch statistics differ from the population ones. This allows the above layers to observe the "correct" activations -namely, the ones that would be generated by the inference model.</p><formula xml:id="formula_4">x i − µ σ = x i − µ B σ B •r+d, where r = σ B σ , d = µ B − µ σ If σ = E[σ B ] and µ = E[µ B ],</formula><p>In practice, it is beneficial to train the model for a certain number of iterations with batchnorm alone, without the correction, then ramp up the amount of allowed correction. We do this by imposing bounds on r and d, which initially constrain them to 1 and 0, respectively, and then are gradually relaxed.</p><p>Algorithm 1 presents Batch Renormalization. Unlike batchnorm, where the moving averages are computed during training but used only for inference, Batch Renorm does use µ and σ during training to perform the correction. We use a fairly high rate of update α for these averages, to ensure that they benefit from averaging multiple batches but do not become stale relative to the model parameters. We explicitly update the exponentially-decayed moving averages µ and σ, and optimize the rest of the model using gradient optimization, with the gradients cal-Input: Values of x over a training mini-batch B = {x 1...m }; parameters γ, β; current moving mean µ and standard deviation σ; moving average update rate α; maximum allowed correction r max , d max . Output: </p><formula xml:id="formula_5">{y i = BatchRenorm(x i )}; updated µ, σ. µ B ← 1 m m i=1 x i σ B ← ǫ + 1 m m i=1 (x i − µ B ) 2 r ← stop gradient clip [1/rmax,rmax] σ B σ d ← stop gradient clip [−dmax,dmax] µ B − µ σ x i ← x i − µ B σ B • r + d y i ← γ x i + β µ := µ + α(µ B − µ) // Update moving averages σ := σ + α(σ B − σ) Inference: y ← γ • x − µ σ + β</formula><formula xml:id="formula_6">∂ℓ ∂ x i = ∂ℓ ∂y i • γ ∂ℓ ∂σ B = m i=1 ∂ℓ ∂ x i • (x i − µ B ) • −r σ B ∂ℓ ∂µ B = m i=1 ∂ℓ ∂ x i • −r σ B ∂ℓ ∂x i = ∂ℓ ∂ x i • r σ B + ∂ℓ ∂σ B • x i − µ B mσ B + ∂ℓ ∂µ B • 1 m ∂ℓ ∂γ = m i=1 ∂ℓ ∂y i • x i ∂ℓ ∂β = m i=1 ∂ℓ ∂y i</formula><p>These gradient equations reveal another interpretation of Batch Renormalization. Because the loss ℓ is unaffected when all x i are shifted or scaled by the same amount, the functions ℓ({x i + t}) and ℓ({x i • (1 + t)}) are constant in t, and computing their derivatives at t = 0 gives m i=1 ∂ℓ ∂xi = 0 and m i=1 x i ∂ℓ ∂xi = 0. Therefore, if we consider the m-dimensional vector ∂ℓ ∂xi (with one element per example in the minibatch), and further consider two vectors p 0 = (1, . . . , 1) and p 1 = (x 1 , . . . , x m ), then ∂ℓ ∂xi lies in the null-space of p 0 and p 1 . In fact, it is easy to see from the Batch Renorm backprop formulas that to compute the gradient ∂ℓ ∂xi from ∂ℓ ∂ xi , we need to first scale the latter by r/σ B , then project it onto the null-space of p 0 and p 1 . For r = σB σ , this is equivalent to the backprop for the transformation x−µ σ , but combined with the null-space projection. In other words, Batch Renormalization allows us to normalize using moving averages µ, σ in training, and makes it work using the extra projection step in backprop.</p><p>Batch Renormalization shares many of the beneficial properties of batchnorm, such as insensitivity to initialization and ability to train efficiently with large learning rates. Unlike batchnorm, our method ensures that that all layers are trained on internal representations that will be actually used during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>To evaluate Batch Renormalization, we applied it to the problem of image classification. Our baseline model is Inception v3 <ref type="bibr" target="#b11">[12]</ref>, trained on 1000 classes from ImageNet training set <ref type="bibr" target="#b7">[8]</ref>, and evaluated on the ImageNet validation data. In the baseline model, batchnorm was used after convolution and before the ReLU <ref type="bibr" target="#b6">[7]</ref>. To apply Batch Renorm, we simply swapped it into the model in place of batchnorm. Both methods normalize each feature map over examples as well as over spatial locations. We fix the scale γ = 1, since it could be propagated through the ReLU and absorbed into the next layer.</p><p>The training used 50 synchronized workers <ref type="bibr" target="#b2">[3]</ref>. Each worker processed a minibatch of 32 examples per training step. The gradients computed for all 50 minibatches were aggregated and then used by the RMSProp optimizer <ref type="bibr" target="#b12">[13]</ref>. As is common practice, the inference model used exponentially-decayed moving averages of all model parameters, including the µ and σ computed by both batchnorm and Batch Renorm.</p><p>For Batch Renorm, we used r max = 1, d max = 0 (i.e. simply batchnorm) for the first 5000 training steps, after which these were gradually relaxed to reach r max = 3 at 40k steps, and d max = 5 at 25k steps. These final values resulted in clipping a small fraction of rs, and none of ds. However, at the beginning of training, when the learning rate was larger, it proved important to increase r max slowly: otherwise, occasional large gradients were observed to suddenly and severely increase the loss. To account for the fact that the means and variances change as the model trains, we used relatively fast updates to the moving statistics µ and σ, with α = 0.01. Because of this and keeping r max = 1 for a relatively large number of steps, we did not need to apply initialization bias correction <ref type="bibr">[? ]</ref>. All the hyperparameters other than those related to normalization were fixed between the models and across experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline</head><p>As a baseline, we trained the batchnorm model using the minibatch size of 32. More specifically, batchnorm was applied to each of the 50 minibatches; each example was normalized using 32 examples, but the resulting gradients were aggregated over 50 minibatches. This model achieved the top-1 validation accuracy of 78.3% after 130k training steps.</p><p>To verify that Batch Renorm does not diminish performance on such minibatches, we also trained the model with Batch Renorm, see <ref type="figure" target="#fig_1">Figure 1</ref>. The test accuracy of this model closely tracked the baseline, achieving a slightly higher test accuracy (78.5%) after the same number of steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Small minibatches</head><p>To investigate the effectiveness of Batch Renorm when training on small minibatches, we reduced the number of examples used for normalization to 4. Each minibatch of size 32 was thus broken into "microbatches" each having 4 examples; each microbatch was normalized independently, but the loss for each minibatch was computed as before. In other words, the gradient was still aggregated over 1600 examples per step, but the normalization  involved groups of 4 examples rather than 32 as in the baseline. <ref type="figure" target="#fig_3">Figure 2</ref> shows the results.</p><p>The validation accuracy of the batchnorm model is significantly lower than the baseline that normalized over minibatches of size 32, and training is slow, achieving 74.2% at 210k steps. We obtain a substantial improvement much faster (76.5% at 130k steps) by replacing batchnorm with Batch Renorm, However, the resulting test accuracy is still below what we get when applying either batchnorm or Batch Renorm to size 32 minibatches. Although Batch Renorm improves the training with small minibatches, it does not eliminate the benefit of having larger ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Non-i.i.d. minibatches</head><p>When examples in a minibatch are not sampled independently, batchnorm can perform rather poorly. However, sampling with dependencies may be necessary for tasks such as for metric learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. We may want to ensure that images with the same label have more similar representations than otherwise, and to learn this we require that a reasonable number of same-label image pairs can be found within the same minibatch.</p><p>In this experiment <ref type="figure">(Figure 3</ref>), we selected each minibatch of size 32 by randomly sampling 16 labels (out of the total 1000) with replacement, then randomly selecting 2 images for each of those labels. When training with batchnorm, the test accuracy is much lower than for i.i.d. minibatches, achieving only 67%. Surprisingly, even the training accuracy is much lower (72.8%) than the test accuracy in the i. consistent with overfitting. We suspect that this is in fact what happens: the model learns to predict labels for images that come in a set, where each image has a counterpart with the same label. This does not directly translate to classifying images individually, thus producing a drop in the accuracy computed on the training data. To verify this, we also evaluated the model in the "training mode", i.e. using minibatch statistics µ B , σ B instead of moving averages µ, σ, where each test minibatch had size 50 and was obtained using the same procedure as the training minibatches -25 labels, with 2 images per label. As expected, this does much better, achieving 76.5%, though still below the baseline accuracy. Of course, this evaluation scenario is usually infeasible, as we want the image representation to be a deterministic function of that image alone. We can improve the accuracy for this problem by splitting each minibatch into two halves of size 16 each, so that for every pair of images belonging to the same class, one image is assigned to the first half-minibatch, and the other to the second. Each half is then more i.i.d., and this achieves a much better test accuracy (77.4% at 140k steps), but still below the baseline. This method is only applicable when the number of examples per label is small (since this determines the number of microbatches that a minibatch needs to be split into).</p><p>With Batch Renorm, we simply trained the model with minibatch size of 32. The model achieved the same test accuracy (78.5% at 120k steps) as the equivalent model on i.i.d. minibatches, vs. 67% obtained with batchnorm. By replacing batchnorm with Batch Renorm, we ensured that the inference model can effectively classify individual images. This has completely eliminated the effect of overfitting the model to image sets with a biased label distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have demonstrated that Batch Normalization, while effective, is not well suited to small or non-i.i.d. training minibatches. We hypothesized that these drawbacks are due to the fact that the activations in the model, which are in turn used by other layers as inputs, are computed differently during training than during inference. We address this with Batch Renormalization, which replaces batchnorm and ensures that the outputs computed by the model are dependent only on the individual examples and not the entire minibatch, during both training and inference.</p><p>Batch Renormalization extends batchnorm with a perdimension correction to ensure that the activations match between the training and inference networks. This correction is identity in expectation; its parameters are computed from the minibatch but are treated as constant by the optimizer. Unlike batchnorm, where the means and variances used during inference do not need to be computed until the training has completed, Batch Renormalization benefits from having these statistics directly participate in the training. Batch Renormalization is as easy to implement as batchnorm itself, runs at the same speed during both training and inference, and significantly improves training on small or non-i.i.d. minibatches. Our method does have extra hyperparameters: the update rate α for the moving averages, and the schedules for correction limits d max , r max . A more extensive investigation of the effect of these is a part of future work.</p><p>Batch Renormalization offers a promise of improving the performance of any model that would normally use batchnorm. This includes Residual Networks <ref type="bibr" target="#b4">[5]</ref>. Another application is Generative Adversarial Networks <ref type="bibr" target="#b8">[9]</ref>, where the non-determinism introduced by batchnorm has been found to be an issue, and Batch Renorm may provide a solution.</p><p>Finally, Batch Renormalization may benefit applications where applying batch normalization has been difficult -such as recurrent networks. There, batchnorm would require each timestep to be normalized independently, but Batch Renormalization may make it possible to use the same running averages to normalize all timesteps, and then update those averages using all timesteps. This remains one of the areas that warrants further exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>then E[r] = 1 and E[d] = 0 (the expectations are w.r.t. a minibatch B). Batch Normalization, in fact, simply sets r = 1, d = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Validation top-1 accuracy of Inception-v3 model with batchnorm and its Batch Renorm version, trained on 50 synchronized workers, each processing minibatches of size 32. The Batch Renorm model achieves a marginally higher validation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Validation accuracy for models trained with either batchnorm or Batch Renorm, where normalization is performed for sets of 4 examples (but with the gradients aggregated over all 50 × 32 examples processed by the 50 workers). Batch Renorm allows the model to train faster and achieve a higher accuracy, although normalizing sets of 32 examples performs better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>i.d. case, and in fact exhibits a drop that is</figDesc><table><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>50 60</cell><cell></cell><cell></cell><cell cols="3">Batchnorm (67.0%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Batchnorm, train accuracy (72.8%)</cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell cols="3">Non-i.i.d. in test, using µ B , σ B (76.5%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Batchnorm on half-minibatches (77.4%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Batch Renormalization (78.6%)</cell></row><row><cell></cell><cell>30</cell><cell>0k</cell><cell>20k</cell><cell>40k</cell><cell>60k</cell><cell>80k 100k 120k 140k</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Training steps</cell></row><row><cell cols="7">Figure 3: Validation accuracy when training on non-i.i.d.</cell></row><row><cell cols="7">minibatches, obtained by sampling 2 images for each of</cell></row><row><cell cols="7">16 (out of total 1000) random labels. This distribution</cell></row><row><cell cols="7">bias results not only in a low test accuracy, but also low</cell></row><row><cell cols="7">accuracy on the training set, with an eventual drop. This</cell></row><row><cell cols="7">indicates overfitting to the particular minibatch distribu-</cell></row><row><cell cols="7">tion, which is confirmed by the improvement when the test</cell></row><row><cell cols="7">minibatches also contain 2 images per label, and batch-</cell></row><row><cell cols="7">norm uses minibatch statistics µ B , σ B during inference. It</cell></row><row><cell cols="7">improves further if batchnorm is applied separately to 2</cell></row><row><cell cols="7">halves of a training minibatch, making each of them more</cell></row><row><cell cols="7">i.i.d. Finally, by using Batch Renorm, we are able to just</cell></row><row><cell cols="7">train and evaluate normally, and achieve the same vali-</cell></row><row><cell cols="7">dation accuracy as we get for i.i.d. minibatches in Fig.</cell></row><row><cell>1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Normalization propagation: A parametric technique for removing internal covariate shift in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename><surname>Kota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01431</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00981</idno>
		<title level="m">Revisiting distributed synchronous sgd</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 17</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="901" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno>abs/1503.03832</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Lecture 6.5 -rmsprop. COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
