<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-07-22">22 Jul 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-07-22">22 Jul 2015</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1506.05869v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require handcrafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domainspecific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Advances in end-to-end training of neural networks have led to remarkable progress in many domains such as speech recognition, computer vision, and language processing. Recent work suggests that neural networks can do more Proceedings of the 31 st International Conference on Machine Learning, Lille, France, 2015. JMLR: W&amp;CP volume 37. Copyright 2015 by the author(s). than just mere classification, they can be used to map complicated structures to other complicated structures. An example of this is the task of mapping a sequence to another sequence which has direct applications in natural language understanding <ref type="bibr" target="#b12">(Sutskever et al., 2014)</ref>. The main advantage of this framework is that it requires little feature engineering and domain specificity whilst matching or surpassing state-of-the-art results. This advance, in our opinion, allows researchers to work on tasks for which domain knowledge may not be readily available, or for tasks which are simply too hard to design rules manually.</p><p>Conversational modeling can directly benefit from this formulation because it requires mapping between queries and reponses. Due to the complexity of this mapping, conversational modeling has previously been designed to be very narrow in domain, with a major undertaking on feature engineering. In this work, we experiment with the conversation modeling task by casting it to a task of predicting the next sequence given the previous sequence or sequences using recurrent networks <ref type="bibr" target="#b12">(Sutskever et al., 2014)</ref>. We find that this approach can do surprisingly well on generating fluent and accurate replies to conversations. We test the model on chat sessions from an IT helpdesk dataset of conversations, and find that the model can sometimes track the problem and provide a useful answer to the user. We also experiment with conversations obtained from a noisy dataset of movie subtitles, and find that the model can hold a natural conversation and sometimes perform simple forms of common sense reasoning. In both cases, the recurrent nets obtain better perplexity compared to the n-gram model and capture important long-range correlations. From a qualitative point of view, our model is sometimes able to produce natural conversations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our approach is based on recent work which proposed to use neural networks to map sequences to sequences <ref type="bibr" target="#b5">(Kalchbrenner &amp; Blunsom, 2013;</ref><ref type="bibr" target="#b12">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>. This framework has been used for neural machine translation and achieves improvements on the English-French and English-German translation tasks from the WMT'14 dataset <ref type="bibr" target="#b7">(Luong et al., 2014;</ref><ref type="bibr" target="#b3">Jean et al., 2014)</ref>. It has also been used for other tasks such as parsing <ref type="bibr" target="#b15">(Vinyals et al., 2014a)</ref> and image captioning <ref type="bibr" target="#b16">(Vinyals et al., 2014b)</ref>. Since it is well known that vanilla RNNs suffer from vanishing gradients, most researchers use variants of Long Short Term Memory (LSTM) recurrent neural networks <ref type="bibr" target="#b2">(Hochreiter &amp; Schmidhuber, 1997)</ref>.</p><p>Our work is also inspired by the recent success of neural language modeling <ref type="bibr" target="#b1">(Bengio et al., 2003;</ref><ref type="bibr" target="#b9">Mikolov et al., 2010;</ref><ref type="bibr" target="#b8">Mikolov, 2012)</ref>, which shows that recurrent neural networks are rather effective models for natural language. More recently, work by <ref type="bibr" target="#b11">Sordoni et al. (Sordoni et al., 2015)</ref> and <ref type="bibr" target="#b10">Shang et al. (Shang et al., 2015)</ref>, used recurrent neural networks to model dialogue in short conversations (trained on Twitter-style chats).</p><p>Building bots and conversational agents has been pursued by many researchers over the last decades, and it is out of the scope of this paper to provide an exhaustive list of references. However, most of these systems require a rather complicated processing pipeline of many stages <ref type="bibr" target="#b6">(Lester et al., 2004;</ref><ref type="bibr" target="#b17">Will, 2007;</ref><ref type="bibr" target="#b4">Jurafsky &amp; Martin, 2009)</ref>. Our work differs from conventional systems by proposing an end-to-end approach to the problem which lacks domain knowledge. It could, in principle, be combined with other systems to re-score a short-list of candidate responses, but our work is based on producing answers given by a probabilistic model trained to maximize the probability of the answer given some context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>Our approach makes use of the sequence-to-sequence (seq2seq) framework described in <ref type="bibr" target="#b12">(Sutskever et al., 2014)</ref>. The model is based on a recurrent neural network which reads the input sequence one token at a time, and predicts the output sequence, also one token at a time. During training, the true output sequence is given to the model, so learning can be done by backpropagation. The model is trained to maximize the cross entropy of the correct sequence given its context. During inference, given that the true output sequence is not observed, we simply feed the predicted output token as input to predict the next output. This is a "greedy" inference approach. A less greedy approach would be to use beam search, and feed several candidates at the previous step to the next step. The predicted sequence can be selected based on the probability of the sequence.</p><p>Concretely, suppose that we observe a conversation with two turns: the first person utters "ABC", and second person replies "WXYZ". We can use a recurrent neural network, <ref type="figure">Figure 1</ref>. Using the seq2seq framework for modeling conversations. and train to map "ABC" to "WXYZ" as shown in <ref type="figure">Figure  above</ref>. The hidden state of the model when it receives the end of sequence symbol "&lt;eos&gt;" can be viewed as the thought vector because it stores the information of the sentence, or thought, "ABC".</p><p>The strength of this model lies in its simplicity and generality. We can use this model for machine translation, question/answering, and conversations without major changes in the architecture. Applying this technique to conversation modeling is also straightforward: the input sequence can be the concatenation of what has been conversed so far (the context), and the output sequence is the reply.</p><p>Unlike easier tasks like translation, however, a model like sequence-to-sequence will not be able to successfully "solve" the problem of modeling dialogue due to several obvious simplifications: the objective function being optimized does not capture the actual objective achieved through human communication, which is typically longer term and based on exchange of information rather than next step prediction. The lack of a model to ensure consistency and general world knowledge is another obvious limitation of a purely unsupervised model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>In our experiments we used two datasets: a closed-domain IT helpdesk troubleshooting dataset and an open-domain movie transcript dataset. The details of the two datasets are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">IT Helpdesk Troubleshooting dataset</head><p>In our first set of experiments, we used a dataset which was extracted from a IT helpdesk troubleshooting chat service. In this service, costumers face computer related issues, and a specialist help them by conversing and walking through a solution. Typical interactions (or threads) are 400 words long, and turn taking is clearly signaled. Our training set contains 30M tokens, and 3M tokens were used as validation. Some amount of clean up was performed, such as removing common names, numbers, and full URLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">OpenSubtitles dataset</head><p>We also tested our model on the OpenSubtitles dataset <ref type="bibr" target="#b13">(Tiedemann, 2009)</ref>. This dataset consists of movie conversations in XML format. It contains sentences uttered by characters in movies. We applied a simple processing step removing XML tags and obvious non-conversational text (e.g., hyperlinks) from the dataset. As turn taking is not clearly indicated, we treated consecutive sentences assuming they were uttered by different characters. We trained our model to predict the next sentence given the previous one, and we did this for every sentence (noting that this doubles our dataset size, as each sentence is used both for context and as target). Our training and validation split has 62M sentences (923M tokens) as training examples, and the validation set has 26M sentences (395M tokens). The split is done in such a way that each sentence in a pair of sentences either appear together in the training set or test set but not both. Unlike the previous dataset, the OpenSubtitles is quite large, and rather noisy because consecutive sentences may be uttered by the same character. Given the broad scope of movies, this is an open-domain conversation dataset, contrasting with the technical troubleshooting dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we describe the experimental results with the two datasets and show some samples of the interactions with the system that we trained. We also compare the performance of our system against a popular rule-based bot (CleverBot 1 ) using human evaluations on a set of 200 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">IT Helpdesk Troubleshooting experiments</head><p>In this experiment, we trained a single layer LSTM with 1024 memory cells using stochastic gradient descent with gradient clipping. The vocabulary consists of the most common 20K words, which includes special tokens indicating turn taking and actor.</p><p>At convergence, this model achieved a perplexity of 8, whereas an n-gram model achieved 18. Below is a few samples of simulated troubleshooting sessions for most three of the most common IT problems (remote access, software crashes, and password issues). In these conversations, Machine is our Neural Conversational Model, and Human the human actor interacting with it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">OpenSubtitles experiments</head><p>In this experiment, we trained a two-layered LSTM using AdaGrad with gradient clipping. Each layer of the LSTM has 4096 memory cells, and we built a vocabulary consisting of the most frequent 100K words. To speed up the softmax, we project the memory cells to 2048 linear units before feeding the information to the classifier.</p><p>At convergence the perplexity of the recurrent model on the validation set is 17. Our smoothed 5-gram model achieves a perplexity of 28. Interestingly, adding the soft attention mechanism of (Bahdanau et al., 2014) did not significantly improve the perplexity on neither training or validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">MODEL SAMPLES</head><p>In addition to the perplexity measure, our simple recurrent model does often produce plausible answers. Here are a few sample question-answer pairs that allowed us to understand the capabilities of the model when trained on the noisy but large dataset of OpenSubtitles from movies. Again, in these conversations, Machine is our Neural Conversational Model, and Human the human actor interacting with it. Conversation 6: Opinions (notice the model outputs "he" , "she" or "it" depending on the subject being asked) We find it encouraging that the model can remember facts, understand contexts, perform common sense reasoning without the complexity in traditional pipelines. What surprises us is that the model does so without any explicit knowledge representation component except for the parameters in the word vectors.</p><p>Perhaps most practically significant is the fact that the model can generalize to new questions. In other words, it does not simply look up for an answer by matching the question with the existing database. In fact, most of the questions presented above, except for the first conversation, do not appear in the training set.</p><p>Nonetheless, one drawback of this basic model is that it only gives simple, short, sometimes unsatisfying answers to our questions as can be seen above. Perhaps a more problematic drawback is that the model does not capture a consistent personality. Indeed, if we ask not identical but semantically similar questions, the answers can sometimes be inconsistent. This is expected due to the simplicity of our model and the dataset in our experiments. The conversation below illustrates such failure: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">HUMAN EVALUATION</head><p>In order to fairly and objectively compare our model against CleverBot, we picked 200 questions 3 , and asked four different humans to rate our model (NCM) versus Cle-verBot (CB). Human judges were asked to pick which of the two bots they preferred, and were also allowed to declare a tie if both answers were of equal quality.</p><p>If three out of four human judges agreed, we record that score. Our model was preferred in 97 out of 200 questions, whereas CleverBot was picked in 60 out of 200. There was a tie in 20 questions, and in 23 questions the judges were in disagreement.</p><p>Here are some samples of how the judges evaluated the systems (in bold we show their preferred answer, the third example shows a tie, and the fourth examples shows diagreement):</p><p>Human: What is the deepest spot on the world ? NCM: The pacific ocean. We however believe that it remains an open research problem of designing a good metric to quickly measure the quality of a conversational model. We did so by manual inspection, by using Mechanical Turk to get human evaluations comparing with a rule-based bot, and by computing the perplexity. However, there are obvious drawbacks on using those, and we are actively pursuing this and other research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this paper, we show that a simple language model based on the seq2seq framework can be used to train a conversational engine. Our modest results show that it can generate simple and basic conversations, and extract knowledge from a noisy but open-domain dataset. Even though the model has obvious limitations, it is surprising to us that a purely data driven approach without any rules can produce rather proper answers to many types of questions. However, the model may require substantial modifications to be able to deliver realistic conversations. Amongst the many limitations, the lack of a coherent personality makes it difficult for our system to pass the Turing test <ref type="bibr" target="#b14">(Turing, 1950)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Machine: is it a windows machine ? Machine: we have linux on your machine . Machine: is it windows xp or windows 7 ? Human: i am using linux Machine: let me take a remote of your machine and check if issue has been pushed properly .</figDesc><table><row><cell></cell><cell>Machine: hi Machine: ok great</cell></row><row><cell></cell><cell>Human: hello Human: can you tell me how i can recover it ?</cell></row><row><cell></cell><cell>Machine: could you please let me know what are Machine: i ' ll need to verify who it is at that</cell></row><row><cell></cell><cell>the operating systems you are using ? machine</cell></row><row><cell></cell><cell>Human: i am using linux Machine: can we do a hangout</cell></row><row><cell></cell><cell>Machine: ok it was in the ssl vpn on which you Machine: ?</cell></row><row><cell>Human: ok</cell><cell>stated it was broken Human: yes</cell></row><row><cell>Machine: please open terminal and run the be-</cell><cell>Machine: please check if you are logged in to Machine: &lt;anonymized hangout instructions</cell></row><row><cell>low commands in one computer :</cell><cell>corp network and verifications&gt;</cell></row><row><cell>Machine: sudo apt -get update</cell><cell>Machine: what is the error that you are running Human: cool , i am good now</cell></row><row><cell>Machine: sudo apt -get upgrade</cell><cell>please Machine: how ' s this looking good ?</cell></row><row><cell>Machine: sudo &lt;COMMAND&gt; -updater</cell><cell>Human: i am seeing an error related to vpn Machine: i ' ll go ahead and close this chat .</cell></row><row><cell>Human: done</cell><cell>Machine: what is the error message that you are Machine: take care . : )</cell></row><row><cell>Machine: let me try connecting now</cell><cell>getting when connecting to vpn using network Human: bye</cell></row><row><cell>Human: ok</cell><cell>connect ? Machine: bye .</cell></row><row><cell>Machine: its done</cell><cell>Human: connection refused or something like</cell></row><row><cell>Machine: should use the second or hit run on</cell><cell>that</cell></row><row><cell>the background</cell><cell>Machine: may i know the version of network</cell></row><row><cell>Human: done</cell><cell>connect you connect ?</cell></row><row><cell>Machine: try signing out now and follow the</cell><cell>Human: i am not sure i know that</cell></row><row><cell>prompts as per the message given above</cell><cell>Machine: is the network connect prompting for</cell></row><row><cell>Machine: shd i reboot the machine .</cell><cell>any user certificate</cell></row><row><cell>Human: ok , please go ahead</cell><cell>Machine: ?</cell></row><row><cell>Machine: once its completed please try to login</cell><cell>Human: yes</cell></row><row><cell>.</cell><cell>Machine: are you at home or at work at the mo-</cell></row><row><cell>Human: sure</cell><cell>ment ?</cell></row><row><cell>Machine: please ping me back once you have</cell><cell>Machine: office ?</cell></row><row><cell>tried to login</cell><cell>Human: i am at home</cell></row><row><cell>Machine: are we still connected</cell><cell>Machine: try this</cell></row><row><cell>Human: i now logged in</cell><cell>Machine: goto &lt;URL&gt; page and enter your</cell></row><row><cell>Machine: thanks for your confirmation</cell><cell>name and password</cell></row><row><cell>Human: what should i do now ?</cell><cell>Human: done</cell></row><row><cell>Machine: wait for 1 5 minutes to complete the</cell><cell>Machine: try logging in with &lt;NAME&gt; and</cell></row><row><cell>login policy</cell><cell>check if its working fine now</cell></row><row><cell>Machine: and then try logging in</cell><cell>Human: yes , now it works !</cell></row><row><cell>Human: ok</cell><cell>Machine: great</cell></row><row><cell>Machine: is there anything else to help you ?</cell><cell>Machine: anything else that i can help</cell></row><row><cell>Human: let my try browsing a little bit and if it</cell><cell>Human: no , that was very useful</cell></row><row><cell>keeps crashing i will let you know</cell><cell>Machine: you are other than &lt;NAME&gt; : )</cell></row><row><cell>Machine: sure will do</cell><cell>Machine: have a nice night</cell></row><row><cell>Machine: seems the mail is not loading</cell><cell></cell></row><row><cell>Machine: are we still connected to the chat ?</cell><cell>(The &lt;URL&gt; indeed contains information about vpn ac-</cell></row><row><cell>Machine: &lt;reinvite&gt;</cell><cell>cess.)</cell></row><row><cell>Human: it seems good now , cool , thank you , bye</cell><cell>Conversation 2: Browser issues.</cell></row><row><cell>Machine: you , too Machine: bye</cell><cell>Describe your problem: browser keeps crashing Machine: hi</cell></row><row><cell></cell><cell>Human: hi</cell></row><row><cell>Conversation 1: VPN issues. Conversation 3: Password issues.</cell><cell>Machine: could you please let me know what is</cell></row><row><cell></cell><cell>the operating system you are using ?</cell></row><row><cell>Describe your problem: i forgot my password Describe your problem: i am having issues ac-Machine: hi , this is &lt;NAME&gt; from helpdesk cessing vpn connect , what can i help you with today ?</cell><cell>Human: linux Machine: ok this is the issue with the system per-formance , nasser , tools .</cell></row><row><cell>1 http://www.cleverbot.com/ Human: hi , i forgot my password</cell><cell>Human: what is that ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>What is the name of your husband.</figDesc><table><row><cell>CleverBot: What is the objective of sistetic life ?</cell><cell></cell></row><row><cell>Human: Does a cat have a wing ?</cell><cell></cell></row><row><cell>CleverBot: Human: Can a cat fly ?</cell><cell></cell></row><row><cell>CleverBot:: What is your husband's name ?</cell><cell></cell></row><row><cell>Human: Who is skywalker ?</cell><cell></cell></row><row><cell>CleverBot: There are two of them, Anakin Sky-walker and Luke Skywalker, both characters from</cell><cell>Conversation 7: Job and Personality</cell></row><row><cell>star Wars. Human: How many legs does a cat have ? CleverBot: An ant. Human: How many legs does a centipede have ?</cell><cell>Human: what is your job ? Machine: i 'm a lawyer . Human: what do you do ? Machine: i 'm a doctor .</cell></row><row><cell>CleverBot: A centipede's number of legs varies</cell><cell></cell></row><row><cell>from thirty to two hundred.</cell><cell>Lastly, as a comparison of other traditional bots that are</cell></row><row><cell>Human: What is your job ?</cell><cell>rule-based, we provide the output of CleverBot for some of</cell></row><row><cell>CleverBot: I work in mental health.</cell><cell>the questions in our test: 2</cell></row><row><cell>Human: What do you do ?</cell><cell></cell></row><row><cell>CleverBot: I have no job.</cell><cell>Human: Are you a follower or a leader ?</cell></row><row><cell></cell><cell>CleverBot: No !!</cell></row><row><cell></cell><cell>Human: My name is david . what is my name ?</cell></row><row><cell></cell><cell>CleverBot: How old are you David ?</cell></row><row><cell></cell><cell>Human: My name is john . what is my name ?</cell></row><row><cell></cell><cell>CleverBot: John</cell></row><row><cell></cell><cell>Human: What is the purpose of being intelligent</cell></row><row><cell></cell><cell>?</cell></row><row><cell></cell><cell>CleverBot: The purpose of life is to love God</cell></row><row><cell></cell><cell>and love people.</cell></row><row><cell></cell><cell>Human: Is sky blue or black ?</cell></row><row><cell></cell><cell>CleverBot: I am 200 years old.</cell></row><row><cell></cell><cell>Human: What is the color of a yellow car ?</cell></row><row><cell></cell><cell>CleverBot: Brown.</cell></row><row><cell></cell><cell>Human: What is the usual color of a leaf ?</cell></row><row><cell></cell><cell>CleverBot: What is the color of the apple in the</cell></row><row><cell></cell><cell>fridge ?</cell></row><row><cell></cell><cell>Human: What is the purpose of life ?</cell></row><row><cell></cell><cell>CleverBot: My purpose it to forward my species,</cell></row><row><cell></cell><cell>in other words to make it easier for future gener-</cell></row><row><cell></cell><cell>ations of mankind to live.</cell></row><row><cell></cell><cell>Human: What do you think about tesla ?</cell></row><row><cell></cell><cell>CleverBot: I am tesla.</cell></row><row><cell></cell><cell>Human: What do you think about cleopatra ?</cell></row><row><cell></cell><cell>CleverBot: Cool is cool.</cell></row><row><cell></cell><cell>Human: What do you think about england dur-</cell></row><row><cell></cell><cell>ing the reign of elizabeth ?</cell></row><row><cell></cell><cell>CleverBot: I like him. Do you ?</cell></row><row><cell></cell><cell>Human: What do you think about bill gates ?</cell></row><row><cell></cell><cell>CleverBot: Lets talk about Link.</cell></row><row><cell></cell><cell>Human: What is moral ?</cell></row><row><cell></cell><cell>CleverBot: Morality is the quality of being in</cell></row><row><cell></cell><cell>accord with standards of right or good conduct.</cell></row><row><cell></cell><cell>Human: What is immoral ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>love Mexican food. Human:NCM: You're the one who's crazy. CB: Who are you ?NCM: Sixty ? CB: What should we talk about ?</head><label></label><figDesc>Who are you crazy about ? /ai.stanford.edu/˜quocle/QAresults.pdf Human: What's thirty plus forty ?</figDesc><table><row><cell cols="3">CB: What ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Human: Do you like Mexican food or Indian</cell><cell></cell></row><row><cell cols="2">food ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">NCM: I hate it.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CB: I The</cell><cell>questions</cell><cell>we</cell><cell>used</cell><cell cols="2">together</cell></row><row><cell>with</cell><cell>the</cell><cell>answers</cell><cell>can</cell><cell>be</cell><cell>found</cell><cell>in</cell></row><row><cell>http:/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The outputs of CleverBot are non-deterministic so the answers may be different for different trials.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Greg Corrado, Andrew Dai, Jeff Dean, Tom Dean, Matthieu Devin, Rajat Monga, Mike Schuster, Noam Shazeer, Ilya Sutskever and the Google Brain team for the help with the project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janvin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Pearson International</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Branting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Internet Computing</title>
		<imprint>
			<publisher>Chapman &amp; Hall</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8206</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Statistical Language Models based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">News from OPUS -A collection of multilingual parallel corpora with tools and interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing</title>
		<editor>Nicolov, N., Bontcheva, K., Angelova, G., and Mitkov, R.</editor>
		<meeting><address><addrLine>Philadelphia, Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">V</biblScope>
			<biblScope unit="page" from="90" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Computing machinery and intelligence. Mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Turing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<biblScope unit="page" from="433" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7449</idno>
		<title level="m">Grammar as a foreign language</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Creating a Dynamic Speech Dialogue. VDM Verlag Dr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Will</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
