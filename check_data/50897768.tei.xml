<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Texture Networks: Feed-forward Synthesis of Textures and Stylized Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
							<email>dmitry.ulyanov@skoltech.ru</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
							<email>vedaldi@robots.ox.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
							<email>lempitsky@skoltech.ru</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Skolkovo Institute of Science and Technology &amp; Yandex</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Skolkovo Institute of Science and Technology &amp; Yandex</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Skolkovo Institute of Science and Technology</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Texture Networks: Feed-forward Synthesis of Textures and Stylized Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memoryconsuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably lightweight and can generate textures of quality comparable to Gatys et al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Several recent works demonstrated the power of deep neural networks in the challenging problem of generating images. Most of these proposed generative networks that produce images as output, using feed-forward calculations from a random seed; however, very impressive results were obtained by <ref type="bibr" target="#b4">(Gatys et al., 2015a;</ref><ref type="bibr">b</ref>) by using networks descriptively, as image statistics. Their idea is to reduce image generation to the problem of sampling at random from The source code and pretrained models are available at https: //github.com/DmitryUlyanov/texture_nets the set of images that match a certain statistics. In texture synthesis <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref>, the reference statistics is extracted from a single example of a visual texture, and the goal is to generate further examples of that texture. In style transfer <ref type="bibr" target="#b5">(Gatys et al., 2015b)</ref>, the goal is to match simultaneously the visual style of a first image, captured using some low-level statistics, and the visual content of a second image, captured using higher-level statistics. In this manner, the style of an image can be replaced with the one of another without altering the overall semantic content of the image.</p><p>Matching statistics works well in practice, is conceptually simple, and demonstrates that off-the-shelf neural networks trained for generic tasks such as image classification can be re-used for image generation. However, the approach of <ref type="bibr" target="#b4">(Gatys et al., 2015a;</ref><ref type="bibr">b)</ref> has certain shortcomings too. Being based on an iterative optimization procedure, it requires backpropagation to gradually change the values of the pixels until the desired statistics is matched. This iterative procedure requires several seconds in order to generate a relatively small image using a high-end GPU, while scaling to large images is problematic because of high memory requirements. By contrast, feed-forward generation networks can be expected to be much more efficient because they require a single evaluation of the network and do not incur in the cost of backpropagation.</p><p>In this paper we look at the problem of achieving the synthesis and stylization capability of descriptive networks using feed-forward generation networks. Our contribution is threefold. First, we show for the first time that a generative approach can produce textures of the quality and diversity comparable to the descriptive method. Second, we propose a generative method that is two orders of magnitude faster and one order of magnitude more memory efficient than the arXiv:1603.03417v1 <ref type="bibr">[cs.CV]</ref>   <ref type="figure">Figure 1</ref>. Texture networks proposed in this work are feed-forward architectures capable of learning to synthesize complex textures based on a single training example. The perceptual quality of the feed-forwardly generated textures is similar to the results of the closely related method suggested in <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref>, which use slow optimization process.</p><p>descriptive one. Using a single forward pass in networks that are remarkably compact make our approach suitable for video-related and possibly mobile applications. Third, we devise a new type of multi-scale generative architecture that is particularly suitable for the tasks we consider.</p><p>The resulting fully-convolutional networks (that we call texture networks) can generate textures and process images of arbitrary size. Our approach also represents an interesting showcase of training conceptually-simple feedforward architectures while using complex and expressive loss functions. We believe that other interesting results can be obtained using this principle.</p><p>The rest of the paper provides the overview of the most related approaches to image and texture generation (Sect. 2), describes our approach (Sect. 3), and provides extensive extensive qualitative comparisons on challenging textures and images (Sect. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and related work</head><p>Image generation using neural networks. In general, one may look at the process of generating an image x as the problem of drawing a sample from a certain distribution p(x). In texture synthesis, the distribution is induced by an example texture instance x 0 (e.g. a polka dots image), such that we can write x ∼ p(x|x 0 ). In style transfer, the distribution is induced by an image x 0 representative of the visual style (e.g. an impressionist painting) and a second image x 1 representative of the visual content (e.g. a boat), such that x ∼ p(x|x 0 , x 1 ). <ref type="bibr" target="#b13">(Mahendran &amp; Vedaldi, 2015;</ref><ref type="bibr" target="#b4">Gatys et al., 2015a;</ref><ref type="bibr">b)</ref> reduce this problem to the one of finding a pre-image of a certain image statistics Φ(x) ∈ R d and pose the latter as an optimization problem. In particular, in order to synthesize a texture from an example image x 0 , the pre-image problem is:</p><formula xml:id="formula_0">argmin x∈X Φ(x) − Φ(x 0 ) 2 2 .<label>(1)</label></formula><p>Importantly, the pre-image x : Φ(x) ≈ Φ(x 0 ) is usually not unique, and sampling pre-images achieves diversity. In practice, samples are extracted using a local optimization algorithm A starting from a random initialization z. Therefore, the generated image is the output of the function</p><formula xml:id="formula_1">localopt x∈X ( Φ(x) − Φ(x 0 ) 2 2 ; A, z), z ∼ N (0, Σ). (2)</formula><p>This results in a distribution p(x|x 0 ) which is difficult to characterise, but is easy to sample and, for good statistics Φ, produces visually pleasing and diverse images. Both <ref type="bibr" target="#b13">(Mahendran &amp; Vedaldi, 2015)</ref> and <ref type="bibr" target="#b4">(Gatys et al., 2015a;</ref><ref type="bibr">b)</ref> base their statistics on the response that x induces in deep neural network layers. Our approach reuses in particular the statistics based on correlations of convolutional maps proposed by <ref type="bibr" target="#b4">(Gatys et al., 2015a;</ref><ref type="bibr">b)</ref>.</p><p>Descriptive texture modelling. The approach described above has strong links to many well-known models of visual textures. For texture, it is common to assume that p(x) is a stationary Markov random field (MRF). In this case, the texture is ergodic and one may considers local spatiallyinvariant statistics ψ • F (x; i), i ∈ Ω, where i denotes a spatial coordinate. Often F is the output of a bank of linear filters and ψ an histogramming operator. Then the spatial average of this local statistics on the prototype texture x 0 approximates its sample average</p><formula xml:id="formula_2">φ(x 0 ) = 1 |Ω| |Ω| i=1 ψ •F (x 0 ; i) ≈ E x∼p(x) [ψ •F l (x; 0)]. (3)</formula><p>The FRAME model of <ref type="bibr" target="#b18">(Zhu et al., 1998)</ref> uses this fact to induce the maximum-entropy distribution over textures</p><formula xml:id="formula_3">p(x) ∝ exp(− λ, φ(x) ),</formula><p>where λ is a parameter chosen so that the marginals match their empirical estimate, i.e.</p><formula xml:id="formula_4">E x∼p(x) [φ(x)] = φ(x 0 ).</formula><p>A shortcoming of FRAME is the difficulty of sampling from the maxent distribution. <ref type="bibr" target="#b14">(Portilla &amp; Simoncelli, 2000)</ref> addresses this limitation by proposing to directly find images x that match the desired statistics Φ(x) ≈ Φ(x 0 ), pioneering the pre-image method of (1).</p><p>Where <ref type="bibr" target="#b18">(Zhu et al., 1998;</ref><ref type="bibr" target="#b14">Portilla &amp; Simoncelli, 2000)</ref> use linear filters, wavelets, and histograms to build their texture statistics, <ref type="bibr" target="#b13">(Mahendran &amp; Vedaldi, 2015;</ref><ref type="bibr" target="#b4">Gatys et al., 2015a</ref>;a) extract statistics from pre-trained deep neural networks. <ref type="bibr" target="#b5">(Gatys et al., 2015b)</ref> differs also in that it considers the style transfer problem instead of the texture synthesis one.</p><p>Generator deep networks. An alternative to using a neural networks as descriptors is to construct generator networks x = g(z) that produce directly an image x starting from a vector of random or deterministic parameters z.</p><p>Approaches such as <ref type="bibr" target="#b2">(Dosovitskiy et al., 2015</ref>) learn a mapping from deterministic parameters z (e.g. the type of object imaged and the viewpoint) to an image x. This is done by fitting a neural network to minimize the discrepancy</p><formula xml:id="formula_5">x i − g(z i ) for known image-parameter pairs (x i , z i )</formula><p>. While this may produce visually appealing results, it requires to know the relation (x, z) beforehand and cannot express any diversity beyond the one captured by the parameters.</p><p>An alternative is to consider a function g(z) where the parameters z are unknown and are sampled from a (simple) random distribution. The goal of the network is to map these random values to plausible images x = g(z). This requires measuring the quality of the sample, which is usually expressed as a distance between x and a set of example images x 1 , . . . , x n . The key challenge is that the distance must be able to generalize significantly from the available examples in order to avoid penalizing sample diversity.</p><p>Generative Adversarial Networks (GAN; <ref type="bibr" target="#b6">(Goodfellow et al., 2014)</ref>) address this problem by training, together with the generator network g(z), a second adversarial network f (x) that attempts to distinguish between samples g(z) and natural image samples. Then f can be used as a measure of quality of the samples and g can be trained to optimize it. LAPGAN <ref type="bibr" target="#b1">(Denton et al., 2015</ref>) applies GAN to a Laplacian pyramid of convolutional networks and DC-GAN <ref type="bibr" target="#b15">(Radford et al., 2015)</ref> further optimizes GAN and learn is from very large datasets.</p><p>Moment matching networks. The maximum entropy model of <ref type="bibr" target="#b18">(Zhu et al., 1998)</ref> is closely related to the idea of Maximum Mean Discrepancy (MMD) introduced in <ref type="bibr" target="#b7">(Gretton et al., 2006)</ref>. Their key observation the expected value</p><formula xml:id="formula_6">µ p = E x∼p(x) [φ(x)]</formula><p>of certain statistics φ(x) uniquely identifies the distribution p. <ref type="bibr" target="#b11">(Li et al., 2015;</ref><ref type="bibr" target="#b3">Dziugaite et al., 2015)</ref> derive from it a loss function alternative to GAN by comparing the statistics averaged over network</p><formula xml:id="formula_7">samples 1 m m i=1 φ • g(z i ) to the statistics averaged over empirical samples 1 m m i=1 φ(x i ).</formula><p>They use it to train a Moment Matching Network (MMN) and apply it to generate small images such as MNIST digits. Our networks are similar to moment matching networks, but use very specific statistics and applications quite different from the considered in <ref type="bibr" target="#b11">(Li et al., 2015;</ref><ref type="bibr" target="#b3">Dziugaite et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Texture networks</head><p>We now describe the proposed method in detail. At a highlevel (see <ref type="figure">Figure 2</ref>), our approach is to train a feed-forward generator network g which takes a noise sample z as input and produces a texture sample g(z) as output. For style transfer, we extend this texture network to take both a noise sample z and a content image y and then output a new image g(y, z) where the texture has been applied to y as a visual style. A separate generator network is trained for each texture or style and, once trained, it can synthesize an arbitrary number of images of arbitrary size in an efficient, feed-forward manner.</p><p>A key challenge in training the generator network g is to construct a loss function that can assess automatically the quality of the generated images. For example, the key idea of GAN is to learn such a loss along with the generator network. We show in Sect. 3.1 that a very powerful loss can be derived from pre-trained and fixed descriptor networks using the statistics introduced in <ref type="bibr" target="#b4">(Gatys et al., 2015a;</ref><ref type="bibr">b)</ref>. Given the loss, we then discuss the architecture of the generator network for texture synthesis (Sect. 3.2) and then generalize it to style transfer (Sect 3.3). <ref type="figure">Figure 2</ref>. Overview of the proposed architecture (texture networks). We train a generator network (left) using a powerful loss based on the correlation statistics inside a fixed pre-trained descriptor network (right). Of the two networks, only the generator is updated and later used for texture or image synthesis. The conv block contains multiple convolutional layers and non-linear activations and the join block upsampling and channel-wise concatenation. Different branches of the generator network operate at different resolutions and are excited by noise tensors zi of different sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Texture and content loss functions</head><p>Our loss function is derived from <ref type="bibr" target="#b4">(Gatys et al., 2015a;</ref><ref type="bibr">b)</ref> and compares image statistics extracted from a fixed pretrained descriptor CNN (usually one of the VGG CNN (Simonyan &amp; Zisserman, 2014; Chatfield et al., 2014) which are pre-trained for image classification on the ImageNet ILSVRC 2012 data). The descriptor CNN is used to measure the mismatch between the prototype texture x 0 and the generated image x. Denote by F l i (x) the i-th map (feature channel) computed by the l-th convolutional layer by the descriptor CNN applied to image x. The Gram matrix G l (x) is defined as the matrix of scalar (inner) products between such feature maps:</p><formula xml:id="formula_8">G l ij (x) = F l i (x), F l j (x) .<label>(4)</label></formula><p>Given that the network is convolutional, each inner product implicitly sums the products of the activations of feature i and j at all spatial locations, computing their (unnormalized) empirical correlation. Hence G l ij (x) has the same general form as (3) and, being an orderless statistics of local stationary features, can be used as a texture descriptor.</p><p>In practice, <ref type="bibr" target="#b4">(Gatys et al., 2015a;</ref><ref type="bibr">b)</ref> use as texture descriptor the combination of several Gram matrices G l , l ∈ L T , where L T contains selected indices of convolutional layer in the descriptor CNN. This induces the following texture loss between images x and x 0 :</p><formula xml:id="formula_9">L T (x; x 0 ) = l∈L T G l (x) − G l (x 0 ) 2 2 .<label>(5)</label></formula><p>In addition to the texture loss (5), <ref type="bibr" target="#b5">(Gatys et al., 2015b)</ref> propose to use as content loss the one introduced by (Mahendran &amp; Vedaldi, 2015), which compares images based on the output F l i (x) of certain convolutional layers l ∈ L C (without computing further statistics such as the Gram matrices). In formulas</p><formula xml:id="formula_10">L C (x; y) = l∈L C N l i=1 F l i (x) − F l i (y) 2 2 ,<label>(6)</label></formula><p>where N l is the number of maps (feature channels) in layer l of the descriptor CNN. The key difference with the texture loss (5) is that the content loss compares feature activations at corresponding spatial locations, and therefore preserves spatial information. Thus this loss is suitable for content information, but not for texture information.</p><p>Analogously to <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref>, we use the texture loss (5) alone when training a generator network for texture synthesis, and we use a weighted combination of the texture loss (5) and the content loss (6) when training a generator network for stylization. In the latter case, the set L C does not includes layers as shallow as the set L T as only the high-level content should be preserved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generator network for texture synthesis</head><p>We now discuss the architecture and the training procedure for the generator network g for the task of texture synthesis. We denote the parameters of the generator network as θ. The network is trained to transform a noise vector z sampled from a certain distribution Z (which we set to be uniform i.i.d.) into texture samples that match, according to the texture loss (5), a certain prototype texture x 0 :</p><formula xml:id="formula_11">θ x0 = argmin θ E z∼Z [ L T (g(z; θ), x 0 ) ] .<label>(7)</label></formula><p>Network architecture. We experimented with several architectures for the generator network g. The simplest are chains of convolutional, non-linear activation, and upsampling layers that start from a noise sample z in the form of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content</head><p>Texture nets (ours) Gatys et al. Style <ref type="figure">Figure 3</ref>. Our approach can also train feed-forward networks to transfer style from artistic images (left). After training, a network can transfer the style to any new image (e.g. right) while preserving semantic content. For some styles (bottom row), the perceptual quality of the result of our feed-forward transfer is comparable with the optimization-based method <ref type="bibr" target="#b5">(Gatys et al., 2015b)</ref>, though for others the results are not as impressive (top row and (Supp.Material)).</p><p>a small feature map and terminate by producing an image. While models of this type produce reasonable results, we found that multi-scale architectures result in images with smaller texture loss and better perceptual quality while using fewer parameters and training faster. The reference texture x 0 is a tensor R M ×M ×3 containing three color channels. For simplicity, assume that the spatial resolution M is a power of two. The input noise z comprises K random tensors</p><formula xml:id="formula_12">z i ∈ R M i × M i , i = 1, 2, .</formula><p>. . , K (we use M = 256 and K = 5) whose entries are i.i.d. sampled from a uniform distribution. Each random noise tensor is first processed by a sequence of convolutional and non-linear activation layers, then upsampled by a factor of two, and finally concatenated as additional feature channels to the partially processed tensor from the scale below. The last full-resolution tensor is ultimately mapped to an RGB image x by a bank of 1 × 1 filters. <ref type="figure">Figure 2</ref> contains three convolutional layers, each of which is followed by a ReLU activation layer. The convolutional layers contain respectively × 3, 3 × 3 and 1 × 1 filters. Filers are computed densely (stride one) and applied using circular convolution to remove boundary effects, which is appropriate for textures. The number of feature channels, which equals the number of filters in the preceding bank, grows from a minimum of to a maximum of 40. The supplementary material specifies in detail the network configuration which has only ∼65K parameters, and can be compressed to ∼300 Kb of mem-ory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Each convolution block in</head><p>Upsampling layers use simple nearest-neighbour interpolation (we also experimented strided full-convolution <ref type="bibr" target="#b12">(Long et al., 2015;</ref><ref type="bibr" target="#b15">Radford et al., 2015)</ref>, but the results were not satisfying). We found that training benefited significantly from inserting batch normalization layers <ref type="bibr" target="#b8">(Ioffe &amp; Szegedy, 2015)</ref> right after each convolutional layer and, most importantly, right before the concatenation layers, since this balances gradients travelling along different branches of the network.</p><p>Learning. Learning optimizes the objective (7) using stochastic gradient descent (SGD). At each iteration, SGD draws a mini-batch of noise vectors z k , k = 1, . . . , B, performs forward evaluation of the generator network to obtained the corresponding images x k = g(z k , θ), performs forward evaluation of the descriptor network to obtain Gram matrices G l (x k ), l ∈ L T , and finally computes the loss (5) (note that the corresponding terms G l (x 0 ) for the reference texture are constant). After that, the gradient of the texture loss with respect to the generator network parameters θ is computed using backpropagation, and the gradient is used to update the parameters. Note that LAP-GAN <ref type="bibr" target="#b1">(Denton et al., 2015)</ref> also performs multi-scale processing, but uses layer-wise training, whereas our generator is trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Style transfer</head><p>In order to extend the method to the task of image stylization, we make several changes. Firstly, the generator net-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Gatys et al.</p><p>Texture nets (ours) Portilla, Simoncelli DCGAN <ref type="figure">Figure 4</ref>. Further comparison of textures generated with several methods including the original statistics matching method <ref type="bibr" target="#b14">(Portilla &amp; Simoncelli, 2000)</ref> and the DCGAN <ref type="bibr" target="#b15">(Radford et al., 2015)</ref> approach. Overall, our method and <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref> provide better results, our method being hundreds times faster.</p><p>work x = g(y, z; θ) is modified to take as input, in addition to the noise variable z, the image y to which the noise should be applied. The generator network is then trained to output an image x that is close in content to y and in texture/style to a reference texture x 0 . For example, y could be a photo of a person, and x 0 an impressionist painting.</p><p>Network architecture. The architecture is the same as the one used for texture synthesis with the important difference that now the noise tensors z i , i = 1, . . . , K at the K scales are concatenated (as additional feature channels) with downsampled versions of the input image y. For this application, we found beneficial to increased the number of scales from K = 5 to K = 6.</p><p>Learning. Learning proceeds by sampling noise vectors z i ∼ Z and natural images y i ∼ Y and then adjusting the parameters θ of the generator g(y i , z i ; θ) in order to minimize the combination of content and texture loss:</p><formula xml:id="formula_13">θ x0 = argmin θ E z∼Z; y∼Y [ (8) L T (g(y, z; θ), x 0 ) + α L C (g(y, z; θ), y) ] .</formula><p>Here Z is the same noise distribution as for texture synthesis, Y empirical distribution on naturals image (obtained from any image collection), and α a parameter that trades off preserving texture/style and content. In practice, we found that learning is surprisingly resilient to overfitting and that it suffices to approximate the distribution on natural images Y with a very small pool of images (e.g. 16).</p><p>In fact, our qualitative results degraded using too many example images. We impute this to the fact that stylization by a convolutional architecture uses local operations; since the same local structures exist in different combinations and proportions in different natural images y, it is difficult for local operators to match in all cases the overall statistics of the reference texture x 0 , where structures exist in a fixed arbitrary proportion. Despite this limitation, the perceptual quality of the generated stylized images is usually very good, although for some styles we could not match the quality of the original stylization by optimization of <ref type="bibr" target="#b5">(Gatys et al., 2015b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Further technical details. The generator network weights were initialized using Xavier's method. Training used Torch7's implementation of Adam <ref type="bibr" target="#b10">(Kingma &amp; Ba, 2014)</ref>, running it for 2000 iteration. The initial learning rate of 0.1 was reduced by a factor 0.7 at iteration 1000 and then again every 200 iterations. The batch size was set to 16. Similar to <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref>, the texture loss uses the layers L T = {relu1 1, relu2 1, relu3 1, relu4 1, relu5 1} of VGG-19 and the content loss the layer L C = {relu4 2}. Fully training a single model required just two hours on an NVIDIA Tesla K40, and visually appealing results could be obtained much faster, after just a few epochs.</p><p>Texture synthesis. We compare our method to <ref type="bibr" target="#b4">(Gatys et al., 2015a;</ref><ref type="bibr">b)</ref> using the popular implementation of (Johnson, 2015), which produces comparable if not better results k = 0.01 k = 0.1 k = 1 k = <ref type="figure">Figure 5</ref>. Our architecture for image stylization takes the content image and the noise vector as inputs. By scaling the input noise by different factors k we can affect the balance of style and content in the output image without retraining the network.</p><p>than the implementation eventually released by the authors. We also compare to the DCGAN <ref type="bibr" target="#b15">(Radford et al., 2015)</ref> version of adversarial networks <ref type="bibr" target="#b6">(Goodfellow et al., 2014)</ref>.</p><p>Since DCGAN training requires multiple example images for training, we extract those as sliding 64 × 64 patches from the 256 × 256 reference texture x 0 ; then, since DC-GAN is fully convolutional, we use it to generate larger × 256 images simply by inputting a larger noise tensor. Finally, we compare to <ref type="bibr" target="#b14">(Portilla &amp; Simoncelli, 2000)</ref>. <ref type="figure">Figure 4</ref> shows the results obtained by the four methods on two challenging textures of <ref type="bibr" target="#b14">(Portilla &amp; Simoncelli, 2000)</ref>. Qualitatively, our generator CNN and <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref>'s results are comparable and superior to the other methods; however, the generator CNN is much more efficient (see Sect. 4.1). <ref type="figure">Figure 1</ref> includes further comparisons between the generator network and <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref> and many others are included in the supplementary material.</p><p>Style transfer. For training, example natural images were extracted at random from the ImageNet ILSVRC data. As for the original method of (Gatys et al., 2015b), we found that style transfer is sensitive to the tradeoff parameter α between texture and content loss in (6). At test time this parameter is not available in our method, but we found that the trade-off can still be adjusted by changing the magnitude of the input noise z (see <ref type="figure">Figure 5</ref>).</p><p>We compared our method to the one of <ref type="bibr" target="#b5">(Gatys et al., 2015b;</ref><ref type="bibr" target="#b9">Johnson, 2015)</ref> using numerous style and content images, including the ones in <ref type="bibr" target="#b5">(Gatys et al., 2015b)</ref>, and found that results are qualitatively comparable. Representative comparisons (using a fixed parameter α) are included in <ref type="figure">Figure</ref> 3 and many more in the supplementary material. Other qualitative results are reported in <ref type="figure" target="#fig_2">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Speed and memory</head><p>We compare quantitatively the speed of our method and of the iterative optimization of <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref> by measuring how much time it takes for the latter and for our gen-  <ref type="figure">Figure 6</ref>. The objective values (log-scale) within the optimization-based method <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref> for three randomly chosen textures are plotted as functions of time. Horizontal lines show the style loss achieved by our feedforward algorithm (mean over several samples) for the same textures. It takes the optimization within <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref> around 10 seconds (500x slower than feedforward generation) to produce samples with comparable loss/objective. erator network to reach a given value of the loss L T (x, x 0 ). <ref type="figure">Figure 6</ref> shows that iterative optimization requires about 10 seconds to generate a sample x that has a loss comparable to the output x = g(z) of our generator network. Since an evaluation of the latter requires ∼20ms, we achieve a 500× speed-up, which is sufficient for real-time applications such as video processing. There are two reasons for this significant difference: the generator network is much smaller than the VGG-19 model evaluated at each iteration of <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref>, and our method requires a single network evaluation. By avoiding backpropagation, our method also uses significantly less memory (170 MB to generate a 256 × 256 sample, vs 1100 MB of <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We have presented a new deep learning approach for texture synthesis and image stylization. Remarkably, the ap- proach is able to generate complex textures and images in a purely feed-forward way, while matching the texture synthesis capability of <ref type="bibr" target="#b4">(Gatys et al., 2015a)</ref>, which is based on multiple forward-backward iterations. In the same vein as <ref type="bibr" target="#b6">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b3">Dziugaite et al., 2015;</ref><ref type="bibr" target="#b11">Li et al., 2015)</ref>, the success of this approach highlights the suitability of feed-forward networks for complex data generation and for solving complex tasks in general. The key to this success is the use of complex loss functions that involve different feed-forward architectures serving as "experts" assessing the performance of the feed-forward generator.</p><p>While our method generally obtains very good result for texture synthesis, going forward we plan to investigate better stylization losses to achieve a stylization quality comparable to <ref type="bibr" target="#b5">(Gatys et al., 2015b</ref>) even for those cases (e.g. <ref type="figure">Figure 3.</ref>top) where our current method achieves less impressive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Sample 1</head><p>Sample 2 Sample 512x256  Ablation example: here we generate examples from the pretrained networks after zeroing out all inputs to the multi-scale architecture except for one scale. This let us analyze the processes inside the generator. For peppers image we observe that depth K = 4 is enough for generator to perform well. Texture elements in starry night are bigger therefore the deepest input blob is used. Note that the generator is limited by the VGG network capacity and cannot capture larger texture elements than the receptive field of the last convolution layer. <ref type="figure">Figure 13</ref>. Style transfer results for more styles. As before, each row represents a network trained with particular style with the original style image on the left, and columns correspond to different content images, with the original image on the top. <ref type="figure">Figure 14</ref>. Being fully convolutional, our architecture is not bound to image resolution it was trained with. Above, a network trained to stylize 256×256 images was applied to 1024×768 reproduction of Pieter Bruegel's "The Tower of Babel".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure containsa high-level representation of our reference multi-scale architecture, which we describe next.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Stylization results for various styles and inputs (one network per row). Our approach can handle a variety of styles. The generated images are of 256x256 resolution and are computed in about 20 milliseconds each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Various samples drawn from three texture networks trained for the samples shown on the left. While training was done for 256x256 samples, in the right column we show the generated textures for a different resolution. More comparison with Gatys et al. for texture synthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Ablation example: here we generate examples from the pretrained networks after zeroing out all inputs to the multi-scale architecture except for one scale. This let us analyze the processes inside the generator. For peppers image we observe that depth K = 4 is enough for generator to perform well. Texture elements in starry night are bigger therefore the deepest input blob is used. Note that the generator is limited by the VGG network capacity and cannot capture larger texture elements than the receptive field of the last convolution layer.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material</head><p>Below, we provide several additional qualitative results demonstrating the performance of texture networks and comparing them to Gatys et al. approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">A note on generator architecture</head><p>Since the generator is only restricted to produce good images in terms of texture loss, nothing stops it from generating samples with small variance between them. Therefore, if a model archives lower texture loss it does not implicate that this model is preferable. The generator should be powerful enough to combine texture elements, but not too complex to degrade to similar samples. If degrading effect is noticed the noise amount increasing can help in certain cases. <ref type="figure">Figure 9</ref> shows a bad case, with too much iteration performed. This degrading effect is similar to overfitting but there is no obvious way to control it as there is no analogue of validation set available.  . The example of an overfitted generator. Although every "star" differs from each other in details you may notice a common structure. This also illustrates generators love (when overfitted) for diagonal, horizontal and vertical lines as 3x3 filters are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content</head><p>Texture nets (ours) Gatys et al. Style <ref type="figure">Figure 15</ref>. More style transfer comparisons with Gatys et al. For the styles above, the results of our approach are inferior to Gatys et al. It remains to be seen if more complex losses or deeper networks can narrow down or bridge this performance gap.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soumith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1506.05751</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tobias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>Conference on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1538" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training generative neural networks via maximum mean discrepancy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gintare</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karolina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>abs/1505.03906</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno>abs/1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems,NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning, ICML</title>
		<meeting>International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://github.com/jcjohnson/neural-style" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning, ICML</title>
		<meeting>International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition,CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="70" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soumith</surname></persName>
		</author>
		<idno>abs/1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Supp</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
