<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FAWN: A Fast Array of Wimpy Nodes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Franklin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Len</forename><surname>Key</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Data</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Data Log In-memory Hash Index Log Entry</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FAWN: A Fast Array of Wimpy Nodes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.4.7 [Operating Systems]: Organization and Design-Distributed Systems</term>
					<term>D.4.2 [Operating Systems]: Storage Management</term>
					<term>D.4.5 [Operating Systems]: Reliability-Faulttolerance</term>
					<term>D.4.8 [Operating Systems]: Performance-Measurements Design, Energy Efficiency, Performance, Measurement, Cluster Computing, Flash</term>
				</keywords>
			</textClass>
			<abstract>
				<p>This paper presents a new cluster architecture for low-power data-intensive computing. FAWN couples low-power embedded CPUs to small amounts of local flash storage, and balances computation and I/O capabilities to enable efficient, massively parallel access to data. The key contributions of this paper are the principles of the FAWN architecture and the design and implementation of FAWN-KV-a consistent, replicated, highly available, and high-performance key-value storage system built on a FAWN prototype. Our design centers around purely log-structured datastores that provide the basis for high performance on flash storage, as well as for replication and consistency obtained using chain replication on a consistent hashing ring. Our evaluation demonstrates that FAWN clusters can handle roughly 350 key-value queries per Joule of energy-two orders of magnitude more than a disk-based system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale data-intensive applications, such as highperformance key-value storage systems, are growing in both size and importance; they now are critical parts of major Internet services such as Amazon (Dynamo <ref type="bibr" target="#b9">[10]</ref>), LinkedIn (Voldemort <ref type="bibr" target="#b40">[41]</ref>), and Facebook (memcached <ref type="bibr" target="#b32">[33]</ref>).</p><p>The workloads these systems support share several characteristics: they are I/O, not computation, intensive, requiring random access over large datasets; they are massively parallel, with thousands of concurrent, mostly-independent operations; their high load requires large clusters to support them; and the size of objects stored is typically small, e.g., 1 KB values for thumbnail images, 100s of bytes for wall posts, twitter messages, etc.</p><p>The clusters that serve these workloads must provide both high performance and low cost operation. Unfortunately, small-object random-access workloads are particularly illserved by conventional disk-based or memory-based clusters. The poor seek performance of disks makes disk-based systems inefficient in terms of both system performance and performance per watt. High performance DRAM-based clusters, storing terabytes or petabytes of data, are both expensive and consume a surprising amount of power-two 2 GB DIMMs consume as much energy as a 1 TB disk.</p><p>The power draw of these clusters is becoming an increasing fraction of their cost-up to 50% of the three-year total cost of owning a computer. The density of the datacenters that house them is in turn limited by their ability to supply and cool 10-20 kW of power per rack and up to 10-20 MW per datacenter <ref type="bibr" target="#b24">[25]</ref>. Future datacenters may require as much as 200 MW <ref type="bibr" target="#b24">[25]</ref>, and datacenters are being constructed today with dedicated electrical substations to feed them.</p><p>These challenges necessitate the question: Can we build a cost-effective cluster for data-intensive workloads that uses less than a tenth of the power required by a conventional architecture, but that still meets the same capacity, availability, throughput, and latency requirements?</p><p>In this paper, we present the FAWN architecture-a Fast Array of Wimpy Nodes-that is designed to address this question. FAWN couples low-power, efficient embedded CPUs with flash storage to provide efficient, fast, and cost-effective access to large, random-access data. Flash is significantly faster than disk, much cheaper than the equivalent amount of DRAM, and consumes less power than either. Thus, it is a particularly suitable choice for FAWN and its workloads. FAWN creates a well-matched system architecture around flash: each node can use the full capacity of the flash without memory or bus bottlenecks, but does not waste excess power.</p><p>To show that it is practical to use these constrained nodes as the core of a large system, we have designed and built the FAWN-KV cluster-based key-value store, which provides storage functionality similar to that used in several large enterprises <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b32">33]</ref>. FAWN-KV is designed specifically with the FAWN hardware in mind, and is able to exploit the advantages and avoid the limitations of wimpy nodes with flash memory for storage.</p><p>The key design choice in FAWN-KV is the use of a logstructured per-node datastore called FAWN-DS that provides high performance reads and writes using flash memory. This append-only data log provides the basis for replication and strong consistency using chain replication <ref type="bibr" target="#b53">[54]</ref> between nodes. Data is distributed across nodes using consistent hashing, with data split into contiguous ranges on disk such that all replication and node insertion operations involve only a fully in-order traversal of the subset of data that must be copied to a new node. Together with the log structure, these properties combine to provide fast failover and fast node insertion, and they minimize the time the affected datastore's key range is locked during such operations-for a single node failure and recovery, the affected key range is blocked for at most 100 milliseconds.</p><p>We have built a prototype 21-node FAWN cluster using 500 MHz embedded CPUs. Each node can serve up to 1300 256-byte queries per second, exploiting nearly all of the raw I/O capability of their attached flash devices, and consumes under 5 W when network and support hardware is taken into account. The FAWN cluster achieves 364 queries per Jouletwo orders of magnitude better than traditional disk-based clusters.</p><p>In Section 5, we compare a FAWN-based approach to other architectures, finding that the FAWN approach provides significantly lower total cost and power for a significant set of large, high-query-rate applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Why FAWN?</head><p>The FAWN approach to building well-matched cluster systems has the potential to achieve high performance and be fundamentally more energy-efficient than conventional architectures for serving massive-scale I/O and data-intensive workloads. We measure system performance in queries per second and measure energy-efficiency in queries per Joule (equivalently, queries per second per Watt). FAWN is inspired by several fundamental trends:</p><p>Increasing CPU-I/O Gap: Over the last several decades, the gap between CPU performance and I/O bandwidth has continually grown. For data-intensive computing workloads, storage, network, and memory bandwidth bottlenecks often cause low CPU utilization.</p><p>FAWN Approach: To efficiently run I/O-bound dataintensive, computationally simple applications, FAWN uses wimpy processors selected to reduce I/O-induced idle cycles while maintaining high performance. The reduced processor speed then benefits from a second trend:</p><p>CPU power consumption grows super-linearly with speed. Operating processors at higher frequency requires more energy, and techniques to mask the CPU-memory bottleneck come at the cost of energy efficiency. Branch prediction, speculative execution, out-of-order/superscalar execution and increasing the amount of on-chip caching all require additional processor die area; modern processors dedicate as much as half their die to L2/3 caches <ref type="bibr" target="#b20">[21]</ref>. These techniques do not increase the speed of basic computations, but do increase power consumption, making faster CPUs less energy efficient.</p><p>FAWN Approach: A FAWN cluster's slower CPUs dedicate more transistors to basic operations. These CPUs execute significantly more instructions per Joule than their faster counterparts: multi-GHz superscalar quad-core processors can execute approximately 100 million instructions per Joule, assuming all cores are active and avoid stalls or mispredictions. Lower-frequency in-order CPUs, in contrast, can provide over 1 billion instructions per Joule-an order of magnitude more efficient while still running at 1/3rd the frequency.</p><p>Worse yet, running fast processors below their full capacity draws a disproportionate amount of power:</p><p>Dynamic power scaling on traditional systems is surprisingly inefficient. A primary energy-saving benefit of dynamic voltage and frequency scaling (DVFS) was its ability to reduce voltage as it reduced frequency <ref type="bibr" target="#b55">[56]</ref>, but modern CPUs already operate near minimum voltage at the highest frequencies.</p><p>Even if processor energy was completely proportional to load, non-CPU components such as memory, motherboards, and power supplies have begun to dominate energy consumption <ref type="bibr" target="#b2">[3]</ref>, requiring that all components be scaled back with demand. As a result, running a modern, DVFS-enabled system at 20% of its capacity may still consume over 50% of its peak power <ref type="bibr" target="#b51">[52]</ref>. Despite improved power scaling technology, systems remain most energy-efficient when operating at peak utilization.</p><p>A promising path to energy proportionality is turning machines off entirely <ref type="bibr" target="#b6">[7]</ref>. Unfortunately, these techniques do not apply well to FAWN-KV's target workloads: key-value systems must often meet service-level agreements for query response throughput and latency of hundreds of milliseconds; the inter-arrival time and latency bounds of the requests prevents shutting machines down (and taking many seconds to wake them up again) during low load <ref type="bibr" target="#b2">[3]</ref>.</p><p>Finally, energy proportionality alone is not a panacea: systems ideally should be both proportional and efficient at 100% load. In this paper, we show that there is significant room to improve energy efficiency, and the FAWN approach provides a simple way to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design and Implementation</head><p>We describe the design and implementation of the system components from the bottom up: a brief overview of flash storage (Section 3.2), the per-node FAWN-DS datastore (Section 3.3), and the FAWN-KV cluster key-value lookup system (Section 3.4), including caching, replication, and consistency. <ref type="figure" target="#fig_0">Figure 1</ref> gives an overview of the entire FAWN system. Client requests enter the system at one of several front-ends. The front-end nodes forward the request to the back-end FAWN-KV node responsible for serving that particular key. The backend node serves the request from its FAWN-DS datastore and returns the result to the front-end (which in turn replies to the client). Writes proceed similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Overview</head><p>The large number of back-end FAWN-KV storage nodes are organized into a ring using consistent hashing. As in systems such as Chord <ref type="bibr" target="#b47">[48]</ref>, keys are mapped to the node that follows the key in the ring (its successor). To balance load and reduce failover times, each physical node joins the ring as a small number (V ) of virtual nodes, each virtual node representing a virtual ID ("VID ") in the ring space. Each physical node is thus responsible for V different (noncontiguous) key ranges. The data associated with each virtual ID is stored on flash using FAWN-DS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Understanding Flash Storage</head><p>Flash provides a non-volatile memory store with several significant benefits over typical magnetic hard disks for randomaccess, read-intensive workloads-but it also introduces several challenges. Three characteristics of flash underlie the design of the FAWN-KV system described throughout this section:</p><p>1. Fast random reads: ( 1 ms), up to 175 times faster than random reads on magnetic disk <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref>. 2. Efficient I/O: Flash devices consume less than one Watt even under heavy load, whereas mechanical disks can consume over 10 W at load. Flash is over two orders of magnitude more efficient than mechanical disks in terms of queries/Joule. 3. Slow random writes: Small writes on flash are very expensive. Updating a single page requires first erasing an entire erase block (128 KB-256 KB) of pages, and then writing the modified block in its entirety. As a result, updating a single byte of data is as expensive as writing an entire block of pages <ref type="bibr" target="#b36">[37]</ref>. Modern devices improve random write performance using write buffering and preemptive block erasure. These techniques improve performance for short bursts of writes, but recent studies show that sustained random writes still perform poorly on these devices <ref type="bibr" target="#b39">[40]</ref>.</p><p>These performance problems motivate log-structured techniques for flash filesystems and data structures <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b22">23]</ref>. These same considerations inform the design of FAWN's node storage management system, described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The FAWN Data Store</head><p>FAWN-DS is a log-structured key-value store. Each store contains values for the key range associated with one virtual ID. It acts to clients like a disk-based hash table that supports Store, Lookup, and Delete. <ref type="bibr" target="#b0">1</ref> FAWN-DS is designed specifically to perform well on flash storage and to operate within the constrained DRAM available on wimpy nodes: all writes to the datastore are sequential, and reads require a single random access. To provide this property, FAWN-DS maintains an in-DRAM hash table (Hash Index) that maps keys to an offset in the append-only Data Log on flash <ref type="figure">(Figure 2a</ref>). This log-structured design is similar to several append-only filesystems <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b14">15]</ref>, which avoid random seeks on magnetic disks for writes.</p><p>Mapping a Key to a Value. FAWN-DS uses an in-memory (DRAM) Hash Index to map 160-bit keys to a value stored in the Data Log. It stores only a fragment of the actual key in memory to find a location in the log; it then reads the full key (and the value) from the log and verifies that the key it read was, in fact, the correct key. This design trades a small and configurable chance of requiring two reads from flash (we set it to roughly 1 in 32,768 accesses) for drastically reduced memory requirements (only six bytes of DRAM per key-value pair). <ref type="figure" target="#fig_2">Figure 3</ref> shows the pseudocode that implements this design for Lookup. FAWN-DS extracts two fields from the 160-bit key: the i low order bits of the key (the index bits) and the The constants involved (15 bits of key fragment, 4 bytes of log pointer) target the prototype FAWN nodes described in Section 4. A typical object size is between 256 B to 1 KB, and the nodes have 256 MB of DRAM and approximately 4 GB of flash storage. Because each node is responsible for V key ranges (each of which has its own datastore file), a single physical node can address 4 GB * V bytes of data. Expanding the in-memory storage to 7 bytes per entry would permit FAWN-DS to address 512 GB of data per key range. While some additional optimizations are possible, such as rounding the size of objects stored in flash or reducing the number of bits used for the key fragment (and thus incurring, e.g., a 1-in-1000 chance of having to do two reads from flash), the current design works well for the target key-value workloads we study.</p><p>Reconstruction. Using this design, the Data Log contains all the information necessary to reconstruct the Hash Index from scratch. As an optimization, FAWN-DS periodically checkpoints the index by writing the Hash Index and a pointer to the last log entry to flash. After a failure, FAWN-DS uses the checkpoint as a starting point to reconstruct the in-memory Hash Index quickly.  Virtual IDs and Semi-random Writes. A physical node has a separate FAWN-DS datastore file for each of its virtual IDs, and FAWN-DS appends new or updated data items to the appropriate datastore. Sequentially appending to a small number of files is termed semi-random writes. Prior work by Nath and Gibbons observed that with many flash devices, these semi-random writes are nearly as fast as a single sequential append <ref type="bibr" target="#b35">[36]</ref>. We take advantage of this property to retain fast write performance while allowing key ranges to be stored in independent files to speed the maintenance operations described below. We show in Section 4 that these semi-random writes perform sufficiently well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Basic functions: Store, Lookup, Delete</head><p>Store appends an entry to the log, updates the corresponding hash table entry to point to this offset within the Data Log, and sets the valid bit to true. If the key written already existed, the old value is now orphaned (no hash entry points to it) for later garbage collection.</p><p>Lookup retrieves the hash entry containing the offset, indexes into the Data Log, and returns the data blob.</p><p>Delete invalidates the hash entry corresponding to the key by clearing the valid flag and writing a Delete entry to the end of the data file. The delete entry is necessary for faulttolerance-the invalidated hash table entry is not immediately committed to non-volatile storage to avoid random writes, so a failure following a delete requires a log to ensure that recovery will delete the entry upon reconstruction. Because of its log structure, FAWN-DS deletes are similar to store operations with 0-byte values. Deletes do not immediately reclaim space and require compaction to perform garbage collection. This design defers the cost of a random write to a later sequential write operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Maintenance: Split, Merge, Compact</head><p>Inserting a new virtual node into the ring causes one key range to split into two, with the new virtual node gaining responsibility for the first part of it. Nodes handling these VIDs must therefore Split their datastore into two datastores, one for each key range. When a virtual node departs the system, two adjacent key ranges must similarly Merge into a single datastore. In addition, a virtual node must periodically Compact its datastores to clean up stale or orphaned entries created by Split, Store, and Delete.</p><p>The design of FAWN-DS ensures that these maintenance functions work well on flash, requiring only scans of one datastore and sequential writes into another. We briefly discuss each operation in turn.</p><p>Split parses the Data Log sequentially, writing each entry in a new datastore if its key falls in the new datastore's range. Merge writes every log entry from one datastore into the other datastore; because the key ranges are independent, it does so as an append. Split and Merge propagate delete entries into the new datastore.</p><p>Compact cleans up entries in a datastore, similar to garbage collection in a log-structured filesystem. It skips entries that fall outside of the datastore's key range, which may be left-over after a split. It also skips orphaned entries that no in-memory hash table entry points to, and then skips any delete entries corresponding to those entries. It writes all other valid entries into the output datastore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Concurrent Maintenance and Operation</head><p>All FAWN-DS maintenance functions allow concurrent read and write access to the datastore. Stores and Deletes only modify hash table entries and write to the end of the log.</p><p>The maintenance operations (Split, Merge, and Compact) sequentially parse the Data Log, which may be growing due to deletes and stores. Because the log is appendonly, a log entry once parsed will never be changed. These operations each create one new output datastore logfile. The maintenance operations therefore run until they reach the end of the log, and then briefly lock the datastore, ensure that all values flushed to the old log have been processed, update the FAWN-DS datastore list to point to the newly created log, and release the lock <ref type="figure">(Figure 2c</ref>). The lock must be held while writing in-flight appends to the log and updating datastore   <ref type="figure" target="#fig_3">Figure 4</ref> depicts FAWN-KV request processing. Client applications send requests to front-ends using a standard put/get interface. Front-ends send the request to the back-end node that owns the key space for the request. The back-end node satisfies the request using its FAWN-DS and replies to the front-ends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The FAWN Key-Value System</head><p>In a basic FAWN implementation, clients link against a front-end library and send requests using a local API. Extending the front-end protocol over the network is straightforward-for example, we have developed a drop-in replacement for the memcached distributed memory cache, enabling a collection of FAWN nodes to appear as a single, robust memcached server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Consistent Hashing: Key Ranges to Nodes</head><p>A typical FAWN cluster will have several front-ends and many back-ends. FAWN-KV organizes the back-end VIDs into a storage ring-structure using consistent hashing, similar to the Chord DHT <ref type="bibr" target="#b47">[48]</ref>. FAWN-KV does not use DHT routing-instead, front-ends maintain the entire node membership list and directly forward queries to the back-end node that contains a particular data item.</p><p>Each front-end node manages the VID membership list and queries for a large contiguous chunk of the key space (in other words, the circular key space is divided into piewedges, each owned by a front-end). A front-end receiving queries for keys outside of its range forwards the queries to the appropriate front-end node. This design either requires clients to be roughly aware of the front-end mapping, or doubles the traffic that front-ends must handle, but it permits front ends to cache values without a cache consistency protocol.</p><p>The key space is allocated to front-ends by a single management node; we envision this node being replicated using a small Paxos cluster <ref type="bibr" target="#b26">[27]</ref>, but we have not (yet) implemented this. There would be 80 or more back-end nodes per front-end node with our current hardware prototypes, so the amount of information this management node maintains is small and changes infrequently-a list of 125 front-ends would suffice for a 10,000 node FAWN cluster. <ref type="bibr" target="#b1">2</ref> When a back-end node joins, it obtains the list of front-end IDs. Each of its virtual nodes uses this list to determine which front-end to contact to join the ring, one VID at a time. We chose this design so that the system would be robust to frontend node failures: The back-end node identifier (and thus, what keys it is responsible for) is a deterministic function of the back-end node ID. If a front-end node fails, data does not move between back-end nodes, though virtual nodes may have to attach to a new front-end.</p><p>The FAWN-KV ring uses a 160-bit circular ID space for VIDs and keys. Virtual IDs are hashed identifiers derived from the node's address. Each VID owns the items for which it is the item's successor in the ring space (the node immediately clockwise in the ring). As an example, consider the cluster depicted in <ref type="figure" target="#fig_4">Figure 5</ref> with five physical nodes, each of which has two VIDs. The physical node A appears as VIDs A1 and A2, each with its own 160-bit identifiers. VID A1 owns key range R1, VID B1 owns range R2, and so on.</p><p>Consistent hashing provides incremental scalability without global data movement: adding a new VID moves keys only at the successor of the VID being added. We discuss below (Section 3.4.4) how FAWN-KV uses the single-pass, sequential Split and Merge operations in FAWN-DS to handle such changes efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Caching Prevents Wimpy Hot-Spots</head><p>FAWN-KV caches data using a two-level cache hierarchy. Back-end nodes implicitly cache recently accessed data in their filesystem buffer cache. While our current nodes (Section 4) can read at about 1300 queries per second from flash, they can locally retrieve 85,000 queries per second if the working set fits completely in buffer cache. The FAWN frontend maintains a small, high-speed query cache that helps reduce latency and ensures that if the load becomes skewed to <ref type="bibr" target="#b1">2</ref> We do not use consistent hashing to determine this mapping because the number of front-end nodes may be too small to achieve good load balance.  only one or a few keys, those keys are served by a fast cache instead of all hitting a single back-end node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Replication and Consistency</head><p>FAWN-KV offers a configurable replication factor for fault tolerance. Items are stored at their successor in the ring space and at the R − 1 following virtual IDs. FAWN-KV uses chain replication <ref type="bibr" target="#b53">[54]</ref> to provide strong consistency on a per-key basis. Updates are sent to the head of the chain, passed along to each member of the chain via a TCP connection between the nodes, and queries are sent to the tail of the chain. By mapping the chain replication to the consistent hashing ring, each virtual ID in FAWN-KV is part of R different chains: it is the "tail" for one chain, a "mid" node in R − 2 chains, and the "head" for one. <ref type="figure" target="#fig_5">Figure 6</ref> depicts a ring with six physical nodes, where each has two virtual IDs (V = 2), using a replication factor of three. In this figure, node C1 is thus the tail for range R1, mid for range R2, and tail for range R3. <ref type="figure">Figure 7</ref> shows a put request for an item in range R1. The front-end routes the put to the key's successor, VID A1, which is the head of the replica chain for this range. After storing the value in its datastore, A1 forwards this request to B1, which similarly stores the value and forwards the request to the tail, C1. After storing the value, C1 sends the put response back to the front-end, and sends an acknowledgment back up the chain indicating that the response was handled properly.</p><p>For reliability, nodes buffer put requests until they receive the acknowledgment. Because puts are written to an appendonly log in FAWN-DS and are sent in-order along the chain, this operation is simple: nodes maintain a pointer to the last unacknowledged put in their datastore, and increment it when they receive an acknowledgment. By using a purely log structured datastore, chain replication with FAWN-KV becomes simply a process of streaming the growing datastore from node to node.</p><p>Gets proceed as in chain replication-the front-end directly routes the get to the tail of the chain for range R1, node C1, which responds to the request. Chain replication ensures that any update seen by the tail has also been applied by other replicas in the chain.  <ref type="figure">Figure 7</ref>: Lifecycle of a put with chain replication-puts go to the head and are propagated through the chain. Gets go directly to the tail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Joins and Leaves</head><p>When a node joins a FAWN-KV ring:</p><p>1. The new virtual node causes one key range to split into two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The new virtual node must receive a copy of the R ranges</head><p>of data it should now hold, one as a primary and R − 1 as a replica. 3. The front-end must begin treating the new virtual node as a head or tail for requests in the appropriate key ranges. 4. Virtual nodes down the chain may free space used by key ranges they are no longer responsible for.</p><p>The first step, key range splitting, occurs as described for FAWN-DS. While this operation can occur concurrently with the rest (the split and data transmission can overlap), for clarity, we describe the rest of this process as if the split had already taken place.</p><p>After the key ranges have been split appropriately, the node must become a working member of R chains. For each of these chains, the node must receive a consistent copy of the datastore file corresponding to the key range. The process below does so with minimal locking and ensures that if the node fails during the data copy operation, the existing replicas are unaffected. We illustrate this process in detail in <ref type="figure" target="#fig_6">Figure 8</ref> where node C1 joins as a new middle replica for range R2.</p><p>Phase 1: Datastore pre-copy. Before any ring membership changes occur, the current tail for the range (VID E1) begins sending the new node C1 a copy of the datastore log file. This operation is the most time-consuming part of the join, potentially requiring hundreds of seconds. At the end of this phase, C1 has a copy of the log that contains all records committed to the tail.</p><p>Phase 2: Chain insertion, log flush and play-forward. After C1's pre-copy phase has completed, the front-end sends a chain membership message that flushes through the chain. This message plays two roles: first, it updates each node's neighbor state to add C1 to the chain; second, it ensures that any in-flight updates sent after the pre-copy phase completed are flushed to C1.</p><p>More specifically, this message propagates in-order through B1, D1, and E1, and is also sent to C1. Nodes B1, C1, and D1 update their neighbor list, and nodes in the current chain forward the message to their successor in the chain. Updates arriving at B1 after the reception of the chain membership message now begin streaming to C1, and C1 relays them properly to D1. D1 becomes the new tail of the chain. At this point, B1 and D1 have correct, consistent views of the datastore, but C1 may not: A small amount of time passed between the time that the pre-copy finished and when C1 was inserted into the chain. To cope with this, C1 logs updates from B1 in a temporary datastore, not the actual datastore file for range R2, and does not update its in-memory hash table.</p><p>During this phase, C1</p><p>is not yet a valid replica. All put requests sent to B1 after it received the chain membership message are replicated at B1, C1, and D1, and D1 forwards the chain membership message directly to E1. Thus, the receipt of the chain membership message at E1 signals that no further updates to this range will arrive at E1. The old tail E1 then pushes all entries that might have arrived in the time after C1 received the log copy and before C1 was inserted in the chain, and C1 adds these entries to the R2 datastore. At the end of this process, E1 sends the chain membership message back to C1, confirming that all in-flight entries have been flushed. C1 then merges (appends) the temporary log to the end of the R2 datastore, updating its in-memory hash table as it does so. The node briefly locks the temporary log at the end of the merge to flush these in-flight writes.</p><p>After phase 2, C1 is a functioning member of the chain with a fully consistent copy of the datastore. This process occurs R times for the new virtual ID-e.g., if R = 3, it must join as a new head, a new mid, and a new tail for one chain.</p><p>Joining as a head or tail: In contrast to joining as a middle node, joining as a head or tail must be coordinated with the front-end to properly direct requests to the correct node. The process for a new head is identical to that of a new mid. To join as a tail, a node joins before the current tail and replies to put requests. It does not serve get requests until it is consistent (end of phase 2)-instead, its predecessor serves as an interim tail for gets.</p><p>Leave: The effects of a voluntary or involuntary (failuretriggered) leave are similar to those of a join, except that the replicas must merge the key range that the node owned. As above, the nodes must add a new replica into each of the R chains that the departing node was a member of. This replica addition is simply a join by a new node, and is handled as above.</p><p>Failure Detection: Nodes are assumed to be fail-stop <ref type="bibr" target="#b46">[47]</ref>. Each front-end exchanges heartbeat messages with its backend nodes every t hb seconds. If a node misses f d threshold heartbeats, the front-end considers it to have failed and initiates the leave protocol. Because the Join protocol does not insert a node into the chain until the majority of log data has been transferred to it, a failure during join results only in an additional period of slow-down, not a loss of redundancy.</p><p>We leave certain aspects of failure detection for future work. In addition to assuming fail-stop, we assume that the dominant failure mode is a node failure or the failure of a link or switch, but our current design does not cope with a communication failure that prevents one node in a chain from communicating with the next while leaving each able to communicate with the front-ends. We plan to augment the heartbeat exchange to allow nodes to report their neighbor connectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We begin by characterizing the I/O performance of a wimpy node. From this baseline, we then evaluate how well FAWN-DS performs on this same node, finding that its performance is similar to the node's baseline I/O capability. To further illustrate the advantages of FAWN-DS's design, we compare its performance to an implementation using the general-purpose Berkeley DB, which is not optimized for flash writes.</p><p>After characterizing individual node performance, we then study a prototype FAWN-KV system running on a 21-node cluster. We evaluate its energy efficiency, in queries per second per Watt, and then measure the performance effects of node failures and arrivals. In the following section, we then compare FAWN to a more traditional cluster architecture designed to store the same amount of data and meet the same query rates.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Individual Node Performance</head><p>We benchmark the I/O capability of the FAWN nodes using iozone <ref type="bibr" target="#b21">[22]</ref> and Flexible I/O tester <ref type="bibr" target="#b0">[1]</ref>. The flash is formatted with the ext2 filesystem and mounted with the noatime option to prevent random writes for file access <ref type="bibr" target="#b34">[35]</ref>. These tests read and write 1 KB entries, the lowest record size available in iozone. The filesystem I/O performance using a 3.5 GB file is shown in <ref type="table" target="#tab_4">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">FAWN-DS Single Node Local Benchmarks</head><p>Lookup Speed: This test shows the query throughput achieved by a local client issuing queries for randomly distributed, existing keys on a single node. We report the average of three runs (the standard deviations were below 5%). <ref type="table" target="#tab_5">Table 2</ref> shows FAWN-DS 1 KB and 256 byte random read queries/sec as a function of the DS size. If the datastore fits in the buffer cache, the node locally retrieves 50-85 thousand queries per second. As the datastore exceeds the 256 MB of RAM available on the nodes, a larger fraction of requests go to flash. FAWN-DS imposes modest overhead from hash lookups, data copies, and key comparisons, and it must read slightly more data than the iozone tests (each stored entry has a header). The resulting query throughput, however, remains high: tests reading a 3.5 GB datastore using 1 KB values achieved 1,150 queries/sec compared to 1,424 queries/sec from the filesystem. Using the 256 byte entries that we focus on below achieved 1,298 queries/sec from a 3.5 GB datastore. By comparison, the raw filesystem achieved 1,454 random 256 byte reads per second using Flexible I/O. Bulk store Speed: The log structure of FAWN-DS ensures that data insertion is entirely sequential. As a consequence, inserting two million entries of 1 KB each (2 GB total) into a single FAWN-DS log sustains an insert rate of 23.2 MB/s (or nearly 24,000 entries per second), which is 96% of the raw speed that the flash can be written through the filesystem.</p><p>Put Speed: In FAWN-KV, each FAWN node has R * V FAWN-DS files: each virtual ID adds one primary data range, plus an additional R − 1 replicated ranges. A node receiving puts for different ranges will concurrently append to a small number of files ("semi-random writes"). Good semi-random write performance is central to FAWN-DS's per-range data layout that enables single-pass maintenance operations. We therefore evaluate its performance using five flash-based storage devices.</p><p>Semi-random performance varies widely by device. <ref type="figure" target="#fig_7">Figure 9</ref> shows the aggregate write performance obtained when inserting 2GB of data into FAWN-DS using five different flash drives as the data is inserted into an increasing number of datastore files. All SATA-based flash drives measured below use an Intel Atom-based chipset because the Alix3c2 lacks a SATA port. The relatively low-performance CompactFlash write speed slows with an increasing number of files. The 2008 Intel X25-M and X25-E, which use log-structured writing and preemptive block erasure, retain high performance with up to 256 concurrent semi-random writes for the 2 GB of data we inserted; both the Mtron Mobi and Memoright GT drop in performance as the number of files increases. The key take-away from this evaluation is that Flash devices are capable of handling the FAWN-DS write workload extremely well-but a system designer must exercise care in selecting devices that actually do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Comparison with BerkeleyDB</head><p>To understand the benefit of FAWN-DS's log structure, we compare with a general purpose disk-based database that is not optimized for Flash. BerkeleyDB provides a simple put/get interface, can be used without heavy-weight transactions or rollback, and performs well versus other memory or disk-based databases. We configured BerkeleyDB using both its default settings and using the reference guide suggestions for Flash-based operation <ref type="bibr" target="#b3">[4]</ref>. The best performance we achieved required 6 hours (B-Tree) and 27 hours (Hash) to insert seven million, 200 byte entries to create a 1.5 GB database. This corresponds to an insert rate of 0.07 MB/s. The problem was, of course, small writes: When the BDB store was larger than the available RAM on the nodes (&lt; 256 MB), both the B-Tree and Hash implementations had to flush pages to disk, causing many writes that were much smaller than the size of an erase block.</p><p>That comparing FAWN-DS and BDB seems unfair is exactly the point: even a well-understood, high-performance database will perform poorly when its write pattern has not been specifically optimized to Flash's characteristics. We evaluated BDB on top of NILFS2 <ref type="bibr" target="#b38">[39]</ref>, a log-structured Linux filesystem for block devices, to understand whether logstructured writing could turn the random writes into sequential writes. Unfortunately, this combination was not suitable because of the amount of metadata created for small writes for use in filesystem checkpointing and rollback, features not needed for FAWN-KV-writing 200 MB worth of 256 B key-value pairs generated 3.5 GB of metadata. Other existing Linux log-structured flash filesystems, such as JFFS2 <ref type="bibr" target="#b22">[23]</ref>, are designed to work on raw flash, but modern SSDs, compact flash and SD cards all include a Flash Translation Layer that hides the raw flash chips. While future improvements to filesystems can speed up naive DB performance on flash, the pure log structure of FAWN-DS remains necessary even if we could use a more conventional backend: it provides the basis for replication and consistency across an array of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Read-intensive vs. Write-intensive Workloads</head><p>Most read-intensive workloads have at least some writes. For example, Facebook's memcached workloads have a 1:6 ratio of application-level puts to gets <ref type="bibr" target="#b23">[24]</ref>. We therefore measured the aggregate query rate as the fraction of puts ranged from 0 (all gets) to 1 (all puts) on a single node <ref type="figure" target="#fig_0">(Figure 10)</ref>.</p><p>FAWN-DS can handle more puts per second than gets because of its log structure. Even though semi-random write performance across eight files on our CompactFlash devices is worse than purely sequential writes, it still achieves higher  throughput than pure random reads. When the put-ratio is low, the query rate is limited by the get requests. As the ratio of puts to gets increases, the faster puts significantly increase the aggregate query rate. On the other hand, a pure write workload that updates a small subset of keys would require frequent cleaning. In our current environment and implementation, both read and write rates slow to about 700-1000 queries/sec during compaction, bottlenecked by increased thread switching and system call overheads of the cleaning thread. Last, because deletes are effectively 0-byte value puts, delete-heavy workloads are similar to insert workloads that update a small set of keys frequently. In the next section, we mostly evaluate readintensive workloads because it represents the target workloads for which FAWN-KV is designed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">FAWN-KV System Benchmarks</head><p>In this section, we evaluate the query rate and power draw of our 21-node FAWN-KV system.</p><p>System Throughput: To measure query throughput, we populated the KV cluster with 20 GB of values, and then measured the maximum rate at which the front-end received query responses for random keys. We disabled front-end caching for this experiment. <ref type="figure" target="#fig_0">Figure 11</ref> shows that the cluster sustained roughly 36,000 256 byte gets per second (1,700 per second per node) and 24,000 1 KB gets per second (1,100 per second per node). A single node serving a 512 MB datastore over the network could sustain roughly 1,850 256 byte gets per second per node, while <ref type="table" target="#tab_5">Table 2</ref> shows that it could serve the queries locally at 2,450 256 byte queries per second per node. Thus, a single node serves roughly 70% of the sustained rate that a single FAWN-DS could handle with local queries. The primary reason for the difference is the addition of network overhead and request marshaling and unmarshaling. Another reason for difference is load balance: with random key distribution, some back-end nodes receive more queries than others, slightly reducing system performance. <ref type="bibr" target="#b2">3</ref> System Power Consumption: Using a WattsUp <ref type="bibr" target="#b54">[55]</ref> power meter that logs power draw each second, we measured the power consumption of our 21-node FAWN-KV cluster and two network switches. <ref type="figure" target="#fig_0">Figure 12</ref> shows that, when idle, the cluster uses about 83 W, or 3 Watts per node and 10 W per switch. During gets, power consumption increases to 99 W, and during insertions, power consumption is 91 W. <ref type="bibr" target="#b3">4</ref> Peak get performance reaches about 36,000 256 B queries/sec for the cluster serving the 20 GB dataset, so this system, excluding the front-end, provides 364 queries/Joule.</p><p>The front-end has a 1 Gbit/s connection to the backend nodes, so the cluster requires about one low-power frontend for every 80 nodes-enough front-ends to handle the aggregate query traffic from all the backends (80 nodes * 1500 queries/sec/node * 1 KB / query = 937 Mbit/s). Our prototype front-end uses 27 W, which adds nearly 0.5 W per node amortized over 80 nodes, providing 330 queries/Joule for the entire system.</p><p>Network switches currently account for 20% of the power used by the entire system. Our current cluster size affords the use of a flat network hierarchy, but providing full bisection bandwidth for a large cluster would require many more network switches, increasing the ratio of network power to FAWN node power. Scaling networks to support large deployments is a problem that affects today's clusters and remains an active area of research <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. While improving the network energy consumption of large FAWN clusters is a topic of ongoing work, we note that recent fat-tree network topology designs using many small commodity, low-power switches <ref type="bibr" target="#b1">[2]</ref> would impose only a fixed per-node network power overhead. Should the application design permit, sacrificing full bisection bandwidth can trade reduced communica- tion flexibility for improved network energy efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of Ring Membership Changes</head><p>Node joins, leaves, or failures require existing nodes to split merge, and transfer data while still handling puts and gets. In this section we evaluate the impact of node joins on system query throughput and the impact of maintenance operations such as local splits and compaction on single node query throughput and latency.</p><p>Query Throughput During Node Join: In this test, we start a 20-node FAWN-KV cluster populated with 10 GB of key-value pairs and begin issuing get requests uniformly at random to the entire key space. At t=25, we add a node to the ring and continue to issue get requests to the entire cluster. For this experiment, we set R = 3 and V = 1. <ref type="figure" target="#fig_0">Figure 13</ref> shows the resulting cluster query throughput during a node join.</p><p>The joining node requests pre-copies for R = 3 ranges, one range for which it is the tail and two ranges as the head and mid. The three nodes that pre-copy their datastores to the joining node experience a one-third reduction in external query throughput, serving about 1,000 queries/sec. Pre-copying data does not cause significant I/O interference with external requests for data-the pre-copy operation requires only a sequential read of the datastore and bulk sends over the network.</p><p>The lack of seek penalties for concurrent access on flash together with the availability of spare network capacity results in only a small drop in performance during pre-copying. The other 17 nodes in our cluster are not affected by this join operation and serve queries at their normal rate. The join operation completes long after pre-copies finished in this experiment due to the high external query load, and query throughput returns back to the maximum rate.</p><p>The experiment above stresses the cluster by issuing requests at the maximum rate the cluster can handle. But most systems offer performance guarantees only for loads below maximum capacity. We run the same experiment above but with an external query load at about 30% of the maximum supported query rate. The three nodes sending pre-copies have enough spare resources available to perform their precopy without affecting their ability to serve external queries, so the system's throughput does not drop when the new node is introduced. The join completes shortly after the pre-copies finishes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Throughput During Maintenance Operations:</head><p>Maintenance operations perform sequential reads of one file and sequential writes into another. In the node join experiment above, we deferred performing the local split/merge operations until after the node join completed to minimize the performance impact during the node join. <ref type="figure" target="#fig_0">Figure 14(top)</ref> shows the impact of split, merge, and compaction on external get queries sent at high load to the 512 MB datastore. In this experiment, the key range is initially split unevenly: 25% of the original key space is split into a second FAWN-DS datastore. As a result, the split operation only writes 25% of its records into the second datastore. Merging the two datastores back into one is more "intense" than a split because the merge requires a read and write of nearly every record in the datastore being merged rather than just a fraction of the records. Consequently, the FAWN-DS file with fewer records should always be merged into the larger store to minimize the completion time of the merge operation.</p><p>Compaction has a query impact between both split and merge-compaction must write most of the entries in the log, except for out-of-range, deleted, or orphaned entries. However, because it must read and write every valid record in the datastore, the length of the operation is typically longer than either split and merge. <ref type="figure" target="#fig_0">Figure 14</ref>(bottom) shows the same experiment with a query rate set at 30% of the maximum supported, showing that the impact of maintenance operations on query rate is minimal when the incoming rate is below half of the node's maximum query capacity. <ref type="figure" target="#fig_0">Figure 15</ref> shows the distribution of query latency for three workloads: a pure get workload issuing gets at the maximum rate (Max Load), a 500 requests per second workload with a concurrent Split (Split-Low Load), and a 1500 requests per second workload with a Split (Split-High Load).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Split on Query Latency:</head><p>In general, accesses that hit buffer cache are returned in 300 µs including processing and network latency. When the accesses go to flash, the median response time is 800 µs. Even during a split, the median response time remains under 1 ms. The median latency increases with load, so the max load, get-only workload has a slightly higher median latency than the lower load splits.</p><p>Many key-value systems care about 99.9th percentile latency guarantees as well as fast average-case performance. During normal operation, request latency is very low: 99.9% of requests take under 26.3 ms, and 90% take under 2 ms. During a split with low external query load, the additional processing and locking extend 10% of requests above 10 ms.</p><p>Query latency increases briefly at the end of a split when the datastore is locked to atomically add the new datastore. The lock duration is 20-30 ms on average, but can rise to 100 ms if the query load is high, increasing queuing delay for incoming requests during this period. The resulting 99.9%-ile response time during the low-activity split is 491 ms. For a high-rate request workload, the incoming request rate is occasionally higher than can be serviced during the split. Incoming requests are buffered and experience additional queuing delay: the 99.9%-ile response time is 611 ms. Fortunately, these worst-case response times are still on the same order as those worst-case times seen in production key-value systems <ref type="bibr" target="#b9">[10]</ref>.</p><p>With larger values (1KB), query latency during Split increases further due to a lack of flash device parallelism-a large write to the device blocks concurrent independent reads, resulting in poor worst-case performance. Modern SSDs, in contrast, support and require request parallelism to achieve high flash drive performance <ref type="bibr" target="#b39">[40]</ref>; a future switch to these devices could greatly reduce the effect of background operations on query latency.</p><p>We also measured the latency of put requests during normal operation. With R=1, median put latency was about 500µs, with 99.9%ile latency extending to 24.5 ms. With R=3, put requests in chain replication are expected to incur additional latency as the requests get routed down the chain. Median latency increased by roughly three times to 1.58 ms, with 99.9%ile latency increasing only to 30 ms. <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Alternative Architectures</head><p>When is the FAWN approach likely to beat traditional architectures? We examine this question in two ways. First, we examine how much power can be saved on a conventional system using standard scaling techniques. Next, we compare the three-year total cost of ownership (TCO) for six systems: three "traditional" servers using magnetic disks, flash SSDs, and DRAM; and three hypothetical FAWN-like systems using the same storage technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Characterizing Conventional Nodes</head><p>We first examine a low-power, conventional desktop node configured to conserve power. The system uses an Intel quadcore Q6700 CPU with GB DRAM, an Mtron Mobi SSD, and onboard gigabit Ethernet and graphics.</p><p>Power Saving Techniques: We configured the system to use DVFS with three p-states (2.67 GHz, 2.14 GHz, 1.60 GHz). To maximize idle time, we ran a tickless Linux When the workload consisted of a mixture of puts and gets, 99.9%ile latency increased significantly-our naive implementation used a single queue for all requests, so puts propagating between neighbors would often get queued behind a large set of external get requests, further increasing latency. Using separate queues for external messages and neighbor messages would reduce this worst-case latency.  As expected from the small idle-active power gap of the desktop (64 W idle, 83 W active), the system had little room for "scaling down"-the queries/Joule became drastically worse as the load decreased. The idle power of the desktop is dominated by fixed power costs, while half of the idle power consumption of the 3-node FAWN cluster comes from the idle (and under-populated) Ethernet switch. <ref type="table" target="#tab_7">Table 3</ref> extends this comparison to clusters of several other systems. <ref type="bibr" target="#b5">6</ref> As expected, systems with disks are limited by seek times: the desktop above serves only 171 queries per second, and so provides only 1.96 queries/Joule-two orders of magnitude lower than a fully-populated FAWN. This performance is not far off from what the disks themselves can do: they draw 10 W at load, providing only 17 queries/Joule. Low-power laptops with magnetic disks fare little better. The desktop (above) with an SSD performs best of the alternative systems, but is still far from the efficiency of a FAWN cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">General Architectural Comparison</head><p>A general comparison requires looking not just at the queries per Joule, but the total system cost. In this section, we examine the 3-year total cost of ownership (TCO), which we define as the sum of the capital cost and the 3-year power cost at 10 cents per kWh.</p><p>The Soekris is a five-year-old embedded communications board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Cost W  Because the FAWN systems we have built use several-yearold technology, we study a theoretical 2009 FAWN node using a low-power CPU that consumes 10-20 W and costs ∼$150 in volume. We in turn give the benefit of the doubt to the server systems we compare against-we assume a 1 TB disk exists that serves 300 queries/sec at 10 W.</p><note type="other">QPS Queries Joule GB Watt TCO GB TCO</note><p>Our results indicate that both FAWN and traditional systems have their place-but for the small random access workloads we study, traditional systems are surprisingly absent from much of the solution space, in favor of FAWN nodes using either disks, SSDs, or DRAM.</p><p>Key to the analysis is a question: why does a cluster need nodes? The answer is, of course, for both storage space and query rate. Storing a DS gigabyte dataset with query rate QR requires N nodes:</p><formula xml:id="formula_0">N = max DS gb node , QR qr node</formula><p>With large datasets with low query rates, the number of nodes required is dominated by the storage capacity per node: thus, the important metric is the total cost per GB for an individual node. Conversely, for small datasets with high query rates, the per-node query capacity dictates the number of nodes: the dominant metric is queries per second per dollar. Between these extremes, systems must provide the best tradeoff between per-node storage capacity, query rate, and power cost. <ref type="table" target="#tab_9">Table 4</ref> shows these cost and performance statistics for several candidate systems. The "traditional" nodes use 200W servers that cost $1,000 each. Traditional+Disk pairs a single server with five 5 TB high-speed disks capable of 300 queries/sec, each disk consuming 10 W. Traditional+SSD uses two PCI-E Fusion-IO 80 GB Flash SSDs, each also consuming about 10 W (Cost: $3k). Traditional+DRAM uses eight 8 GB server-quality DRAM modules, each consuming 10 W. FAWN+Disk nodes use one 2 TB 7200 RPM disk: FAWN nodes have fewer connectors available on the board. FAWN+SSD uses one 32 GB Intel SATA Flash SSD capable of 35,000 random reads/sec <ref type="bibr" target="#b39">[40]</ref> and consuming 2 W ($400). FAWN+DRAM uses a single 2 GB, slower DRAM module, also consuming 2 W. <ref type="figure" target="#fig_0">Figure 16</ref> shows which base system has the lowest cost for a particular dataset size and query rate, with dataset sizes  between 100 GB and 10 PB and query rates between 100 K and billion per second. The dividing lines represent a boundary across which one system becomes more favorable than another.</p><p>Large Datasets, Low Query Rates: FAWN+Disk has the lowest total cost per GB. While not shown on our graph, a traditional system wins for exabyte-sized workloads if it can be configured with sufficient disks per node (over 50), though packing 50 disks per machine poses reliability challenges.</p><p>Small Datasets, High Query Rates: FAWN+DRAM costs the fewest dollars per queries/second, keeping in mind that we do not examine workloads that fit entirely in L2 cache on a traditional node. This somewhat counterintuitive result is similar to that made by the intelligent RAM project, which coupled processors and DRAM to achieve similar benefits <ref type="bibr" target="#b4">[5]</ref> by avoiding the memory wall. We assume the FAWN nodes can only accept 2 GB of DRAM per node, so for larger datasets, a traditional DRAM system provides a high query rate and requires fewer nodes to store the same amount of data (64 GB vs 2 GB per node).</p><p>Middle Range: FAWN+SSDs provide the best balance of storage capacity, query rate, and total cost. As SSD capacity improves, this combination is likely to continue expanding into the range served by FAWN+Disk; as SSD performance improves, so will it reach into DRAM territory. It is therefore conceivable that FAWN+SSD could become the dominant architecture for a wide range of random-access workloads.</p><p>Are traditional systems obsolete? We emphasize that this analysis applies only to small, random access workloads. Sequential-read workloads are similar, but the constants depend strongly on the per-byte processing required. Traditional cluster architectures retain a place for CPU-bound workloads, but we do note that architectures such as IBM's BlueGene successfully apply large numbers of low-power, efficient processors to many supercomputing applications <ref type="bibr" target="#b13">[14]</ref>-but they augment their wimpy processors with custom floating point units to do so.</p><p>Our definition of "total cost of ownership" also ignores several notable costs: In comparison to traditional architectures, FAWN should reduce power and cooling infrastructure, but may increase network-related hardware and power costs due to the need for more switches. Our current hardware prototype improves work done per volume, thus reducing costs associated with datacenter rack or floor space. Finally, of course, our analysis assumes that cluster software developers can engineer away the human costs of management-an optimistic assumption for all architectures. We similarly discard issues such as ease of programming, though we ourselves selected an x86-based wimpy platform precisely for ease of development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>FAWN follows in a long tradition of ensuring that systems are balanced in the presence of scaling challenges and of designing systems to cope with the performance challenges imposed by hardware architectures.</p><p>System Architectures: JouleSort <ref type="bibr" target="#b43">[44]</ref> is a recent energyefficiency benchmark; its authors developed a SATA diskbased "balanced" system coupled with a low-power (34 W) CPU that significantly out-performed prior systems in terms of records sorted per joule. A major difference with our work is that the sort workload can be handled with large, bulk I/O reads using radix or merge sort. FAWN targets even more seek-intensive workloads for which even the efficient CPUs used for JouleSort are excessive, and for which disk is inadvisable.</p><p>More recently, several projects have begun using lowpower processors for datacenter workloads to reduce energy consumption <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. The Gordon <ref type="bibr" target="#b5">[6]</ref> hardware architecture argues for pairing an array of flash chips and DRAM with low-power CPUs for low-power data intensive computing. A primary focus of their work is on developing a Flash Translation Layer suitable for pairing a single CPU with several raw flash chips. Simulations on general system traces indicate that this pairing can provide improved energy-efficiency. Our work leverages commodity embedded low-power CPUs and flash storage for cluster key-value applications, enabling good performance on flash regardless of FTL implementation. CEMS <ref type="bibr" target="#b19">[20]</ref>, AmdahlBlades <ref type="bibr" target="#b49">[50]</ref>, and Microblades <ref type="bibr" target="#b29">[30]</ref> also leverage low-cost, low-power commodity components as a building block for datacenter systems, similarly arguing that this architecture can provide the highest work done per dollar and work done per joule. Microsoft has recently begun exploring the use of a large cluster of lowpower systems called Marlowe <ref type="bibr" target="#b33">[34]</ref>. This work focuses on taking advantage of the very low-power sleep states provided by this chipset (between 2-4 W) to turn off machines and migrate workloads during idle periods and low utilization, initially targeting the Hotmail service. We believe these ad-vantages would also translate well to FAWN, where a lull in the use of a FAWN cluster would provide the opportunity to significantly reduce average energy consumption in addition to the already-reduced peak energy consumption that FAWN provides. Dell recently designed and has begun shipping VIA Nano-based servers consuming 20-30 W each for large webhosting services <ref type="bibr" target="#b10">[11]</ref>.</p><p>Considerable prior work has examined ways to tackle the "memory wall." The Intelligent RAM (IRAM) project combined CPUs and memory into a single unit, with a particular focus on energy efficiency <ref type="bibr" target="#b4">[5]</ref>. An IRAM-based CPU could use a quarter of the power of a conventional system to serve the same workload, reducing total system energy consumption to 40%. FAWN takes a thematically similar view-placing smaller processors very near flash-but with a significantly different realization. Similar efforts, such as the Active Disk project <ref type="bibr" target="#b42">[43]</ref>, focused on harnessing computation close to disks. Schlosser et al. proposed obtaining similar benefits from coupling MEMS with CPUs <ref type="bibr" target="#b45">[46]</ref>.</p><p>Databases and Flash: Much ongoing work is examining the use of flash in databases, examining how database data structures and algorithms can be modified to account for flash storage strengths and weaknesses <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29]</ref>. Recent work concluded that NAND flash might be appropriate in "read-mostly, transaction-like workloads", but that flash was a poor fit for high-update databases <ref type="bibr" target="#b34">[35]</ref>. This work, along with FlashDB <ref type="bibr" target="#b36">[37]</ref> and FD-Trees <ref type="bibr" target="#b28">[29]</ref>, also noted the benefits of a log structure on flash; however, in their environments, using a log-structured approach slowed query performance by an unacceptable degree. Prior work in sensor networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref> has employed flash in resource-constrained sensor applications to provide energy-efficient filesystems and single node object stores. In contrast to the above work, FAWN-KV sacrifices range queries by providing only primary-key queries, which eliminates complex indexes: FAWN's separate data and index can therefore support log-structured access without reduced query performance. Indeed, with the log structure, FAWN's performance actually increases with a moderate percentage of writes. FAWN-KV also applies log-structured data organization to speed maintenance and failover operations in a clustered, datacenter environment.</p><p>Filesystems for Flash: Several filesystems are specialized for use on flash. Most are partially log-structured <ref type="bibr" target="#b44">[45]</ref>, such as the popular JFFS2 (Journaling Flash File System) for Linux. Our observations about flash's performance characteristics follow a long line of research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref>. Past solutions to these problems include the eNVy filesystem's use of battery-backed SRAM to buffer copy-on-write log updates for high performance <ref type="bibr" target="#b56">[57]</ref>, followed closely by purely flashbased log-structured filesystems <ref type="bibr" target="#b25">[26]</ref>.</p><p>High-throughput Storage and Analysis: Recent work such as Hadoop or MapReduce <ref type="bibr" target="#b8">[9]</ref> running on GFS <ref type="bibr" target="#b14">[15]</ref> has examined techniques for scalable, high-throughput computing on massive datasets. More specialized examples include SQLcentric options such as the massively parallel data-mining appliances from Netezza <ref type="bibr" target="#b37">[38]</ref>. As opposed to the randomaccess workloads we examine for FAWN-KV, these systems provide bulk throughput for massive datasets with low selectivity or where indexing in advance is difficult. We view these workloads as a promising next target for the FAWN approach.</p><p>Distributed Hash <ref type="table">Tables: Related cluster and wide-area  hash table-like services include Distributed data structure  (DDS)</ref>  <ref type="bibr" target="#b16">[17]</ref>, a persistent data management layer designed to simplify cluster-based Internet services. FAWN's major points of differences with DDS are a result of FAWN's hardware architecture, use of flash, and focus on energy efficiency-in fact, the authors of DDS noted that a problem for future work was that "disk seeks become the overall bottleneck of the system" with large workloads, precisely the problem that FAWN-DS solves. These same differences apply to systems such as Dynamo <ref type="bibr" target="#b9">[10]</ref> and Voldemort <ref type="bibr" target="#b40">[41]</ref>. Systems such as Boxwood <ref type="bibr" target="#b30">[31]</ref> focus on the higher level primitives necessary for managing storage clusters. Our focus was on the lower-layer architectural and data-storage functionality.</p><p>Sleeping Disks: A final set of research examines how and when to put disks to sleep; we believe that the FAWN approach compliments them well. Hibernator <ref type="bibr" target="#b58">[59]</ref>, for instance, focuses on large but low-rate OLTP database workloads (a few hundred queries/sec). Ganesh et al. proposed using a logstructured filesystem so that a striping system could perfectly predict which disks must be awake for writing <ref type="bibr" target="#b12">[13]</ref>. Finally, Pergamum <ref type="bibr" target="#b48">[49]</ref> used nodes much like our wimpy nodes to attach to spun-down disks for archival storage purposes, noting that the wimpy nodes consume much less power when asleep. The system achieved low power, though its throughput was limited by the wimpy nodes' Ethernet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>FAWN pairs low-power embedded nodes with flash storage to provide fast and energy efficient processing of random read-intensive workloads. Effectively harnessing these more efficient but memory and compute-limited nodes into a usable cluster requires a re-design of many of the lower-layer storage and replication mechanisms. In this paper, we have shown that doing so is both possible and desirable. FAWN-KV begins with a log-structured per-node datastore to serialize writes and make them fast on flash. It then uses this log structure as the basis for chain replication between cluster nodes, providing reliability and strong consistency, while ensuring that all maintenance operations-including failure handling and node insertion-require only efficient bulk sequential reads and writes. Our 4-year-old FAWN nodes delivered over an order of magnitude more queries per Joule than conventional disk-based systems, and our preliminary experience using Intel Atom-based systems paired with SATA-based Flash drives shows that they can provide over 1000 queries/Joule, demonstrating that the FAWN architecture has significant potential for many I/O-intensive workloads.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FAWNFigure 1 :</head><label>1</label><figDesc>FAWN-KV Architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>/</head><label></label><figDesc>* KEY = 0x93df7317294b99e3e049, 16 index bits */ INDEX = KEY &amp; 0xffff; /* = 0xe049; */ KEYFRAG = (KEY &gt;&gt; 16) &amp; 0x7fff; /* = 0x19e3; */ for i = 0 to NUM HASHES do bucket = hash[i](INDEX); if bucket.valid &amp;&amp; bucket.keyfrag==KEYFRAG &amp;&amp; readKey(bucket.offset)==KEY then return bucket; end if {Check next chain element...} end for return NOT FOUND;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Pseudocode for hash bucket lookup in FAWN-DS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>FAWN-KV Interfaces-Front-ends manage back-ends, route requests, and cache responses. Backends use FAWN-DS to store key-value pairs. list pointers, which typically takes 20-30 ms at the end of a Split or Merge (Section 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Consistent Hashing with 5 physical nodes and 2 virtual IDs each.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Overlapping Chains in the Ring -Each node in the consistent hashing ring is part of R = 3 chains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Phases of join protocol on node arrival.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Sequentially writing to multiple FAWN-DS files results in semi-random writes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>FAWN supports both read-and writeintensive workloads. Small writes are cheaper than random reads due to the FAWN-DS log structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Query throughput on 21-node FAWN-KV system for 1 KB and 256 B entry sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Power consumption of 21-node FAWN-KV system for 256 B values during Puts/Gets. Get query rates during node join for max load (top) and low load (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Get query rates during background operations for high (top) and low (bottom) external query loads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Query latency CDF for normal and split workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>Solution space for lowest 3-year TCO as a function of dataset size and query rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>DS reads the record off of the flash. If the stored full key in the on-flash record matches the desired lookup key, the operation is complete. Otherwise, FAWN-DS resumes its hash chaining search of the in-memory hash table and searches additional records. With the 15-bit key fragment, only 1 in 32,768 retrievals from the flash will be incorrect and require fetching an additional record.</figDesc><table><row><cell>Datastore List</cell><cell>Data in original range Data in new range</cell><cell>Datastore List</cell><cell>Atomic Update of Datastore List</cell></row><row><cell>(a)</cell><cell>(b)</cell><cell></cell><cell>(c)</cell></row><row><cell cols="4">Figure 2: (a) FAWN-DS appends writes to the end of the Data Log. (b) Split requires a sequential scan of the data region,</cell></row><row><cell cols="4">transferring out-of-range entries to the new store. (c) After scan is complete, the datastore list is atomically updated to</cell></row><row><cell cols="3">add the new store. Compaction of the original store will clean up out-of-range entries.</cell><cell></cell></row><row><cell>next 15 low order bits (the key fragment). FAWN-DS uses</cell><cell></cell><cell></cell><cell></cell></row><row><cell>the index bits to select a bucket from the Hash Index, which</cell><cell></cell><cell></cell><cell></cell></row><row><cell>contains 2 i hash buckets. Each bucket is only six bytes: a</cell><cell></cell><cell></cell><cell></cell></row><row><cell>15-bit key fragment, a valid bit, and a 4-byte pointer to the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>location in the Data Log where the full entry is stored.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Lookup proceeds, then, by locating a bucket using the index</cell><cell></cell><cell></cell><cell></cell></row><row><cell>bits and comparing the key against the key fragment. If the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>fragments do not match, FAWN-DS uses hash chaining to</cell><cell></cell><cell></cell><cell></cell></row><row><cell>continue searching the hash table. Once it finds a matching</cell><cell></cell><cell></cell><cell></cell></row><row><cell>key fragment, FAWN-</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Evaluation Hardware: Our FAWN cluster has 21 backend nodes built from commodity PCEngine Alix 3c2 devices, commonly used for thin-clients, kiosks, network firewalls, wireless routers, and other embedded applications. These devices have a single-core 500 MHz AMD Geode LX processor, 256 MB DDR SDRAM operating at 400 MHz, and 100 Mbit/s Ethernet. Each node contains one 4 GB Sandisk Extreme IV CompactFlash device. A node consumes 3 W when idle and a maximum of 6 W when deliberately using 100% CPU, network and flash. The nodes are connected to each other and to a 27 W Intel Atom-based front-end node using two 16-port Netgear GS116 GigE Ethernet switches.Seq. Read Rand Read Seq. Write Rand. Write 28.5 MB/s 1424 QPS 24 MB/s 110 QPS Baseline CompactFlash statistics for 1 KB entries. QPS = Queries/second.</figDesc><table><row><cell cols="3">DS Size 1 KB Rand Read 256 B Rand Read</cell></row><row><cell></cell><cell>(in queries/sec)</cell><cell>(in queries/sec)</cell></row><row><cell>KB</cell><cell>72352</cell><cell>85012</cell></row><row><cell>125 MB</cell><cell>51968</cell><cell>65412</cell></row><row><cell>250 MB</cell><cell>6824</cell><cell>5902</cell></row><row><cell>500 MB</cell><cell>2016</cell><cell>2449</cell></row><row><cell>1 GB</cell><cell>1595</cell><cell>1964</cell></row><row><cell>2 GB</cell><cell>1446</cell><cell>1613</cell></row><row><cell>3.5 GB</cell><cell>1150</cell><cell>1298</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Local random read performance of FAWN-DS. Evaluation Workload: FAWN-KV targets read-intensive, small object workloads for which key-value systems are of- ten used. The exact object sizes are, of course, application dependent. In our evaluation, we show query performance for 256 byte and 1 KB values. We select these sizes as proxies for small text posts, user reviews or status messages, image thumbnails, and so on. They represent a quite challenging regime for conventional disk-bound systems, and stress the limited memory and CPU of our wimpy nodes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Query performance and efficiency for different machine configurations.</figDesc><table><row><cell>kernel (version 2.6.27) and disabled non-system critical back-</cell></row><row><cell>ground processes. We enabled power-relevant BIOS settings</cell></row><row><cell>including ultra-low fan speed and processor C1E support.</cell></row><row><cell>Power consumption was 64 W when idle with only system</cell></row><row><cell>critical background processes and 83-90 W with significant</cell></row><row><cell>load.</cell></row><row><cell>Query Throughput: Raw (iozone) random reads achieved</cell></row><row><cell>4,771 (256 B) queries/sec and FAWN-DS achieved 4,289</cell></row><row><cell>queries/second. The resulting full-load query efficiency was</cell></row><row><cell>52 queries/Joule, compared to the 346 queries/Joule of a fully</cell></row><row><cell>populated FAWN cluster. Even a three-node FAWN cluster</cell></row><row><cell>that achieves roughly the same query throughput as the desk-</cell></row><row><cell>top, including the full power draw of an unpopulated 16-port</cell></row><row><cell>gigabit Ethernet switch (10 W), achieved 240 queries/Joule.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Traditional and FAWN node statistics</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This problem is fundamental to random load-balanced systems. Terrace and Freedman<ref type="bibr" target="#b50">[51]</ref> recently devised a mechanism for allowing queries to go to any node using chain replication; in future work, we plan to incorporate this to allow us to direct queries to the least-loaded replica, which has been shown to drastically improve load balance.<ref type="bibr" target="#b3">4</ref> Flash writes and erase require higher currents and voltages than reads do, but the overall put power was lower because FAWN's log-structured writes enable efficient bulk writes to flash, so the system spends more time idle.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by gifts from Network Appliance, Google, and Intel Corporation, and by grant CNS-0619525 from the National Science Foundation. Jason Franklin is supported in part by an NSF Graduate Research Fellowship. Amar Phanishayee was supported by an IBM Fellowship. Vijay Vasudevan is supported by a fellowship from APC by Schneider Electric. We extend our thanks to the excellent feedback from our OSDI and SOSP reviewers, Vyas Sekar, Mehul Shah, and to Lorenzo Alvisi for shepherding the work for SOSP. Iulian Moraru provided both feedback and extensive performance tuning assistance on the wimpy nodes.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I/O</forename><surname>Flexible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tester</surname></persName>
		</author>
		<ptr target="http://freshmeat.net/projects/fio/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A scalable, commodity, data center network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Fares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukissas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIG-COMM</title>
		<meeting>ACM SIG-COMM</meeting>
		<imprint>
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The case for energy-proportional computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hölzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="33" to="37" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<ptr target="http://www.oracle.com/technology/documentation/berkeley-db/db/ref/program/ram.html" />
	</analytic>
	<monogr>
		<title level="j">BerkeleyDB Reference Guide. Memory-only or Flash configurations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluation of existing architectures in IRAM systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cardwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Romer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Mixing Logic and DRAM, 24th International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="1997-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using flash memory to build fast, power-efficient clusters for data-intensive applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Grupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS09)</title>
		<imprint>
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Managing energy and server resources in hosting centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Thakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>18th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2001-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ELF: an efficient logstructured flash file system for micro sensor nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neufeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Embedded Networked Sensor Systems (SenSys)</title>
		<meeting>the ACM Conference on Embedded Networked Sensor Systems (SenSys)</meeting>
		<imprint>
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th USENIX OSDI</title>
		<meeting>6th USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s highly available key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vosshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>21st ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xs11-Vx8. Dell</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fortuna</surname></persName>
		</author>
		<ptr target="http://www1.euro.dell.com/content/topics/topic.aspx/emea/corporate/pressoffice/2009/uk/en/2009_05_20_brk_000?c=uk&amp;l=en" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Storage alternatives for mobile computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caceres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st USENIX OSDI</title>
		<meeting>1st USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="1994-11" />
			<biblScope unit="page" from="25" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimizing power consumption in large scale storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Weatherspoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Birman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HotOS XI</title>
		<meeting>HotOS XI</meeting>
		<imprint>
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overview of the Blue Gene/L system architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Blumrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-T</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res and Dev</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2/3</biblScope>
			<date type="published" when="2005-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Google file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>19th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2003-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VL2: A scalable and flexible data center network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scalable, distributed data structures for Internet service construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Gribble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Culler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th USENIX OSDI</title>
		<meeting>4th USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2000-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DCell: A scalable and fault-tolerant network structure for data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BCube: A high performance, servercentric network architecture for modular data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cooperative expendable micro-slice servers (CEMS): Low cost, low power servers for Internet scale services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamilton</surname></persName>
		</author>
		<ptr target="http://mvdirona.com/jrh/TalksAndPapers/JamesHamilton_CEMS.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel</surname></persName>
		</author>
		<ptr target="http://www.intel.com/pressroom/archive/releases/20070328fact.htm" />
		<imprint>
			<publisher>Penryn Press Release</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Filesystem</forename><surname>Iozone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benchmark</surname></persName>
		</author>
		<ptr target="http://www.iozone.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="http://sources.redhat.com/jffs2/" />
		<title level="m">JFFS2. The Journaling Flash File System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Johnson</forename><surname>Facebook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
	<note>personal communication</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tech titans building boom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A flash-memory based file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nishioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Annual Technical Conference</title>
		<meeting>USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="1995-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The part-time parliament</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lamport</surname></persName>
		</author>
		<idno>0734-2071</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="169" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A case for flash memory SSD in enterprise database applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tree indexing on flash disks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 25th International Conference on Data Engineering</title>
		<meeting>25th International Conference on Data Engineering</meeting>
		<imprint>
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Understanding and designing new server architectures for emerging warehouse-computing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Boxwood: abstractions as the foundation for storage infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Thekkath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th USENIX OSDI</title>
		<meeting>6th USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Capsule: an energy-optimized object storage system for memoryconstrained sensor devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Desnoyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Embedded Networked Sensor Systems (SenSys)</title>
		<meeting>the ACM Conference on Embedded Networked Sensor Systems (SenSys)</meeting>
		<imprint>
			<date type="published" when="2006-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A distributed memory object caching system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memcached</surname></persName>
		</author>
		<ptr target="http://www.danga.com/memcached/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Peering into future of cloud computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Marlowe</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/en-us/news/features/ccf-022409.aspx" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">On the use of NAND flash memory in highperformance relational databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Myers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-02" />
		</imprint>
	</monogr>
	<note type="report_type">M.S. Thesis, MIT</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online maintenance of very large random samples on flash storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">FlashDB: Dynamic self-tuning database for NAND flash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM/IEEE International Conference on Information Processing in Sensor Networks</title>
		<meeting>ACM/IEEE International Conference on Information Processing in Sensor Networks</meeting>
		<imprint>
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Business intelligence data warehouse appliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Netezza</surname></persName>
		</author>
		<ptr target="http://www.netezza.com/" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<ptr target="http://www.nilfs.org" />
		<title level="m">Continuous snapshotting filesystem for Linux</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Enabling enterprise solid state disks performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Polte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop on Integrating Solid-state Memory into the Storage Hierarchy</title>
		<meeting>Workshop on Integrating Solid-state Memory into the Storage Hierarchy</meeting>
		<imprint>
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Project Voldemort. A distributed key-value storage system</title>
		<ptr target="http://project-voldemort.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Venti: A new approach to archival storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dorward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Conference on File and Storage Technologies (FAST)</title>
		<meeting>USENIX Conference on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2002-01" />
			<biblScope unit="page" from="89" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Active disks for large-scale data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nagle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">JouleSort: A balanced energy-efficient benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rivoire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The design and implementation of a log-structured file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="52" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Filling the memory access gap: A case for on-chip magnetic storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Schlosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<idno>CMU-CS-99-174</idno>
		<imprint>
			<date type="published" when="1999-11" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Byzantine generals in action: implementing fail-stop processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
		<idno>0734-2071</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="154" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Chord: A scalable peer-to-peer lookup service for Internet applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2001-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pergamum: Replacing tape with energy efficient, reliable, diskbased archival storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Storer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Greenan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Voruganti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Conference on File and Storage Technologies</title>
		<meeting>USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2008-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Low power Amdahl blades for data intensive computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szalay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Terzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Object storage on CRAQ: High-throughput chain replication for read-mostly workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Terrace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Annual Technical Conference</title>
		<meeting>USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Delivering energy proportionality with non energyproportional systems -optimizing the ensemble</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tolia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marwah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HotPower</title>
		<meeting>HotPower</meeting>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Query processing techniques for solid state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsirogiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harizopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Wiener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Graefe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Chain replication for supporting high throughput and availability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th USENIX OSDI</title>
		<meeting>6th USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><surname>Wattsup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Net Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meter</surname></persName>
		</author>
		<ptr target="http://wattsupmeters.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scheduling for reduced CPU energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st USENIX OSDI</title>
		<meeting>1st USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="1994-11" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">eNVy: A non-volatile, main memory storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th International Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>6th International Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="1994-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Microhash: An efficient index structure for flash-based sensor devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeinalipour-Yazti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalogeraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Najjar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th USENIX Conference on File and Storage Technologies</title>
		<meeting>4th USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hibernator: Helping disk arrays sleep through the winter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>20th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2005-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
