<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">European Union regulations on algorithmic decision-making and a &quot;right to explanation&quot;</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-08-31">31 Aug 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryce</forename><surname>Goodman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Oxford Internet Institute</orgName>
								<address>
									<settlement>Oxford</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">St Giles&apos;</orgName>
								<address>
									<postCode>OX1 3LB</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
							<email>flaxman@stats.ox.ac.uk.</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<addrLine>24-29 St Giles&apos;</addrLine>
									<postCode>OX1 3LB</postCode>
									<settlement>Oxford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">European Union regulations on algorithmic decision-making and a &quot;right to explanation&quot;</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-08-31">31 Aug 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1606.08813v3[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We summarize the potential impact that the European Union&apos;s new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on userlevel predictors) which &quot;significantly affect&quot; users. The law will also effectively create a &quot;right to explanation,&quot; whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In April 2016, for the first time in over two decades, the European Parliament adopted a set of comprehensive regulations for the collection, storage and use of personal information, the General Data Protection Regulation (GDPR) 1 <ref type="bibr" target="#b25">[26]</ref>. The new regulation has been described as a "Copernican Revolution" in data protection law, "seeking to shift its focus away from paper-based, bureaucratic requirements and towards compliance in practice, harmonization of the law, and individual empowerment" <ref type="bibr" target="#b21">[22]</ref>. Much of the regulations are clearly aimed at perceived gaps and inconsistencies in the EU's current approach to data protection. This includes, for example, the codification of the "right to be forgotten" (Article 17), and regulations for foreign companies collecting data from European citizens (Article 44).</p><p>However, while the bulk of language deals with how data is collected and stored, the regulation contains Article 22: Automated individual decision-making, including profiling (see figure 1) potentially prohibiting a wide swath of algorithms currently in use in, e.g. recommendation systems, credit and insurance risk assessments, computational advertising, and social networks. This raises important issues that are of particular concern to the machine learning community. In its current form, the GDPR's requirements could require a complete overhaul of standard and widely used algorithmic techniques. The GDPR's policy on the right of citizens to receive an explanation for algorithmic decisions highlights the pressing importance of human interpretability in algorithm design. If, as expected, the GDPR takes effect in its current form in mid-2018, there will be a pressing need for effective algorithms which can operate within this new legal framework. <ref type="bibr">Article 22</ref>. Automated individual decision making, including profiling 1. The data subject shall have the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her.</p><p>2. Paragraph 1 shall not apply if the decision:</p><p>(a) is necessary for entering into, or performance of, a contract between the data subject and a data controller;</p><p>(b) is authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject's rights and freedoms and legitimate interests; or (c) is based on the data subject's explicit consent.</p><p>3. In the cases referred to in points (a) and (c) of paragraph 2, the data controller shall implement suitable measures to safeguard the data subject's rights and freedoms and legitimate interests, at least the right to obtain human intervention on the part of the controller, to express his or her point of view and to contest the decision.</p><p>4. Decisions referred to in paragraph 2 shall not be based on special categories of personal data referred to in Article 9(1), unless point (a) or (g) of Article 9(2) apply and suitable measures to safeguard the data subject's rights and freedoms and legitimate interests are in place. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The General Data Protection Regulation is slated to go into effect in April 2018, and will replace the EU's 1995 Data Protection Directive (DPD). On the surface, the GDPR merely reaffirms the DPD's right to explanation and restrictions on automated decision-making. However, this reading ignores a number of critical differences between the two pieces of legislation. First, it is important to note the difference between a Directive and a Regulation. While a Directive "set[s] out general rules to be transferred into national law by each country as they deem appropriate", a Regulation is "similar to a national law with the difference that it is applicable in all EU countries" <ref type="bibr" target="#b12">[13]</ref>. In other words, the 1995 Directive was subject to national interpretation, and was only ever indirectly implemented through subsequent laws passed within individual member states <ref type="bibr" target="#b14">[15]</ref>. The GDPR, on the other hand, requires no enabling legislation to take effect. It does not direct the law of EU member states, it simply is the law for member states (or will be, when it takes effect).</p><p>Second, the DPD and GDPR are worlds apart in terms of the penalties that can be imposed on violators. Under the DPD, there are no explicit maximum fines. Instead, fines are determined on a country by country basis. By contrast, the GDPR introduces EU-wide maximum penalties of 20 million euro or 4% of global revenue, whichever is greater (Article 83, Paragraph 5). For companies like Google and Facebook, this could mean fines in the billions.</p><p>Third, the scope of the GDPR is explicitly global (cf. Article 3, Paragraph 1). Its requirements do not just apply to companies that are headquartered in the EU but, rather, to any companies processing EU residents' personal data. For the purposes of determining jurisdiction, it is irrelevant whether that data is processed within the EU territory, or abroad.</p><p>Before proceeding with analysis, we summarize some of the key terms employed in the GDPR as defined in Article 4: Definitions:</p><p>• Personal data is "any information relating to an identified or identifiable natural person"</p><p>• Data subject is the natural person to whom data relates</p><p>• Processing is "any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means"</p><p>• Profiling is "any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person"</p><p>Thus profiling should be construed as a subset of processing, under two conditions: the processing is automated, and the processing is for the purposes of evaluation.</p><p>The GDPR calls particular attention to profiling aimed at "analys[ing] or predict[ing] aspects concerning that natural person's performance at work, economic situation, health, personal preferences, interests, reliability, behavior, location or movements" (Article 4, Paragraph 4). Given the breadth of categories, it stands to reason that the GDPR's desideratum for profiling errs on the side of inclusion, to say the least.</p><p>Article 22: Automated individual decision-making, including profiling, paragraph 1 (see figure 1) prohibits any "decision based solely on automated processing, including profiling" which "significantly affects" a data subject. Paragraph 2 specifies that exceptions can be made "if necessary for entering into, or performance of, a contract", authorized by "Union or Member State law" or "based on the data subjects explicit consent." However, paragraph 3 states that, even in the case of exceptions, data controllers must "provide appropriate safeguards" including "the right to obtain human intervention...to express his or her point of view and to contest the decision." . Paragraph 4 specifically prohibits automated processing "based on special categories of personal data" unless "suitable measures to safeguard the data subjects rights and freedoms and legitimate interests are in place".</p><p>Note that this section does not address the conditions under which it is ethically permissible to access sensitive data-this is dealt with elsewhere (e.g. Article 7). Rather, it is implicitly assumed in this section that the data is legitimately obtained. Thus the provisions for algorithmic profiling are an additional constraint that apply even if the data processor has informed consent from data subjects. <ref type="bibr" target="#b1">2</ref> These provisions present a number of practical challenges for the design and deployment of machine learning algorithms. This paper focuses on two: issues raised by the GDPR's stance on discrimination and the GDPR's "right to explanation." Throughout, we highlight opportunities for researchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Non-discrimination</head><p>In general, discrimination might be defined as the unfair treatment of an individual because of his or her membership in a particular group, e.g. race, gender, etc. <ref type="bibr" target="#b2">[3]</ref>. The right to non-discrimination is deeply embedded in the normative framework that underlies the EU, and can be found in Article 21 of the Charter of Fundamental Rights of the European Union, Article 14 of the European Convention on Human Rights, and in Articles 18-25 of the Treaty on the Functioning of the European Union.</p><p>The use of algorithmic profiling for the allocation of resources is, in a certain sense, inherently discriminatory: profiling takes place when data subjects are grouped in categories according to various variables, and decisions are made on the basis of subjects falling within so-defined groups. It is thus not surprising that concerns over discrimination have begun to take root in discussions over the ethics of big data. Barocas and Selbst sum the problem up succinctly: "Big data claims to be neutral. It isn't" <ref type="bibr" target="#b3">[4]</ref>. As the authors point out, machine learning depends upon data that has been collected from society, and to the extent that society contains inequality, exclusion or other traces of discrimination, so too will the data. Consequently, "unthinking reliance on data mining can deny members of vulnerable groups full participation in society" <ref type="bibr" target="#b3">[4]</ref>. Indeed, machine learning can reify existing patterns of discrimination-if they are found in the training dataset, then by design an accurate classifier will reproduce them. In this way, biased decisions are presented as the outcome of an 'objective' algorithm.</p><p>Paragraph 71 of the recitals (the preamble to the GDPR, which explains the rationale behind it but is not itself law) explicitly requires data controllers to "implement appropriate technical and organizational measures" that "prevents, inter alia, discriminatory effects" on the basis of processing sensitive data. According to Article 9: Processing of special categories of personal data, sensitive data includes: personal data revealing racial or ethnic origin, political opinions, religious or philosophical beliefs, or trade-union membership, and the processing of genetic data, biometric data for the purpose of uniquely identifying a natural person, data concerning health or data concerning a natural person's sex life or sexual orientation...</p><p>It is important to note that paragraph 71 and Article 22 paragraph 4 specifically address discrimination from profiling that makes use of sensitive data. In unpacking this mandate, we must distinguish between two potential interpretations. The first minimal interpretation is that this directive only pertains to cases where an algorithm is making direct use of data that is explicitly sensitive. This would include, for example, variables that code for race, finances, or any of the other categories of sensitive information referred to in Article 9. However, it is widely acknowledged that simply removing certain variables from a model does not ensure predictions that are, in effect, uncorrelated to those variables (e.g. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17]</ref>). For example, if a certain geographic region has a high number of low income or minority residents, an algorithm that employs geographic data to determine loan eligibility is likely to produce results that are, in effect, informed by race and income.</p><p>Thus a second maximal interpretation, takes a broader view of 'sensitive data' to include not only those variables which are explicitly named, but also any variables with which they are correlated. This would put the onus on a data processor to ensure that algorithms are not provided with datasets containing variables that are correlated with the "special categories of personal data" in Article 9.</p><p>However, this interpretation also suffers from a number of complications in practice. With relatively small datasets it may be possible to both identify and account for correlations between sensitive and 'non-sensitive' variables. However, removing all data correlated with sensitive variables may make the resulting predictor virtually useless. As Calders and Verwer note, "postal code can reveal racial information and yet at the same time, still give useful, non-discriminatory information on loan defaulting" <ref type="bibr" target="#b8">[9]</ref>.</p><p>Furthermore, as datasets become increasingly large, correlations can become increasingly complex and difficult to detect. The link between geography and income may be obvious, but less obvious correlations-say between IP address and race-are likely to exist within large enough datasets and could lead to discriminatory effects. For example, at an annual conference of actuaries, consultants from Deloitte explained that they can now "use thousands of 'non-traditional' third party data sources, such as consumer buying history, to predict a life insurance applicant's health status with an accuracy comparable to a medical exam" <ref type="bibr" target="#b27">[28]</ref>. With sufficiently large data sets, the task of exhaustively identifying and excluding data features correlated with "sensitive categories" a priori may be impossible. Companies may also be reluctant to exclude certain covariates-web-browsing patterns are a very good predictor for various recommendation systems, but they are also correlated with sensitive categories.</p><p>A final challenge, which purging variables from the dataset does not address, is posed by what we term uncertainty bias. This bias arises when two conditions are met:</p><p>• One group is underrepresented in the sample <ref type="bibr" target="#b2">3</ref> , so there is more uncertainty associated with predictions about that group</p><p>• The algorithm is risk averse, so it will ceteris paribus prefer to make decisions based on predictions about which they are more confident (i.e. those with smaller confidence intervals <ref type="bibr" target="#b1">[2]</ref>)</p><p>In practice, this could mean that predictive algorithms (e.g. for loan approval) favor groups that are better represented in the training data, since there will be less uncertainty associated with those predictions. Uncertainty bias is illustrated in <ref type="figure">Figure 2</ref>. The population consists of two groups, white and non-whites. An algorithm is used to decide whether to extend a loan, based on the predicted probability that the individual will repay the loan. We repeatedly generated synthetic datasets of size 500, varying the true proportion of non-whites in the population. In every case, we set the true probability of repayment to be independent of group membership: all individuals have a 95% probability of repayment regardless of race. Using a logistic regression classifier, we consider a case in which loan decisions are made in a risk averse manner, by using the following decision rule: check whether the lower end of the 95% confidence interval for an individual is above a fixed "approval threshold" of 90%. In all cases, all white individuals will be offered credit since the true probability is 95% and the sample size is large enough for the confidence interval to be small. However, when the non-white population is any fraction less than 30% of the total population, they will not be extended credit due to the uncertainty inherent in the small sample.</p><p>Note that in practice, more complicated combinations of categories (occupation, location, consumption patterns, etc.) would be considered by a classifier and rare combinations will have very few observations. This issue is compounded in an active learning setting: consider the same setting, where non-whites and whites are equally likely to default. A small initial bias towards the better represented groups due will be compounded over time as the active learning acquires more examples of the better represented group and their overrepresentation grows. The  <ref type="figure">Figure 2</ref>: An illustration of uncertainty bias: a hypothetical algorithm is used to predict the probability of loan repayment in a setting in which the ground truth is that non-whites and whites are equally likely to repay. The algorithm is risk averse, so it makes an offer when the lower end of the 95% confidence interval for its predictions lie above a fixed approval threshold of 90% (dashed line). When non-whites are less than 30% of the population, and assuming a simple random sample, the algorithm exhibits what we term "uncertainty bias"-the underrepresentation of non-whites means that predictions for non-whites have less certainty, so they are not offered loans. As the non-white percentage approaches 50% the uncertainty approaches that of whites and everyone is offered loans.</p><p>GDPR thus presents us with a dilemma with two horns: under the minimal interpretation the non-discrimination requirement is ineffective, under the maximal interpretation it is infeasible. However it would be premature to conclude that non-discrimination measures are without merit. Rather, the complexity and multifaceted nature of algorithmic discrimination suggests that appropriate solutions will require an understanding of how it arises in practice. This highlights the need for human-intelligible explanations of algorithmic decision making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Right to explanation</head><p>The provisions outlined in Articles 13-15 specify that data subjects have the right to access information collected about them, and also requires data processors to ensure data subjects are notified about the data collected. However, it is important to distinguish between these rights, which may be termed the right to access and notification, and additional "safeguards for the rights and freedoms of the data subject" required under Article 22 when profiling takes place. Although the Article does not elaborate what these safeguards are beyond "the right to obtain human intervention" 4 , Articles 13 and 14 state that, when profiling takes place, a data subject has the right to "meaningful information about the logic involved." This requirement prompts the question: what does it mean, and what is required, to explain an algorithm's decision?</p><p>Standard supervised machine learning algorithms for regression or classification are inherently based on discovering reliable associations / correlations to aid in accurate out-of-sample prediction, with no concern for causal reasoning or "explanation" beyond the statistical sense in which it is possible to measure the amount of variance explained by a predictor. As Mildebrandt writes, "correlations stand for a probability that things will turn out the same in the future. What they do not reveal is why this should be the case" <ref type="bibr" target="#b17">[18]</ref>. The use of algorithmic decisions in an increasingly wider range of applications has led some (e.g. <ref type="bibr" target="#b26">[27]</ref>) to caution against the rise of a "black box" society and demand increased transparency in algorithmic decision-making. The nature of this requirement, however, is not always clear.</p><p>Burrell distinguishes between three barriers to transparency <ref type="bibr" target="#b7">[8]</ref>:</p><p>• Intentional concealment on the part of corporations or other institutions, where decision making procedures are kept from public scrutiny</p><p>• Gaps in technical literacy which mean that, for most people, simply having access to underlying code is insufficient</p><p>• A "mismatch between the mathematical optimization in high-dimensionality characteristic of machine learning and the demands of human-scale reasoning and styles of interpretation"</p><p>Within the GDPR, Article 13: Information to be made available or given to the data subject goes some way 5 towards the first barrier, stipulating that data processors inform data subjects when and why data is collected, processed, etc. Article 12: Communication and modalities for exercising the rights of the data subject attempts to solve the second by requiring that communication with data subjects is in "concise, intelligible and easily accessible form." The third barrier, however, poses additional challenges that are particularly relevant to algorithmic selection and design. As Lisboa notes, "machine learning approaches are alone in the spectrum in their lack of interpretability" <ref type="bibr" target="#b24">[25]</ref>.</p><p>Putting aside any barriers arising from technical fluency, and also ignoring the importance of training the model, it stands to reason that an algorithm can only be explained if the trained model can be articulated and understood by a human. It is reasonable to suppose that any adequate explanation would, at a minimum, provide an account of how input features relate to predictions, allowing one to answer questions such as: Is the model more or less likely to recommend a loan if the applicant is a minority? Which features play the largest role in prediction?</p><p>There is of course a tradeoff between the representational capacity of a model and its interpretability, ranging from linear models (which can only represent simple relationships but are easy to interpret) to nonparametric methods like support vector machines and Gaussian processes (which can represent a rich class of functions but are hard to interpret). Ensemble methods like random forests pose a particular challenge, as predictions result from an aggregation or averaging procedure. Neural networks, especially with the rise of deep learning, pose perhaps the biggest challenge-what hope is there of explaining the weights learned in a multilayer neural net with a complex architecture? These issues have recently gained attention within the machine learning community and are becoming an active area of research <ref type="bibr" target="#b20">[21]</ref>. One promising avenue of research concerns developing algorithms to quantify the degree of influence of input variables on outputs, given black-box access to a trained prediction algorithm <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper has focused on two sets of issues raised by the forthcoming GDPR that are directly relevant to machine learning: the right to non-discrimination and the right to explanation. This is by no means a comprehensive overview of the potential challenges that will be faced by engineers as they adapt to the new framework. The ability of humans to intervene in algorithmic decision making, or for data subjects to provide input to the decisionmaking process, will also likely impose requirements on algorithmic design and require further investigation.</p><p>While the GDPR presents a number of problems for current applications in machine learning they are, we believe, good problems to have. The challenges described in this paper emphasize the importance of work that ensures that algorithms are not merely efficient, but transparent and fair. Research is underway in pursuit of rendering algorithms more amenable to ex post and ex ante inspection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref>. Furthermore, a number of recent studies have attempted to tackle the issue of discrimination within algorithms by introducing tools to both identify <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref> and rectify <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref> cases of unwanted bias. It remains to be seen whether these techniques are adopted in practice. One silver lining of this research is to show that, for certain types of algorithmic profiling, it is possible to both identify and implement interventions to correct for discrimination. This is in contrast to cases where discrimination arises from human judgment. The role of extraneous and ethically inappropriate factors in human decision making is well documented (e.g., <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1]</ref>), and discriminatory decision making is pervasive in many of the sectors where algorithmic profiling might be introduced (e.g. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7]</ref>). We believe that, properly applied, algorithms can not only make more accurate predictions, but offer increased transparency and fairness over their human counterparts (cf. <ref type="bibr" target="#b22">[23]</ref>).</p><p>Above all else, the GDPR is a vital acknowledgement that, when algorithms are deployed in society, few if any decisions are purely "technical". Rather, the ethical design of algorithms requires coordination between technical and philosophical resources of the highest caliber. A start has been made, but there is far to go. And, with less than two years until the GDPR takes effect, the clock is ticking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Excerpt from the General Data Protection Regulation,<ref type="bibr" target="#b25">[26]</ref> </figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Regulation (EU) 2016/679 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) [2016] OJ L119/1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Cf. "consent of the data subject should not provide in itself a legal ground for processing such sensitive data"<ref type="bibr" target="#b25">[26]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that the underrepresentation of a minority in a sample can arise through historical discrimination or less access to technology, but it is also a feature of a random sample in which groups are by construction represented at their population rates. In public health and public policy research, minorities are sometimes oversampled to address this problem.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The exact meaning and nature of the intended intervention is unspecified, and the requirement raises a number of important questions that are beyond our current scope.<ref type="bibr" target="#b4">5</ref> It is not clear whether companies will be required to disclose their learning algorithms or training datasets and, if so, whether that information will be made public.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Do judges vary in their treatment of race</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Legal Studies</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="347" to="383" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical theories of discrimination in labor markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Aigner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Cain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Industrial and Labor Relations Review</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Altman</surname></persName>
		</author>
		<editor>Zalta, E. N., editor</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>The Stanford Encyclopedia of Philosophy. Stanford University</orgName>
		</respStmt>
	</monogr>
	<note>fall 2015 edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Big data&apos;s disparate impact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">California Law Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploring Discrimination: A User-centric Evaluation of Discrimination-Aware Data Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Preibusch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="344" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better decision support through exploratory discrimination-aware data mining: foundations and empirical evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Berendt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Preibusch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="209" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Shape of the River. Long-Term Consequences of Considering Race in College and University Admissions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>ERIC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How the machine &quot;thinks&quot;: Understanding opacity in machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data &amp; Society</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Three naive bayes approaches for discrimination-free classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="292" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extraneous factors in judicial decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Danziger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Avnaim-Pesso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="6889" to="6892" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Algorithmic transparency via quantitative input influence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An approach for discrimination prevention in data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dive</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khedkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Application or Innovation in Engineering and Management</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">European Commission</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Legislation</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Certifying and removing disparate impact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The European Union Data Privacy Directive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Fromholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Berkeley Technology Law Journal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="461" to="484" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Discrimination prevention in data mining for intrusion and crime detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hajian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Domingo-Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez-Balleste</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IEEE</publisher>
			<biblScope unit="page">4754</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">How big data is unfair: Understanding sources of unfairness in data driven decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Defining profiling: a new type of knowledge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hildebrandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page">1745</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mortgage redlining: Race, risk, and demand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Finance</title>
		<imprint>
			<biblScope unit="page">8199</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Malioutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<title level="m">Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI</title>
		<meeting>the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The European Commission&apos;s proposed data protection regulation: A Copernican revolution in European data protection law</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kuner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bloomberg BNA Privacy and Security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Machines learning justice: The case for judgmental bootstrapping of legal decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Laqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Copus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Available at SSRN</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The new profiling: Algorithms, black boxes, and the failure of anti-discriminatory safeguards in the european union</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Security Dialogue</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">494511</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Interpretability in Machine LearningPrinciples and Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Lisboa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page">1521</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">General Data Protection Regulation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Parliament and Council of the European Union</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The Black Box Society: The Secret Algorithms That Control Money and Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pasquale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
	<note>1 edition edition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Civil Rights, Big Data, and Our Algorithmic Future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rieke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Justice and Technology</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Auditing algorithms: Research methods for detecting discrimination on internet platforms. Data and Discrimination: Converting Critical Concerns into Productive Inquiry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sandvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karahalios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Langbort</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Judgment under uncertainty: Heuristics and biases. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page">11241131</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Making machine learning models interpretable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vellido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Martn-Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Lisboa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Citeseer</publisher>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Handling Conditional Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zliobaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="992" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
