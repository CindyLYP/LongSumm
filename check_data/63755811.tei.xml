<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tracking Occluded Objects and Recovering Incomplete Trajectories by Reasoning about Containment Relations and Human Actions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liang</surname></persName>
							<email>liangwei@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="department" key="dep2">Center for Vision</orgName>
								<orgName type="department" key="dep3">Cognition, Learning, and Autonomy</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
							<email>yixin.zhu@ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
							<email>sczhu@stat.ucla.edu</email>
						</author>
						<title level="a" type="main">Tracking Occluded Objects and Recovering Incomplete Trajectories by Reasoning about Containment Relations and Human Actions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>This paper studies a challenging problem of tracking severely occluded objects in long video sequences. The proposed method reasons about the containment relations and human actions, thus infers and recovers occluded objects identities while contained or blocked by others. There are two conditions that lead to incomplete trajectories: i) Contained. The occlusion is caused by a containment relation formed between two objects, e.g., an unobserved laptop inside a backpack forms containment relation between the laptop and the backpack. ii) Blocked. The occlusion is caused by other objects blocking the view from certain locations, during which the containment relation does not change. By explicitly distinguishing these two causes of occlusions, the proposed algorithm formulates tracking problem as a network flow representation encoding containment relations and their changes. By assuming all the occlusions are not spontaneously happened but only triggered by human actions, an MAP inference is applied to jointly interpret the trajectory of an object by detection in space and human actions in time. To quantitatively evaluate our algorithm, we collect a new occluded object dataset captured by Kinect sensor, including a set of RGB-D videos and human skeletons with multiple actors, various objects, and different changes of containment relations. In the experiments, we show that the proposed method demonstrates better performance on tracking occluded objects compared with baseline methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>We study the problem of tracking occluded objects during human daily activities in cluttered scenes, such as packing, playing, working, etc. <ref type="figure">Figure 1</ref> shows an example of a daily indoor scenario captured by a RGB-D sensor: an agent 1 enters a room; 2 puts down her backpack; 3 takes a laptop out of the backpack and puts it on the table; 4 grabs a cup, fetches some water from a water dispenser; 5 sits back and puts down the cup next to her. During the course of this event, objects disappear and then re-appear frequently.</p><p>Tracking objects in such scenarios is a challenging problem due to severe occlusions caused by two conditions: -Contained. The occlusion is caused by a new containment relation formed between two objects, e.g., a person puts a laptop into a bag, which is view-independent;</p><p>Figure 1: A scenario for tracking occluded objects in an indoor scene. The dashed lines represent the inferred trajectories and different colors indicate different objects in the scene. By explicitly reasoning about containment relations, the proposed algorithm is capable of recovering full trajectories of objects even they are contained or occluded by other objects in the video.</p><p>-Blocked. The occlusion is caused by other objects observed from certain camera views, in which the containment relations unchanged, e.g., a laptop is sitting in front of a cup, which blocks the view of the cup from the current camera view and is view-dependent.</p><p>We argue such problem is not merely a vision task compared to traditional visual tracking tasks, which primarily focuses on reliable object detectors and data association methods. Instead, significant reasoning processes are involved. To address this problem, we believe an explicit model of relations among objects as well as relations between objects and agents are needed.</p><p>The proposed framework is shown in <ref type="figure">Figure 2</ref>. Given a RGB-D video with extracted human skeleton sequence in a scene, state-of-the-art detection algorithms are applied to detect regions of interest of each object and human actions over <ref type="figure">Figure 2</ref>: The framework of the proposed method. (a) Sensor input: a sequence of RGB-D images and human skeleton captured by a Kinect sensor. (b) Off-the-shelf state-of-the-art object detection and human action detection algorithms were applied to extract the object location and human actions per frame. (c) Inference on a network flow representation. The solid red lines denote the observations in space. The dashed red lines denote that the present state of the object is hidden and there is no observation. The blue lines denote the observations in time. S and E are the start and the end of the trajectories, respectively. A dynamic programming scheme is applied to search, optimize and recover the complete trajectory of each object. time; the detection results serve as the initial proposals and the input to our algorithm. We pose the problem of recovering object trajectories as Maximizing a Posteriori (MAP) problem using a network flow representation, in which a trajectory of an object is jointly interpreted and constrained by both object detection and containment relations in space as well as human actions over time. A dynamic programming scheme is applied to search, optimize and recover the complete trajectory of each object.</p><p>This paper makes three contributions: 1. We propose a method to recover incomplete trajectories of objects by taking account of containment relations and two causes of occlusions: contained and blocked. 2. We assume that human action is the only cause that leads to occlusions and object status changes, and use it as a constraint to interpret trajectories of objects over time. 3. We introduce a new dataset including a set of RGB-D videos of human interacting with occluded objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Spatial Reasoning. Spatial reasoning plays an essential role in human daily life. Although quantitative approaches can provide the most precise information, numerical information is often unnecessary or unavailable at human level. In computer vision, quantitative approaches usually study objects tracking problem, of which the literature is too expansive to survey here; we refer readers to recent survey and benchmark <ref type="bibr" target="#b31">(Wu, Lim, and Yang 2015;</ref><ref type="bibr" target="#b22">Smeulders et al. 2014;</ref><ref type="bibr" target="#b31">Wu, Lim, and Yang 2015)</ref>. Here, we focus on spatial reasoning methods related to the presented work.</p><p>As a typical example, container has been used to study spatial reasoning problem <ref type="bibr" target="#b4">(Bredeweg and Forbus 2003;</ref><ref type="bibr" target="#b10">Frank 1996)</ref>. Using physical-based simulations, <ref type="bibr" target="#b16">(Liang et al. 2015</ref>) evaluated human cognition of containing relations through human studies. <ref type="bibr" target="#b5">(Davis, Marcus, and Chen 2013)</ref> developed a knowledge base for qualitative reasoning about containers, expressed in a first-order language of time, geometry, objects, histories, and events. Some exemplary tasks include i) computational approaches for reasoning about liquid transfer <ref type="bibr" target="#b13">(Kubricht et al. 2016;</ref><ref type="bibr" target="#b34">Yu, Duncan, and Yeung 2015;</ref><ref type="bibr" target="#b18">Mottaghi et al. 2017)</ref>, ii) reason about containability and containment relations <ref type="bibr" target="#b17">(Liang et al. 2016;</ref><ref type="bibr" target="#b34">Yu, Duncan, and Yeung 2015;</ref><ref type="bibr" target="#b27">Wang, Liang, and Yu 2017)</ref>, and iii) occlusion modeling and reasoning <ref type="bibr" target="#b6">(Eichner and Ferrari 2010;</ref><ref type="bibr">Enzweiler et al. 2010;</ref><ref type="bibr" target="#b30">Wojek et al. 2011)</ref>. Compared to prior work, we integrate qualitative and quantitative approach and explicitly model the occlusions by containment relations when an object is contained or blocked by others.</p><p>Detection using Context. Context has been widely explored in human-object interactions (HOI) and multi-object tracking. Some typical approaches and setups include: i) Learning deformable action templates <ref type="bibr" target="#b33">(Yao et al. 2014)</ref>, actionlet <ref type="bibr" target="#b28">(Wang, Liu, and Wu 2014)</ref> and animated pose templates <ref type="bibr" target="#b33">(Yao et al. 2014)</ref>. ii) Combining spatial and functional constraints between human and objects <ref type="bibr" target="#b11">(Gupta, Kembhavi, and Davis 2009;</ref><ref type="bibr" target="#b32">Yang, Wu, and Hua 2009;</ref><ref type="bibr" target="#b29">Wei et al. 2013)</ref>. iii) Task-oriented action recognition <ref type="bibr" target="#b39">(Zhu, Zhao, and Zhu 2015)</ref> and utility learning <ref type="bibr" target="#b38">(Zhu et al. 2016;</ref><ref type="bibr" target="#b21">Shukla et al. 2017)</ref>, including complex cooking tasks <ref type="bibr" target="#b19">(Rohrbach et al. 2012;</ref><ref type="bibr" target="#b0">Aksoy et al. 2011)</ref> Different from the literature in context modeling, we explicitly model human action as a context cue. Specifically, we assume that human action is the only cause that leads to the changes of containment relations, and use the human action as a constraint to improve the tracking.</p><p>Although some recent work adopted deep neural networks to extract contexts for object detection and tracking, these data-driven feedforward methods have well-known problems: i) They are black-box models that cannot be explained and only applicable with supervised training by fitting the typical context of the object, thus difficult to generalize to new tasks. ii) Lacking explicit representation to handle occlusions, low resolution, and lighting variations-there are millions of ways to occlude an object in a given im- <ref type="figure">Figure 3</ref>: Two causes of occlusions. i) Blocked: An apple (a) can be detected at the beginning, but later (b) becomes occluded by a bowl. ii) Contained: an apple is contained by a person (c) and a bowl (d), respectively.</p><p>age , making it impossible to have enough data for training and testing such black box models. In this paper, we go beyond passive recognition by reasoning about time-varying containment relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic Formulation</head><p>A key concept in the present paper is "containment relation". An object which contains or holds another object can serve as a container, forming containment relation with the object it contains. For instance, when a laptop is inside a backpack, a containment relation is formed between the laptop and the backpack, where the backpack plays the role of container.</p><p>We make the following assumptions for containers and containment relations:</p><p>-When an object is contained by a container, the object will inherit the same trajectory from its container. For example, consider a case that a laptop is inside a backpack. If a person carries the backpack around, the laptop will move together with the backpack, sharing the same trajectory.</p><p>-The containment relation is a partially ordered relation constrained by the volume of the object and its container, i.e., if a container's volume is smaller than an object's, the object cannot be contained by this container.</p><p>-An object can only be contained directly by one container.</p><p>Suppose there are K objects in a scene. Our goal is to recover trajectories of all</p><formula xml:id="formula_0">K objects T = {T 1 , T 2 , • • • , T K } from a RGB-D image sequence I = {I 1 , I 2 , • • • , I τ },</formula><p>where τ is the length of the image sequence. T k is defined as an ordered set of object states</p><formula xml:id="formula_1">T k = {x k 1 , x k 2 , • • • , x k τ }, where x k</formula><p>t is the state of the kth object in space at time t:</p><formula xml:id="formula_2">x k t = (l k t , c k t ),</formula><p>where l k t is the location of the kth object at time t and c k t ∈ {1, 2, • • • , K} is an object index, representing the inferred container of kth object at time t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Hypotheses by Containment Relations</head><p>At each frame t, instead of purely relying on detection results, our algorithm further proposes two types of hypotheses generated based on the possible causes of occlusion. These two hypotheses provides additional cues, essentially competing with the detection results. As a result, such extra info recovered by the containment relations could later help overcome the miss or wrong detection described in the next section. The two types of hypotheses are: <ref type="figure">Figure 4</ref>: Human action features at time t in a sliding window with the length of 2 . The red line represents the distance between the hand and the spine of the person. The yellow line represents the distance change between the hand and the object. The green line represents the location change of the object in the current sliding window.</p><p>Contained. In these situations, occlusion happens due to forming new containment relations as shown in <ref type="figure">Figure 3</ref> (c) and 3 (d), where hypotheses are shown in dashed box. Formally, suppose such occlusion happens to the kth object at time t, the algorithm proposes that the location of the kth object the same as its container while keeping it's container the same as in the previous frame:</p><formula xml:id="formula_3">x k t = (l c k t t , c k t ), c k t = k. Blocked.</formula><p>In such cases, an object is occluded due to another object sitting in between the object and the camera from certain camera views, as shown in <ref type="figure">Figure 3</ref> (a) and 3 (b), where an apple is occluded by a bowl and a person. The dashed box is the proposal for the apple's present location, which is the same as the location in the last frame before occlusion happened. Formally, suppose such occlusion happens to the kth object at time t, the algorithm proposes the object state as the same in previous state:</p><formula xml:id="formula_4">x k t = x k t−1 = (l k t−1 , c k t−1 )</formula><p>, where the location and containment relation remain the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Temporal Hypotheses by Human Actions</head><p>Across different frames, we consider human as the cause and the only cause of object state changes, assuming no other external disturbance in a scene. In other words, if there is no human action occurring, the objects should remain the same location and the containment relations will not change. As a result, human actions impose a hard constraint to rule out the implausible sudden jumps from the object detection, resulting in a smooth and plausible trajectory.</p><p>In this paper, we represent the human action as a skeleton sequence</p><formula xml:id="formula_5">H = {H 1 , H 2 , • • • , H τ },</formula><p>where τ is the length of the sequence. At time t, 25 joints of human skeleton captured by a Kinect sensor were used:</p><formula xml:id="formula_6">H t = (h 1 t , h 2 t , • • • , h 25 t ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recovering Incomplete Trajectories</head><p>We recover incomplete trajectories using MAP by reasoning about containment relations and human actions:</p><formula xml:id="formula_7">T * = arg max T P (T |I) = arg max T P (T |X , H) (1) ∝ arg max T P (X |T )P (T |H) (2) = arg max T k P (X |T k )P (T k |H),<label>(3)</label></formula><p>where X and H are the object detection in space and human action in time, respectively. P (X |T k ) = τ t=1 P (X t |x k t ) models the likelihood for object detector response</p><formula xml:id="formula_8">X = {X 1 , X 2 , • • • , X τ } P (T k |H)</formula><p>is a dynamic model which is a smoothness term for trajectory, and can be decomposed as</p><formula xml:id="formula_9">P (T k |H) = P ({x k 1 , x k 2 , • • • , x k τ }|H) (4) = PS(x k 0 ) τ t=1 P (x k t |x k t−1 , Ht−1) PE(x k τ ),<label>(5)</label></formula><p>where P S (x k 0 ) and P E (x k τ ) are the probability for initialization and termination, respectively, and P (x k t |x k t−1 , H k t−1 ) is the transition probability of two consecutive frames, which models the probability that the object status changes from time t − 1 to t based on the observation of human action H t−1 . Intuitively, this probability evaluates the consistency between the location of an object and human actions. As we discuss in previous section, the location changes of an object can be interpreted by the occurrence of human actions. <ref type="figure">Figure 4</ref> illustrates three examples of the object location changes and the corresponding human actions, including (a) a person taking an apple from a bowl, (b) a person throwing a frisbee, and (c) the object keeps the same location without human action.</p><p>The transition probability of two consecutive states is</p><formula xml:id="formula_10">− log P (x k t |x k t−1 , H k t−1 ) = ω , θ [ ] ,<label>(6)</label></formula><p>where ω is the template parameter. θ [ ] is the extracted human action feature in a time interval</p><formula xml:id="formula_11">[t − 1 − , t − 1 + ].</formula><p>For θ, we consider three types of features in a sliding window on the time axis: human pose, relative movements between the human and the object, and the object movements. Suppose that the sliding window size is 2 , the feature vector sequence at time t is</p><formula xml:id="formula_12">F m = (F h m , F r m , F o m ), m ∈ [t − 1 − t − 1 + ]. Specifically,</formula><p>-F h m is the relative distance of all the skeletons to three base points (two shoulders and one spine point), which encodes human action. In <ref type="figure">Figure 4</ref>, we show one component of F h m in red lines: the distance between the hand and the spine point.</p><p>-F r m is the distance between human hand and the location of the object, which is denoted in yellow lines in <ref type="figure">Figure 4</ref>.</p><p>-F o m is the distance between the locations of the object at time m and t, depicted by green lines in <ref type="figure">Figure 4</ref>.</p><p>A sequence clip is first interpolated to a certain length. The wavelet transform is then applied to F m . The coefficients at the low frequency are kept as the action feature. The window sizes and sliding steps are both in multiple scales.</p><p>Substituting Eq. 4 into Eq. 1, we then have</p><formula xml:id="formula_13">T * = arg max T K k=1 τ t=1 [P (Xt|x k t )• (7) PS(x k 0 ) • P (x k t |x k t−1 , Ht−1) • PE(x k τ )].</formula><p>We can reformulate Eq. 7 as an Integer Linear Programming problem: where</p><formula xml:id="formula_14">f * = arg min f C(f ),<label>(8)</label></formula><formula xml:id="formula_15">C(f ) = i c s i f s i + i,j cijfij + i cifi + i c e i f e i (9) cij = − log P (xj|xi, Hi) (10) ci = − log P (xi|T k ) (11) c e i = − log PE(xi) (12) c s i = − log PS(xi) (13) s.t. fij, fi, f s i , f e i ∈ {0, 1}.<label>(14)</label></formula><p>This is equivalent to finding a min-cost path in network flow with source S and sink E as shown in <ref type="figure">Figure 2</ref>: the red arrows denotes the detection on input RGB-D images with cost on the edge c i , the dashed red arrows indicates that the object is hidden at the present state and there is no observation from current frame, and each transition between successive frames is denoted by blue lines with cost c ij given by human actions, serving as a smoothness term. Dynamic programming is applied to optimize Eq. 9. By assuming objects will not affect each other's trajectory, we optimize the trajectory for each object individually. Firstly, we run K-Shortest Paths Algorithm <ref type="bibr" target="#b3">(Berclaz et al. 2011)</ref>, which generates a set of tracklets. Then we use the Viterbi algorithm to connect these tracklets, which yields continuous trajectories for each object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset</head><p>We collected a 3D dataset with diverse scenes, multiple actions and various objects to evaluate the proposed method ( <ref type="figure" target="#fig_0">Figure 5</ref>). 1346 video clips in 10 scene categories were captured by Kinect sensors. RGB and depth images, 3D human skeletons as well as point cloud data were recorded in each video clip. Compared with existing dataset, the proposed dataset focuses on occluded objects for visual tracking, which consists of a large variety of human actions causing object location changes in different scenarios, such as throwing, catching, picking up, putting down, fetching, lifting, etc.</p><p>Each frame in the dataset was manually annotated with ground truth by drawing bounding boxes for each object. When an object is occluded, we annotate the ground truth based on two types of causes for the occlusions. i) Contained. The object shares the location with its container, <ref type="figure">Figure 6</ref>: Transition probability of the object location in the green bounding box. The solid boxes depict that the object is tracked by object detectors. The dashed boxes depict that the object is recovered by inference. (a), (c) and and (e) show detected bounding boxes and human skeletons on point cloud. (b) and (d) are the transition probabilities between two possible locations. In (b), the bottom four bars with low probability keep the same since we constrain the impossible object moving that are not caused by human actions. Transitions in DP: an In-depth Example <ref type="figure">Figure 6</ref> shows an example of the trajectory inference process of an object bounded by a green box. The tracking results are visualized in the bottom panel, where the solid boxes denote the detected location, and the dashed boxes denote the inferred results. Specifically, we employed the stateof-the-art RGB-D based detectors (Song and Xiao 2013) on a RGB-D image sequence. The detected objects are bounded by boxes with different colors shown in <ref type="figure">Figure 6</ref> (a), (c) and (e). The human skeletons from Kinect are in red color. <ref type="figure">Figure 6</ref> (b) and (d) illustrate the partial transition probabilities changes between two consecutive states in an interval (frame 11 to frame 51, frame 53 to frame 108), equivalent to the probability of human actions and calculated by Eq. 6. The left panel of (b) and (d) are some possible transitions. Take the first bar in (b) as an example. The green and red dot represent the location of the object bounded by green bounding box and the person, respectively. The bar depicts the probabilities of the transition from the green box location to the human hand location over time. We can see that the probability increases from frame 11 to frame 51. At frame 51, the person picked up the object. From frame 59 to frame 108, the object was held by the person. The first bar of <ref type="figure">Figure 6 (d)</ref> shows the probability of the object being carried by this person.</p><p>It is worth noting that the bottom four bars in Figure 6 (b) have low transition probabilities which are close to zero. Take the last bar in <ref type="figure">Figure 6 (b)</ref> as an example. It shows the probability of the object bounded by the green box moving to the location of the object bounded by the orange box. This movement was not caused by human action and violated our assumption, which was ruled out during the inference.</p><p>From frame 51 to 109, the object was contained and thus cannot be visually detected. Human action provided a strong cue for the object location: a person picked up this object and moved it to a container bounded by a yellow bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablative Analysis: Roles of Interactions in HOI</head><p>In this section, we evaluate the roles and importance of HOI quantitatively by turning on and off certain components in the proposed method.</p><p>We consider the HOI as a binary classification problem: if the object movement is consistent with human action, it should be classified as positive; otherwise it is negative. We define whether the object movement is consistent with human action using two criteria: i) if no human action, the object should remain stationary, and vice versa; ii) if there is an object location change, the object should follow the trajectory of human action.</p><p>We first consider the simplest method using human pose only, i.e., Eq. 6 with feature vector F m = (F h m ). As showed in <ref type="figure" target="#fig_1">Figure 7a</ref>, using human pose only is not sufficient to achieve reasonable performance. This was mainly caused by the lack of object context, disallowing a good classification between certain actions, e.g., putting down and picking up.</p><p>Next, we consider the method using both human pose and object context, i.e., Eq. 6 with feature vector</p><formula xml:id="formula_16">F m = (F h m , F r m , F o m )</formula><p>. Although achieving reasonable results as shown in <ref type="figure" target="#fig_1">Figure 7b</ref>, this method only looks at local window m ∈ [t−1− , t−1+ ], thus lacking of global optimization. With back propagation using DP as described in Eq. 7 and Eq. 9, the proposed method globally adjust the inference, resulting in the best performance among three methods as shown in <ref type="figure" target="#fig_1">Figure 7c</ref>.</p><p>All the results report here were trained by SVM on the same training data. To address the problem of different scales of interaction, different step sizes and different sliding window sizes along time axis were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablative Analysis: Spatial/Temporal Suppression</head><p>In this section, we design two experiments (baseline1 and baseline2) to evaluate how spatial and temporal information influence the tracking. We compare the results of these two experiments with the approach of tracking with occlusion model (baseline3) and the proposed method (full model).</p><p>As an example, we show comparisons of results from different methods using a video of 530 frames <ref type="figure" target="#fig_2">(Figure 8)</ref>. In this video, three actors threw and caught a ball highlighted by a yellow bounding box. The ball traveled fairly fast, appearing and then disappearing frequently. Directions, scales, and views of the ball also varied. Severe occlusions by hands or other body parts occurred. Temporal Suppression (baseline1). In this setting, we do not consider the human actions, i.e., set Eq. 10 to a constant. As a result, it is equivalent to an online tracking problem: the trajectory of an object is determined only by the response of detectors. Non-maximum suppression was applied on all detection candidates per frame. <ref type="figure" target="#fig_2">Figure 8 (b)</ref> shows the results. Spatial Suppression (baseline2). In this setting, we set Eq. 11 to a constant, i.e., not considering the detection score, but inferring object location only by human actions in time. In other words, the trajectory of an object is determined only by the transition probabilities modeled by human actions.</p><p>Results were shown in <ref type="figure" target="#fig_2">Figure 8 (d)</ref>. Failure cases mostly fall into two categories: i) when human skeleton, the object and the container are occluded at the same time, and ii) when human skeleton or the object are partially occluded, it is difficult to distinguish the throwing action from the catching action as the lack of action cues or spatial context. Tracking with occlusion model (baseline3). A related topic in computer vision is multi-object tracking. Some re-cent efforts were trying to infer and recover both shortterm and long-term occluded objects by occlusion assumption <ref type="bibr" target="#b36">(Zhang, Li, and Nevatia 2008;</ref><ref type="bibr" target="#b1">Andriyenko and Schindler 2011)</ref>. In this paper, we use <ref type="bibr" target="#b36">(Zhang, Li, and Nevatia 2008)</ref> as the baseline representing the state-of-the-art multi-object tracking algorithm with occlusion assumptions, which adopted an Explicit Occlusion Model (EOM) to track with long-term inter-object occlusions, adding occluded object hypothesis to model occlusions. Full model. The results of full model are shown in <ref type="bibr">Figure 8 (e)</ref>. Benefit from both spatial and temporal terms with back propagation, most of the occlusions were successfully recovered. The failure cases happened when the object was transferred continuously between containers without any valid object detection in space. For example, from frame 265-320, the ball was passed from actor 1 to actor 2 and then passed to actor 3. Later, at frame 320, the ball was passed back to actor 1. In this case, the ball was not detected during the entire process. As the result, our method believed the ball was in the hand of actor 1 all the time. Results. To evaluate our method quantitatively, we extract two subsets of the video clips from the proposed dataset based on two causes of occlusions: contained by another object and the blocked camera views. We evaluate the accuracy on these two subsets as well as on the entire dataset. Success rate was adopted for quantitative analysis, defined as the ratio between the number of frames with correct object localization and the number of all frames. Given an estimated bounding box of an object b e and the ground truth bounding box b g , the overlap score is defined as r = be∩bg be∪bg , where ∩ and ∪ are the intersection and union of two regions. An object bounding box is considered correct if r ≥ r 0 . The <ref type="figure">Figure 9</ref>: Different overlap ratios evaluated on different subsets. The red, yellow, green, and blue line represent the results of full model, baseline1, baseline2, and baseline3, respectively. The horizontal axis is the threshold axis, ranging from 0 to 1. The vertical axis is the success rate. accuracy of the tracking results are shown in <ref type="table" target="#tab_0">Table 1</ref> with r 0 = 0.5. We further evaluate success rate when varying different overlap ratios r 0 . Results are shown in in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluations on Existing Datasets</head><p>In addition to our proposed dataset designed for tracking severe objects which are "contained" or "blocked", we further test our method on some existing datasets for modeling HOI: CAD-120 <ref type="bibr" target="#b24">(Sung et al. 2012)</ref>, CMU interaction dataset <ref type="bibr" target="#b11">(Gupta, Kembhavi, and Davis 2009)</ref>, MSR action recognition dataset <ref type="bibr" target="#b35">(Yuan, Liu, and Wu 2009)</ref>, and NW-UCLA Multiview Action 3D dataset . The major differences between these four datasets and other public available datasets (e.g., the multiple objects tracking datasets) is: these four datasets focus on rich HOI, severe occlusions between human and objects, and large appearance variations of object, which is the main focus of this paper.</p><p>To evaluate our method on these datasets, we apply the RGB-D detectors <ref type="bibr" target="#b23">(Song and Xiao 2013)</ref> for RGB-D datasets <ref type="bibr" target="#b24">(Sung et al. 2012;</ref><ref type="bibr" target="#b35">Yuan, Liu, and Wu 2009)</ref>, and RGB detectors <ref type="bibr" target="#b12">(Kalal, Mikolajczyk, and Matas 2012)</ref> for RGB-only dataset <ref type="bibr" target="#b11">(Gupta, Kembhavi, and Davis 2009)</ref>. For action detection, we train a classifier on 2D data for CAD 120 and CMU interaction datasets which have no skeleton data. Examples of qualitative results are shown in <ref type="figure" target="#fig_3">Figure 10</ref>.</p><p>The quantitative tracking accuracy is shown in <ref type="table" target="#tab_1">Table 2</ref>. The performance of our method on MSR action recognition and Northwestern-UCLA dataset is better than the results on CAD-120 and CMU dataset. We believe two reasons contributed to the performance differences: i) Some errors were caused by the unreliable action detections in 2D space compared to 3D space. ii) Small object detections are more challenging in 2D cases, such as pouring from a cup, lighting a flash light in the CMU dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Discussions</head><p>We propose an algorithm to infer occluded objects and recover the incomplete trajectories for objects in a cluttered indoor scene by reasoning about containment relations and human actions. We assume that the movements of objects are only caused by human actions, and explicitly model occlusions from two causes: contained by others, or blocked camera views. A network flow representation is adopted to globally optimize trajectories based on two occlusion causes. In the experiment, we test our method on the collected occluded objects dataset and other four existing datasets, demonstrating the proposed method can provide better performance in challenging scenarios. The current work is limited in the following aspects: i) When the object detection is noisy, the performance of our method is likely to degenerate, especially when continuous transitions between occluded objects happen. High level knowledge may help to improve the results, e.g., integrating an inference algorithm for the intention of the agent.</p><p>ii) We currently limit the scenarios where human is the only cause that leads to the object status changes, thus are unable to handle situations where objects move only by invisible force field, e.g., gravity. Such challenging situations would require a much deeper understanding of the 3D scenes, particular the "dark matter" that is invisible <ref type="bibr" target="#b20">(Shu et al. 2015)</ref>, e.g., functionality <ref type="bibr" target="#b37">(Zheng et al. 2013;</ref><ref type="bibr" target="#b39">Zhu, Zhao, and Zhu 2015)</ref> and causality <ref type="bibr" target="#b9">(Fire and Zhu 2013)</ref>.</p><p>iii) The majority of computer vision community is focusing on rigid body. However, properly modeling fluid (e.g., water <ref type="bibr" target="#b2">(Bates et al. 2015;</ref><ref type="bibr" target="#b13">Kubricht et al. 2016)</ref>) and granular material (e.g., sand <ref type="bibr" target="#b14">(Kubricht et al. 2017)</ref>) is important for inferring containment relations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Our occluded object tracking dataset. (a) Statistic of the dataset. (b) Some examples of the activities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>Confusion matrix of HOI. 1 denotes that the object movement is consistent with HOI, whereas 2 denotes that the object movement is not consistent with HOI. (a) Human pose sequence only. (b) Human pose sequence with objects context. (c) Joint inference in our method.forming a new containment relation. ii) Blocked. The object is stationary, and the containment relation remains the same. For the situation that a person serves as a container, we draw a bounding box on the person's hand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>An example of the experiment results. The goal is to track the yellow ball. In each bar, the yellow represents the correct results, and the black represents the wrong results. The overlap ratio of bounding boxes were set to 0.5. Different colors denote different objects: actor 1 (green), actor 2 (blue), actor 3 (red) and ball (yellow).(a) Examples of tracking results. The dashed boxes depict the object is occluded. (b) Temporal-suppression results. (c) The scores of consistency between object movement and human action. (d) Spatial-suppression results. (e) Full model results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 10 :</head><label>10</label><figDesc>More qualitative results. Solid boxes are detected by tracking algorithm and the dashed boxes are inferred. Top two rows: (a) CAD-120, (b) CMU Dataset, (c) MSR Dataset, and (d) NW-UCLA Dataset. The bottom three rows (e) are the results on our proposed occluded objects dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Tracking accuracy of full model compared with three baselines on different subsets of the proposed dataset.</figDesc><table><row><cell></cell><cell cols="4">baseline1 baseline2 baseline3 full</cell></row><row><cell>all</cell><cell>0.57</cell><cell>0.32</cell><cell>0.59</cell><cell>0.69</cell></row><row><cell>blocked</cell><cell>0.21</cell><cell>0.08</cell><cell>0.25</cell><cell>0.47</cell></row><row><cell>contained</cell><cell>0.15</cell><cell>0.02</cell><cell>0.16</cell><cell>0.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Tracking accuracy on other datasets.</figDesc><table><row><cell></cell><cell cols="4">baseline1 baeline2 baseline3 full</cell></row><row><cell>CAD-120</cell><cell>0.30</cell><cell>0.13</cell><cell>0.33</cell><cell>0.47</cell></row><row><cell>CMU</cell><cell>0.28</cell><cell>0.12</cell><cell>0.25</cell><cell>0.43</cell></row><row><cell>MSR</cell><cell>0.43</cell><cell>0.21</cell><cell>0.44</cell><cell>0.60</cell></row><row><cell>NW-UCLA</cell><cell>0.56</cell><cell>0.25</cell><cell>0.56</cell><cell>0.72</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The work reported herein was supported by a Natural Science Foundation of China (NSFC) grant No.61472038 and  No.61375044 (to Liang), DARPA XAI grant N66001-17-2-4029 and ONR MURI grant N00014-16-1-2007 (to Zhu).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning the semantics of object-action relations by observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abramov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dörr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wörgötter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1229" to="1249" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-target tracking by continuous energy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andriyenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1265" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Humans predict liquid dynamics using probabilistic simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Cognitive Science Society</title>
		<meeting>the 37th Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple object tracking using k-shortest paths optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Turetken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1806" to="1819" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Qualitative modeling in education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bredeweg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reasoning from radically incomplete information: The case of containers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Annual Conference on Advances in Cognitive Systems (ACS)</title>
		<meeting>the Second Annual Conference on Advances in Cognitive Systems (ACS)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="page">288</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">We are family: Joint pose estimation of multiple persons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="228" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eigenstetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-cue pedestrian classification with partial occlusion handling</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="990" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning perceptual causality from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Fire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop: Learning Rich Representations from Low-Level Sensors</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Qualitative spatial reasoning: Cardinal directions as an example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">U</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Geographical Information Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="290" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Observing human-object interactions: Using spatial and functional compatibility for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1775" to="1789" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tracking-learningdetection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probabilistic simulation predicts human performance on viscous fluid-pouring task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kubricht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting of the Cognitive Science Society</title>
		<meeting>the 38th Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1805" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Consistent probabilistic simulation underlying human judgment in substance dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kubricht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th</title>
		<meeting>the 39th</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m">Annual Meeting of the Cognitive Science Society</title>
		<imprint>
			<biblScope unit="page" from="700" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating human cognition of containing relations with physical simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Cognitive Science Society</title>
		<meeting>the 37th Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="782" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">What is where: Inferring containment relations from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3418" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">See the glass half full: Reasoning about liquid containers, their volume and content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schenck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint inference of groups, events and human roles in aerial videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4576" to="4584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning human utility from video demonstrations for deductive planning in robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shukla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="448" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<title level="m">Visual tracking: An experimental survey. T-PAMI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tracking revisited using rgbd camera: Unified benchmark and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unstructured human activity detection from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="842" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04451</idno>
		<title level="m">Visual concepts and compositional voting</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transferring objects: Joint inference of container and human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2933" to="2941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Action Recognition with Depth Cameras</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="11" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling 4d human-object interactions for event and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3272" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monocular 3d scene understanding with explicit occlusion reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Context-aware visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1195" to="1209" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Animated pose templates for modeling and detecting human actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="436" to="452" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fill and transfer: A simple physics-based approach for containability reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="711" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminative subvolume search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2442" to="2449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond point clouds: Scene understanding by reasoning geometry and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3127" to="3134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Inferring forces and learning human utilities from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3823" to="3833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding tools: Task-oriented object modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2855" to="2864" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
