<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Information Maximizing Exploration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-08-17">17 Aug 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Sciences</orgName>
								<orgName type="department" key="dep2">Department of Information Technology ‡ OpenAI</orgName>
								<orgName type="institution">Ghent University -iMinds</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Sciences</orgName>
								<orgName type="department" key="dep2">Department of Information Technology ‡ OpenAI</orgName>
								<orgName type="institution">Ghent University -iMinds</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Sciences</orgName>
								<orgName type="department" key="dep2">Department of Information Technology ‡ OpenAI</orgName>
								<orgName type="institution">Ghent University -iMinds</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Sciences</orgName>
								<orgName type="department" key="dep2">Department of Information Technology ‡ OpenAI</orgName>
								<orgName type="institution">Ghent University -iMinds</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>De Turck</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Sciences</orgName>
								<orgName type="department" key="dep2">Department of Information Technology ‡ OpenAI</orgName>
								<orgName type="institution">Ghent University -iMinds</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Sciences</orgName>
								<orgName type="department" key="dep2">Department of Information Technology ‡ OpenAI</orgName>
								<orgName type="institution">Ghent University -iMinds</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Sciences</orgName>
								<orgName type="department" key="dep2">Department of Information Technology ‡ OpenAI</orgName>
								<orgName type="institution">Ghent University -iMinds</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Information Maximizing Exploration</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-08-17">17 Aug 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1605.09674v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent&apos;s belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reinforcement learning (RL) studies how an agent can maximize its cumulative reward in a previously unknown environment, which it learns about through experience. A long-standing problem is how to manage the trade-off between exploration and exploitation. In exploration, the agent experiments with novel strategies that may improve returns in the long run; in exploitation, it maximizes rewards through behavior that is known to be successful. An effective exploration strategy allows the agent to generate trajectories that are maximally informative about the environment. For small tasks, this trade-off can be handled effectively through Bayesian RL <ref type="bibr">[1]</ref> and PAC-MDP methods <ref type="bibr">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, which offer formal guarantees. However, these guarantees assume discrete state and action spaces. Hence, in settings where state-action discretization is infeasible, many RL algorithms use heuristic exploration strategies. Examples include acting randomly using -greedy or Boltzmann exploration <ref type="bibr" target="#b6">[7]</ref>, and utilizing Gaussian noise on the controls in policy gradient methods <ref type="bibr" target="#b7">[8]</ref>. These heuristics often rely on random walk behavior which can be highly inefficient, for example Boltzmann exploration requires a training time exponential in the number of states in order to solve the well-known n-chain MDP <ref type="bibr" target="#b8">[9]</ref>. In between formal methods and simple heuristics, several works have proposed to address the exploration problem using less formal, but more expressive methods <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. However, none of them fully address exploration in continuous control, as discretization of the state-action space scales exponentially in its dimensionality. For example, the Walker2D task <ref type="bibr" target="#b14">[15]</ref> has a 26-dim state-action space. If we assume a coarse discretization into 10 bins for each dimension, a table of state-action visitation counts would require 10 26 entries. This paper proposes a curiosity-driven exploration strategy, making use of information gain about the agent's internal belief of the dynamics model as a driving force. This principle can be traced back to the concepts of curiosity and surprise <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. Within this framework, agents are encouraged to take actions that result in states they deem surprising-i.e., states that cause large updates to the dynamics model distribution. We propose a practical implementation of measuring information gain using variational inference. Herein, the agent's current understanding of the environment dynamics is represented by a Bayesian neural networks (BNN) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. We also show how this can be interpreted as measuring compression improvement, a proposed model of curiosity <ref type="bibr" target="#b20">[21]</ref>. In contrast to previous curiosity-based approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>, our model scales naturally to continuous state and action spaces. The presented approach is evaluated on a range of continuous control tasks, and multiple underlying RL algorithms. Experimental results show that VIME achieves significantly better performance than naïve exploration strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In Section 2.1, we establish notation for the subsequent equations. Next, in Section 2.2, we explain the theoretical foundation of curiosity-driven exploration. In Section 2.3 we describe how to adapt this idea to continuous control, and we show how to build on recent advances in variational inference for Bayesian neural networks (BNNs) to make this formulation practical. Thereafter, we make explicit the intuitive link between compression improvement and the variational lower bound in Section 2.4. Finally, Section 2.5 describes how our method is practically implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>This paper assumes a finite-horizon discounted Markov decision process (MDP), defined by (S, A, P, r, ρ 0 , γ, T ), in which S ⊆ R n is a state set, A ⊆ R m an action set, P : S × A × S → R ≥0 a transition probability distribution, r : S × A → R a bounded reward function, ρ 0 : S → R ≥0 an initial state distribution, γ ∈ (0, 1] a discount factor, and T the horizon. States and actions viewed as random variables are abbreviated as S and A. The presented models are based on the optimization of a stochastic policy π α : S × A → R ≥0 , parametrized by α. Let µ(π α ) denote its expected discounted return:</p><formula xml:id="formula_0">µ(π α ) = E τ [ T t=0 γ t r(s t , a t )]</formula><p>, where τ = (s 0 , a 0 , . . .) denotes the whole trajectory, s 0 ∼ ρ 0 (s 0 ), a t ∼ π α (a t |s t ), and s t+1 ∼ P(s t+1 |s t , a t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Curiosity</head><p>Our method builds on the theory of curiosity-driven exploration <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, in which the agent engages in systematic exploration by seeking out state-action regions that are relatively unexplored. The agent models the environment dynamics via a model p(s t+1 |s t , a t ; θ), parametrized by the random variable Θ with values θ ∈ Θ. Assuming a prior p(θ), it maintains a distribution over dynamic models through a distribution over θ, which is updated in a Bayesian manner (as opposed to a point estimate). The history of the agent up until time step t is denoted as ξ t = {s 1 , a 1 , . . . , s t }.</p><p>According to curiosity-driven exploration <ref type="bibr" target="#b16">[17]</ref>, the agent should take actions that maximize the reduction in uncertainty about the dynamics. This can be formalized as maximizing the sum of reductions in entropy</p><formula xml:id="formula_1">t (H(Θ|ξ t , a t ) − H(Θ|S t+1 , ξ t , a t )) ,<label>(1)</label></formula><p>through a sequence of actions {a t }. According to information theory, the individual terms equal the mutual information between the next state distribution S t+1 and the model parameter Θ, namely I (S t+1 ; Θ|ξ t , a t ). Therefore, the agent is encouraged to take actions that lead to states that are maximally informative about the dynamics model. Furthermore, we note that</p><formula xml:id="formula_2">I (S t+1 ; Θ|ξ t , a t ) = E st+1∼P(•|ξt,at) D KL [p(θ|ξ t , a t , s t+1 ) p(θ|ξ t )] ,<label>(2)</label></formula><p>the KL divergence from the agent's new belief over the dynamics model to the old one, taking expectation over all possible next states according to the true dynamics P. This KL divergence can be interpreted as information gain.</p><p>If calculating the posterior dynamics distribution is tractable, it is possible to optimize Eq. (2) directly by maintaining a belief over the dynamics model <ref type="bibr" target="#b16">[17]</ref>. However, this is not generally the case. Therefore, a common practice <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> is to use RL to approximate planning for maximal mutual information along a trajectory t I (S t+1 ; Θ|ξ t , a t ) by adding each term I (S t+1 ; Θ|ξ t , a t ) as an intrinsic reward, which captures the agent's surprise in the form of a reward function. This is practically realized by taking actions a t ∼ π α (s t ) and sampling s t+1 ∼ P(•|s t , a t ) in order to add D KL [p(θ|ξ t , a t , s t+1 ) p(θ|ξ t )] to the external reward. The trade-off between exploitation and exploration can now be realized explicitly as follows:</p><formula xml:id="formula_3">r (s t , a t , s t+1 ) = r(s t , a t ) + ηD KL [p(θ|ξ t , a t , s t+1 ) p(θ|ξ t )],<label>(3)</label></formula><p>with η ∈ R + a hyperparameter controlling the urge to explore. In conclusion, the biggest practical issue with maximizing information gain for exploration is that the computation of Eq. <ref type="formula" target="#formula_3">3</ref>requires calculating the posterior p(θ|ξ t , a t , s t+1 ), which is generally intractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Variational Bayes</head><p>We propose a tractable solution to maximize the information gain objective presented in the previous section. In a purely Bayesian setting, we can derive the posterior distribution given a new state-action pair through Bayes' rule as</p><formula xml:id="formula_4">p(θ|ξ t , a t , s t+1 ) = p(θ|ξ t )p(s t+1 |ξ t , a t ; θ) p(s t+1 |ξ t , a t ) ,<label>(4)</label></formula><p>with p(θ|ξ t , a t ) = p(θ|ξ t ) as actions do not influence beliefs about the environment <ref type="bibr" target="#b16">[17]</ref>. Herein, the denominator is computed through the integral</p><formula xml:id="formula_5">p(s t+1 |ξ t , a t ) = Θ p(s t+1 |ξ t , a t ; θ)p(θ|ξ t )dθ.<label>(5)</label></formula><p>In general, this integral tends to be intractable when using highly expressive parametrized models (e.g., neural networks), which are often needed to accurately capture the environment model in high-dimensional continuous control.</p><p>We propose a practical solution through variational inference <ref type="bibr" target="#b23">[24]</ref>. Herein, we embrace the fact that calculating the posterior p(θ|D) for a data set D is intractable. Instead we approximate it through an alternative distribution q(θ; φ), parameterized by φ, by minimizing</p><formula xml:id="formula_6">D KL [q(θ; φ) p(θ|D)]</formula><p>. This is done through maximization of the variational lower bound L[q(θ; φ), D]:</p><formula xml:id="formula_7">L[q(θ; φ), D] = E θ∼q(•;φ) [log p(D|θ)] − D KL [q(θ; φ) p(θ)].<label>(6)</label></formula><p>Rather than computing information gain in Eq. (3) explicitly, we compute an approximation to it, leading to the following total reward:</p><formula xml:id="formula_8">r (s t , a t , s t+1 ) = r(s t , a t ) + ηD KL [q(θ; φ t+1 ) q(θ; φ t )],<label>(7)</label></formula><p>with φ t+1 the updated and φ t the old parameters representing the agent's belief. Natural candidates for parametrizing the agent's dynamics model are Bayesian neural networks (BNNs) <ref type="bibr" target="#b18">[19]</ref>, as they maintain a distribution over their weights. This allows us to view the BNN as an infinite neural network ensemble by integrating out its parameters:</p><formula xml:id="formula_9">p(y|x) = Θ p(y|x; θ)q(θ; φ)dθ.<label>(8)</label></formula><p>In particular, we utilize a BNN parametrized by a fully factorized Gaussian distribution <ref type="bibr" target="#b19">[20]</ref>. Practical BNN implementation details are deferred to Section 2.5, while we give some intuition into the behavior of BNNs in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Compression</head><p>It is possible to derive an interesting relationship between compression improvement-an intrinsic reward objective defined in <ref type="bibr" target="#b24">[25]</ref>, and the information gain of Eq. (2). In <ref type="bibr" target="#b24">[25]</ref>, the agent's curiosity is equated with compression improvement, measured through</p><formula xml:id="formula_10">C(ξ t ; φ t−1 ) − C(ξ t ; φ t ), where C(ξ; φ)</formula><p>is the description length of ξ using φ as a model. Furthermore, it is known that the negative variational lower bound can be viewed as the description length <ref type="bibr" target="#b18">[19]</ref>. Hence, we can write compression</p><formula xml:id="formula_11">improvement as L[q(θ; φ t ), ξ t ] − L[q(θ; φ t−1 ), ξ t ].</formula><p>In addition, an alternative formulation of the variational lower bound in Eq. (6) is given by</p><formula xml:id="formula_12">log p(D) = L[q(θ;φ),D] Θ q(θ; φ) log p(θ, D) q(θ; φ) dθ +D KL [q(θ; φ) p(θ|D)].<label>(9)</label></formula><p>Thus, compression improvement can now be written as</p><formula xml:id="formula_13">(log p(ξ t ) − D KL [q(θ; φ t ) p(θ|ξ t )]) − (log p(ξ t ) − D KL [q(θ; φ t−1 ) p(θ|ξ t )]) .<label>(10)</label></formula><p>If we assume that φ t perfectly optimizes the variational lower bound for the history ξ t , then D KL [q(θ; φ t ) p(θ|ξ t )] = 0, which occurs when the approximation equals the true posterior, i.e., q(θ; φ t ) = p(θ|ξ t ). Hence, compression improvement becomes</p><formula xml:id="formula_14">D KL [p(θ|ξ t−1 ) p(θ|ξ t )].<label>(11)</label></formula><p>Therefore, optimizing for compression improvement comes down to optimizing the KL divergence from the posterior given the past history ξ t−1 to the posterior given the total history ξ t . As such, we arrive at an alternative way to encode curiosity than information gain, namely</p><formula xml:id="formula_15">D KL [p(θ|ξ t ) p(θ|ξ t , a t , s t+1 )]</formula><p>, its reversed KL divergence. In experiments, we noticed no significant difference between the two KL divergence variants. This can be explained as both variants are locally equal when introducing small changes to the parameter distributions. Investigation of how to combine both information gain and compression improvement is deferred to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Implementation</head><p>The complete method is summarized in Algorithm 1. We first set forth implementation and parametrization details of the dynamics BNN. The BNN weight distribution q(θ; φ) is given by the fully factorized Gaussian distribution <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_16">q(θ; φ) = |Θ| i=1 N (θ i |µ i ; σ 2 i ).<label>(12)</label></formula><p>Hence, φ = {µ, σ}, with µ the Gaussian's mean vector and σ the covariance matrix diagonal. This is particularly convenient as it allows for a simple analytical formulation of the KL divergence. This is described later in this section. Because of the restriction σ &gt; 0, the standard deviation of the Gaussian BNN parameter is parametrized as σ = log(1 + e ρ ), with ρ ∈ R <ref type="bibr" target="#b19">[20]</ref>.</p><p>Now the training of the dynamics BNN through optimization of the variational lower bound is described. The second term in Eq. (6) is approximated through sampling <ref type="bibr" target="#b19">[20]</ref>. Optimizing the variational lower bound in Eq. (6) in combination with the reparametrization trick is called stochastic gradient variational Bayes (SGVB) <ref type="bibr" target="#b25">[26]</ref> or Bayes by Backprop <ref type="bibr" target="#b19">[20]</ref>. Furthermore, we make use of the local reparametrization trick proposed in <ref type="bibr" target="#b25">[26]</ref>, in which sampling at the weights is replaced by sampling the neuron pre-activations, which is more computationally efficient and reduces gradient variance. The optimization of the variational lower bound is done at regular intervals during the RL training process, by sampling D from a FIFO replay pool that stores recent samples (s t , a t , s t+1 ). This is to break up the strong intratrajectory sample correlation which destabilizes learning in favor of obtaining i.i.d. data <ref type="bibr" target="#b6">[7]</ref>. Moreover, it diminishes the effect of compounding posterior approximation errors.</p><formula xml:id="formula_17">E θ∼q(•;φ) [log p(D|θ)] ≈ 1 N N i=1 log p(D|θ i ) with N samples drawn according to θ ∼ q(•; φ)</formula><p>The posterior distribution of the dynamics parameter, which is needed to compute the KL divergence in the total reward function r of Eq. (7), can be computed through the following minimization</p><formula xml:id="formula_18">φ = arg min φ (q(θ;φ),st) D KL [q(θ; φ) q(θ; φ t−1 )] KL (q(θ;φ)) −E θ∼q(•;φ) [log p(s t |ξ t , a t ; θ)] ,<label>(13)</label></formula><p>where we replace the expectation over θ with samples θ ∼ q(•; φ). Because we only update the model periodically based on samples drawn from the replay pool, this optimization can be performed in parallel for each s t , keeping φ t−1 fixed. Once φ has been obtained, we can use it to compute the intrinsic reward. </p><formula xml:id="formula_19">∆φ = H −1 ( )∇ φ (q(θ; φ), s t ),<label>(14)</label></formula><p>in which H( ) is the Hessian of (q(θ; φ), s t ).</p><p>Since we assume that the variational approximation is a fully factorized Gaussian, the KL divergence from posterior to prior has a particularly simple form:</p><formula xml:id="formula_20">D KL [q(θ; φ) q(θ; φ )] = 1 |Θ| i=1 σi σ i 2 + 2 log σ i − 2 log σ i + (µ i −µi) 2 σ 2 i − |Θ| .<label>(15)</label></formula><p>Because this KL divergence is approximately quadratic in its parameters and the log-likelihood term can be seen as locally linear compared to this highly curved KL term, we approximate H by only calculating it for the term KL term KL (q(θ; φ)). This can be computed very efficiently in case of a fully factorized Gaussian distribution, as this approximation becomes a diagonal matrix. Looking at Eq. (15), we can calculate the following Hessian at the origin. The µ and ρ 1 entries are defined as</p><formula xml:id="formula_21">∂ 2 KL ∂µ 2 i = 1 log 2 (1 + e ρi )</formula><p>and</p><formula xml:id="formula_22">∂ 2 KL ∂ρ 2 i = 2e 2ρi (1 + e ρi ) 2 1 log 2 (1 + e ρi ) ,<label>(16)</label></formula><p>while all other entries are zero. Furthermore, it is also possible to approximate the KL divergence through a second-order Taylor expansion as 1 ∆φH∆φ = 1 H −1 ∇ H H −1 ∇ , since both the value and gradient of the KL divergence are zero at the origin. This gives us</p><formula xml:id="formula_23">D KL [q(θ; φ + λ∆φ) q(θ; φ)] ≈ 1 2 λ 2 ∇ φ H −1 ( KL )∇ φ .<label>(17)</label></formula><p>Note that H −1 ( KL ) is diagonal, so this expression can be computed efficiently.</p><p>Instead of using the KL divergence D KL [q(θ; φ t+1 ) q(θ; φ t )] directly as an intrinsic reward in Eq. <ref type="formula" target="#formula_8">7</ref>, we normalize it by division through the average of the median 2 KL divergences taken over a fixed number of previous trajectories. Rather than focusing on its absolute value, we emphasize relative difference in KL divergence between samples. This accomplishes the same effect since the variance of KL divergence converges to zero, once the model is fully learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we investigate (i) whether VIME can succeed in domains that have extremely sparse rewards, (ii) whether VIME improves learning when the reward is shaped to guide the agent towards its goal, and (iii) how η, as used in in Eq. (3), trades off exploration and exploitation behavior.</p><p>All experiments make use of the rllab <ref type="bibr" target="#b14">[15]</ref> benchmark code base and the complementary continuous control tasks suite. The following tasks are part of the experimental setup:</p><formula xml:id="formula_24">CartPole (S ⊆ R 4 , A ⊆ R 1 ), CartPoleSwingup (S ⊆ R 4 , A ⊆ R 1 ), DoublePendulum (S ⊆ R 6 , A ⊆ R 1 ), MountainCar (S ⊆ R 3 , A ⊆ R 1 ), locomotion tasks HalfCheetah (S ⊆ R 20 , A ⊆ R 6 ), Walker2D (S ⊆ R 20 , A ⊆ R 6</formula><p>), and the hierarchical task SwimmerGather (S ⊆ R 33 , A ⊆ R 2 ). Performance is measured through the average return (not including the intrinsic rewards) over the trajectories generated (y-axis) at each iteration (x-axis). More specifically, the darker-colored lines in each plot represent the median performance over a fixed set of 10 random seeds while the shaded areas show the interquartile range at each iteration. Moreover, the number in each legend shows this performance measure, averaged over all iterations. The exact experimental setup is described in the Appendix (Supplementary Material). Domains with sparse rewards are difficult to solve through naïve exploration behavior because, before the agent obtains any reward, it lacks a feedback signal on how to improve its policy. This allows us to test whether an exploration strategy is truly capable of systematic exploration, rather than improving existing RL algorithms by adding more hyperparameters. Therefore, VIME is compared with heuristic exploration strategies on the following tasks with sparse rewards. A reward of +1 is given when the car escapes the valley on the right side in MountainCar; when the pole is pointed upwards in CartPoleSwingup; and when the cheetah moves forward over five units in HalfCheetah. We compare VIME with the following baselines: only using Gaussian control noise <ref type="bibr" target="#b14">[15]</ref> and using the 2 BNN prediction error as an intrinsic reward, a continuous extension of <ref type="bibr" target="#b9">[10]</ref>. TRPO <ref type="bibr" target="#b7">[8]</ref> is used as the RL algorithm, as it performs very well compared to other methods <ref type="bibr" target="#b14">[15]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows the performance results. We notice that using a naïve exploration performs very poorly, as it is almost never able to reach the goal in any of the tasks. Similarly, using 2 errors does not perform well. In contrast, VIME performs much better, achieving the goal in most cases. This experiment demonstrates that curiosity drives the agent to explore, even in the absence of any initial reward, where naïve exploration completely breaks down.</p><p>To further strengthen this point, we have evaluated VIME on the highly difficult hierarchical task SwimmerGather in <ref type="figure">Figure 5</ref> whose reward signal is naturally sparse. In this task, a two-link robot needs to reach "apples" while avoiding "bombs" that are perceived through a laser scanner. However, before it can make any forward progress, it has to learn complex locomotion primitives in the absence of any reward. None of the RL methods tested previously in <ref type="bibr" target="#b14">[15]</ref> were able to make progress with naïve exploration. Remarkably, VIME leads the agent to acquire coherent motion primitives without any reward guidance, achieving promising results on this challenging task.</p><p>Next, we investigate whether VIME is widely applicable by (i) testing it on environments where the reward is well shaped, and (ii) pairing it with different RL methods. In addition to TRPO, we choose to equip REINFORCE <ref type="bibr" target="#b26">[27]</ref> and ERWR <ref type="bibr" target="#b27">[28]</ref> with VIME because these two algorithms usually suffer from premature convergence to suboptimal policies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>, which can potentially be alleviated by better exploration. Their performance is shown in <ref type="figure" target="#fig_2">Figure 2</ref> on several well-established continuous control tasks. Furthermore, <ref type="figure">Figure 3</ref> shows the same comparison for the Walker2D locomotion task.</p><p>In the majority of cases, VIME leads to a significant performance gain over heuristic exploration.</p><p>Our exploration method allows the RL algorithms to converge faster, and notably helps REINFORCE and ERWR avoid converging to a locally optimal solution on DoublePendulum and MountainCar. We note that in environments such as CartPole, a better exploration strategy is redundant as following the policy gradient direction leads to the globally optimal solution. Additionally, we tested adding Gaussian noise to the rewards as a baseline, which did not improve performance. To give an intuitive understanding of VIME's exploration behavior, the distribution of visited states for both naïve exploration and VIME after convergence is investigated. <ref type="figure" target="#fig_0">Figure 1d</ref> shows that using Gaussian control noise exhibits random walk behavior: the state visitation plot is more condensed and ball-shaped around the center. In comparison, VIME leads to a more diffused visitation pattern, exploring the states more efficiently, and hence reaching the goal more quickly.</p><p>Finally, we investigate how η, as used in in Eq. <ref type="formula" target="#formula_3">3</ref>, trades off exploration and exploitation behavior. On the one hand, higher η values should lead to a higher curiosity drive, causing more exploration. On the other hand, very low η values should reduce VIME to traditional Gaussian control noise. <ref type="figure">Figure 4</ref> shows the performance on MountainCar for different η values. Setting η too high clearly results in prioritizing exploration over getting additional external reward. Too low of an η value reduces the method to the baseline algorithm, as the intrinsic reward contribution to the total reward r becomes negligible. Most importantly, this figure highlights that there is a wide η range for which the task is best solved, across different algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>A body of theoretically oriented work demonstrates exploration strategies that are able to learn online in a previously unknown MDP and incur a polynomial amount of regret-as a result, these algorithms find a near-optimal policy in a polynomial amount of time. Some of these algorithms are based on the principle of optimism under uncertainty: E 3 <ref type="bibr">[3]</ref>, R-Max <ref type="bibr" target="#b3">[4]</ref>, UCRL <ref type="bibr" target="#b29">[30]</ref>. An alternative approach is Bayesian reinforcement learning methods, which maintain a distribution over possible MDPs <ref type="bibr">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>. The optimism-based exploration strategies have been extended to continuous state spaces, for example, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, however these methods do not accommodate nonlinear function approximators (in contrast to VIME).</p><p>Practical RL algorithms often rely on simple exploration heuristics, such as -greedy and Boltzmann exploration <ref type="bibr" target="#b31">[32]</ref>. However, these heuristics exhibit random walk exploratory behavior, which can lead to exponential regret even in case of small MDPs <ref type="bibr" target="#b8">[9]</ref>.</p><p>Our proposed method of utilizing information gain can be traced back to <ref type="bibr" target="#b21">[22]</ref>, and has been further explored in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. Other metrics for curiosity have also been proposed, including prediction error <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref>, prediction error improvement <ref type="bibr" target="#b35">[36]</ref>, and leverage <ref type="bibr" target="#b13">[14]</ref>. All these methods have only been tested on small problems, and are not directly applicable to high dimensional continuous control tasks. We refer the reader to <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref> for an extensive review on curiosity and intrinsic rewards.</p><p>Recently, there have been various exploration strategies proposed in the context of deep RL. <ref type="bibr" target="#b9">[10]</ref> proposes to use the 2 prediction error as the intrinsic reward. <ref type="bibr" target="#b11">[12]</ref> performs approximate visitation counting in a learned state embedding using Gaussian kernels. <ref type="bibr" target="#b10">[11]</ref> proposes a form of Thompson sampling, training multiple value functions using bootstrapping. Although these approaches can scale up to high-dimensional state spaces, they generally assume discrete action spaces. Finally, <ref type="bibr" target="#b37">[38]</ref> proposes a variational method for information maximization in the context of optimizing empowerment, which, as noted by <ref type="bibr" target="#b38">[39]</ref>, does not explicitly favor exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have proposed Variational Information Maximizing Exploration (VIME), a curiosity-driven exploration strategy for continuous control tasks. Variational inference is used to approximate the posterior distribution of a Bayesian neural network that represents the environment dynamics. Using information gain in this learned dynamics model as intrinsic rewards allows the agent to optimize for both external reward and intrinsic surprise simultaneously. Empirical results show that VIME performs significantly better than heuristic exploration methods across various continuous control tasks and algorithms. As future work, we would like to investigate measuring surprise in the value function and using the learned dynamics model for planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Bayesian neural networks (BNNs)</head><p>We demonstrate the behavior of a BNN <ref type="bibr">[1]</ref> when trained on simple regression data. <ref type="figure" target="#fig_0">Figure 1</ref> shows a snapshot of the behavior of the BNN during training. In this figure, the red dots represent the regression training data, which has a 1-dim input x and a 1-dim output. The input to the BNN is constructed as</p><formula xml:id="formula_25">x = [x, x 2 , x 3 , x 4 ].</formula><p>The green dots represent BNN predictions, each for a differently sampled θ value, according to q(•; φ). The color lines represent the output for different, but fixed, θ samples. The shaded areas represent the sampled output mean plus-minus one and two standard deviations.  The figure shows that the BNN output is very certain in the training data range, while having high uncertainty otherwise. If we introduce data outside of this training range, or data that is significantly different from the training data, it will have a high impact on the parameter distribution q(θ; φ). This is tested in <ref type="figure" target="#fig_2">Figure 2</ref>: previously unseen data is introduced right before training iteration 10,000. The KL divergence from posterior to prior (y-axis) is set out in function of the training iteration number (x-axis). We see a sharp spike in the KL divergence curve, which represents the BNN's surprise about this novel data. This spike diminishes over time as the BNN learns to fit this new data, becoming less surprised about it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental setup</head><p>In case of the classic tasks CartPole, CartPoleSwingup, DoublePendulum, and MountainCar, as well as in the case of the hierarchical task SwimmerGather, the dynamics BNN has one hidden layer of 32 units. For the locomotion tasks Walker2D and HalfCheetah, the dynamics BNN has two hidden layers of 64 units each. All hidden layers have rectified linear unit (ReLU) nonlinearities, while no nonlinearity is applied to the output layer. The number of samples drawn to approximate the variational lower bound expectation term is fixed to 10. The batch size for the policy gradient methods is set to 5,000 samples, except for the SwimmerGather task, where it is set to 50,000. The replay pool has a fixed size of 100,000 samples, with a minimum size of 500 samples for all but the SwimmerGather task. In this latter case, the replay pool has a size of 1,000,000 samples. The dynamics BNN is updated each epoch, using 500 iterations of Adam <ref type="bibr">[2]</ref>, with a batch size of 10, except for the SwimmerGather task, in which 5,000 iterations are used. The Adam learning rate is set to 0.0001 while the batches are drawn randomly with replacement from the replay pool. In the second-order KL divergence update step, λ is set to 0.01. The BNN prior weight distribution is a fully factorized Gaussian with µ sampled from a different Gaussian distribution N (0, I), while ρ is fixed to log(1 + e 0.5 ).</p><p>The classic tasks make use of a neural network policy with one layer of 32 tanh units, while the locomotion tasks make use of a two-layer neural network of 64 and 32 tanh units. The outputs are modeled by a fully factorized Gaussian distribution N (µ, σ 2 I), in which µ is modeled as the network output, while σ is a parameter. The classic tasks make use of a neural network baseline with one layer of 32 ReLU units, while the locomotion tasks make use linear baseline function.</p><p>All tasks are implemented as described in <ref type="bibr">[3]</ref>. The tasks have the following state and action dimensions: CartPole, S ⊆ R 4 , A ⊆ R 1 ; CartPoleSwingup, S ⊆ R 4 , A ⊆ R 1 ; DoublePendulum, S ⊆ R 6 , A ⊆ R 1 ; MountainCar S ⊆ R 3 , A ⊆ R 1 ; locomotion tasks HalfCheetah, S ⊆ R 20 , A ⊆ R 6 ; and Walker2D, S ⊆ R 20 , A ⊆ R 6 ; and hierarchical task SwimmerGather, S ⊆ R 33 , A ⊆ R 2 . The time horizon is set to T = 500 for all tasks. For the sparse reward experiments, the tasks have been modified as follows. In MountainCar, the agent receives a reward of +1 when the goal state is reached, namely escaping the valley from the right side. In CartPoleSwingup, the agent receives a reward of +1 when cos(β) &gt; 0.8, with β the pole angle. Therefore, the agent has to figure out how to swing up the pole in the absence of any initial external rewards. In HalfCheetah, the agent receives a reward of +1 when x body &gt; 5. As such, it has to figure out how to move forward without any initial external reward.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a,b,c) TRPO+VIME versus TRPO on tasks with sparse rewards; (d) comparison of TRPO+VIME (red) and TRPO (blue) on MountainCar: visited states until convergence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :Figure 4 :Figure 5 :</head><label>2345</label><figDesc>Performance of TRPO (top row), ERWR (middle row), and REINFORCE (bottom row) with (+VIME) and without exploration for different continuous control tasks. Performance of TRPO with and without VIME on the high-dimensional Walker2D locomotion task. VIME: performance over the first few iterations for TRPO, REINFORCE, and ERWR i.f.o. η on MountainCar. Performance of TRPO with and without VIME on the challenging hierarchical task SwimmerGather.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Just before iteration 10,000 we introduce data outside the training data range to the BNN. This results in a KL divergence spike, showing the model's surprise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Variational Information Maximizing Exploration (VIME)1 for each epoch n do 2 for each timestep t in each trajectory generated during n do Generate action a t ∼ π α (s t ) and sample state st+1 ∼ P(•|ξ t , a t ), get r(s t , a t ). intrinsic reward D KL [q(θ; φ n+1 ) q(θ; φ n+1 )] through approximation ∇ H −1 ∇,following Eq.(17), or by optimizing Eq.(13)to obtain φ n+1 .Divide D KL [q(θ; φ n+1 ) q(θ; φ n+1 )] by median of previous KL divergences.</figDesc><table><row><cell>Compute</cell></row></table><note>37 Construct r (s t , a t , s t+1 ) ← r(s t , a t ) + ηD KL [q(θ; φ n+1 ) q(θ; φ n+1 )], following Eq. (7).8 Minimize D KL [q(θ; φ n ) p(θ)] − E θ∼q(•;φn) [log p(D|θ)] following Eq. (6), with D sampled randomly from R, leading to updated posterior q(θ; φ n+1 ). Use rewards {r (s t , a t , s t+1 )} to update policy π α using any standard RL method. To optimize Eq. (13) efficiently, we only take a single second-order step. This way, the gradient is rescaled according to the curvature of the KL divergence at the origin. As such, we compute D KL [q(θ; φ + λ∆φ) q(θ; φ)], with the update step ∆φ defined as</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Recall that the standard deviation is parametrized as σ = log(1 + e ρ ).2  The median is used as it is more robust to KL divergence outliers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by DARPA, the Berkeley Vision and Learning Center (BVLC), the Berkeley Artificial Intelligence Research (BAIR) laboratory, and Berkeley Deep Drive (BDD). Rein Houthooft is supported by a Ph.D. Fellowship of the Research Foundation -Flanders (FWO).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian reinforcement learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="359" to="483" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploration in metric state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="306" to="312" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Near-optimal reinforcement learning in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">R-Max -a general polynomial time algorithm for near-optimal reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using confidence bounds for exploitation-exploration trade-offs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PAC optimal exploration in continuous space Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generalization and exploration via randomized value functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.0635</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Incentivizing exploration in reinforcement learning with deep predictive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00814</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep exploration via bootstrapped DQN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Action-conditional video prediction using deep networks in Atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2845" to="2853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intrinsically motivated model learning for developing curious robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploration from demonstration for interactive reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Isbell</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAMAS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Benchmarking deep reinforcement learning for continous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Curious model-building control systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="1458" to="1463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Planning to be surprised: Optimal Bayesian exploration in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial General Intelligence</title>
		<imprint>
			<biblScope unit="page" from="41" to="51" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bayesian surprise attracts human attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="547" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NIPS</title>
		<imprint>
			<biblScope unit="page" from="2348" to="2356" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Formal theory of creativity, fun, and intrinsic motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Auton. Mental Develop</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="247" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reinforcement driven information acquisition in nondeterministic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Storck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICANN</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="159" to="164" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Near-Bayesian exploration in polynomial time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Camp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple algorithmic principles of discovery, subjective beauty, selective attention, curiosity &amp; creativity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Discovery Science</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="26" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Policy search for motor primitives in robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reinforcement learning by reward-weighted regression for operational space control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="745" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Near-optimal regret bounds for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaksch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ortner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayes-adaptive simulation-based search with value function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Introduction to reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An information-theoretic approach to curiosity-driven reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Still</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Biosci</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="139" to="148" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning and exploration in action-perception loops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Closing the Loop Around Neural Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">295</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient exploration in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Thrun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploration in model-based reinforcement learning by empirically estimating learning progress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="206" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">What is intrinsic motivation? a typology of computational approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Neurorobot</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Variational information maximisation for intrinsically motivated reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2116" to="2124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Guided self-organization: Inception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Salge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Glackin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Polani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ch. Empowerment-An Introduction</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="67" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Benchmarking deep reinforcement learning for continous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
