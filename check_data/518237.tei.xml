<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Classification for Analysing Social Media</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Thang</forename><surname>Duong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Lebret</surname></persName>
							<email>remi.lebret@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Abereŕ</surname></persName>
							<email>karl.aberer@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique Fédérale de Lausanne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Classification for Analysing Social Media</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Classification of social media data is an important approach in understanding user behavior on the Web. Although information on social media can be of different modalities such as texts, images, audio or videos, traditional approaches in classification usually leverage only one prominent modality. Techniques that are able to leverage multiple modalities are often complex and susceptible to the absence of some modalities. In this paper, we present simple models that combine information from different modalities to classify social media content and are able to handle the above problems with existing techniques. Our models combine information from different modalities using a pooling layer and an auxiliary learning task is used to learn a common feature space. We demonstrate the performance of our models and their robustness to the missing of some modalities in the emotion classification domain. Our approaches, although being simple, can not only achieve significantly higher accuracies than traditional fusion approaches but also have comparable results when only one modality is available.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the advance of social media networks, people are sharing user-generated contents in an unprecedented scale <ref type="bibr" target="#b17">[18]</ref>. Social media networks have shifted from being specialized as users can only share either texts or images to general-purpose as users can now share texts, images, audio segments or even video clips. As recent statistics show that posts with images get more interaction <ref type="bibr" target="#b0">1</ref> while video tweets fuels discovery and drives engagement 2 , we expect an enormous increase of multimodal posts. The contents posted by users on social networks are a way for them to express themselves such as their emotions, feelings or to share life events. As a result, these posts provide precious information about the users that if these information is analyzed systematically, we would have a strong understanding about the users. For instance, knowing the emotions of the users by analyzing these contents can bring enormous benefits. An ad campaign can customize its ad based on the emotion expressed in a recent post by a user, which would reduce marketing cost but increase user engagement. A further step is to induce an emotion in a user proactively using a specific image or piece of text. A use case would be using happy images or joyful texts to encourage users to buy a product. There are many approaches to social media analysis and classification is usually the tool of choice when we want to study an aspect of the user base in depth such as their sentiment or emotion. Although classifying social media content for user understanding is highly important, traditional works tend to focus on one modality <ref type="bibr" target="#b4">[5]</ref>. For instance, works in emotion classification leverage only the visual information, textual information, which is also important, is often left out. Although images can express users' emotions more vividly, textual information can also convey various emotional aspects. <ref type="table" target="#tab_0">Table 1</ref> shows some posts with the availability of both image and text. In the first example, both the text and the image express the same emotion joy, which is also the emotion expressed by the post. On the other hand, in the second example, using only the text, it is easy for us to tell it conveys the emotion joy but the visual signal is the dominant one in this case and it expresses the emotion fear. In the last example, without considering the text, the image may describe the emotion contentment due to the presence of a pet. However, the text provides the context which changes the meaning and the emotion expressed in the image completely. These examples have shown that when information is available in different modalities, only by leveraging all of them can give us a complete picture.</p><p>Given the drawbacks of unimodal approaches, several works have been done on combining multimodal information for social media analysis. These works are able to improve the classification accuracy significantly <ref type="bibr" target="#b27">[28]</ref>. However, most of these approaches are usually complicated and tailored to specific problems and modalities, which makes them hard to apply to new problems or add other modalities <ref type="bibr" target="#b4">[5]</ref>. In addition, these classification techniques also assume the availability of all the required modalities, which is not the case in practice. For instance, a tweet classification technique that combines visual and textual information to classify may not work if the tweets contain only texts. Inspired by these observations, we propose techniques to combine information from multiple modalities based on neural network models. Our models combine information from different modalities using a pooling layer and an auxiliary learning task is used to learn a common feature space for all modalities. Our techniques are able to achieve high accuracy without the use of complex model while allowing to classify even when some modalities are missing. In addition, our techniques are easily scalable to new problems or more modalities.</p><p>We demonstrate the performance of our approaches in the emotion classification domain. Emotion classification is a multiclass classification problem with many applications in practice. We focus on two prominent modalities which are textual and visual resources. To the best of our knowledge, we are the first to propose techniques that combine information from different modalities in the emotion classification domain. In this paper, we make the following contributions.</p><p>-We propose novel generic techniques that can combine information from multiple modalities to classify social media content. In particular, we leverage advances in neural networks to build our fusion approaches that are scalable to new modalities and new problems. -Our proposed techniques are also robust to different types of input as they are able to handle the missing of some modalities. In the emotion classification demonstration, they can tackle three cases: only image or text or both are available. -We construct a dataset which contains data from textual and visual modalities to test our techniques. The dataset also contains strong labels for training and testing. We also enrich an image-only dataset with textual data. These datasets will be made available to foster research in the emotion analysis field.</p><p>The rest of the paper is organized as follows. Section 2 introduces related works on the field of multimodal classification and emotion analysis. Section 3 discusses a general fusion model and traditional fusion approaches. Section 4 explains in detail our proposed techniques. Experimental evaluation and analysis are presented in Section 5 while Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multimodal classification for social media. Multimodal classification techniques can be classified into two main classes depending on how the information from multiple modalities are combined. In late fusion, separate classification results obtained from different modalities are constructed first and the fusion is done based on these results at the decision level <ref type="bibr" target="#b4">[5]</ref>. Late fusion implies an assumption that the underlying data from different modalities are independent. This assumption is not practical as information from different modalities still describe the same event/object, which means they could be correlated <ref type="bibr" target="#b21">[22]</ref>. In a recent work <ref type="bibr" target="#b26">[27]</ref>, a variant of late fusion is discussed where the authors used KL divergence to enforce the results from different modalities to be similar. Early fusion takes a different approach as the fusion is done at the feature level where information from different modalities are appended. Classification is then done on the appended representation <ref type="bibr" target="#b27">[28]</ref>. Various variants of early fusion are proposed to classify social media content. In sentiment analysis, instead of concatenating information from different modalities, the work in <ref type="bibr" target="#b24">[25]</ref> use LSTM to combine visual and textual information. One approach <ref type="bibr" target="#b27">[28]</ref> in social event classification constructed a hierarchical classifier on the concatenated features to classify non-event and other event types. Several intermediate fusion techniques are also available such as using LDA to extract joint latent topics <ref type="bibr" target="#b0">[1]</ref> or by using statistical methods such as CCA <ref type="bibr" target="#b10">[11]</ref> for image classification. Existing techniques for multimodal classification are usually complex and they often require the presence of all modalities. Although our proposed approaches can be considered as a variant of early fusion, they differ from previous techniques as they can handle the absence of some modalities while being simpler. Our model seems to be similar to a Siamese network <ref type="bibr" target="#b11">[12]</ref> which contains two identical subnetworks with the same parameters and weights. However, our approach differs from a Siamese network as the image and text subnetworks architecture are different, hence, their parameters are completely different.</p><p>Emotion analysis. Techniques for analyzing emotions can be classified into handcrafted features and deep features. Early works in emotion analysis leverage features based on art and psychology theory. These artistic features are usually low-level such as shape <ref type="bibr" target="#b12">[13]</ref>, color and texture <ref type="bibr" target="#b14">[15]</ref>. Several higher-level features which are the combination of these low-level features are also used <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17]</ref>. In a recent work <ref type="bibr" target="#b28">[29]</ref>, the authors proposed several features based on principles of arts in which combinations of these features can evoke different emotions. After these features are defined, a classifier such as SVM is constructed on top of these features to classify the emotions. The inherent drawback of techniques that uses hand-crafted features is the required availability of these features. However, designing these features is a tedious process which requires expert knowledge and it is not guaranteed that all features that can capture the emotions are covered. Given this drawback, techniques that do not require a feature design process are proposed. These features are constructed automatically using CNNs. Only a recent work <ref type="bibr" target="#b2">[3]</ref> has leveraged deep features to emotion analysis and already achieved better accuracy in comparison with hand-crafted features. However, the technique in <ref type="bibr" target="#b2">[3]</ref> only deals with images, which leaves out the textual information completely. As we show in Table1, considering only textual or visual information is not enough to analyze emotions. This work is also the closest to our work as we also use build image representations using Convolutional Neural Networks(CNNs). However, we differ from this work significantly as we also take into account textual information and the combination of them. To the best of our knowledge, our technique is the first to combine visual and textual information to classify emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multimodal Classification for Analysing Social Media</head><p>In this section, we denote the modalities in social media that we use in our models, and we present the traditional approaches for combining them in classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multimodality in Social Media</head><p>Multimodality describes communication practices in terms of the textual, aural, linguistic, spatial, and visual resources -or modes -used to compose messages (or posts) <ref type="bibr" target="#b13">[14]</ref>. In this paper, we focus on combining the two prominent modalities of social media, i.e. textual and visual resources. It is worth mentioning that the proposed models is easily scalable to more modalities.</p><p>We define x ∈ X a post that can be composed of an image or a text, or both an image and a text. When both image and text are present, we assume that they are semantically related, e.g. the text describes the image. Each image i is represented with an image feature vector γ(i) ∈ R n . Thanks to the recent progress in computer vision using deep learning, γ(i) can be extracted from CNNs trained on millions of images. A text can be as long as a paragraph or as short as a phrase. We represent each piece of text s with a textual feature vector ψ(s) ∈ R m . Such textual feature vectors can be obtained with classical bag-of-words models or with approaches based on word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal Classification using</head><p>Feature Level Fusion Given a post x ∈ X and a set of classes Y , we define a model that assigns probabilities to all y ∈ Y . The predicted class is then the one with the highest probability, y = arg max</p><formula xml:id="formula_0">y P(Y = y|x).<label>(1)</label></formula><p>When x = i or x = s, unimodal classifiers are trained on the entire training set in each modality. As we are interested in combining image and text (i.e. when x = {i, s}), we discuss in this part two traditional fusion approaches: late fusion and early fusion. The difference in these approaches is when the fusion is done in the classification process. In early fusion, the fusion happens before the classifier is constructed (hence the name early) while in late fusion, the fusion is done after the classifiers are created.</p><p>Late fusion Late fusion requires the construction of two independent classifiers: one for image and one for text. The predicted class is then the highest product between the two classifiers,ŷ = arg max y P(Y = y|i)P(Y = y|s).</p><p>Early fusion Early fusion, on the other hand, does not require the construction of two separate classifiers as the fusion applies in the feature space. More precisely, it requires modeling the post x as a feature vector:</p><formula xml:id="formula_2">x = [γ(i); ψ(s)]<label>(3)</label></formula><p>where x ∈ R (n+m) is the vector for the post x and</p><formula xml:id="formula_3">[v 1 ; v 2 ] denotes vector concatenation.</formula><p>The model to classify x can be constructed on top of the feature vectors x. Classification techniques such as SVM can be used. In our work, the classification is done using neural network as discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Joint Fusion with Neural Network Models</head><p>Although late fusion and early fusion are able to combine visual and textual data for classification, they also suffer from some drawbacks. Late fusion requires the construc-tion of two classifiers. In many cases, the construction of two separate classifiers would negate the purpose of combining them as the improvement (if any) from using only one classifier could be insignificant. On the other hand, while early fusion does not need two separate classifiers, it requires the availability of both the image and the text, which may be too stringent. In this section, we propose two fusion approaches that enjoys the simplicity of early fusion and the flexibility of late fusion. We call these approaches joint fusion and common space fusion. Our fusion approaches are based on neural networks as they provide many advantages: 1) neural networks allow to train the classifier in an end-to-end manner without involving the tedious feature engineering process, 2) neural networks are also highly-customizable as the model can be changed easily by adding or removing some layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mathematical notations and layers</head><p>A feedforward neural network estimates P(Y = y|x) with a parametric function φ θ (Equation 1), where θ refers to all learnable parameters of the network. Given an input x, this function φ θ applies a combination of functions such as</p><formula xml:id="formula_4">φ θ (x) = φ L (φ L−1 (. . . φ 1 (x) . . .)),<label>(4)</label></formula><p>with L the total number of layers in the network. We denote matrices as bold upper case letters (X, Y, Z), and vectors as bold lowercase letters (a, b, c). A i represents the i th row of matrix A and [a] i denotes the i th element of vector a. Unless otherwise stated, vectors are assumed to be column vectors. We now introduce the two standard layers when training linear classifiers with neural networks: the linear layer and the softmax layer.</p><p>Linear layer This layer applies a linear transformation to its inputs x:</p><formula xml:id="formula_5">φ l (x) = W l x + b l (5)</formula><p>where W l and b l are the trainable parameters with W l being the weight matrix, and b l is the bias term. For instance, when training an image classifier, x = γ(i) ∈ R n with W l ∈ R |Y |×n and b l ∈ R n . And x = ψ(s) ∈ R m with W l ∈ R |Y |×m and b l ∈ R m , when training a classifier with only textual content.</p><p>Softmax layer Given an input x, the penultimate layer outputs a score for each class y ∈ Y , φ L−1 (x) ∈ R |Y | . The probability distribution is obtained by applying the softmax activation function:</p><formula xml:id="formula_6">P(Y = y|x) ∝ φ θ (x, y) = exp(φ L−1 (x, y)) ∑ |Y | k=1 exp(φ L−1 (x, y k ))<label>(6)</label></formula><p>Using the above layers, early fusion can be described as a neural network as in the following example.</p><p>Example 1 (Early fusion as a neural network model). Early fusion can be modelled as a neural network with 3 layers: φ θ (x) = φ 3 (φ 2 (φ 1 (x))). The first layer φ 1 is a fusion layer that applies the concatenation operation on the image and text feature vectors, γ(i), ψ(s). The output of this layer is the post vector x = [γ(i); ψ(s)]. The second layer φ 2 is a linear layer that takes the concatenation of both image and text feature vectors, x = [γ(i); ψ(s)] ∈ R <ref type="bibr">(n+m)</ref> . The learnable parameters of this layer are W 2 ∈ R |Y |×(n+m) and b 2 ∈ R <ref type="bibr">(n+m)</ref> . The final layer φ 3 is a softmax layer which converts the score for each class obtained from the second layer to probability. The parameters of early fusion are θ = {W 2 , b 2 }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature Vectors</head><p>Before diving into the detail of our fusion models, we discuss the process to represent each image and text as feature vectors (γ(i), ψ(s)) as early fusion and our proposed approaches require an underlying image and text representation. A good image and text representation can affect the performance of these approaches heavily.</p><p>Image Representations To represent the images, we use a CNN to extract features, i.e. represent each image i as a feature vector γ(i) ∈ R n . This is motivated by the fact that CNNs are able to obtain state-of-the-art results in many object classification tasks <ref type="bibr" target="#b3">[4]</ref>. Therefore, the CNN may capture features which are suitable to classify objects <ref type="bibr" target="#b22">[23]</ref>. These features may also be applicable for other types of classification, such as emotion. Technically, we take a CNN <ref type="bibr" target="#b23">[24]</ref> trained on the ImageNet dataset for the object classification task and remove the last fully-connected layer while keeping the other layers the same. By feeding each image through this pretrained CNN, we obtain an image vector in the output.</p><p>It is worth noting that by keeping other layers the same, we use no parameter to obtain the image vector. However, we can also retrain the CNN with our dataset. In this case, the parameter for γ(i) is all the parameters of the CNN. They will be trained together with other parameters of the joint fusion models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Representations</head><p>The process of converting a text s from its original format (i.e. words) to a d-dimensional vector is modelled by a function ψ µ (s), which takes a sequence of words s = w 1 , w 2 , ..., w n as input, where each word w t comes from a predefined vocabulary V . This function is a composition of an embedding layer and an aggregation layer.</p><p>Embedding layer. Given a parameter matrix E ∈ R |V |×d , the embedding layer is a lookup table that performs an array indexing operation:</p><formula xml:id="formula_7">ψ 1 (w t ) = E t ∈ R d ,<label>(7)</label></formula><p>where E t corresponds to the embedding of the element w i at row i. This matrix is usually initialized with pretrained word embeddings. d is the dimensionality of the word vectors which is a hyperparameter chosen by the users. By feeding each word of a text s = w 1 , w 2 , ..., w n through the lookup table, we obtain the following output matrix:</p><formula xml:id="formula_8">ψ 1 (s) = (E 1 , E 2 , ..., E n ) ∈ R d×n .<label>(8)</label></formula><p>Aggregation layer. The second layer ψ 2 of the network is an aggregation layer that takes the matrix ψ 1 from the previous layer as input and returns a vector representation for the text s. Formally, the aggregation layer applies the following operation:</p><formula xml:id="formula_9">ψ 2 (s) = agg ψ 1 (w t ), ∀w t ∈ s ,<label>(9)</label></formula><p>where agg is an aggregation function. This function can be either the average or a component-wise max. More complex functions could be used for aggregating the word embeddings, such as a convolutional or recurrent neural network. But it has been shown that these simpler aggregating function with no parameters have similar performance on classification tasks <ref type="bibr" target="#b9">[10]</ref>.</p><p>As the network ψ µ has only two layers and the second layer has no parameter, µ = {E}. This parameter will be trained together with the parameters of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint Fusion Models</head><p>As discussed above, our motivation for joint fusion is to have models that can classify posts having only image or text or both without constructing two classifiers like late fusion. To achieve this goal, our joint fusion models change how the post vector x is constructed from the image and text vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Fusion Learning</head><p>In early fusion, the post vector x is constructed in the fusion layer by concatenating the image vector γ(i) and the text vector ψ(s). This way of combining the vectors has two important implications. First, both the image and text vector must be available, which prevents our intention to classify using only image or text. Second, we still consider the textual and visual information as separate as each of them affects the classification independently. In joint fusion models, we change the way the text and image vectors are fused in the fusion layer while keeping other layers similar to early fusion. Fusion layer. The fusion layer takes the image vector and text vector γ(i), ψ(s) as input and applies the pooling operation to obtain the post vector x:</p><p>x = pooling(γ(i), ψ(s))</p><p>The pooling function can be either a component-wise max pooling, or an average pooling. It is worth noting that the pooling operation requires the vectors γ(i) ∈ R n and ψ(s) ∈ R m to have the same size. In <ref type="figure" target="#fig_0">Figure 1c</ref>, we assume the image vector has higher dimension and we project its feature vector into the textual feature space. This can be done by adding an extra linear layer to γ (i.e. the network that extracts image feature vector). Assuming n &gt; m, the linear layer is as follows:</p><formula xml:id="formula_11">γ(i) =Wγ(i) +b<label>(11)</label></formula><p>whereγ(i) ∈ R m ,W ∈ R n×m andb ∈ R m . The input to the fusion layer is then two vectorsγ(i) and ψ(s).</p><p>The parameters of joint fusion are θ = {W 2 , b 2 ,W,b}. Training. The parameter θ is obtained by training the joint fusion neural network by minimizing the negative log-likelihood using stochastic gradient descent(SGD):</p><formula xml:id="formula_12">L(θ) = ∑ (x,y) − log P(Y = y|x) ∝ ∑ (x,y) − log φ θ (x, y) .<label>(12)</label></formula><p>Common Feature Space Fusion Although joint fusion allows to classify even when only image or text is available, the accuracy of these unimodal classifiers are not the same (c.f. Section 5) as joint fusion still considers visual and textual signals of the same post as different. Motivated by this observation, we propose common space fusion which aims to enforce visual and textual vectors of the same post to be similar i.e. to be in the same feature space. We achieve this using an auxiliary learning task in addition to the classification main task. The neural network of common space fusion is similar to that of joint fusion with the exception of the addition of the auxiliary task. Auxiliary learning task. The goal of the auxiliary learning task is to make the image and text vector γ(i), ψ(s) of the post x to be similar while the image vector γ(i) of the post x and the text vectors {ψ(s − )} of posts from different classes are different. The vectors γ(i), ψ(s + ) of the post x is called a positive pair while the vectors γ(i), ψ(s − ) is called a negative pair. We measure the similarity and difference between the pairs using a distance metric d(γ(i), ψ(s)). Intuitively, we want the distance of the positive pair to be low while the distance of the negative pairs to be high. This objective is captured by the loss function for the auxiliary task.</p><p>Training. Traditionally, the objective of the auxiliary task is captured using a marginbased loss function. However, a recent work has shown that using a probabilistic interpretation of the margin-based loss function yields better result <ref type="bibr" target="#b5">[6]</ref>. The loss function for the auxiliary task is defined as follows:</p><formula xml:id="formula_13">L(i, s + , {s − j } j=1,g ) = − ∑ j=1,g log exp(−d(γ(i), ψ(s + ))) exp(−d(γ(i), ψ(s + ))) + exp(−d(γ(i), ψ(s − j )))<label>(13)</label></formula><p>where g is the number of negative pairs to be used in training. With θ = {W 2 , b 2 ,W,b}, we minimize the following loss function involving the auxiliary learning task and the classification main task for a given training sample (x, y), where x = {i, s + }:</p><formula xml:id="formula_14">L(x, y; θ) = −λ log φ θ (x, y) − ∑ j=1,g log exp(−d(γ(i), ψ(s + ))) exp(−d(γ(i), ψ(s + ))) + exp(−d(γ(i), ψ(s − j )))<label>(14)</label></formula><p>where the g negative text samples s − are chosen randomly at each iteration, and λ is a hyperparameter specifying the weight of the classification main task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments with Emotion Classification</head><p>In this section, we evaluate our proposed fusion approaches on the emotion classification application. To the best of our knowledge, we are the first one to study combining visual and textual data for emotion classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Emotion as Discrete Categories</head><p>In emotion classification, the problem we want to solve is given a post x, we want to construct a classifier to find out the post's membership i.e., x belongs to which emotion class. The set of emotion classes is denoted as Y = {y 1 , ..., y k } where each class y i ∈ Y and ∀i, j : y i ∩ y j = / 0. For the emotion classes, we use the most well-known Plutchik's classification of emotions <ref type="bibr" target="#b20">[21]</ref>. An illustration of the Plutchik's wheel of emotions is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Datasets</head><p>To the best of our knowledge, there is no large-scale dataset for emotion classification that contains both visual and textual data. Even for visual content only, most of the datasets are relatively small except for the flickr dataset <ref type="bibr" target="#b25">[26]</ref>. This motivates us to build a dataset from scratch while also adding textual data to the flickr dataset. Enriching an image-only dataset. The flickr dataset was produced by You et. al. <ref type="bibr" target="#b25">[26]</ref>, which contains images from a popular image sharing website <ref type="bibr" target="#b2">3</ref> . To obtain the labels for these images, at least 5 workers from Amazon Mechanical Turks were hired to label them into 8 emotion classes: amusement, anger, awe, contentment, disgust, fear and sadness. We first crawl the images from this dataset while keeping only the ones where the majority of the workers agree that the image belongs to a specific emotion class. Then, for each image, we collect its title and description to use as textual data. Although these information are not provided in the dataset, they are still available from the image sharing website. We only keep the images with English title and description and the total of words in the title and description is more than 5. The statistics of the flickr dataset is shown in <ref type="table" target="#tab_1">Table 2</ref>. Building an emotion dataset from scratch. In order to test our approaches in different settings, we build another emotion dataset by crawling data from Reddit 4 . Reddit is a discussion website where discussions are organized by topics (i.e. subreddits). Reddit  also has a reputation system where submissions are vote up or down. This reputation system enforces that 1) submissions in a subreddit are always belong to a topic, 2) each submission is "labelled" by a number of users. These characteristics make Reddit an attractive social media to build an emotion dataset as it provides strongly-labelled data while a submission always contains text in its title and sometimes contains image. We focus on 4 popular subreddits (happy, creepy, rage, gore) which are related to emotion and contain high amount of image submissions. These subreddits correspond to the emotion joy, fear, anger, disgust in the Plutchik's model for emotion classification. For each subreddit, we crawled 1000 submissions with the highest number of upvotes. We kept only posts containing image and discarded the rest. This created an imbalance in the number of posts between the emotion classes. For this reason, we collected submissions with at least 100 upvotes for two classes with the least amount of posts. It is worth noting that one post may contain several images. All the images are then converted to jpg format. The statistics of the reddit dataset is shown in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines</head><p>We compare our proposed approaches (joint fusion and common space fusion) with two unimodal baselines (a text-based and an image-based classifier) and two traditional multimodal fusion techniques (early and late fusion). Regarding the text-based classifier, we chose to use fastText <ref type="bibr" target="#b9">[10]</ref> which is a shallow network model for text classification. FastText is able to achieve state-of-the-art result without sacrificing much performance in comparison with other deep models <ref type="bibr" target="#b9">[10]</ref>. For the image-based classifier, we use a pretrained InceptionNet <ref type="bibr" target="#b23">[24]</ref> which is a CNN for object classification task. To make an image classifier for our setting, we replace the last layer in InceptionNet with a linear layer and train this layer with our dataset while keeping other layers the same. In addition to serving as a baseline, we also use InceptionNet to extract the image features as mentioned in Section 4, which results in a total of 2048 features per image. Regarding late fusion, for comparison purpose, we also reuse the text-based and image-based classifiers to obtain the class probabilities. It is worth noting that we have considered other baselines such as using SVM as the classifier. However, as these techniques have worse performance than deep-learning approaches, we do not include them in the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experimental settings</head><p>For the embedding layer, we used the GloVe word vectors <ref type="bibr" target="#b19">[20]</ref> trained on Twitter data as they are in the same social media domain as Flickr and Reddit. For regularization, we used a dropout layer with a dropout probability of 0.25 right after the lookup table to reduce overfitting. In addition, it is reported that factorizing the linear classifier into low rank matrices may improve the classification accuracy <ref type="bibr" target="#b15">[16]</ref>. We also followed this approach by adding a linear layer right before the last layer to map the concatenated vector (in the case of early fusion) and the pooled vector (in the case of joint fusion models) to a hidden vector space with a size of h. Regarding the hyperparameters, we tested different values of them on the validation set and select the ones that gave the best results. <ref type="table" target="#tab_2">Table 3</ref> describes other hyperparameters. Our models were trained with a learning rate set to 0.01. The models were trained on a server equipped with a Tesla GPU. For testing, we use the top 10% posts with the highest number of upvotes for the reddit dataset and the highest number of agreements for the flickr dataset. The next 10% of these dataset are used for validation. We use the same splits for all the models in our experiments. All the source codes and the datasets are available at https://emoclassifier.github.io/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Quantitative analysis</head><p>Results on the Reddit dataset <ref type="table" target="#tab_3">Table 4</ref> shows the experimental results for the Reddit dataset. The results show that visual and textual classifiers have similar accuracy on the reddit dataset as their difference in accuracy is only 1%. In addition, all the fusion techniques have better accuracy than the baselines. The traditional fusion approaches improve the accuracy by at least 5%. This clearly demonstrates the benefits of combing visual and textual data. Our proposed techniques perform the best as they improves the accuracy by 8% and 2% in comparison with using single modality and early/late fusion, respectively. Among the proposed techniques, common space fusion has higher accuracy. However, the difference is small as it is only 0.6%.</p><p>Results on the Flickr dataset The experimental results for the Flickr dataset is also shown in <ref type="table" target="#tab_3">Table 4</ref>. The dataset is interesting as there is a large discrepancy in accuracy (30%) between image-based and text-based classifiers. On the contrary, the discrepancy between these classifiers in the Reddit dataset is only 1%. The reasons for this could be 1) the title and description of an image post from flickr are more descriptive as they tend to be longer than the title of a submission from reddit, 2) the labels from the reddit dataset are more reliable as they are "labelled" by at least 100 users in comparison with only 5 for the flickr dataset. Similar to the reddit dataset, all the fusion approaches have better accuracy than unimodal classifiers and our proposed approaches outperform the rest with the common space fusion has better accuracy.</p><p>The experimental results on two datasets with different characteristics show that combining textual and visual data can improve the accuracy of classification significantly. In addition, our proposed techniques are robust with different setting as they always achieve the highest accuracy throughout two datasets.</p><p>Handling the absence of one modality The goal of joint and common space fusion is not only able to achieve higher accuracy by combining visual and textual signals but also able to provide correct classification when only either text or image is available.</p><p>In this section, we analyze the performance of our proposed approaches when only one modality is used as input. <ref type="table" target="#tab_4">Table 5</ref> shows the experimental results for two different datasets, we also replicate the results with the image and text-only classifiers for comparison purposes. The results clearly demonstrate the benefits of creating a common space as the difference in accuracy between using only image or text as input for common space fusion is significantly lower than that of joint fusion. For the reddit dataset, the difference is only 0.8% for common space fusion while it is 34% for joint fusion. The discrepancy for common space fusion is lower as it considers image and text vector of a post as equally important. As a result, using either image or text to classify, we get similar accuracy. On the other hand, joint fusion considers textual information as more important. This makes the classification using text significantly better than using only image. Another key finding is that using only image or text as input, the common space fusion has better result on the flickr dataset than single-input classifier. We achieve a gain of 13% for image and 0.8% for text. As the common space contains information from two modalities, even when we use only one modality as input, we can also leverage information from other modality. However, by enforcing a common space from two modalities, we may lose information if one modality is less informative than the other. This happens with the reddit dataset where the titles are shorter and less descriptive than the descriptions of flickr images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Qualtitative analysis</head><p>Results with multimodal input We also compare and analyze the results to find out in which case our proposed approaches, image-only or text-only approach is better. <ref type="table" target="#tab_5">Table  6</ref> shows some noteworthy examples. The first two examples demonstrate the benefits of creating the common space as it allows the classifier to put more weights on the visual signal. As the image descriptors of these examples are sarcasm, focusing only on the text give incorrect predictions. For instance, the image descriptor contains the word "present" which is a strong indicator for the emotion joy. However, the image shows that the present is a creepy doll in a closet, which expresses the emotion fear. Only by taking into account both the image and the text, a classifier can make a correct prediction i.e. this is the advantage of fusion approaches. The next two examples shows that when a signal is stronger than the other, the fusion approaches will make the same prediction as the stronger signal. In very rare cases as in the fifth example, the stronger signal is an incorrect one, which makes the fusion's prediction incorrect. Results with the absence of one modality <ref type="table" target="#tab_6">Table 7</ref> shows several examples where either image or text is used as input to our fusion approaches. As one type of input is missing, single-modality fusion approaches may misclassify. This is illustrated in the first two example where unimodal classifiers classify incorrectly while fusion approaches give correct results. In the first example, the presence of the words "wife","baby" usually indicates the emotion joy while the image indicates the emotion fear. In the next two examples, common space fusion makes correct predictions while other approaches fail. The text from the third example contains the words "wake up", "morning", which indicates the emotion joy. This makes joint fusion misclassify as it considers textual signal as more important. On the other hand, common space fusion is able to balance between textual and visual signals and come up with correct predictions. The fifth example shows a case where the image-only common space fusion is even better than the image classifier. While the image classifier predicts the emotion fear due to the presence of the color red and people's skin, the image actually expresses the emotion contentment, which is correctly classified by the image-only common space fusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we propose simple models that are able to combine information from different modalities to analyze social media. Our models are robust with different types of input as they can handle the missing of some modalities. In addition, we show that our models, despite being simple, can achieve high accuracy on the emotion classification application. In order to showcase our models, we also constructed two multimodal datasets which allow us to test our approaches in different settings. Future research directions will go towards analyzing the performance of our models in problems that involve other modalities such as structured data <ref type="bibr" target="#b18">[19]</ref> and user-annotated data <ref type="bibr" target="#b6">[7]</ref> and other applications beside emotion classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Traditional techniques (a,b) and our techniques (c,d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Plutchik's classification of emotions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>blog.bufferapp.com/the-power-of-twitters-new-expanded-images[..] 2 blog.twitter.com/2015/new-research-twitter-users[..] arXiv:1708.02099v1 [cs.CL] 7 Aug 2017 Example posts on social media with both image and image descriptor</figDesc><table><row><cell>This is my</cell><cell>[...] I decided</cell><cell cols="2">Poor dog was</cell></row><row><cell>happy.</cell><cell>to leave a little</cell><cell>wearing</cell><cell>[...]</cell></row><row><cell></cell><cell>present for the</cell><cell cols="2">bricks [...] to be</cell></row><row><cell></cell><cell>next tenants.</cell><cell cols="2">a fighting dog</cell></row><row><cell></cell><cell></cell><cell>:(</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the two datasets</figDesc><table><row><cell></cell><cell cols="2">emotion #posts</cell><cell></cell><cell>emotion</cell><cell>#posts</cell></row><row><cell></cell><cell>joy</cell><cell>1119</cell><cell></cell><cell cols="2">amusement 1259</cell></row><row><cell>reddit</cell><cell>fear anger</cell><cell>697 613</cell><cell></cell><cell>anger awe</cell><cell>407 1561</cell></row><row><cell></cell><cell cols="2">disgust 810 Total 3239</cell><cell>flickr</cell><cell cols="2">contentment 2389 disgust 852 excitement 1451</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>fear</cell><cell>434</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sadness</cell><cell>984</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Total</cell><cell>9337</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Model hyperparameters</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Word embedding size</cell><cell>d = 200</cell></row><row><cell>Hidden vector space</cell><cell>h = 100</cell></row><row><cell># negative samples (reddit)</cell><cell>g = 3</cell></row><row><cell># negative samples (flickr)</cell><cell>g = 4</cell></row><row><cell cols="2">Classification main task weight λ = 3</cell></row><row><cell>Aggregation layer function</cell><cell>max</cell></row><row><cell>Fusion layer function</cell><cell>max</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance of different fusion models and two baselines on two datasets</figDesc><table><row><cell>Model</cell><cell></cell><cell>Reddit</cell><cell></cell><cell></cell><cell>Flickr</cell><cell></cell></row><row><cell></cell><cell cols="6">Accuracy F-Macro F-Micro Accuracy F-Macro F-Micro</cell></row><row><cell cols="2">Image-based classifier 77.48</cell><cell>0.77</cell><cell>0.78</cell><cell>56.87</cell><cell>0.67</cell><cell>0.73</cell></row><row><cell>Text-based classifier</cell><cell>78.4</cell><cell>0.77</cell><cell>0.78</cell><cell>88.3</cell><cell>0.85</cell><cell>0.88</cell></row><row><cell>Late fusion</cell><cell>83.33</cell><cell>0.82</cell><cell>0.83</cell><cell>91.83</cell><cell>0.87</cell><cell>0.91</cell></row><row><cell>Early fusion</cell><cell>84.11</cell><cell>0.81</cell><cell>0.83</cell><cell>92.69</cell><cell>0.89</cell><cell>0.92</cell></row><row><cell>Joint fusion</cell><cell>86.29</cell><cell>0.84</cell><cell>0.86</cell><cell>93.01</cell><cell>0.90</cell><cell>0.93</cell></row><row><cell cols="2">Common space fusion 86.92</cell><cell>0.85</cell><cell>0.87</cell><cell>93.44</cell><cell>0.91</cell><cell>0.934</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Accuracies of joint and common space fusion when only one modality is available</figDesc><table><row><cell></cell><cell>reddit</cell><cell>flickr</cell></row><row><cell></cell><cell cols="2">image text image text</cell></row><row><cell cols="3">single-input classifier 77.48 78.4 56.869 88.3</cell></row><row><cell>joint fusion</cell><cell cols="2">35.83 69.47 29.46 85.81</cell></row><row><cell cols="3">common space fusion 71.03 71.96 69.03 89.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Qualitative results when both image and text are available</figDesc><table><row><cell>Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Image descriptor Happy Happy</cell><cell>Today</cell><cell>i'm</cell><cell>Early</cell><cell>diving</cell><cell>but john , why</cell><cell>I took this long</cell></row><row><cell></cell><cell>Joy Joy!!!</cell><cell>moving,</cell><cell>so</cell><cell cols="2">equipment</cell><cell>do n ' t you like</cell><cell>exposure shot</cell></row><row><cell></cell><cell></cell><cell cols="2">I decided to</cell><cell></cell><cell></cell><cell>your roommate</cell><cell>of a pumpkin</cell></row><row><cell></cell><cell></cell><cell cols="2">leave a little</cell><cell></cell><cell></cell><cell>?</cell><cell>as my lense</cell></row><row><cell></cell><cell></cell><cell cols="2">present for the</cell><cell></cell><cell></cell><cell></cell><cell>unknowingly</cell></row><row><cell></cell><cell></cell><cell cols="2">next tenants.</cell><cell></cell><cell></cell><cell></cell><cell>fogged up</cell></row><row><cell>Ground truth</cell><cell>fear</cell><cell>fear</cell><cell></cell><cell></cell><cell>fear</cell><cell>anger</cell><cell>fear</cell></row><row><cell>Text classifier</cell><cell>joy</cell><cell>joy</cell><cell></cell><cell cols="2">disgust</cell><cell>anger</cell><cell>fear</cell></row><row><cell>Image classifier</cell><cell>fear</cell><cell>fear</cell><cell></cell><cell></cell><cell>fear</cell><cell>fear</cell><cell>joy</cell></row><row><cell>Joint fusion</cell><cell>joy</cell><cell>joy</cell><cell></cell><cell></cell><cell>fear</cell><cell>anger</cell><cell>joy</cell></row><row><cell>Common space</cell><cell>fear</cell><cell>fear</cell><cell></cell><cell></cell><cell>fear</cell><cell>anger</cell><cell>joy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Qualitative results when either image or text is available</figDesc><table><row><cell>Image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Image descriptor My wife heard</cell><cell>New</cell><cell>Year's</cell><cell cols="2">I live on the</cell><cell>A Long Way</cell><cell>Zelda at the</cell></row><row><cell></cell><cell>her name being</cell><cell cols="2">Resolutions</cell><cell>second</cell><cell>story</cell><cell>from Home Oil</cell><cell>height of con-</cell></row><row><cell></cell><cell>whispered [...]</cell><cell cols="2">Taken on my</cell><cell cols="2">and a painter</cell><cell>on canvas by</cell><cell>tentment. Lily's</cell></row><row><cell></cell><cell>she looked at</cell><cell cols="2">way home for</cell><cell cols="2">made me al-</cell><cell>Michael Kent</cell><cell>going</cell><cell>away</cell></row><row><cell></cell><cell>the baby moni-</cell><cell cols="2">christmas 2005.</cell><cell cols="2">most crap my</cell><cell>picnic at Bal-</cell></row><row><cell></cell><cell>tor to find our</cell><cell></cell><cell></cell><cell cols="2">pants when i</cell><cell>last Point Park</cell></row><row><cell></cell><cell>son like this.</cell><cell></cell><cell></cell><cell cols="2">woke up this</cell><cell>in Balmain.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>morning</cell><cell></cell></row><row><cell>Ground truth</cell><cell>fear</cell><cell cols="2">disgust</cell><cell>fear</cell><cell></cell><cell>sadness</cell><cell>contentment</cell></row><row><cell>Text classifier</cell><cell>anger</cell><cell cols="2">contentment</cell><cell>joy</cell><cell></cell><cell>contentment</cell><cell>contentment</cell></row><row><cell>Image classifier</cell><cell>fear</cell><cell cols="2">disgust</cell><cell>fear</cell><cell></cell><cell>fear</cell><cell>disgust</cell></row><row><cell>Joint fusion</cell><cell>fear</cell><cell cols="2">disgust</cell><cell>joy</cell><cell></cell><cell>contentment</cell><cell>contentment</cell></row><row><cell>Joint fusion -text</cell><cell>joy</cell><cell cols="2">contentment</cell><cell>joy</cell><cell></cell><cell>contentment</cell><cell>contentment</cell></row><row><cell>Joint fusion -image</cell><cell>disgust</cell><cell cols="2">amusement</cell><cell>fear</cell><cell></cell><cell>amusement</cell><cell>amusement</cell></row><row><cell>Common space</cell><cell>fear</cell><cell cols="2">disgust</cell><cell>fear</cell><cell></cell><cell>sadness</cell><cell>contentment</cell></row><row><cell>Common space -</cell><cell>joy</cell><cell cols="2">contentment</cell><cell>fear</cell><cell></cell><cell>sadness</cell><cell>contentment</cell></row><row><cell>text</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Common space -</cell><cell>fear</cell><cell cols="2">disgust</cell><cell>fear</cell><cell></cell><cell>contentment</cell><cell>contentment</cell></row><row><cell>image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">www.flickr.com 4 www.reddit.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research. We would also like to thank the IT support team for their help.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching words and pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sentibank: large-scale ontology and classifiers for detecting sentiment and emotions in visual content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning deep features for image emotion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP. IEEE</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maria Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multiple feature fusion for social media applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<editor>SIGMOD. ACM</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep unsupervised learning through spatial contrasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00243</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Minimizing efforts in validating crowd answers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Q V</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weidlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD. pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="999" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Affective audio-visual words and latent topic driving model for realizing movie affective scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Satou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Can we understand van gogh&apos;s mood?: learning to infer affects from images in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative learning and recognition of image set classes using canonical correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML Deep Learning Workshop</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On shape and the computability of emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Suryanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Adams</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multimodal composition: A critical sourcebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lutkewitte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Bedford/St. Martin&apos;s</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Argument discovery via crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weidlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Retaining data from streams of social platforms with minimal regret</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weidlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V H</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Result selection and summarization for web table search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weidlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aberer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="231" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The nature of emotions human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Audiovisual synchronization and fusion using canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Sargin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yemez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erzin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Robust visual-textual sentiment analysis: When attention meets tree-structured recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Building a large scale dataset for image emotion recognition: The fine print and the benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02677</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Cross-modality consistent regression for joint visualtextual sentiment analysis of social multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>WSDM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multimodal classification of events in social media. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeppelzauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schopfhauser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploring principles-of-art features for image emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
