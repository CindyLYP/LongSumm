<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AI 2 : Training a big data machine to defend</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><forename type="middle">Veeramachaneni</forename><surname>Csail</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ignacio Arnaldo PatternEx</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>MA</roleName><forename type="first">Mit</forename><surname>Cambridge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ignacio Arnaldo PatternEx</orgName>
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Cuesta-Infante</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsi</forename><surname>Korrapati</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costas</forename><surname>Bassias</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>San Jose</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AI 2 : Training a big data machine to defend</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We present an analyst-in-the-loop security system, where analyst intuition is put together with stateof-the-art machine learning to build an end-to-end active learning system. The system has four key features: a big data behavioral analytics platform, an ensemble of outlier detection methods, a mechanism to obtain feedback from security analysts, and a supervised learning module. When these four components are run in conjunction on a daily basis and are compared to an unsupervised outlier detection method, detection rate improves by an average of 3.41×, and false positives are reduced fivefold. We validate our system with a real-world data set consisting of 3.6 billion log lines. These results show that our system is capable of learning to defend against unseen attacks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today, information security solutions generally fall into two categories: analyst-driven, or unsupervised machine learning-driven. Analyst-driven solutions rely on rules determined by fraud and security experts, and usually lead to high rates of undetected attacks (false negatives), as well as delays between attack detection and implementation of preventative countermeasures. Moreover, bad actors often figure out current rules, and design newer attacks that can sidestep detection.</p><p>Using unsupervised machine learning to detect rare or anomalous patterns can improve detection of new attacks. However, it may also trigger more false positive alarms and alerts, which can themselves require substantial investigative efforts before they are dismissed. Such false alarms can cause alarm fatigue and distrust, and over time, can cause reversion to analyst-driven solutions, with their attendant weaknesses.</p><p>We identified three major challenges facing the information security industry, each of which could be addressed by machine learning solutions:</p><p>Lack of labeled data: Many enterprises lack labeled examples from previous attacks, undercutting the ability to use supervised learning models.</p><p>Constantly evolving attacks: Even when supervised learning models are possible, attackers constantly change their behaviors, making said models irrelevant. Limited investigative time and budget: Relying on analysts to investigate attacks is costly and time-consuming. A solution that properly addresses these challenges must use analysts' time effectively, detect new and evolving attacks in their early stages, reduce response times between detection and attack prevention, and have an extremely low false positive rate. We present a solution that combines analysts' experience and intuition with state-of-the-art machine learning techniques to provide an end-to-end, artificially intelligent solution. We call this system AI 2 . AI 2 learns and automatically creates models that, when executed on new data, produce predictions as intelligent as those deduced by human analysts. Backed by big data infrastructure, we achieve this in close to real time.</p><p>Our contributions through this paper are as follows:</p><p>1. Developed an Active Model Synthesis approach, which: (a) computes the behaviors of different entities within a raw big data set, (b) presents the analyst with an extremely small set of events (k &lt;&lt;&lt; N ), generated by an unsupervised, machine learning-based outlier detection system, (c) collects analyst feedback (labels) about these events, (d) learns supervised models using the feedback, (e) uses these supervised models in conjunction with the unsupervised models to predict attacks, and (f) continuously repeats steps (a) -(e). 2. Designed multivariate methods that are capable of modeling the joint behaviors of mixed variable types (numeric and discrete ordinal). These methods include density-based, matrix decomposition-based, and replicator neural networks. 3. Demonstrated performance of the AI 2 system by monitoring a web-scale platform that generated millions of log lines per day over a period of 3 months, for a total of 3.6 billion log lines. Summary of results: In <ref type="figure" target="#fig_0">Figure 1</ref>, we present a snapshot of our system's progress after 12 weeks of use. With 3 months' worth of data, and with awareness of attacks, we evaluate whether our solution can improve attack detection rates (recall) while reducing the number of alerts shown to the analyst ("daily investigation budget" k).</p><p>Using analyst time effectively: The AI 2 system achieves a detection rate of 86.8% even at an extremely low daily investigative budget of k = 200 events. This represents more than tenfold improvement 1 over the unsupervised outlier detection approach rate, which is 7.9%. Fixing the daily investigation budget at 200 keeps the false positive rate at 4.4%.</p><p>Reducing false-positives by a factor 5: If we allow for an higher daily investigative budget (for example, up to 1000), the unsupervised outlier detection based method can still only achieve a 73.7% detection rate, and the false positive rate is &gt; 22%. AI 2 achieves &gt; 86% for a false positive rate of 4.4% a reduction by factor of 5. On our choice of the title "Training a big data machine to defend": We define a big data system or machine as a software infrastructure that is able to ingest data in real time, compute and generate quantities that can then be analyzed, either by data scientists or a machine learning system. A machine learning substrate that sits on top of this system can analyze the data and automatically produce outliers. We provide a system that collects and incorporates analyst feedback, generates, and uses these models continuously without any involvement from its original developers -that is us. Thus, we are able to deliver a fully automatic system that could be trained by analysts.</p><p>In Section 2, we present an overview of the system, and the challenges encountered while building it. Section 3 summarizes related work in this area and Section 4 describes the data analyzed by our platform. In Section 5 we present the big data platform for behavioral analytics. Section 6 presents the outlier detection system. With the two key components in place, Section 7 presents the active model synthesis framework. Section 8 presents the experimental setup and the results achieved. Section 9 presents our key findings and conclusions.</p><p>This result corresponds to the 12th and last week of deployment while the 3.41× improvement claimed in the abstract is the average improvement over the 12 weeks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">AI 2</head><p>In this paper, we present an end-to-end system that learns over time thanks to feedback from security analysts. <ref type="figure">Figure 2</ref> presents a schematic of our system, which is made up of the following components:</p><p>• Big data processing system: A platform that can quantify the behaviors (a.k.a features) of different entities, and compute them from raw data. With high-volume, high-velocity data, this first component requires processing at a challenging scale. We describe this system and what it accomplishes in Section 5 • Outlier detection system: This system learns a descriptive model of those features extracted from the data via unsupervised learning, using one of three methods: density, matrix decomposition, or replicator neural networks. To achieve confidence and robustness when detecting rare and extreme events, we fuse multiple scores into a final score that indicates how far a certain entity or event is from others. These methods are described in detail in Section 6. • Feedback mechanism and continuous learning: This component incorporates analyst input through a user interface. It shows the top k outlier events or entities, and asks the analyst to deduce whether or not they are malicious. This feedback is then fed into the supervised learning module. The value of k and the feedback frequency (e.g. daily or weekly) are both decided by the end user. • Supervised learning module: Given the analyst's feedback, the supervised learning module learns a model that predicts whether a new incoming event is normal or malicious. As more feedback is gathered, the model is constantly refined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Our work exploits ideas from a wide range of fields, including outlier analysis, ensemble learning, active learning, information security, behavioral analytics, and big data computing. Outlier analysis methods have been reviewed in Hodge and Austin <ref type="bibr">[2004]</ref>, <ref type="bibr" target="#b2">Chandola et al. [2009]</ref>  </p><formula xml:id="formula_0">Figure 2: AI 2 .</formula><p>Features describing the entities in the data set are computed at regular intervals from the raw data. An unsupervised learning module learns a model that is able to identify extreme and rare events in data. The rare events are ranked based on a predefined metric, and are shown to the analyst, who labels them as 'normal' or as pertaining to a particular type of attack. These "labels" are provided to the supervised learning module, which produces a model that can then predict, from features, whether there will be attacks in the following days.</p><p>Principal Component Analysis <ref type="bibr" target="#b15">(Shyu et al. [2003]</ref>), neural networks <ref type="bibr" target="#b4">(Hawkins et al. [2002]</ref>; <ref type="bibr" target="#b10">Scholz and Vigário [2002]</ref>; <ref type="bibr" target="#b11">Scholz et al. [2008]</ref>), and statistical models. Ensemble learning can enhance the robustness of outlier analysis <ref type="bibr" target="#b12">(Schubert et al. [2012]</ref>; Aggarwal [2013b]; <ref type="bibr" target="#b17">Zimek et al. [2014]</ref>). This approach has received attention only recently, due to two main constraints: first, it is difficult to interpret and compare outlier scores retrieved with different methods <ref type="bibr" target="#b3">(Gao and Tan [2006]</ref>; <ref type="bibr" target="#b6">Kriegel et al. [2011]</ref>), and second, it is difficult to weight confidence in different methods, since there is no ground truth that can be used for learning. In fact, most works assume a semi-supervised setting, where a set of labels is initially available <ref type="bibr" target="#b8">(Micenková et al. [2014]</ref>).</p><p>The active learning framework <ref type="bibr" target="#b14">(Seung et al. [1992]</ref>) has been applied to the outlier analysis task in the past <ref type="bibr" target="#b9">(Pelleg and Moore [2004]</ref>; <ref type="bibr" target="#b0">Abe et al. [2006]</ref>). In these works, the most ambiguous examples are shown to the user for labeling. This approach is in line with the uncertainty sampling method introduced in Lewis and Catlett <ref type="bibr">[1994]</ref>.</p><p>Behavioral predictive analytics have shown promising results for network intrusion <ref type="bibr" target="#b16">(Yen [2011]</ref>) and internal threat detection <ref type="bibr" target="#b13">(Senator et al. [2013]</ref>). However, to the best of our knowledge, we present the first big data security system capable of detecting threats in real time, and of collecting analysts' feedback to improve detection rates over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data characteristics</head><p>In this section, we present the typical characteristics of the data ingested by our platform (also summarized in <ref type="table">Table 1</ref>). Data sources and applications: Our platform processes both web logs and firewall logs. In a typical enterprise system, these logs are delivered in real, streaming time from widely distributed data sources. Web log analysis is aimed at the detection of web attacks, while mining firewall logs allows us to prevent data ex-filtration in enterprise setups.  <ref type="table">Table 1</ref>: Summary of the characteristics of the data ingested by our platform. The provided ranges are defined according to the values seen at the different enterprise setups where our platform has been deployed.</p><p>Data dimensions and unique entities: The computational effort associated with analyzing data can be reasonably estimated by the data size and the number of unique entities. The first refers to the volume of the raw data, and is generally reported in the form of size metrics (GB, TB) and/or number of log lines (for instance, a midsized enterprise platform easily generates tens of millions of log lines on a daily basis). The second is specific to behavioral analytics, and corresponds to the number of unique entities (IP addresses, users, sessions, etc) analyzed on a daily basis.</p><p>The data set used in this paper corresponds to three months' worth of logs, generated by an enterprise platform. This platform records millions of log lines per day, each corresponding to a specific user interaction, and has hundreds of thousands of daily active users. <ref type="table">Table 1</ref> presents the typical ranges we see in our current use cases. Malicious activity prevalence: Under normal circumstances, malicious activities are extremely rare in an enter- </p><formula xml:id="formula_1">6 • 10 −4 8 • 10 −4 1 • 10 −3 1.2 • 10 −3 1.4 • 10 −3 1.6 • 10 −3</formula><p>Week Ratio of malicious users <ref type="figure">Figure 3</ref>: Weekly ratio of reported malicious users to the total number of active users. prise setup. Attack cases represent a minor fraction of total events (generally &lt; 0.1%). To illustrate this fact, <ref type="figure">Figure 3</ref> shows the ratio of reported malicious users to the total number of active users in the studied dataset. Three additional observations are worth remarking on:</p><p>• This dearth of malicious activity results in extreme class imbalance when learning a supervised model, and increases the difficulty of the detection process. • It is safe to assume that not all malicious activities are systematically reported, either because their incident responses were inconclusive, or because they were not detected in the first place. This introduces noise into the data, since unreported attacks will be considered legitimate activity. • Attack vectors can take a wide variety of shapes. Even when malicious activities are reported, we are not always aware of the specific vectors involved. Thus ,it is important to develop robust defense strategies that are capable of detecting as many attacks as possible. ok</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">BigData platform for behavioral analytics</head><p>Our approach rests on the computation of behavioral descriptors for different entities, such as IP addresses, users, or sessions. These entities can be independent or connected; for instance, the same IP address may be associated with two or more users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Behavioral signatures</head><p>A typical attack has a behavioral signature, which comprises the series of steps involved in committing it. The information necessary to quantify these signatures is buried deep in the raw data, and is often delivered as logs. These quantitative values can be specified by security experts, and generally correspond to indicators an expert would use to investigate an attack. Also known as variables or features in the field of machine learning, they are usually extracted on a per-entity, per-time-segment basis. Using the platform, we calculate a total of 24 variables per user per day. Take, for example, the per-user behavioral features shown in <ref type="figure">Figure 4</ref>. A platform capable of computing behavioral features in real time is itself valuable, since it allows analysts to monitor applications more dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Design requirements</head><p>A big data system for real-time behavioral analytics on webscale applications must meet the following criteria:</p><p>1. Capable of analyzing the behavior of 10+ million entities on a daily basis. 2. Capable of updating and retrieving the behavioral signatures of active entities, on demand and in real time. The platform needs to be able to retrieve behavioral signatures for up to 50 thousand entities at once. In the following section, we describe the key aspects that enable our big data system to process logs from many data sources, extract entities, transform raw logs into features, and keep these features up-to-date in real time. The system is designed to horizontally scale, in order to address billions of log lines per day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">From raw logs to behaviors in real time</head><p>To calculate behavioral features for one user over a particular time segment, one must isolate all relevant historic log lines and perform the aggregations that feature definition demands-for example, aggregating the money this user spent during that time segment. This process must be repeated for all the active users, populating the entity-feature matrix as shown on the right hand side of <ref type="figure">Figure 4</ref>. Such computations are challenging because of high volume, distributed storage of data, and the need to aggregate over historical data to compute the feature. We address this challenge by breaking the extraction of features into two processes: Activity Tracking and Activity Aggregation. Activity Tracking: As the system absorbs the log stream generated by the platform, it identifies the entities involved in each log line (e.g. IP address, user, etc.) and updates the corresponding activity records. These activity records are calculated and stored according to two guidelines:</p><p>1. A very short temporal window. In our experiments, the temporal window over which these activity records are computed and stored is one minute. This way, we can compute behavioral features for different time intervals -30 minutes, 1 hr, 12 hrs and 24 hrs. This allows flexibility in analysis. 2. A design streamlined toward efficient retrieval of the user data necessary for feature computation. Note that, depending on the definition of the feature, aggregating activity records for a larger time window can require anything from simple counters to complex data structures. To elaborate on this second guideline, we show 5 categories of behavioral features, and discuss appropriate structures for efficient data retrieval and aggregation for each category:</p><p>• Counts, averages, and standard deviations: these three metrics can be derived from simple counters. For example: the number of successful logins over the last 24 hours. • Indicators (or boolean variables): Aggregating indicators is also straightforward and requires no additional data structures. For example: whether at least one address verification failed over the last 24 hours.</p><p>• Relational features: these features are calculated using data at the intersection of two entities. For example: the maximum outlier score given to an IP address from which the user has accessed the website. To compute these features efficiently, we build graphs that represent relations between entities in the system. • Temporal behaviors: these variables capture the time elapsed between two or more events, and therefore must be analyzed in chronological order. For example: the minimum time from login to checkout). Computing these features requires timestamping all the relevant events (in this case, logins and checkouts), and comparing the time elapsed between consecutive events. • Unique values: This kind of feature cannot be computed with counters, since duplicated values must be kept track of. We use a dictionary to maintain a set of unique values of the feature, and update it every time new user activity is analyzed. For example: number of different locations from which a user has accessed the website over the last 24 hours. Activity aggregation: Computing behavioral features over an interval of time requires two steps:</p><p>1. Retrieve all activity records that fall within the given interval. Note that, for the purposes of this study, we consider behavioral descriptors that have been aggregated over 24 hours and end at the time of the last user activity. This can be graphically represented as a rolling 24-hour window for feature computation. 2. Aggregate minute-by-minute activity records as the feature demands. Again, this aggregation step depends on the feature type. In the simplest case, counters, one must merely add all the minute-by-minute values together. The more complex case of unique values requires retrieving the unique values of the super set formed by the minute-by-minute sets. Performance considerations: Because the temporal scope of our activity records is 1 minute, we can aggregate records and compute features for flexible time intervals. However, this strategy can result in poor performance. For instance, in the worst case, to compute features over a 24-hour window, we need to retrieve and aggregate 24×60 (minute records)=1440 records. This process has to repeat for millions of entity instances.</p><p>To improve retrieval and aggregation performance, we maintain activity records with different, overlapping time scopes. In particular, we maintain records on:</p><p>• a minute-by-minute basis (starting on the dot),</p><p>• an hourly basis (starting on the dot), • a daily basis (starting at midnight), and • a weekly basis (starting Sunday at midnight).</p><p>This way, if we need to compute features for long intervals, our record retrieval and aggregation requirements remain bounded and satisfy real-time requirements. For example, with this strategy, the computation of features over the previous 24 hours requires the retrieval and aggregation of no more than 23(hour records)+60(minute records)=83 records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Outlier detection methods</head><p>The use of outlier analysis is motivated by the observation that attacks are rare and exhibit distinctive behavior. We combine three unsupervised outlier detection techniques:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Matrix Decomposition-based outlier analysis</head><p>Key idea: Outlier detection methods based on matrix decomposition use Principal Component Analysis to find cases that violate the correlation structure of the main bulk of the data <ref type="bibr" target="#b15">(Shyu et al. [2003]</ref>). To detect these rare cases, PCAbased methods analyze the projection from original variables to the principal components' space, followed by the inverse projection (or reconstruction) from principal components to the original variables (see <ref type="figure" target="#fig_3">Figure 5</ref>). If only the first principal components (the components that explain most of the variance in the data) are used for projection and reconstruction, we ensure that the reconstruction error will be low for the majority of the examples, while remaining high for outliers. This is because the first principal components explain the variance of normal cases, while last principal components explain outlier variance (Aggarwal [2013a]).</p><p>Let X be a p-dimensional dataset. Its covariance matrix Σ can be decomposed as: Σ = P × D × P T , where P is an orthonormal matrix where the columns are the eigenvectors of Σ, and D is the diagonal matrix containing the corresponding eigenvalues λ 1 . . . λ p . Graphically, an eigenvector can be seen as a line in 2D space, or a plane in higherdimensionality spaces, while its corresponding eigenvalue indicates how much the data is stretched in that direction.</p><p>Note that, at this stage, it is common practice to sort the columns of the eigenvector matrix P and eigenvalue matrix D in order of decreasing eigenvalues. In other words, the eigenvectors and their corresponding eigenvalues are sorted in decreasing order of significance (the first eigenvector accounts for the most variance, the second for the second-most, etc.).</p><p>The projection of the dataset into the principal component space is given by Y = XP . Note that this projection can be performed with a reduced number of principal components. Let Y j be the projected dataset using the top j principal components: Y j = X × P j . In the same way, the reverse projection (from principal component space to original space) is given by R j = (P j ×(Y j ) T ) T , where R j is the reconstructed dataset using the top j principal components. This process is schematically depicted in <ref type="figure" target="#fig_3">Figure 5</ref>.</p><p>We define the outlier score of point</p><formula xml:id="formula_2">X i = [x i1 . . . x ip ] as: score(X i ) = p j=1 (|X i − R j i |) × ev(j) (1) ev(j) = j k=1 λ k p k=1 λ k<label>(2)</label></formula><p>Note that ev(j) represents the percentage of variance explained with the top j principal components. As mentioned above, eigenvalues are sorted in decreasing order of significance; therefore, ev(j) will be monotonically increasing. This ... ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Changed address?</head><p>{ "username": "Jane" "ip": "128.21.221.13", "agent": "Mozilla/5.0 ...", "page": "https://mitx.mit.edu/...", "time": "11252015:5.40PM" ... }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>"username": "Jane" "ip": "128.21.221.13", "agent": "Mozilla/5.0 ...", "page": "https://mitx.mit.edu/...", "time": "11252015:5.40PM" ... }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>"username": "Jane" "ip": "128.21.221.13", "agent": "Mozilla/5.0 ...", "page": "https://mitx.mit.edu/...", "time": "08252015:11.12AM" ... } ..... .....</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>"username": "John" "ip": "164.28.233.15", "agent": "Mozilla/5.0 ...", "page": "https://mitx.mit.edu/...", "time": "091252015:9.32AM" ... }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>"username": "Smith" "ip": "168.61.221.13", "agent": "Mozilla/5.0 ...", "page": "https://mitx.mit.edu/...", "time": "121152015:10.03AM" ... }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Day</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Day</head><p>Week Month . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. . . Features</head><p>Jane <ref type="figure">Figure 4</ref>: Extracting behavioral descriptors from big data. Our platform extracts information per entity from large raw log files. The result is a vector of indicators that describe the behavior of an entity over a predefined period of time. means that, the higher is j, the most variance will be accounted for within the components from 1 to j. With this outlier score definition, large deviations in the top principal components are not heavily weighted, while deviations in the last principal components are. This way, outliers present large deviations in the last principal components, and thus will receive high scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Replicator Neural Networks</head><p>Key idea: This method is similar to the previous one, in the sense that it also relies on a compression-reconstruction analysis. However, in this case, we train a multi-layer neural network to compress and reconstruct the data in such a way that the bulk of the data is reconstructed accurately, but outliers are not. This way, the reconstruction error can be directly translated into an outlier score. Replicator Neural Networks (RNNs), or autoencoders, are multi-layer feed-forward neural networks. The input and output layers have the same number of nodes, while intermediate layers are composed of a reduced number of nodes. As depicted in <ref type="figure" target="#fig_4">Figure 6</ref>, we consider RNNs that are composed of three hidden layers. The first and third hidden layers count p/2 neurons, while the second, central layer is composed of p/4 neurons, where p is the dimensionality of the data. The The network is trained to learn identity-mapping from inputs to outputs. The mapping from inputs to intermediate layers compresses the data. The data is then decompressed to reconstruct the inputs, mapping from intermediate layers to outputs. This reconstruction is lossy-that is, introduces an error, and the training process is aimed at minimizing it. The reconstruction error for the the i-th example is given by:</p><formula xml:id="formula_3">e i = p j=1 (x ij − r ij ) 2<label>(3)</label></formula><p>where the input vector x and output vector r are both pdimensional. Given a trained RNN, the reconstruction error is used as the outlier score: test instances incurring a high reconstruction error are considered outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Density-based outlier analysis</head><p>Key idea: Next, we incorporate a technique that fits a multivariate model to the data. This results in a joint probability distribution that can be used to detect rare events, because test instances which fall within a low-density region of the distri-  <ref type="figure">Figure 7</ref>: A discrete valued variable will not have a uniform density for its cdf values (top right). However, if tiny amount of additive white Gaussian noise is added to the variable (essentially making it continuous), the density of its cdf values look closer to uniform (bottom right). We use this trick to be able to model discrete variables using copulas.</p><p>bution are considered outliers. The outlier score is simply the probability density of a point in the multidimensional space.</p><p>To build a multivariate model from marginal distributions which are not all Gaussian, we exploit copula functions. A copula framework provides a means of inference after modeling a multivariate joint probability distribution from training data. Because copula frameworks are less well known than other forms of estimation, we will now briefly review copula theory. We will then describe how we construct the individual non-parametric distributions that make up a copula, and how we then couple them to form a multivariate density function.</p><p>A copula function C(u 1 , . . . u m ; θ) with parameter θ is a joint probability distribution of m continuous random variables, each of them uniformly distributed in [0, 1]. According to Sklar's theorem, any copula function that takes probability distributions F i (x i ) as its arguments defines a valid joint distribution with marginals F i (x i ). Thus, we are able to construct a joint distribution function for x 1 . . . x m with arbitrary marginals as</p><formula xml:id="formula_4">F (x 1 . . . x m ) = C(F 1 (x 1 ) . . . F m (x m ); θ).</formula><p>(4) The joint probability density function (PDF) is obtained by taking the m th order derivative of eqn. (4)</p><formula xml:id="formula_5">f (x 1 . . . x m ) = ∂ m ∂x 1 . . . ∂x m C(F 1 (x 1 ) . . . F m (x m ); θ) = m i=1 f i (x i ) • c(F 1 (x 1 ) . . . F m (x m ); θ) (5)</formula><p>where c(.) is the copula density. Gaussian copula: A multivariate Gaussian copula forms a statistical model for our variables given by C G (u 1 . . . u m ; Σ) = F G (Φ −1 (u 1 ) . . . Φ −1 (u m ); Σ) (6) where F G is the CDF of multivariate normal with zero mean vector and Σ as covariance, and Φ −1 is the inverse of the standard normal. Estimation of parameters: Let Ψ = {Σ, ψ i } i=1...m be the parameters of a joint probability distribution constructed with a copula and m marginals, being ψ i the parameter of marginal i th .</p><p>Given N i.i.d observations of the variables x = <ref type="figure" target="#fig_0">(x 11 , . . . , x mN )</ref>, the log-likelihood function is:</p><formula xml:id="formula_6">L(x; Ψ) = N l=1 log m i=1 f (x il ; ψ i ) c(F (x 1 ) . . . F (x m ); Σ)<label>(7)</label></formula><p>Parameters Ψ are estimated via maximum log-likelihood <ref type="bibr" target="#b5">(Iyengar [2011]</ref>):</p><formula xml:id="formula_7">Ψ = arg max Ψ N l=1 log m i=1 f (x il ; ψ i ) c(F (x 1 ) . . . F (x m ); Σ) (8) Estimation of F i (x i ):</formula><p>The first step in modeling copula density is to model the individual distributions for each of our features, x i . We model each feature using a non-parametric kernel density-based method, described by:</p><formula xml:id="formula_8">f σ (x j i ) = 1 nσ n j=1 K x j i − µ σ (9)</formula><p>where K(.) is a Gaussian kernel with the bandwidth parameter σ. Using this method together with our features, we encounter two problems. First, most of the features produce extremely skewed distributions, making it hard to set the bandwidth for the Gaussian kernel. We set bandwidth parameter using Scott's rule of thumb.</p><p>Second, some of our variables are discrete ordinal. For copula functions to be useful, the probability density of u i = F (x i ) should be uniform, and for discrete-valued variables this condition is not met. In <ref type="figure">Figure 7</ref>, we demonstrate this using one of our features. The top left plot in the figure shows histogram for an original feature x i . The histogram on the right is for u i , which is the cdf values for the feature values. As we can see the histogram for u i is not uniform.</p><p>To overcome this problem, we add additive white Gaussian noise to x i . This simple transformation gives us a continuousvalued feature, given by x c i . In our formulation, we add noise to each feature value given by:</p><formula xml:id="formula_9">x c i = x i + η(0, n p )<label>(10)</label></formula><p>where n p is a variance of the Gaussian distribution η used to add noise. This value is determined by evaluating n p = Ps SN R , where SN R is the desired signal-to-noise ratio. P s is the signal power, estimated based on the distribution of all   <ref type="figure">Figure 7</ref> shows the histogram for the transformed variable x c i and the plot on the right shows the histogram for u c i . This looks closer to uniform. Why Copulas?: In <ref type="figure" target="#fig_7">Figure 8</ref> we demonstrate the efficacy of Copulas in modeling a bi-variate distribution. We took two features, plotted a scatter plot, modeled the features using a Gaussian copula with Weibull marginals and overlaid the contours for the density function. The plot on the left shows the result. On right we see the contours for a bi-variate Gaussian fitted to this data. We can see qualitatively that the joint Copula density function fits the data better. For quantitative comparisons, we evaluated the log-likelihood value to evaluate the fit. The Copula fits the data better by an order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Outlier score interpretation</head><p>The three outlier detection methods presented in the previous section assign a score that indicates each example's incongruity. Therefore, it is possible to rank all the test examples according to the score given by an individual detector. In the same way, one can select the top-k examples, or define a threshold to determine when the score is significant enough to consider the example as an outlier. Three main issues arise from such strategies:</p><p>1. Selecting the top-k examples can lead to false positives, because highly-ranked examples will not necessarily have a high outlier score. Since ranking is determined by comparing examples to each other and not by</p><p>In future we estimate the signal power value for each individual value of the feature xi separately allowing a more customized noise value their absolute score, this scenario may occur if the data has few outliers. 2. Thresholding techniques are difficult to implement because scores are not easily interpretable. For instance, joint probability density values differ by more than 50 orders of magnitude (from 10 −60 to 10 −2 ). 3. Because combining scores from different methods is not straightforward, it is also difficult to exploit the robustness of multi-algorithm outlier detection ensembles. Not only can the range of values returned by different methods be completely different, these methods can also result in opposite categorizations; in some cases, such as the matrix decomposition-based method, outliers receive the highest scores, while in other cases, such as probability density estimation methods, normal data points receive higher scores. One solution for overcoming these limitations while still taking advantage of two or more detectors is to combine ranks instead of scores <ref type="bibr" target="#b17">Zimek et al. [2014]</ref>. However, highly ranked examples will still be considered outliers, regardless of whether their absolute outlier scores are high or not. As a result, this strategy can result in a high false positive rate.</p><p>Another solution is to project all scores into the same space, ideally interpretable as probabilities. We adopt this last strategy; however, as we explain in the following subsection, it comes with its own challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Transforming outlier scores into probabilities</head><p>We model matrix decomposition-based outlier scores with a Weibull distribution, which is flexible, and can model a wide variety of shapes. For a given score S, the outlier probability corresponds to the cumulative density function evaluated in S: F (S) = P (X ≤ S). The exact same technique can be applied to the replicator neural networks scores. Figure 9: For one-day's worth of data, these plots show the histograms for the outlier scores from the three methods and the histogram of the combined score. These scores for each method are after the series of transformations performed on their raw score.</p><p>Joint probability densities require an additional step, because the scores span different orders of magnitude. As a result, most of the information is lost. To mitigate this loss, we first compute the negative logarithm of the scores, and then shift the distribution to have positive support. Once this transformation is performed, we model the transformed scores with a Weibull distribution and determine the outlier probability for each example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Outlier detection ensembles</head><p>Multi-algorithm ensembles are combinations of predictions from different machine learning models. This strategy improves robustness by compensating for the individual biases of models in the ensemble. In this case, we average outlier probabilities obtained separately by each of the methods. Each example must be highly scored by all methods in order to be highly ranked and shown to the end user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Active Model Synthesis</head><p>This system is meant to continually identify new and evolving attacks with the help of an analyst, and to use these identifications to synthesize new models that can predict attacks without the analyst, using behavioral descriptors. For this, we designed a closed-loop system that entwines analyst intuition with machine intelligence.</p><p>We present an outline of the Active Model Synthesisframework in <ref type="figure" target="#fig_0">Figure 10</ref>.</p><p>The algorithm has three phases-TRAINING, DEPLOYMENT and FEEDBACK COLLEC-TION/UPDATING-and cycles through these phases daily. The entity-feature matrix and the labeled data serve as the algorithm's inputs. In an everyday workflow, the system trains unsupervised and supervised models, applies these models to that day's incoming data, identifies k entities as extreme events or attacks, and brings them and their data to the analysts' attention. The analysts then use an interface to sort through these rare events and pick out which could truly be attacks. Finally, we use the analysts' deductions to build a new predictive model for the next day.</p><p>The key advantages of this system are:</p><p>• Overcomes limited analyst bandwidth: The number of events an analyst can feasibly examine is a tiny fraction of the overall event volume, about 10 −5 %. To select these events, we rely on the accurate, robust and multimethod outlier detection system presented in Section 6. We update our models daily and use them the next day, as presented in <ref type="figure" target="#fig_0">Figure 10</ref>. • Overcomes weaknesses of unsupervised learning: One of the key intuitions driving our system is that an event's rarity (or its status as an outlier) does not constitute maliciousness, and that an event's score does not capture the intent behind it. If we consider all top k events as malicious, then a simple threshold-based detector would be enough to diagnose them. A non-linear model enables us to imitate analysts' subjective assessment of the events. • Actively adapts and synthesizes new models: Analyst feedback delivers labeled data on a daily basis. This increases the training data available to the system, allowing us to change models on a daily basis. This setup captures the cascading effect of the human-machine interaction: the more attacks the predictive system detects, the more feedback it will receive from the analysts; this feedback, in turn, will improve the accuracy of future predictions. Therefore, as time progresses and the systems absorb the analysts' feedback, we expect to see clear improvement in the detection rate. In addition, we allow the analysts to sort the attacks into multiple categories, enabling us to build custom models for different attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experimental setup</head><p>To validate our platform, we experimented with a real-world data set, with reported attacks introduced in Section 4. The experiments performed were designed to show how the analyst's feedback improved the threat-detection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Types of attacks</head><p>To illustrate the variety of threats that may compromise enterprise platforms, we describe the behaviors involved in three representative attacks.</p><p>• Account takeover attacks: Account takeover attacks generally consist of two steps. First, attackers will try to access a given website using many user/password pairs from a reduced number of IP addresses. At this stage, the bad actors will figure out which user credentials are active, but will perform few or no checkouts. Note that this step can be detected by looking at elevated numbers of login attempts originating from the same IP; however, strictly speaking, no fraud has yet been committed.</p><p>APPLY MODELS: Given the entity-feature matrix M t at time t, deploy and execute the models U t−1 and S t−1 : STEP 1A: Apply the unsupervised model U t−1 to M t and generate scores given by P. STEP 1B: Apply the supervised model S t to M t and generate scores given by Z.</p><p>SELECT k ENTITIES: Given analyst bandwidth k, scores from unsupervised model, P, and the supervised model Z: STEP 2A: Select top k 2 entities based on the score P. STEP 2B: Select top k 2 entities based on the score Z. COLLECT FEEDBACK AND UPDATE: Show the k entities to the analyst and collect feedback: STEP 3: For each of the k entities collect analyst defined labels and add D k it to the labeled training data D t = D k ∪D t−1 TRAINING: Given the entity-feature matrix M t at time t, and labeled data D t : STEP 4A: Train an unsupervised model U t using M t . The unsupervised training includes multiple modeling techniques and an ensembling method described in Section 6. STEP 4B: If labeled data, D t , is available, train a supervised model S t using a random forest classifier. At a later time, the same or a different bad actor will access the site with stolen "validated" credentials, and perform transactions using the credit cards associated with these accounts. In this case, to avoid raising suspicions, attackers will generally use a single user per IP address.</p><p>• New account fraud: In a new account fraud, a bad actor gains access to a stolen credit card and creates a new account using the credit card owner's personal information. Once the account is created, the bad actor performs transactions with the stolen credit card. • Terms of service abuse: This category covers fraudulent violations of the terms of service agreement. Frauds of this sort have very distinct signatures. Two simple examples are the abusive use of promotional codes, or deleting the web browser's cookies to participate more times than allowed in an online voting platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Analyzing the impact of the analysts' feedback</head><p>We compare the fraud detection rate obtained with a purely unsupervised outlier analysis approach to the Active Model Synthesis strategy explained in Section 7.</p><p>In some scenarios, we may have access to labeled data from the past, even before the rare event detection system is deployed. We call these labels historic labels. We introduce an additional parameter, d ∈ {0, 28} to represent the number of days for which we have (albeit incomplete or noisy) labeled examples. For each strategy, we report the total number of detected attacks on a monthly basis, the recall, and the area under the receiver operating characteristic curve (AUC) of the deployed classifier. <ref type="figure" target="#fig_0">Figure 11</ref> shows the detection rates achieved with userbased features, where the analyst has a fixed daily bandwidth of k = 100 incident investigations. The following observations are worth noting:</p><p>• The Active Model Synthesis setups beat fully unsupervised outlier detection by a large margin. Over the 12 weeks of the simulation, the outlier detection approach caught a total of 42 attacks, while the Active Model Synthesis setups with d=0 and d=28 detected 143 and 211 attacks respectively, out of a total of 318 attacks successfully linked to individual users. week is almost identical between the three Active Model Synthesis setups. In the case of d=0, the AUC of the classifier in the final week reaches 0.940, while the setup considering d=28 reaches 0.946. Based on these results, we present the following key findings:</p><p>• Over the course of three months, our system with d=0, with no initial labeled examples, increased the attack detection rate by 3.41×, compared to state-of-the-art unsupervised outlier detection. • Our platform reduces the number of false positives with respect to state-of-the-art unsupervised outlier analysis. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, once the system is trained, we achieve a recall of 0.868 with k (analyst bandwidth) set to 200, whereas the unsupervised-ML approach achieves 0.737, even when the analyst is shown 1000 entities a day. This observation indicates a simultaneous increase of the attack detection rate and a fivefold false positive reduction; therefore, our system improves the analyst's efficiency and mitigates alarm fatigue issues. • The system learns to defend against unseen attacks and can be bootstrapped without labeled data. Given enough interactions with the analyst, the system reaches a performance similar to that obtained when historic attack examples are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We present an end-to-end system that combines analyst intelligence with state-of-the-art machine learning techniques to detect new attacks and reduce the time elapsed between attack detection and successful prevention. The system presents four key features: a big data behavioral analytics platform,  <ref type="table">Table 2</ref>: False positive rate (FPr) and true positive rate (TPr) of the Active Model Synthesisand unsupervised outlier detection over the 12 weeks of deployment. We report the FPr and TPr of the compared approaches for different daily investigation budgets (k ∈ 50, 100, 200, 500, 1000) an ensemble of outlier detection methods, a mechanism for obtaining feedback from security analysts, and a supervised learning module. We validate our platform with a real-world data set consisting of 3.6 billion log lines. The results show that the system learns to defend against unseen attacks: as time progresses and feedback is collected, the detection rate shows an increasing trend, improving by 3.41× with respect to a state-of-the-art unsupervised anomaly detector, and reducing false positives by more than 5×.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Recall versus bandwidth and recall versus false positive rate of the Active Model Synthesisand unsupervised outlier analysis after 3 months of deployment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Matrix Decomposition outlier detection method: the original variables are projected into the top j principal components space. The dataset is reconstructed with the inverse projection, and the reconstruction error is used as the outlier score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>RNN composed of three intermediate layers. The original variables are compressed via non-linear transformation in the first layers and then decompressed to reconstruct the inputs. The reconstruction error is used as outlier score. tan-sigmoid transfer function is used as an activation function across the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>of the sample = −64194</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>The left panel shows the contour lines of the log PDF of a joint Gaussian copula probability density model with Weibull marginals. The right panel shows the contour lines of a bi-variate normal fitted to the data. Qualitatively, it is clear that the copula model is better; as is evident from the contour lines spanning the entire data. Quantitatively, the log-likelihood of the multivariate normal model is one order of magnitude smaller at -64194 when compared to -6539 achieved by Copula based model. values for the feature x i 2 . For most of our features, the SN R value is set to 20. The bottom left plot in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>ACTIVE MODEL SYNTHESIS algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>•</head><label></label><figDesc>The detection rate of the Active Model Synthesis setups with d=0 and d=28 increases over time, reaching 0.500 and 0.604 respectively at the 12 th and final week of the simulation.• The performance of the classifiers at the end of the 12 th</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>User-based analysis: we report the number of detected attacks, recall rate, and AUC of the three compared approaches: outlier detection, Active Model Synthesiswith d = 0, and Active Model Synthesiswith d = 28</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Aggarwal [2013a]. Our platform integrates outlier detection methods based on</figDesc><table><row><cell cols="2">Unsupervised Learning</cell><cell></cell><cell></cell></row><row><cell>Features</cell><cell>m</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Ranking and selection</cell></row><row><cell>n</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Supervised Learning</cell><cell></cell><cell></cell><cell></cell><cell>Feedback</cell></row><row><cell cols="2">Historic labeled data</cell><cell>ID 22 3069 ..</cell><cell>Attack Label</cell><cell>Labeled examples for learning</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Outlier detection by active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="504" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Outlier ensembles: Position paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal ; Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2013-04" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Outlier Analysis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
		<idno>15:1-15:58</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Converting output scores from outlier detection algorithms into probability estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang-Ning</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Data Mining, ICDM &apos;06</title>
		<meeting>the Sixth International Conference on Data Mining, ICDM &apos;06<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Victoria Hodge and Jim Austin. A survey of outlier detection methodologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Warehousing and Knowledge Discovery</title>
		<editor>Yahiko Kambayashi, Werner Winiwarter, and Masatoshi Arikawa</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002-10" />
			<biblScope unit="volume">2454</biblScope>
			<biblScope unit="page" from="85" to="126" />
		</imprint>
	</monogr>
	<note>Artif. Intell. Rev.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Decision-making with heterogeneous sensors-a copula based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Iyengar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">PhD Dissertation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpreting and unifying outlier scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh SIAM International Conference on Data Mining, SDM 2011</title>
		<meeting>the Eleventh SIAM International Conference on Data Mining, SDM 2011<address><addrLine>Mesa, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Heterogeneous uncertainty sampling for supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh international conference on machine learning</title>
		<meeting>the eleventh international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning outlier ensembles: The best of both worldssupervised and unsupervised</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbora</forename><surname>Micenková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Assent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;14 Workshops: Outlier Detection and Description (ODDˆ2)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Active learning for anomaly and rare-category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nonlinear PCA: a new hierarchical approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Vigário</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Symposium on Artificial Neural Networks (ESANN)</title>
		<meeting>the 10th European Symposium on Artificial Neural Networks (ESANN)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonlinear principal component analysis: Neural network models and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Fraunholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Selbig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principal Manifolds for Data Visualization and Dimension Reduction</title>
		<editor>AlexanderN. Gorban, Balázs Kégl, DonaldC. Wunsch, and AndreiY. Zinovyev</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="44" to="67" />
		</imprint>
	</monogr>
	<note>Heidelberg</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On evaluation of outlier rankings and outlier scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remigius</forename><surname>Wojdanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth SIAM International Conference on Data Mining</title>
		<meeting>the Twelfth SIAM International Conference on Data Mining<address><addrLine>Anaheim, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1047" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting insider threats in a real corporate database of computer usage activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><forename type="middle">E</forename><surname>Senator</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><forename type="middle">G</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Memory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Reardon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmond</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duen Horng</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oded</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oguz</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><surname>Zakrzewska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erica</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iv</forename><forename type="middle">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mappus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lora</forename><surname>Mccoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;13</title>
		<editor>Weng-Keen Wong, Shubhomoy Das, Andrew Emmott, Jed Irvine, Jay-Yoon Lee, Danai Koutra, Christos Faloutsos, Daniel Corkill, Lisa Friedland, Amanda Gentzel, and David Jensen</editor>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Query by committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT &apos;92</title>
		<meeting>the Fifth Annual Workshop on Computational Learning Theory, COLT &apos;92<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel anomaly detection scheme based on principal component classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Ling</forename><surname>Shyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Ching Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanoksri</forename><surname>Sarinnapakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Foundations and New Directions of Data Mining Workshop, in conjunction with the Third IEEE International Conference on Data Mining (ICDM&apos;03</title>
		<meeting>the IEEE Foundations and New Directions of Data Mining Workshop, in conjunction with the Third IEEE International Conference on Data Mining (ICDM&apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="172" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Detecting stealthy malware using behavioral features in network traffic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ting-Fang Yen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ensembles for unsupervised outlier detection: Challenges and research questions a position paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><forename type="middle">J G B</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
