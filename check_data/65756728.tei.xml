<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple Model-based Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2001-11-02">November 2, 2001</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Doya</surname></persName>
							<email>doya@atr.co.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Samejima</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Information Science Laboratories</orgName>
								<address>
									<addrLine>ATR International 2-2-2 Hikaridai</addrLine>
									<postCode>619-0288</postCode>
									<settlement>Seika, Soraku</settlement>
									<region>Kyoto</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Katagiri</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuo</forename><surname>Kawato</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">CREST, Japan Science and Technology Corporation Kawato Dynamic Brain Project, ERATO Japan Science and Technology Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Science and Technology</orgName>
								<orgName type="laboratory">ATR Human Information Processing Research Laboratories Nara</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple Model-based Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2001-11-02">November 2, 2001</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We propose a modular reinforcement learning architecture for non-linear, nonstationary control tasks, which we call multiple model-based reinforcement learning (MMRL). The basic idea is to decompose a complex task into multiple domains in space and time based on the predictability of the environmental dynamics. The</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>system is composed of multiple modules, each of which consists of a state prediction model and a reinforcement learning controller. The "responsibility signal," which is given by the softmax function of the prediction errors, is used to weight the outputs of multiple modules as well as to gate the learning of the prediction models and the reinforcement learning controllers. We formulate MMRL for both discrete-time, finite state case and continuous-time, continuous state case.</p><p>The performance of MMRL was demonstrated for discrete case in a non-stationary hunting task in a grid world and for continuous case in a non-linear, non-stationary control task of swinging up a pendulum with variable physical parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A big issue in the application of reinforcement learning (RL) to real-world control problems is how to deal with non-linearity and non-stationarity. For a nonlinear, high-dimensional system, the conventional discretizing approach necessitates a huge number of states, which makes learning very slow. Standard RL algorithms can perform badly when the environment is non-stationary, or has hidden states. These problems have motivated the introduction of modular or hierarchical RL architectures <ref type="bibr" target="#b18">(Singh 1992;</ref><ref type="bibr" target="#b3">Dayan and Hinton 1993;</ref><ref type="bibr" target="#b12">Littman et al. 1995;</ref><ref type="bibr" target="#b23">Wiering and Schmidhuber 1998;</ref><ref type="bibr" target="#b15">Parr and Russel 1998;</ref><ref type="bibr" target="#b19">Sutton et al. 1999;</ref><ref type="bibr" target="#b13">Morimoto and Doya 2001)</ref>. The basic problem in modular or hierarchical RL is how to decompose a complex task into simpler subtasks. This paper presents a new RL architecture based on multiple modules, each of which is composed of a state prediction model and a RL controller. With this architecture, a non-linear and/or non-stationary control task is decomposed in space and time based on the local predictability of the environmental dynamics.</p><p>The "mixture of experts" architecture <ref type="bibr" target="#b11">(Jacobs et al. 1991)</ref> has previously been applied to non-linear or non-stationary control tasks <ref type="bibr" target="#b6">(Gomi and Kawato 1993;</ref><ref type="bibr" target="#b2">Cacciatore and Nowlan 1994</ref>). However, the success of such modular architecture depends strongly on the capability of the gating network to decide which of the given modules should be recruited at any particular moment.</p><p>An alternative approach is to provide each of the experts with a prediction model of the environment and to utilize the prediction errors for the selection of the controllers. In <ref type="bibr" target="#b14">Narendra et al. (1995)</ref>, the model that makes the smallest prediction error among a fixed set of prediction models is selected, and its associated single controller is used for control. However, when the prediction models are to be trained with little prior knowledge, task decomposition is initially far from optimal. Thus the use of 'hard' competition can lead to sub-optimal task decomposition.</p><p>Based on the Bayesian statistical framework, <ref type="bibr" target="#b16">Pawelzik et al. (1996)</ref> proposed the use of annealing in a 'soft' competition network for time series prediction and segmentation. A similar mechanism is used by <ref type="bibr" target="#b22">Tani and Nolfi (1999)</ref> for hierarchical sequence prediction. The use of the softmax function for module selection and combination was originally proposed for a tracking control paradigm as the "Multiple Paired Forward-Inverse Models (MPFIM)" <ref type="bibr" target="#b7">Haruno et al. 1999)</ref>. It was recently reformulated as "MOdular Selection and Identification for Control (MOSAIC)" <ref type="bibr" target="#b24">(Wolpert and Ghahramani 2000)</ref>.</p><p>In this paper, we apply the idea of a softmax selection of modules to the paradigm of reinforcement learning. The resulting learning architecture, which we call "Multiple Model-based Reinforcement Learning (MMRL)," learns to de-compose a non-linear and/or non-stationary task through the competition and cooperation of multiple prediction models and reinforcement learning controllers.</p><p>In the following sections, we first formulate the basic MMRL architecture (Section 2) and then describe its implementation in discrete-time and continuous-time cases, including "Multiple Linear Quadratic Controllers (MLQC)" (Section 3). We first test the performance of the MMRL architecture for the discrete case in a hunting task with multiple preys in a grid world (Section 4). We also demonstrate the performance of MMRL for continuous case in a non-linear, non-stationary control task of swinging up a pendulum with variable physical parameters (Section 5). The basic idea of this modular architecture is to decompose a non-linear and/or non-stationary task into multiple domains in space and time so that within each of the domains the environmental dynamics is well predictable. The action output of the RL controllers as well as the learning rates of both the predictors and the controllers are weighted by the "responsibility signal," which is a Gaussian softmax function of the errors in the outputs of the prediction models. The advantage of this module selection mechanism is that the areas of specialization of the modules are determined in a bottom-up fashion based on the nature of the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multiple Model-based Reinforcement Learning</head><p>Furthermore, for each area of module specialization, the design of the control strategy is facilitated by the availability of the local model of the environmental dynamics.</p><p>In the following, we consider a discrete-time, finite state environment</p><formula xml:id="formula_0">P (x(t)|x(t − 1), u(t − 1)) = F (x(t), x(t − 1), u(t − 1)), (t = 1, 2, ...),<label>(1)</label></formula><p>where x ∈ {1, ..., N} and u ∈ {1, ..., M} are discrete states and actions, and a continuous-time, continuous-state environmenṫ</p><formula xml:id="formula_1">x(t) = f (x(t), u(t)) + ν(t), (t ∈ [0, ∞)),<label>(2)</label></formula><p>where x ∈ R N and u ∈ R M are state and action vectors, and ν ∈ R N is noise.</p><p>Actions are given by a policy, either a stochastic one</p><formula xml:id="formula_2">P (u(t)|x(t)) = G(u(t), x(t))<label>(3)</label></formula><p>or a deterministic one</p><formula xml:id="formula_3">u(t) = g(x(t)).<label>(4)</label></formula><p>The reward r(t) is given as a function of the state x(t) and the action u(t). The goal of reinforcement learning is to improve the policy so that more rewards are acquired in a long run. The basic strategy of reinforcement learning is to estimate cumulative future reward under the current policy as the "value function" V (x) for each state and then to improve the policy based on the value function. We define the value function of the state x(t) under the current policy as</p><formula xml:id="formula_4">V (x(t)) = E ∞ k=0 γ k r(t + k)<label>(5)</label></formula><p>in discrete case <ref type="bibr" target="#b21">(Sutton and Barto 1998)</ref> and</p><formula xml:id="formula_5">V (x(t)) = E ∞ 0 e − s τ r(t + s)ds<label>(6)</label></formula><p>in continuous case <ref type="bibr" target="#b5">(Doya 2000)</ref>, where 0 ≤ γ ≤ 1 and 0 &lt; τ are the parameters for discounting future reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Responsibility Signal</head><p>The purpose of the prediction model in each module is to predict the next state (discrete-time) or the temporal derivative of the state (continuous-time) based on the observation of the state and the action. The responsibility signal λ i (t) <ref type="bibr" target="#b7">Haruno et al. 1999)</ref> is given by the relative goodness of predictions of multiple prediction models.</p><p>For a unified description, we denote the new state in the discrete case as</p><formula xml:id="formula_6">y(t) = x(t)<label>(7)</label></formula><p>and the temporal derivative of the state in the continuous case as</p><formula xml:id="formula_7">y(t) =ẋ(t).<label>(8)</label></formula><p>The basic formula for the responsibility signal is given by the Bayes rule</p><formula xml:id="formula_8">λ i (t) = P (i|y(t)) = P (i)P (y(t)|i) n j=1 P (j)P (y(t)|j) ,<label>(9)</label></formula><p>where P (i) is the prior probability of selecting module i and P (y(t)|i) is the likelihood of model i given the observation y(t).</p><p>In the discrete case, the prediction model gives the probability distribution of the new statex(t) based on the previous state x(t − 1) and the action u(t − 1) as</p><formula xml:id="formula_9">P (x(t)|x(t − 1), u(t − 1)) = F i (x(t), x(t − 1), u(t − 1)) (i = 1, ...,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n).(10)</head><p>If there is no prior knowledge about module selection, we take the priors as uniform (P (i) = 1/n) and then the responsibility signal is given by</p><formula xml:id="formula_10">λ i (t) = F i (x(t), x(t − 1), u(t − 1)) n j=1 F j (x(t), x(t − 1), u(t − 1)) ,<label>(11)</label></formula><p>where x(t) is the newly observed state.</p><p>In the continuous case, the prediction model gives the temporal derivative of the statê</p><formula xml:id="formula_11">x i (t) = f i (x(t), u(t))<label>(12)</label></formula><p>By assuming that the prediction error is Gaussian with variance σ 2 , the responsibility signal is given by the Gaussian softmax function</p><formula xml:id="formula_12">λ i (t) = e − 1 2σ 2 ||ẋ(t)−x i (t)|| 2 n j=1 e − 1 2σ 2 ||ẋ(t)−x i (t)|| 2 ,<label>(13)</label></formula><p>whereẋ(t) is the actually observed state change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Module Weighting by Responsibility Signal</head><p>In the MMRL architecture, the responsibility signal λ i (t) is used for four purposes: 1) weighting the state prediction outputs; 2) gating the learning of prediction models; 3) weighting the action outputs; and 4) gating the learning of reinforcement learning controller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) State prediction:</head><p>The outputs of the prediction models are weighted by the responsibility signal λ i (t). In the discrete case, the prediction of the next state is given by</p><formula xml:id="formula_13">P (x(t)) = n i=1 λ i (t)F i (x(t), x(t − 1), u(t − 1)).<label>(14)</label></formula><p>In the continuous case, the predicted state derivative is given bŷ</p><formula xml:id="formula_14">x(t) = n i=1 λ i (t)x i (t).<label>(15)</label></formula><p>These predictions are used in model-based RL algorithms and also for the annealing of σ as described later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Prediction model learning:</head><p>The responsibility signal λ i (t) is also used for weighting the parameter update of the prediction models. In general, it is realized by scaling the error signal of prediction model learning by λ i (t).</p><p>3) Action output: The outputs of reinforcement learning controllers are linearly weighted by λ i (t) to make the action output. In the discrete case, the probability of taking an action u(t) is given by</p><formula xml:id="formula_15">P (u(t)) = n i=1 λ i (t)G i (u(t), x(t)).<label>(16)</label></formula><p>In the continuous case, the output is given by the interpolation of modular outputs</p><formula xml:id="formula_16">u(t) = n i=1 λ i (t)u i (t) = n i=1 λ i (t)g i (x(t)).<label>(17)</label></formula><p>4) Reinforcement learning: λ i (t) is also used for weighting the learning of the RL controllers. The actual equation for the parameter update varies with the choice of the RL algorithms, which are detailed in the next section. When a temporal difference (TD) algorithm <ref type="bibr" target="#b0">(Barto et al. 1983;</ref><ref type="bibr" target="#b20">Sutton 1988;</ref><ref type="bibr" target="#b5">Doya 2000)</ref> is used, the TD error,</p><formula xml:id="formula_17">δ(t) = r(t) + γV (x(t + 1)) − V (x(t))<label>(18)</label></formula><p>in the discrete case and</p><formula xml:id="formula_18">δ(t) =r(t) − 1 τ V (t) +V (t) ( 1 9 )</formula><p>in the continuos case, is weighted by the responsibility signal</p><formula xml:id="formula_19">δ i (t) = λ i (t)δ(t) ( 2 0 )</formula><p>for learning of the i-th RL controller.</p><p>Using the same weighting factor λ i (t) for training the prediction models and the RL controllers helps each RL controller to learn an appropriate policy and its value function for the context under which its paired prediction model makes valid predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Responsibility predictors</head><p>When there is some prior knowledge or belief about module selection, we incorporate the "responsibility predictors" <ref type="bibr" target="#b7">Haruno et al. 1999</ref>). By assuming their outputsλ i (t) are proportional to the prior probability of module selection, from (9), the responsibility signal is given by</p><formula xml:id="formula_20">λ i (t) =λ i (t)P (y(t)|i) n j=1λ j (t)P (y(t)|j) .<label>(21)</label></formula><p>In modular decomposition of a task, it is desired that modules do not switch too frequently. This can be enforced by incorporating responsibility priors based on the assumption of temporal continuity and spatial locality of module activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Temporal continuity</head><p>The continuity of module selection is incorporated by taking the previous responsibility signal as the responsibility prediction signal. In the discrete case, we take the responsibility prediction based on the previous responsibilitŷ</p><formula xml:id="formula_21">λ i (t) = λ i (t − 1) α ,<label>(22)</label></formula><p>where 0 &lt; α &lt; 1 is a parameter that controls the strength of the memory effect.</p><p>From <ref type="formula" target="#formula_20">21</ref>and <ref type="formula" target="#formula_21">22</ref>, the responsibility signal at time t is given by the product of likelihoods of past module selection</p><formula xml:id="formula_22">λ i (t) = 1 Z(t) t k=0 P (x(t − k)|i) α k ,<label>(23)</label></formula><p>where Z(t) denotes the normalizing factor, i.e.,</p><formula xml:id="formula_23">Z(t) = n j=1 t k=0 P (x(t − k)|j) α k .</formula><p>In the continuous case, we choose the prior</p><formula xml:id="formula_24">λ i (t) = λ i (t − ∆t) ∆tα ∆t ,<label>(24)</label></formula><p>where ∆t is an arbitrarily small time difference (note (24) coincides with (22) with ∆t = 1).</p><p>Since the likelihood of the module i is given by the Gaussian P (ẋ(t)|i) =</p><formula xml:id="formula_25">e − 1 2σ 2 ||ẋ(t)−x i (t)|| 2</formula><p>, from recursion as in (23), the responsibility signal at time t is given by</p><formula xml:id="formula_26">λ i (t) = 1 Z(t) t/∆t k=0 P (ẋ(t − k∆t)|i) ∆tα k∆t = 1 Z(t) e − 1 2σ 2 ∆t t/∆t k=0 ||ẋ(t−k∆t)−x i (t−k∆t)|| 2 α k∆t ,<label>(25)</label></formula><p>that is, a Gaussian softmax function of temporally weighted squared errors. In the limit of ∆t → 0, (25) can be represented as</p><formula xml:id="formula_27">λ i (t) = e − 1 2σ 2 E i (t) n j=1 e − 1 2σ 2 E j (t)<label>(26)</label></formula><p>where E i (t) is a low-pass filtered prediction erroṙ</p><formula xml:id="formula_28">E i (t) = log α E i (t) + ||ẋ(t) −x i (t)|| 2 .<label>(27)</label></formula><p>The use of this low-pass filtered prediction errors for responsibility prediction is helpful in avoiding chattering of the responsibility signal <ref type="bibr" target="#b16">(Pawelzik et al. 1996)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Spatial locality</head><p>In the continuous case, we consider a Gaussian spatial prior</p><formula xml:id="formula_29">λ i (t) = e − 1 (x(t)−c i ) M −1 i (x(t)−c i ) n j=1 e − 1 (x(t)−c j ) M −1 j (x(t)−c j ) ,<label>(28)</label></formula><p>where c i is the center of the area of specialization, M i is a covariance matrix that specifies the shape, and denotes transpose. These parameters are updated so that they approximate the distribution of the input state x(t) weighted by the responsibility signal, namely,</p><formula xml:id="formula_30">c i = η c λ i (t)(−c i + x(t)),<label>(29)</label></formula><formula xml:id="formula_31">M i = η M λ i (t)[−M i + (x(t) − c i )(x(t) − c i ) ],<label>(30)</label></formula><p>where η c and η M are update rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation of MMRL Architecture</head><p>For the RL controllers, it is generally possible to use model-free RL algorithms, such as actor-critic and Q-learning. However, because the prediction models of the environmental dynamics are intrinsic components of the architecture, it is advantageous to utilize these prediction models not just for module selection but also for designing RL controllers. In the following, we describe the use of model-based RL algorithms for discrete-time and continuous-time cases. One special implementation for continuous-time is the use of multiple "linear quadratic controllers"</p><p>derived from linear dynamic models and quadratic reward models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discrete-time MMRL</head><p>Now we consider implementation of the MMRL architecture for discrete-time, finite state and action problems. The standard way of utilizing a predictive model in RL is to use it for action selection by the one-step search</p><formula xml:id="formula_32">u(t) = arg max u E[r(x(t), u) + γV (x(t + 1))],<label>(31)</label></formula><p>wherer(x(t), u) is the predicted immediate reward andx(t + 1) is the next state predicted from the current state x(t) and a candidate action u.</p><p>In order to implement this algorithm, we provide each module with a reward</p><formula xml:id="formula_33">modelr i (x, u), a value function V i (x)</formula><p>, and a dynamic model F i <ref type="bibr">(x, x, u)</ref>. Each candidate action u is then evaluated by</p><formula xml:id="formula_34">q(x(t), u) = E[r(x(t), u) + γV (x(t + 1))|u] = n i=1 λ i (t)[r i (x(t), u) + γ N x=1 V i (x)F i (x, x(t), u)]].<label>(32)</label></formula><p>For the sake of exploration, we use a stochastic version of the greedy action selection (31), where the action u(t) is selected by a Gibbs distribution</p><formula xml:id="formula_35">P (u|x(t)) = e βq(x(t),u) M u =1 e βq(x(t),u ) ,<label>(33)</label></formula><p>where β controls the stochasticity of action selection.</p><p>The parameters are updated by the error signals weighted by the responsibility <ref type="bibr">..., N; c(j, x)</ref> = 1 if j = x and zero otherwise), λ</p><formula xml:id="formula_36">signal, namely, λ i (t)(F i (j, x(t − 1), u(t − 1)) − c(j, x(t))) for the dynamic model (j = 1,</formula><formula xml:id="formula_37">i (t)(r i (x(t), u(t)) − r(t))</formula><p>for the reward model, and λ i (t)δ(t) for the value function model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Continuous-time MMRL</head><p>Next we consider a continuous-time MMRL architecture. A model-based RL algorithm for a continuous-time, continuous-state system (2) is derived from the</p><formula xml:id="formula_38">Hamilton-Jacobi-Bellman (HJB) equation τ V (x(t)) = max u r(x(t), u) + ∂V (x(t)) ∂x f (x(t), u) ,<label>(34)</label></formula><p>where τ is the time constant of reward discount <ref type="bibr" target="#b5">(Doya 2000)</ref>. Under the assumptions that the system is linear with respect to the action and the action cost is convex, a greedy policy is given by</p><formula xml:id="formula_39">u = g ∂f (x, u) ∂u ∂V (x) ∂x ,<label>(35)</label></formula><p>where <ref type="bibr">∂V (x)</ref> ∂x is a vector representing the steepest ascent direction of the value function, <ref type="bibr">∂f(x,u)</ref> ∂u is a matrix representing the input gain of the dynamics, and g is a sigmoid function whose shape is determined by the control cost <ref type="bibr" target="#b5">(Doya 2000)</ref>.</p><p>To implement the HJB based algorithm, we provide each module with a dynamic model f i (x, u) and a value model V i (x). The outputs of the dynamic models (12) are compared with the actually observed state dynamicsẋ(t) to calculate the responsibility signal λ i (t) according to (13).</p><p>The model outputs are linearly weighted by λ i (t) for state predictioṅ</p><formula xml:id="formula_40">x(t) = n i=1 λ i (t)f i (x(t), u(t)),<label>(36)</label></formula><p>and value function estimation</p><formula xml:id="formula_41">V (x) = n i=1 λ i (t)V i (x).<label>(37)</label></formula><p>The derivatives of the dynamic models ∂f i (x,u) ∂u and value models ∂V i (x) ∂x are used to calculate the action for each module</p><formula xml:id="formula_42">u i (t) = g ∂f i (x, u) ∂u ∂V i (x) ∂x x(t) .<label>(38)</label></formula><p>They are then weighted by λ i (t) according to (17) to make the actual action u(t).</p><p>Learning is based on the weighted prediction errors λ i (t)(x i (t) −ẋ(t)) for dynamic models and λ i (t)δ(t) for value function models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multiple Linear Quadratic Controllers</head><p>In a modular architecture like the MMRL, the use of universal non-linear function approximators with large numbers of degrees-of-freedom can be problematic because it can lead to an undesired solution in which a single module tries to handle most of the task domain. The use of linear models for the prediction models and the controllers is a reasonable choice because local linear models have been shown to have good properties of quick learning and good generalization <ref type="bibr" target="#b17">(Schaal and Atkeson 1996)</ref>. Furthermore, if the reward function is locally approximated by a quadratic function, then we can use a linear quadratic controller (see, e.g., <ref type="bibr" target="#b1">Bertsekas 1995)</ref> for the RL controller design.</p><p>We use a local linear dynamic model</p><formula xml:id="formula_43">x(t) = A i (x(t) − x d i ) + B i u(t) ( 3 9 )</formula><p>and a local quadratic reward model</p><formula xml:id="formula_44">r(x(t), u(t)) = r 0 i − 1 2 (x(t) − x r i ) Q i (x(t) − x r i ) − 1 u (t)R i u(t) ( 4 0 )</formula><p>for each module, where </p><formula xml:id="formula_45">x d i , x r i is</formula><formula xml:id="formula_46">V i (x) = v 0 i − 1 (x − x v i ) P i (x − x v i ) ( 4 1 )</formula><p>The matrix P i is given by solving the Riccati equation</p><formula xml:id="formula_47">= 1 τ P i − P i A i − A i P i + P i B i R −1 i B i P i − Q i .<label>(42)</label></formula><p>The center x v i and the bias v 0 i of the value function is given by</p><formula xml:id="formula_48">x v i = (Q i + P i A i ) −1 (Q i x r i + P i A i x d i ),<label>(43)</label></formula><formula xml:id="formula_49">1 τ v 0 i = r 0 i − 1 (x v i − x r i ) Q i (x v i − x r i ) ( 4 4 )</formula><p>Then the optimal feedback control for each module is given by the linear feedback</p><formula xml:id="formula_50">u i (t) = −R −1 i B T i P i (x(t) − x v i ) ( 4 5 )</formula><p>The action output is given by weighting these controller outputs by the responsibility signal λ i (t):</p><formula xml:id="formula_51">u(t) = n i=1 λ i (t)u i (t).<label>(46)</label></formula><p>The parameters of the local linear models A i , B i , and x d i and those of the quadratic reward models r 0 i , Q i , and R i are updated by the weighted prediction</p><formula xml:id="formula_52">errors λ i (t)(x i (t) −ẋ(t)) and λ i (t)(r i (x, u) − r(t))</formula><p>, respectively. When we assume that the update of these models is slow enough, then the Riccati equations <ref type="formula" target="#formula_47">42</ref>may be recalculated only intermittently. We call this method "Multiple Linear Quadratic Controllers" (MLQC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation: Discrete Case</head><p>In order to test the effectiveness of the MMRL architecture, we first applied the discrete MMRL architecture to a non-stationary hunting task in a grid world. The hunter agent tries to catch a prey in a 7×7 torus grid world. There are 47 states representing the position of the prey relative to the hunter. The hunter chooses one of five possible actions: { north (N), east (E), south (S), west (W), stay}. A prey moves in a fixed direction during a trial. At the beginning of each trial, one of four movement directions { NE, NW, SE, SW} is randomly selected and a prey is placed at a random position in the grid world. When the hunter catches the prey, by stepping into the same grid as the prey's, a reward r(t) = 10 is given.</p><p>Each step of movement costs r(t) = −1. A trial is terminated when the hunter catches a prey, or it fails to catch it within 100 steps.</p><p>In order to compare the performance of MMRL with conventional methods, we applied standard Q-learning and compositional Q-learning (CQ-L) <ref type="bibr" target="#b18">(Singh 1992)</ref> to the same task. A major difference between CQ-L and MMRL is the criterion for modular decomposition: CQ-L uses the consistency of the modular value functions while MMRL uses the prediction errors of dynamic models. In CQ-L, the gating network as well as component Q-learning modules are trained so that the composite Q-value well approximates the action value function of the entire problem. In the original CQ-L <ref type="bibr" target="#b18">(Singh 1992)</ref>, the output of the gating network was based on the "augmenting bit" that explicitly signaled the change in the context.</p><p>Since our goal now is to let the agent learn appropriate decomposition of the task without an explicit cue, we used a modified CQ-L (See Appendix for the details of the algorithm and the parameters). <ref type="figure" target="#fig_6">Figure 2</ref> shows the performance difference of standard Q-learning, CQ-L, and MMRL in the hunting task. The modified CQ-L did not perform significantly better than standard, flat Q-learning. Investigation of the modular Q functions of CQ-L revealed that, in most simulation runs, modules did not appropriately differentiate for four different kinds of preys. On the other hand, the performance of MMRL approached close to theoretical optimum. This was because four modules successfully specialized in one of four kinds of prey movement. <ref type="figure" target="#fig_7">Figure 3</ref> shows examples of the value functions and the prediction models learned by MMRL. From the output of the prediction models F i , it can be seen that the modules 1,2,3 and 4 were specialized for the prey moving to NE, NW, accordance with these movement directions of the prey.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Result</head><p>A possible reason for the difference in the performance of CQ-L and MMRL in this task is the difficulty of module selection. In CQ-L, when the prey is far from the hunter, the differences in discounted Q values for different kinds of preys are minor. Thus it would be difficult to differentiate modules based solely on the Q values. In MMRL, on the other hand, module selection based on the state change, in this case prey movement, is relatively easy even when the prey is far from the hunter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Simulation: Continuous Case</head><p>In order to test the effectiveness of the MMRL architecture, we first applied the MLQC algorithm described in Section 3.3 to the task of swinging up a pendulum with limited torque <ref type="figure" target="#fig_3">(Figure 4</ref>) <ref type="bibr" target="#b5">(Doya 2000)</ref>. The driving torque T is limited in [−T max , T max ] with T max &lt; mgl. The pendulum has to be swung back and forth at the bottom to build up enough momentum for a successful swing up.</p><p>The state space was 2-dimensional, i.e. x = (θ,θ) ∈ [−π, π]× R where θ is the joint angle (θ = 0 means the pendulum hanging down). The action was u = T .</p><p>The reward was given by the height of the tip and the negative squared torque</p><formula xml:id="formula_53">r(x, u) = − cos θ − 1 2 RT 2 .<label>(47)</label></formula><p>A trial was started from random joint angle θ ∈ [−π/4, π/4] with no angular velocity, and lasted for 20 seconds. We devised the following automatic annealing process for the parameter σ of the softmax function for the responsibility signal (26).</p><formula xml:id="formula_54">σ k+1 = ηaE k + (1 − η)σ k ,<label>(48)</label></formula><p>where k denotes the number of trial and E k is the average state prediction error during the k-th trial. The parameters were η = 0.25, a = 2, and the initial value set as σ 0 = 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task Decomposition in Space: Non-linear Control</head><p>We first used two modules, each of which had a linear dynamic model <ref type="formula">39</ref>  Initially, the first prediction model predicts the pendulum motion better than the second one, so the responsibility signal λ 1 becomes close to 1. Thus the output of the first RL controller u 1 , which destabilizes the bottom position, is used for control. As the pendulum is driven away from the bottom, the second prediction model predicts the movement better, so λ 2 becomes higher and the second RL controller takes over and stabilizes the upright position.  <ref type="figure" target="#fig_10">Figure 6</ref> shows the dynamic and reward models when there were eight mod-ules. Two modules were specialized for the bottom position and three modules were specialized near the top position, while two other modules were centered somewhere in between. The result shows that proper modularization is possible even when there are redundant modules. <ref type="figure" target="#fig_11">Figure 7</ref> compares the time course of learning by MLQC with two, four, and eight modules and a non-modular actor-critic <ref type="bibr" target="#b5">(Doya 2000)</ref>. Learning was fastest with two modules. The addition of redundant modules resulted in more variability in the time course of learning. This is because there were multiple possible ways of modular decomposition and due to the variability of the sample trajectories, it took longer time for modular decomposition to stabilize. Nevertheless, learning by the 8-module MLQC was still much faster than by the non-modular architecture.</p><p>An interesting feature of the MLQC strategy is that qualitatively different controllers are derived by the solutions of the Riccati equations (42). The controller at the bottom is a positive feedback controller that de-stabilizes the equilibrium where the reward is minimal, while the controller at the top is a typical linear quadratic regulator that stabilizes the upright state. Another important feature of the MLQC is that the modules were flexibly switched simply based on the prediction errors. Successful swing up was achieved without any top-down planning of the complex sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task Decomposition in Time: Non-stationary Pendulum</head><p>We then tested the effectiveness of the MMRL architecture for the both non-linear and non-stationary control task in which mass m and length l of the pendulum were changed every trial.</p><p>We used four modules, each of which had a linear dynamic model (39) and a quadratic reward model (40). The centers x i of the local linear prediction models were initially set randomly. Each trial was started from a random position with θ ∈ [−π/4, π/4] and lasted for 40 seconds.</p><p>We implemented responsibility prediction with τ c = 50, τ M = 200, and τ p = 0.1. The parameters of annealing were η = 0.1, a = 2, and an initial value of σ 0 = 10.</p><p>In the first 50 trials, the physical parameters were fixed at {m = 1.0, l = 1.0}. <ref type="figure" target="#fig_8">Figure 8(a)</ref> shows the change in the position gain ({A 21 } = ∂θ ∂θ ) of the four prediction models. The control performance is shown in <ref type="figure" target="#fig_12">Figure 8(b)</ref>. <ref type="figure" target="#fig_12">Figures   8(c,d,e</ref>) show prediction models in the section of {θ = 0, T = 0}.</p><p>Initial position gains are set randomly <ref type="figure" target="#fig_12">(Figure 8(c)</ref>). After 50 trials, module and module 2 both specialized in the bottom region (θ 0) and learned similar prediction models. Module 3 and module 4 also learned the same prediction model in the top region (θ π) <ref type="figure" target="#fig_12">(Figure 8(d)</ref>). Accordingly, the RL controllers in module 1 and module 2 learned a reward model with a minimum near (0, 0) and a destabilizing feedback policy was given by (15)-(17). Module 3 and module 4 also learned a reward model with a peak near (π, 0) and implemented a stabilizing feedback controller.</p><p>In 50 to 200 trials, the parameters of the pendulum were switched between {m = 1, l = 1.0} and {m = 0.2, l = 10.0} in each trial. At first, the degenerated modules tried to follow the alternating environment <ref type="figure" target="#fig_8">(Figure 8(a)</ref>), and thus swing up was not successful for the new, longer pendulum. The performance for the shorter pendulum was also disturbed <ref type="figure" target="#fig_12">(Figure 8(b)</ref>). After about 80 trials, the prediction models gradually specialized in either new or learned dynamics <ref type="figure">(Fig-ure</ref> 8(e)), and successful swing up was achieved both for the shorter and longer pendulums.</p><p>We found similar module specialization in six of ten simulation runs. In four other runs, due to the bias in initial module allocation, three modules were aggregated in one domain (top or bottom) and one model covered the other domain during the stationary condition. However, after 150 trials in the non-stationary condition, module specialization as shown in <ref type="figure" target="#fig_12">Figure 8</ref>(e) was achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We proposed a multiple model-based reinforcement learning (MMRL) architecture that decomposes a non-linear and/or non-stationary task in space and time based on the local predictability of the system dynamics. We tested the performance of the MMRL in both non-linear and non-stationary control tasks. It was shown in simulations of the pendulum swing up task that multiple prediction models were successfully trained and corresponding model-based controllers were derived. The modules were specialized for different domains in the state space. It was also confirmed in a nonstationary pendulum swing-up task that available modules are flexibly allocated for different domains in space and time based on the task demands.</p><p>The modular control architecture using multiple prediction models was proposed by Wolpert and Kawato as a computational model of the cerebellum . <ref type="bibr">Imamizu et al.</ref> showed in fMRI experiments of novel tool use that a large area of the cerebellum is activated initially and then a smaller area remains to be active after long training. They proposed that such local activation spots are the neural correlates of internal models of tools <ref type="bibr" target="#b10">(Imamizu et al. 2000)</ref>. They also suggested that internal models of different tools are represented in separated areas in the cerebellum <ref type="bibr" target="#b9">(Imamizu et al. 1997</ref>).</p><p>Our simulation results in a non-stationary environment can provide a computational account of these fMRI data. When a new task is introduced, many modules initially compete to learn it. However, after repetitive learning, only a subset of modules are specialized and recruited for the new task.</p><p>One might argue whether MLQC is a reinforcement learning architecture since it uses LQ controllers that were calculated off-line. However, when the linear dynamic models and quadratic reward models are learned on-line, as in our simulations, the entire system realize reinforcement learning. One limitation of MLQC architecture is that the reward function should have helpful gradients in each modular domain. A method for back-propagating the value function of the successor module as the effective reward for the predecessor module is under development.</p><p>In order to construct a hierarchical RL system, it appears necessary to combine both top-down and bottom-up approaches for task decomposition. The MMRL architecture provides one solution for the bottom-up approach. Combination of this bottom-up mechanism with a top-down mechanism is the subject of our ongoing study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Modified Compositional Q-learning</head><p>On each time step, the gating variable g i (t) is given by the prior probability of module selection, in this case from the assumption of temporal continuity (22)</p><formula xml:id="formula_55">g i (t) = λ i (t − 1) α n j=1 λ j (t − 1) α .<label>(49)</label></formula><p>The composite Q-values for state x(t) are then computed bŷ</p><formula xml:id="formula_56">Q(x(t), u) = n i=1 g i (t)Q i (x(t), u) ( 5 0 )</formula><p>and an action u(t) is selected by</p><formula xml:id="formula_57">P (u|x(t)) = e βQ (x(t), u) v∈U e βQ (x(t), v) .<label>(51)</label></formula><p>After the reward r(x(t), u(t)) is acquired and the state changes to x(t + 1), the TD error for the module i is given by</p><formula xml:id="formula_58">e i (t) = r(x(t), u(t)) + γ max uQ (x(t + 1), u) − Q i (x(t), u(t)).<label>(52)</label></formula><p>From Gaussian assumption of value prediction error, the likelihood of module i is given by</p><formula xml:id="formula_59">P (e i (t)|i) = e − 1 2σ 2 e i (t) 2<label>(53)</label></formula><p>and thus the responsibility signal, or the posterior probability for selecting module i, is given by</p><formula xml:id="formula_60">λ i (t) = g i (t)e − 1 2σ 2 e i (t) 2 j g j (t)e − 1 2σ 2 e j (t) 2 .<label>(54)</label></formula><p>The Q values of each module is updated with the weighted TD error λ i (t)e i (t) as the error signal.</p><p>The discount factor was set as γ = 0.9 and the greediness parameters as β = 1 for both MMRL and CQ-L. The decay parameter of temporal responsibility predictor was α = 0.8 for MMRL. We tried different values of α for CQ-L without a success. The value used in <ref type="figure" target="#fig_6">Figures 2 and 3</ref> was α = 0.99.           </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure Legends</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>shows the overall organization of the multiple model-based reinforcement learning (MMRL) architecture. It is composed of n modules, each of which consists of a state prediction model and a reinforcement learning controller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the center of local prediction for state and reward, respectively. The r 0 i is a bias of quadratic reward model.The value function is given by the quadratic form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and a quadratic reward model (40). The centers of the local linear dynamic models were initially placed randomly with the angular component in [−π, π].Each trial was started from a a random position of the pendulum and lasted for 30 seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>shows an example of swing up performance from the bottom position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5</head><label>5</label><figDesc>shows the changes of linear prediction models and quadratic reward models before and after learning. The two linear prediction models approximated the non-linear gravity term. The first model predicted the negative feedback acceleration around the equilibrium state with the pendulum hanging down. The second model predicted the positive feedback acceleration around the unstable equilibrium with the pendulum raised up. The two reward models also approximated the cosine reward function using parabolic curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1</head><label>1</label><figDesc>Schematic diagram of multiple model-based reinforcement learning (MMRL) architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2</head><label>2</label><figDesc>Comparison of the performance of standard Q-learning (gray line), modified CQ-L (dashed line), and MMRL (thick line) in the hunting task. The average number of steps needed for catching a prey during 200 trial epochs in 10 simulation runs are plotted. The dash-dotted line shows the theoretically minimal average steps required for catching the prey.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3</head><label>3</label><figDesc>Example of value functions and prediction models learned by MMRL after 10000 trials. Each slot in the grid shows the position of the prey relative to the hunter, which was used as the state x. (a) The state value functions V i (x). (b) The prediction model outputs F i(x, x, u), where current state x of the prey was fixed as (2, 1), shown by the circle, and the action u was fixed as "stay."Figure 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( a )</head><label>a</label><figDesc>Example of swing-up performance. Dynamics are given by ml 2θ = −mgl sin θ− µθ + T . Physical parameters are m = l = 1, g = 9.8, µ = 0.1, and T max = 5.0. (b) Trajectory from the initial state (0[rad],0.1[rad/s]). o: start, +: goal. Solid line: module 1. Dashed line: module 2. (c) Time course of the state (top), the action (middle), and the responsibility signal (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5</head><label>5</label><figDesc>Development of state and reward prediction of models. (a) and (b): outputs of state prediction models before (a) and after (b) learning. (c) and (d): outputs of the reward prediction model before (c) and after (d) learning. Solid line: module 1. Dashed line: module 2; dotted line: targets (ẍ and r). •: centers of spatial responsibility prediction c i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6</head><label>6</label><figDesc>Outputs of modules. (a) state prediction models. (b) reward prediction models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7</head><label>7</label><figDesc>Learning curves for the pendulum swing up task. The cumulative reward 20 0 r(t)dt during each trial is shown for five simulation runs. (a) n=2 modules. (b) 4 modules. (c) 8 modules. (d) non-modular architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8</head><label>8</label><figDesc>Time course of learning and changes of the prediction models. (a) Changes of a coefficient A 21 = ∂θ ∂θ of the four prediction models. coefficient with angle. (b) Change of average reward during each trial. Thin lines: results of 10 simulation runs. Thick line: average to 10 simulation runs. Note that the average reward with the new, longer pendulum was lower even after successful learning because of its longer period of swinging. (c), (d), and (e): Linear prediction models in the section of {θ = 0, T = 0} before learning (c), after 50 trials with fixed parameters (d), and after 150 trials with changing parameters (e). Slopes of linear models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>FigureFigure 5 :</head><label>5</label><figDesc>Kenji Doya, 2265.   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Figure 6: Kenji Doya, 2265.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 8 :</head><label>8</label><figDesc>Kenji Doya, 2265.   </figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">correspond to A 21 shown in (a).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Masahiko Haruno, Daniel Wolpert, Chris Atkeson, Jun Tani, Hidenori Kimura, and Raju Bapi for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neuronlike adaptive elements that can solve difficult learning control problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="834" to="846" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dynamic Programming and Optimal Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Athena Scientific</publisher>
			<pubPlace>Belmont, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixture of controllers for jump linear and non-linear plants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Cacciatore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System</title>
		<editor>J. D. Cowan, G. Tesauro, and J. Alspector</editor>
		<meeting><address><addrLine>San Mateo, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1994" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Feudal reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>C. L.</editor>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanson</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. D. Cowan</editor>
		<meeting><address><addrLine>San Mateo, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reinforcement learning in continuous time and space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="215" to="245" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognition of manipulated objects by motor learning with modular architecture networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="485" to="497" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multiple paired forwardinverse models for human motor learning and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
		<editor>M. S. Kearns, S. A</editor>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Solla, and D. A. Cohen</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="31" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Separated modules for visuomotor control and learning in the cerebellum: A functional MRI study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Imamizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miyauchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Takino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pütz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeuroImage: Third International Conference on Functional Mapping of the Human Brain</title>
		<editor>A. W. Toga, R. S. J. Frackowiak, and J. C. Mazziotta</editor>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">598</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human cerebellar activity reflecting an acquired internal model of a new tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Imamizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Miyauchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Takino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Putz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="page" from="192" to="195" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning policies for partially observable environments: Scaling up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cassandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaelbling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: Proceedings of the 12th International Conference</title>
		<editor>A. Prieditis and S. Russel</editor>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Acquisition of stand-up behavior by a real robot using hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="37" to="51" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adaptation and learning using multiple models, switching and tuning. IEEE Control Systems Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Narendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ciliz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995-06" />
			<biblScope unit="page" from="37" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reinforcement learning with hierarchies of machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>M. I. Jordan, M. J. Kearns, and S. A. Solla</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1043" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Annealed competition of experts for a segmentation and classification of switching dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pawelzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kohlmorge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="340" to="356" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From isolation to cooperation: An alternative view of a system of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="605" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transfer of learning by composing solutions of elemental sequential tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="323" to="340" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="181" to="211" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to perceive the world as articulated: an approach for hierarchical learning in sensory-motor systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nolfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1131" to="1141" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HQ-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="219" to="246" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Computational principles of movement neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1212" to="1217" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiple paired forward and inverse models for motor control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1317" to="1329" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Internal models in the cerebellum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Miall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="338" to="347" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
