{"1000": "supervised learning approaches have advanced the state of the art on a variety of tasks in natural language processing resulting in highly accurate systems supervised part of speech pos taggers for example approach the level of inter annotator agreement shen et al 2007 accuracy for english unfortunately the best completely unsupervised english pos tagger that does not make use of a tagging dictionary reaches only accuracy christodoulopoulos et al 2010 making its practical usability questionable at best to bridge this gap we consider a practically motivated scenario in which we want to leverage existing resources from a resource rich language like english when building tools for resource poor foreign languages 1 we assume that absolutely no labeled training data is available for the foreign language of interest but that we have access to parallel data with a resource rich language this scenario is applicable to a large set of languages and has been considered by a number of authors in the past alshawi et al 2000 xi and hwa 2005 ganchev et al 2009 2009 study related but different multilingual grammar and tagger induction tasks where it is assumed that no labeled data at all is available to this end we construct a bilingual graph over word types to establish a connection between the two languages 3 and then use graph label propagation to project syntactic information from english to the foreign language 4 these universal pos categories not only facilitate the transfer of pos information from one language to another but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed english labels figure shows an excerpt of a sentence from the italian test set and the tags assigned by four different models as well as the gold tags while the first three models get three to four tags wrong our best model gets only one word wrong and is the most accurate among the four models for this example as a result its pos tag needs to be induced in the no lp case while the 11a word level paired t test is significant at p for danish greek italian portuguese spanish and swedish and p for dutch gold si trovava in un parco con il fidanzato paolo f anni rappresentante em hmm feature hmm no lp with lp conj noun det det noun adp det noun noun pron verb adp det noun conj det noun noun noun verb pron verb adp det noun adp det noun noun noun noun verb verb adp det noun adp det adj noun adj noun verb verb adp det noun adp det noun noun noun noun figure 2 tags produced by the different models along with the reference set of tags for a part of a sentence from the italian test set we have shown the efficacy of graph based label propagation for projecting part of speech information across languages because we are interested in applying our techniques to languages for which no labeled resources are available we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs our results suggest that it is possible to learn accurate pos taggers for languages which do not have any annotated data but have translations into a resource rich language our results outperform strong unsupervised baselines as well as approaches that rely on direct projections and bridge the gap between purely supervised and unsupervised pos tagging models", "1001": "fisher vectors have been shown to provide a significant performance gain on many different applications in the domain of computer vision 39 33 2 in the domain of video action recognition fisher vectors and stacked fisher vectors have recently outperformed state of theart methods on multiple datasets 33 fisher vectors fv have also recently been applied to word embedding e g word2vec 30 and have been shown to provide state of the art results on a variety of nlp tasks 24 as well as on image annotation and image search tasks in all of these contributions the fv of a set of local descriptors is obtained as a sum of gradients of the loglikelihood of the descriptors in the set with respect to the parameters of a probabilistic mixture model that was fitted on a training set in an unsupervised manner in spite of being richer than the mean vector pooling method fisher vectors based on a probabilistic mixture model are invariant to order this makes them less appealing for annotating for example video in which the sequence of events determines much of the meaning this work presents a novel approach for fv representation of sequences using a recurrent neural network rnn the rnn is trained to predict the next element of a sequence given the previous elements several recent works have proposed to use an rnn for sentence representation 44 1 31 the recurrent neural network fisher vector rnn fv method differs from these works in that a sequence is represented by using derived gradient from the rnn as features instead of using a hidden or an output layer of the rnn the regression approach tries to predict the embedding of the following word i e for flickr30k and coco no training splits are given and we use the same splits used by there are three tasks in this benchmark image annotation in which the goal is to retrieve given a query image the five ground truth sentences image search in which given a query sentence the goal is to retrieve the ground truth image and sentence similarity in which the goal is given a sentence to retrieve the other four sentences describing the same image the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs the rnn fv representation surpasses the state of theart results for video action recognition on two challenging datasets when used for representing sentences the rnnfv representation achieves state of the art or competitive results on image annotation and image search tasks since the length of the sentences in these tasks is usually short and therefore the ordering is less crucial we believe that using the rnn fv representation for tasks that use longer text will provide an even larger gap between the conventional fv and the rnn fv a transfer learning result from the image annotation task to the video action recognition task was shown the con ceptual distance between these two tasks makes this result both interesting and surprising it supports a human development like way of training in which visual labeling is learned through natural language as opposed to e g associating bounding boxes with nouns while such training was used in computer vision to learn related image to text tasks and while recently zero shot action recognition was shown 11 55 nlp to video action recognition transfer was never shown to be as general as presented here", "1002": "based on this idea in this paper we propose a new method named talksumm acronym for talk based summarization to automatically generate extractive content based summaries for scientific papers based on video talks our main contributions are as follows 1 we propose a new approach to automatically gener ate summaries for scientific papers based on video talks 2 we create a new dataset that contains summaries for papers from several computer science conferences that can be used as training data 3 we show both automatic and human eval uations for our approach thus to create an extractive paper summary based on the transcript we model the alignment between spo ken words and sentences in the paper assuming the following generative process during the talk the speaker generates words for describing ver bally sentences from the paper one word at each time step however manual inspection reveal many cases of unwanted behaviors in the resulting outputs 1 many resulting sentences are unsupported by the input they contain correct facts about relevant entities but these facts were not mentioned in the input sentence 2 some facts are repeated the same fact is mentioned in multiple output sentences and 3 some facts are missing mentioned in the input but omitted in the output talk transcript let s begin with the motivation so processing long complex sentences is a hard task this is true for arguments like children people with reading disabilities second language learners but this is also true for sentence level and nlp systems for example previous work show that dependency parsers degrade performance when they re introduced with longer and longer sentences in a similar result was shown for neural machine translation where neural machine translation systems introduced with longer sentences starting degrading performance the question rising here is can we automatically break a complex sentence into several simple ones while preserving the meaning or the semantics and this can be a useful component in nlp pipelines for example the split and rephrase task was introduced in the last emnlp by narayan gardent and shimarina where they introduced a dataset an evaluation method and baseline models for this task the task definition can be taking a complex sentence and breaking it into several simple ones with the same meaning for example if you take the sentence alan being joined nasa in nineteen sixty three where he became a member of the apollo twelve mission along with alfa worden and his back a pilot and they ve just got its commander who would like to break the sentence into four sentences which can go as alan bean serves as a crew member of apolo twelve alfa worden was the back pilot will close it was commanded by david scott now be was selected by nasa in nineteen sixty three we can see that the task requires first identifying independence semantics units in the source sentence and then rephrasing those units into a single sentences on the target site in this work we first show the simple neural models seem to perform very well on the original benchmark but this is only due to memorization of the training set we propose a more challenging data split for the task to discourage this memorization and we perform automatic evaluation in error analysis on the new benchmark showing that the task is still very far from being solved", "1003": "emotions are an important element of human nature and detecting them in the textual messages written by users has many applications in information retrieval and human computer interaction a common approach to emotion analysis and modeling is categorization e g according to ekman s basic emotions namely anger disgust fear happiness sadness and surprise approaches to categorical emotion classi cation based on text often employ supervised machine learning classi ers which require labeled training data currently two types of datasets labeled with emotions are publicly available manually labeled and pseudo labeled manual annotations are usually applied to domain speci c datasets e g news headlines to overcome these limitations pseudo labeled datasets are gathered from social media platforms where social media posts are explicitly tagged by the author by using the hashtag symbol or by adding emoticons this tagged data can be used to create large scale training data labeled with emotions in a non speci c domain as in given such a dataset manually or pseudo labeled it is then common to train a linear classi er based on bag of words bow 1url permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page for our experiments we considered the most dominant emotion as the headline label as in fairy tales includes sentences from fairy tales labeled with ve emotions by six annotators for our experiments we used only sentences with high annotation agreement of four identical emotion labels as in blog posts consists of emotion rich sentences collected from blogs labeled with emotions by four annotators we considered only sentences for which the annotators agreed on the emotion category as in customer support dialogs in twitter consists of customer turns from customer support dialogs in twitter labeled by ve annotators with nine emotions relevant to customer care table depicts the macro f1 scores for each dataset and for the di erent models bow is our baseline presented in section en cbow en tfidf and en class are the ensemble models of bow and the corresponding embedded representations presented in section our ensemble methods outperformed the baseline for each document representation method the best result which is also signi cantly better for each dataset is of en class model that achieved an average relative improvement of in f1 score over all datasets these results indicate the advantage in combining both bow and embedded document representations for emotion detection from text this work studied the use of pre trained word vectors for emotion detection we presented class a novel method for representing a document as a dense vector based on the importance of the document s terms in respect to emotion classi cation our results show that an ensemble that combines bow and embedded representations using our class method outperforms previous approaches for domain speci c datasets in comparison to other deep learning methods our approach ts a small number of model parameters and requires little computing power for future work we plan to investigate the use of deep learning models trained on domain adapted pseudo labeled large scale datasets we also plan to investigate transfer learning for multidomain emotion detection", "1004": "a recent study shows that one in five 23 customers in the u s say they have used social media for customer service in 2014 up from in obviously companies hope that such 1url news docs 2014x 2014 global customer uses are associated with a positive experience in this paper we analyze customer support dialogues using the twitter platform and show the utility of such analyses emotions are a cardinal aspect of inter personal communication they are an implicit or explicit part of essentially any communication and of particular importance in the setting of customer service as they relate directly to customer satisfaction and experience oliver 2014 the analysis of emotions being expressed in customer support conversations can take two applications 1 to discern and compute quality of service indicators and 2 to provide real time clues to customer service agents regarding the cus service barometer us pdf tomer emotion expressed in a conversation a possible application here is recommending to customer service agents what should be their emotional response for example in each situation should they apologize should they thank the customer etc several companies are developing text based chat agents typically accessible through corporate web sites and partially automatized in these platforms a computer program handles simple conversations with customers and more complicated dialogues are transferred to a human agent the automation in such systems helps save human resources and with further development based on artificial intelligence more automation in customer service chats is likely to appear given the importance of emotions in service dialogues such systems will benefit from the ability to detect customer emotions and will need to guide employees and machines regarding the right emotional technique in various situations e g apologizing at the right point thus our goal in this paper is to show that the functionality of guiding employees regarding appropriate responses can be developed based on the analysis of textual dialogue data we show first that it is possible to automatically detect emotions being expressed and second that it is possible to predict the emotional technique that is likely to be used by a human agent in a given situation this analysis reflects our ultimate goal to enable a computer system to discern the emotions expressed by human customers and to develop computerized tools that mimic the emotional technique used by a human customer service agent in a particular situation we see the main contributions of this paper as follows 1 to our knowledge this is the first research focusing on automatic analysis of emotions expressed in customer service provided through social media 2 this is the first research using unique dialogue features e g emotions expressed in previous dialogue turns by the agent and customer time between dialogue turns to improve emotion detection in this work we studied emotions being expressed in customer service dialogues in the social media we studied the impact of dialogue features and dialogue history on the quality of the classification and showed improvement in performance for both models and both classification tasks we also showed the robustness of our models across different data sources as for future work we plan to work on several aspects 1 in this work we showed that it is possible to predict the emotional technique in the future we plan to run experiments in which the predicted emotional technique is actually applied in the context of new dialogues to measure the effect of such predictions on real support dialogues 2 distinguish between dialogues that have positive outcomes e g high customer satisfaction and others", "1005": "recently there has been a lot of exciting progress in the field of few shot learning in general and in few shot classification fsc in particular a popular method for approaching fsc is meta learning or learning to learn in meta learning the inputs to both train and test phases are not images but instead a set of few shot tasks ti each k shot n way task containing a small amount k usually 1 5 of labeled support images and some amount of unlabeled query images for each of the n categories of the task the goal of meta learning is to find a base model that is easily adapted to the specific task at hand so that it will generalize well to tasks built from novel unseen categories and fulfill the goal of fsc see section for further review equal contributors corresponding authors sivan doveh sivan dovehibm com and leonid karlinsky leonidkail ibm com ar x iv 1 2v cs c v m ar many successful meta learning based approaches have been developed for fsc 60 55 13 39 51 41 29 advancing its state of the art neural architecture search nas is a very active research field that has contributed significantly to overall improvement of the state of the art in supervised classification at training time instead of the mixed operation defined in equation 1 we define the mixed operation to be oi j x oo zi jo o x 8 where z i j is a continuous approximation of a one hot vector sampled from a gumbel distribution zi j gumbel i j 9 here i j are after softmax normalization and summed to at test time rather than the one hot approximation we use the operation with the top probability zi jk 1 if k argmax i j 0 otherwise 10 using this method we get better results for fc100 1 shot and comparable results for 5 shot compared to vanilla metaoptnet however it does not perform as well as the non stochastic version of metadapt in this work we have proposed metadapt a few shot learning approach that enables meta learned network architecture that is adaptive to novel few shot tasks the proposed approach effectively applies tools from the neural architecture search nas literature extended with the concept of metadapt controllers in order to learn adaptive architectures these tools help mitigate over fitting to the extremely small data of the few shot tasks and domain shift between the training set and the test set we demonstrate that the proposed approach successfully improves state of the art results on two popular few shot benchmarks miniimagenet and fc100 and carefully ablate the different optimization steps and design choices of the proposed approach some interesting future work directions include extending the proposed approach to progressively searching the full network architecture instead of just the last block applying the approach to other few shot tasks such as detection and segmentation and researching into different variants of task adaptivity including global connections modifiers and inter block adaptive wiring", "1006": "automated conversational agents chatbots are becoming widely used for various tasks such as personal assistants or as customer service agents recent studies project that of businesses plan to use chatbots by 20201 and that chatbots will power of customer service interactions by the year this increasing usage is mainly due to advances in artificial intelligence and natural language processing hirschberg and manning 2015 1url 2url along with increasingly capable chat development environments leading to improvements in conversational richness and robustness in the first two turns the chatbot misses the customer s intentions which leads to the customer asking are you a real person as an aid to chatbot improvement analysis of egregious conversations can often point to problems in training data or system logic that can be repaired specifically we consider customer inputs throughout a whole conversation and detect cues such as rephrasing the presence of heightened emotions and queries about whether the chatbot is a human or requests to speak to an actual human the main contributions of this paper are twofold 1 this is the first research focusing on detecting egregious conversations in conversational agent chatbot setting and 2 this is the first research using unique agent customer and customer agent interaction features to detect egregiousness given the full conversation each judge tagged whether the conversation was egregious or not following this guideline conversations which are extraordinarily bad in some way those conversations where you d like to see a human jump in and save the conversation we also studied how robust our features were if our features generalize well performance should not drop much when testing company b with the classifier trained exclusively on the data from company a although company a and company b share similar conversation engine platforms they are completely different in terms of objectives domain terminology etc specifically in our setting the relevant motivations are12 1 natural language understanding nlu error the agent s intent detection is wrong and thus the agent s response is semantically far from the customer s turn 2 language generation lg limitation the intent is detected correctly but the customer is not satisfied by the response for example the response was too generic 3 unsupported intent error the customer s intent is not supported by the agent we further investigated why the egr model was better at identifying egregious conversations i e its recall was higher compared to the baseline models we did not encounter any unsupported intent errors leading to customer rephrasing which affected the ability of the rule based model to classify those conversations as egregious while customer rephrasing was captured by the egr model for the text based model some of the intents were new did not appear in the training data and thus were difficult for the model to capture in this paper we have shown how it is possible to detect egregious conversations using a combination of customer utterances agent responses and customer agent interactional features as explained the goal of this work is to give developers of automated agents tools to detect and then solve problems cre ated by exceptionally bad conversations in this context future work includes collecting more data and using neural approaches e g rnn cnn for analysis validating our models on a range of domains beyond the two explored here we also plan to extend the work to detect egregious conversations in real time e g for escalating to a human operators and create log analysis tools to analyze the root causes of egregious conversations and suggest possible remedies", "1007": "convolutional neural networks cnns originally invented for computer vision have been shown to achieve strong performance on text classification tasks bai et al 2018 kalchbrenner et al 2014 wang et al 2015 zhang et al 2015 johnson and zhang 2015 iyyer et al 2015 as well as other traditional natural language processing nlp tasks collobert et al 2011 even when considering relatively simple one layer models kim 2014 the ability to interpret neural models can be used to increase trust in model predictions analyze errors or improve the model ribeiro et al 2016 the problem of interpretability in machine learning can be divided into two concrete tasks given a trained model model interpretability aims to supply a structured explanation which captures what the model has learned given a trained model and a single example prediction interpretability aims to explain how the model arrived at its prediction in this work we examine and attempt to understand how cnns process text and then use this information for the more practical goals of improving model level and prediction level explanations specifically current common wisdom suggests that cnns classify text by working through the following steps goldberg 2016 1 1 dimensional convolving filters are used as ngram detectors each filter specializing in a closely related family of ngrams in order to identify case negative ngrams we heuristically test whether the changed words scores directly influence the status of the activation relative to the threshold given an already identified negative ngram if the ngram scoresans the bottom k negative slot activations considering a hamming distance of k and given that there are k negative slot activations passes the threshold yet it does not pass the threshold by including the negative slot activations then the ngram is considered a case negative ngram as in computer vision we can now interpret a trained cnn model by visualizing its filters and interpreting the visible shapesin other words defining a high level description of what the filter detects we propose to associate each filter with the following items 1 the class which this filter s strong signals contribute to in the sentiment task positive or negative 2 the threshold value for the filter together with its purity and coverages percentages which essentially capture how informative this filter is 3 a list of semantic patterns identified by this filter finally we can also mark cases of negative ngrams section 5 4 where an ngram has high slot activations for some words but these are negated by a highly negative slot and as a consequence are not selected by max pooling or are selected but do not pass the filter s threshold first we have shown that maxpooling over time induces a thresholding behavior on the convolution layer s output essentially separating between features that are relevant to the final classification and features that are not specifically by maximizing the word level activations by iterating over the vocabulary we observed that filters do not maximize activations at the word level but instead form slot activation patterns that give different types of ngrams similar activation strengths by clustering high scoring ngrams according to their slotactivation patterns we can identify the groups of linguistic patterns captured by a filter we also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words finally we use these findings to suggest improvements to model based and predictionbased interpretability of cnns for text", "1008": "automatic text summarizers condense a given piece of text into a shorter version the summary this is done while trying to preserve the main essence of the original text and keeping the generated summary as readable as possible existing summarization methods can be classified into two main types either extractive or abstractive extractive methods select and order text fragments e g sentences from the original text source such methods are relatively simpler to develop and keep the extracted fragments untouched allowing to preserve important parts e g keyphrases facts opinions etc a common approach is based on the encoder decoder seq to seq paradigm sutskever et al 2014 with the original text sequence being encoded while the summary is the decoded sequence work was done during a summer internship in ibm research ai while such methods usually generate summaries with better readability their quality declines over longer textual inputs which may lead to a higher redundancy paulus et al 2017 a common approach for handling long text sequences in abstractive settings is through attention mechanisms which aim to imitate the attentive reading behaviour of humans chopra et al 2016 compared to previous works whose final summary is either entirely extracted or generated using an abstractive process in this work we suggest a new idea of editorial network editnet a mixed extractive abstractive summarization approach let s denote a summary which was extracted from a given text document d the editorial process is implemented by iterating over sentences in s according to the selection order of the extractor the first decision is to keep the extracted sentence untouched represented by label e in figure 1 therefore the editor may modify summary s by paraphrasing or rejecting some of its sentences resulting in a mixed extractive abstractive summary s let l be the number of sentences in s in each step i 1 2 l in order to make an educated decision the editor considers both sentence representations esi and asi as its input together with two additional auxiliary representations i1 i denote the average r value obtained by decision sequences that start with the prefix 1 based on the soft label y i is then calculated3 as follows y i r 1 we trained validated and tested our approach using the non annonymized version of the cnn dailymail dataset hermann et al 2015 overall editnet provides a highly competitive summary quality where it outperforms most baselines considering the complexity of these models and the slow down that can incur during training and inference we think that editnet still provides a useful high quality and relatively simple extension on top of standard encoder aligned decoder architectures moreover on average per summary editnet keeps only of the original extracted sentences while the rest 67 are abstracted ones this demonstrates that editnet has a high capability of utilizing abstraction while being also able to maintain or reject the original extracted text whenever it is estimated to provide the best benefit for the summary s quality we have proposed editnet a novel alternative summarization approach that instead of solely applying extraction or abstraction mixes both together moreover editnet implements a novel sentence rejection decision allowing to correct initial sentence selection decisions which are predicted to negatively effect summarization quality as future work we plan to evaluate other alternative extractor abstractor configurations and try to train the network end to end we further plan to explore reinforcement learning rl as an alternative decision making approach", "1009": "performing the mental gymnastics of transforming i m hear to i m here or i can t so buttons to i can t sew buttons is familiar to anyone who has encountered autocorrected text messages punny social media posts or just friends with bad grammar although at first glance it may seem that phonetic similarity can only be quantified for audible words this problem is often present in purely textual spaces such as social media posts or text messages incorrect homophones and synophones whether used in error or in jest pose challenges for a wide range of nlp tasks such as named entity identification text normalization and spelling correction chung et al 2011 xia et al 2006 toutanova and moore 2002 twiefel et al 2014 lee et al 2013 kessler 2005 these tasks must therefore successfully transform incorrect words or phrases hear so to their phonetically similar correct counterparts here sew which in turn requires a robust representation of phonetic similarity between word pairs a reli able approach for generating phonetically similar words is equally crucial for chinese text xia et al 2006 unfortunately most existing phonetic similarity algorithms such as soundex archives and administration 2007 and double metaphone dm philips 2000 are motivated by english and designed for indo european languages in contrast the speech sound of a chinese character is represented by a single syllable in pinyin consisting of two or three parts an initial optional a final or compound finals and tone table 1 note that we use pinyin as the phonetic representation because it is a widely accepted romanization system san 2007 iso 2015 of chinese syllables used to teach pronunciation of standard chinese table shows two sentences from chinese microblogs containing informal words derived from phonetic transcription near homonyms of from table are shown in table since both dm and soundex ignore vowels and tones words with dissimilar pronunciations are incorrectly assigned to the same encoding e g this paper presents dimsim a learned ndimensional phonetic encoding for chinese along with a phonetic similarity algorithm which uses the encoding to generate and rank phonetically similar words having determined the phonetic encodings and the mechanism to compute the phonetic similarity using learned phonetic encodings we now describe how to generate and rank similar candidates in algorithm given a word w a similarity threshold th and a chinese pinyin dictionary dict we retrieve the pinyin py of w from dict motivated by phonetic transcription as a widely observed phenomenon in chinese social media and informal language we have designed an accurate phonetic similarity algorithm dimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial final and tone components using a real world dataset we demonstrate that dimsim effectively improves mrr by 7 5x recall by 1 5x and precision by 1 4x over existing approaches the original motivation for this work was to improve the quality of downstream nlp tasks such as named entity identification text normalization and spelling correction these tasks all share a dependency on reliable phonetic similarity as an intermediate step especially for languages such as chinese where incorrect homophones and synophones abound we therefore plan to extend this line of work by applying dimsim to downstream applications such as text normalization", "1010": "knowledge base question answering kbqa systems answer questions by obtaining information from kb tuples berant et al 2013 yao et al kb query which can be executed to retrieve the answers from a kb figure illustrates the process used to parse two sample questions in a kbqa system a a single relation question which can be answered with a single head entity relation tail entity kb tuple fader et al 2013 yih et al 2014 bordes et al 2015 and b a more complex case where some constraints need to be handled for multiple entities in the question the kbqa system in the figure performs two key tasks 1 entity linking which links n grams in questions to kb entities and 2 relation detection which identifies the kb relation s a question refers to first in most general relation detection tasks the number of target relations is limited normally smaller than in contrast in kbqa even a small kb like freebase2m bordes et al 2015 contains more than 6 000 relation types given an input question and a set of candidate entities retrieved by an entity linker based on the question our proposed relation detection model plays a key role in the kbqa process 1 re ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model then we generate the kb queries for q following the four steps illustrated in algorithm algorithm 1 kbqa with two step relation detection input question q knowledge base kb the initial top k entity candidates elk q output top query tuple e r c rc entity re ranking first step relation detection use the raw question text as input for a relation detector to score all relations in the kb that are associated to the entities in elk q use the relation scores to re rank elk q and generate a shorter list el0k0 q containing the top k0 entity candidates section 5 1 relation detection detect relation s using the reformatted question text in which the topic entity is replaced by a special token e section 5 2 query generation combine the scores from step and 2 and select the top pair e r section 5 3 constraint detection optional compute similarity between q and any neighbor entity c of the entities along r connecting by a relation rc add the high scoring c and rc to the query section 5 4 2016 we use s mart yang and chang 2015 entity linking outputs 7 in order to evaluate the relation detection models we create a new relation detection task from the webqsp data set 8 for each question and its labeled semantic parse 1 we first select the topic entity from the parse and then 2 select all the relations and relation chains length 2 connected to the topic entity and set the corechain labeled in the parse as the positive label and all the others as the negative examples in order to highlight the effect of different relation detection models on the kbqa end task we also implemented another baseline that uses our kbqa system but replaces hr bilstm with our implementation of ampcnn for simplequestions or the char 3 gram bicnn for webqsp relation detectors second block in table 3 compared to the baseline relation detector 3rd row of results our method which includes an improved relation detector hr bilstm improves the kbqa end task by 2 3 4th row", "1011": "dictionary expansion is one area where close integration of humans into the discovery loop has been shown to enhance task performance substantially over more traditional post adjudication this is not surprising as dictionary membership is often a fairly subjective judgment e g should a fruit dictionary include tomatoes thus even with a system which finds similar terms e g word2vec guidance is important to keep the system focused on the subject matter expert s notion of lexicon in this work we propose a feature agnostic approach for dictionary expansion based on lightweight neural language models such as word2vec to prevent semantic drift during the dictionary expansion we effectively include humanin the loop huml given an input text corpus and a set of seed examples the proposed approach runs in two phases explore and exploit to identify new potential dictionary entries the explore phase tries to identify similar instances to the dictionary entries that are present in the input text corpus using term vectors from the neural language model to calculate a similarity score the exploit phase tries to construct more complex multi term phrases based on the instances already in the input dictionary to identify multi term phrases most commonly a simple phrase detection model is used which is based on a term s co occurrence score i e terms that often appear together probably are part of the same phrase the phrase detection must be done before the model is built and they remain unchanged after the model is built for example valid phrase combinations may simply not occur e g acute joint pain may appear in the sample corpus but for some reason chronic hip pain may not we use two phrase generation algorithms i modify the phrases by replacing single terms with similar terms from the text corpus e g abnormal behavior can be modified to strange behavior ii extend the instances with terms from the text corpus that are related to the terms in the instance e g abnormal blood clotting problems is a an adverse drug reaction which doesn t appear as such in a large text corpus however the instances abnormal blood count blood clotting and clotting problems appear several times in the corpus which can be used to build the more complex instance in the first approach we first break each instance in to a set of single terms t t1 t2 tn then for each term ti in t we identify a set of similar terms tsti ts1 ts2 tss in the vocabulary vtc using equation in the next step we build new phrases by replacing ti with a term tsi from tsti the new phrases are sorted based on the similarity score and the top n are selected as candidates this paper proposes an interactive dictionary expansion tool using a lightweight neural language model our algorithm is iterative and purely statistical hence does not require any feature extraction beyond tokenization it incorporates human feedback to improve performance and control semantic drift at every iteration cycle the experiments showed high importance of tight huml integration on discovery efficiency in this work we have considered only lightweight language models which can be efficiently built and updated on large text corpora in future work we will analyze more complex language neural network models such as recurrent neural networks rnn long short term memory networks lstm and bidirectional lstm which might improve the search for similar and related terms at the expense of higher training time furthermore future work will include an evaluation of the approach on multiple datasets covering different domains", "1012": "grounding of textual phrases i e finding bounding boxes in images which relate to textual phrases is an important problem for human computer interaction robotics and mining of knowledge bases three applications that are of increasing importance when considering autonomous systems augmented and virtual reality environments for example we may want to guide an autonomous system by using phrases such as the bottle on your left or the plate in the top shelf while those phrases are easy to interpret for a human they pose significant challenges for present day textual grounding algorithms as interpretation of those phrases requires an understanding of objects and their relations more specifically deep net models are designed to extract features from given bounding boxes and textual data which are then compared to measure their fitness while being easy to obtain automatic extraction of region proposals is limiting because the performance of the visual grounding is inherently constrained by the quality of the proposal generation procedure our approach is based on a number of image concepts such as semantic segmentations detections and priors for any number of objects of interest in addition we define a bounding box y via its top left corner y1 y2 and its bottom right corner y3 y4 and subsume the four variables of interest in the tuple y y1 every integral coordinate yi i 1 yi max and y denotes the product space of all four coordinates energy function details our energy function e x y w is based on a set of image concepts such as semantic segmentation of object categories detections or word priors all of which we subsume in the set c importantly all image concepts c c are attached a parametric score map c x wr rwh following the image width w and height h note that those parametric score maps may depend nonlinearly on some parameters wr expected groups of words form for example bicycle bike camera cellphone coffee cup drink man woman snowboarder skier the word vectors capture image spatial relationship of the words meaning items that can be replaced in an image are similar e g a snowboarder can be replaced with a skier and the overall image would still be reasonable the inference speed can be divided into three main parts 1 extracting image features 2 extracting language features and 3 computing scores for extracting image features grounder requires a forward pass on vgg16 for each image region where cca and our approach requires a single forward pass which can be done in ms for extracting language features our method requires index lookups which takes negligible amount of time less than 1e 6 ms cca uses word2vec for processing the text which takes ms grounder uses a long short term memory net which takes ms computing the scores with our c implementation takes 1 05ms on a cpu cca needs to compare projections of the text and image features which takes 13 41ms on a gpu and 609ms on a cpu grounder uses a single fully connected layer which takes ms on a gpu we demonstrated a mechanism for grounding of textual phrases which provides interpretability is easy to extend and permits globally optimal inference in contrast to existing approaches which are generally based on a small set of bounding box proposals we efficiently search over all possible bounding boxes we think interpretability i e linking of word and image concepts is an important concept particularly for textual grounding which deserves more attention", "1013": "the use of features from deep convolutional neural networks dcnns pretrained on imagenet has led to important advances in computer vision dcnn features usually called perceptual features pfs have been used in tasks such as transfer learning 40 16 style transfer and super resolution while there have been previous works on the use of pfs in the context of image generation and transformation 7 17 exploration of pfs as key source of information for learning generative models is not well studied moment matching approaches for generative modeling are based on the assumption that one can learn the data distribution by matching the moments of the model distribution to the empirical data distribution ods of this family are based on maximum mean discrepancy mmd 11 12 and the method of moments mom while mom based methods embed a probability distribution into a finite dimensional vector i e matching of a finite number of moments mmd based methods embed a distribution into an infinite dimensional vector a challenge for mmd methods is to define a kernel function that is statistically efficient and can be used with small minibatch sizes a solution comes by using adversarial learning for the online training of kernel functions 21 however this solution inherits the problematic min max game of adversarial learning more specifically we propose a simple but effective moment matching method that 1 breaks away from the problematic min max game completely 2 does not use online learning of kernel functions and 3 is very efficient with regard to both number of used moments and required minibatch size our proposed approach named generative feature matching networks gfmn learns implicit generative models by performing mean and covariance matching of features extracted from all convolutional layers of pretrained deep convnets the main contributions of this work can be summarized as follows 1 we propose a new effective moment matchingbased approach to train implicit generative models that does not use adversarial or online learning of kernel functions provides stable training and achieves state of the art results 2 we show theoretical results that demonstrate gfmn convergence under the assumption of the universality of perceptual features 3 we propose an adam based moving average method that allows effective training with small minibatches 4 our extensive quantitative and qualitative experimental results demonstrate that pretrained autoencoders and dcnn classifiers can be effectively used as cross domain feature extractors for gfmn training our proposed approach consists in training g by minimizing the following loss function min m j1 jpdata jpg 2 jpdata jpg 2 1 where jpdata expdataej x rdj jpg ezn 0 inz ej g z r dj jpdata expdataej x j pdata 2 dj and is the l2 loss x is a real data point sampled from the data generating distribution pdata z rnz is a noise vector sampled from the normal distribution n 0 inz ej x denotes the output vector feature map of the hidden layer j from e m l is the number of hidden layers used to perform feature matching if a discriminator can distinguish perfectly between real and fake early on the generator cannot learn properly and the min max game becomes unbalanced having no good discriminator gradients for the generator to learn from producing degenerate models in this appendix we present a comparison between the simple moving average ma and adam moving average ama for the case where vgg19 imagenet classifier is used as a feature extractor gfmn trained with ma produces various images with some sort of crossing line artifacts", "1014": "e commerce recommender systems aim to present items with high utility to the consumers utility may be decomposed into form utility the item is desired as it is manifested and time utility the item is desired at the given point in time 28 recommender systems should take both types of utility into account economists define items to be either durable goods or nondurable goods based on how long they are intended to last before being replaced a key characteristic of durable goods is the long duration of time between successive purchases within item categories whereas this duration for nondurable goods is much shorter or even negligible although we have witnessed great success of collaborative filtering in media recommendation we should be careful when expanding its application to general e commerce recommendation involving both durable and nondurable goods due to the following reasons since media such as movies and music are nondurable goods most users are quite receptive to buying or renting them in rapid succession therefore recommending an item for which a user has no immediate demand can hurt user experience and waste an opportunity to drive sales a key assumption made by matrix factorization and completion based collaborative filtering algorithms is that the underlying rating matrix is of low rank since only a few factors typically contribute to an individual s form utility however a user s demand is not only driven by form utility but is the combined effect of both form utility and time utility given purchase triplets user item time and item categories the objective is to make recommendations based on users overall predicted combination of form utility and time utility in this study we replace category prediction with a more strict evaluation metric item prediction 8 which indicates the predicted ranking of item i among all items at time t for each purchase record u i t in the test set as a final note we want to point out that tmall and amazon review may not take full advantage of the proposed algorithm since i their categories are relatively coarse and may contain multiple sub categories with different durations and ii the time stamps of amazon review reflect the review time instead of purchase time and inter review durations could be different from inter purchase durations by choosing a purchase history dataset with a more appropriate category granularity we may obtain more accurate duration estimations and also a better recommendation performance in this paper we examine the problem of demand aware recommendation in settings when interpurchase duration within item categories affects users purchase intention in combination with intrinsic properties of the items themselves we formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter purchase durations and propose a scalable optimization algorithm with a tractable time complexity our empirical studies show that the proposed approach can yield perfect recovery of duration vectors in noiseless settings it is robust to noise and scalable as analyzed theoretically on two real world datasets tmall and amazon review we show that our algorithm outperforms six state of the art recommendation algorithms on the tasks of category item and purchase time predictions", "1015": "automated conversational agents are becoming popular for various tasks such as personal assistants shopping assistants or as customer service agents automated agents benefit from adapting their personality according to the task at hand reeves and nass 1996 tapus and mataric 2008 or to the customer herzig et al 2016 thus it is desirable for automated agents to be capable of generating responses that express a target personality many models of personality exist while the most common one is the big five model digman 1990 including openness conscientiousness extraversion agreeableness and neuroticism these traits were correlated with linguistic choices including lexicon and syntax mairesse and walker 2007 in this paper we study how to encode personality traits as part of neural response generation for conversational agents our approach builds upon a sequence to sequence seq2seq architecture sutskever et al 2014 by adding an additional layer that represents the target set of personality traits and a hidden layer that learns high level personality based features specifically we focus on conversational agents for customer service in this context many studies examined the effect of specific personality traits of human agents on service performance results indicate that conscientiousness a person s tendency to act in an organized or thoughtful way and agreeableness a person s tendency to be compassionate and cooperative toward others correlate with service quality blignaut et al 2014 sackett 2014 the first response in each example is generated by a standard seq2seq response generation system that ignores personality modeling and in effect generates the consensus response of the humans represented in the training data in example 1 the agreeableness agent is more compassionate expresses empathy and is more cooperative asks questions neural response generation can be viewed as a sequence to sequence problem sutskever et al 2014 where a sequence of input language tokens x x1 xm describing the user utterance is mapped to a sequence of output language tokens y1 yn describing the agent response table shows that in this setting we get better performance by utilizing personality based representation our model achieves a relative decrease in perplexity and a relative improvement in bleu score we generated a high trait target personality distribution trait was either agreeableness or conscientiousness where trait was set to a value of 0 9 and all other traits to similarly we created a low trait version where trait was set to for each trait and customer utterance we generated a response for the high trait and low trait versions each triplet a customer utterance followed by high trait and low trait generated responses was evaluated by five master level judges after discarding ties we found that the high trait responses generated by our personality based model were judged either more expressive or somewhat more expressive than the low trait corresponding responses in of cases if we ignore the somewhat more expressive judgments the high trait responses win in of cases we have presented a personality based response generation model and tested it in customer care tasks outperforming baseline seq2seq model in future work we would like to generate responses adapted to the personality traits of the customer as well and to apply our model to other tasks such as education systems to extract personality traits for agents in our experiments we utilized the ibm personality insights service which is publicly available this service infers three models of personality traits namely big five needs and values from social media text it extracts percentile scores for traits1", "1016": "recently developed event based cameras such as dynamic vision sensor dvs 37 and atis 50 inspired by the biological retina encode pixel illumination changes as events a live feed version of the system running on nine truenorth chips is shown to calculate disparity maps per second and the ability to increase this up to 2 000 disparities per second subject to certain trade offs is demonstrated for use with high speed event cameras such as dvs total chip power is the sum of passive power computed by multiplying the idle power by the fraction of the chip s cores under use and active power computed by subtracting idle power from the total power measured when the system is accepting input events the rds is tested on a model using spatial windows left right consistency constraints no morphological erosion dilation after rectification and disparity levels 0 30 plus a no disparity indicator often occurring due to self occlusions the models that run on live davis input are operated at spike injection rate of up to 2 000hz a new input every 1 2 000 seconds and disparity map throughput of 400hz at a 0 5ms tick period 400 distinct disparity maps produced every second across a cluster of truenorth chips by adding a multiplexing spiking network to the network we are able to reuse each feature extraction wta circuit to process the disparities for different pixels effectively decreasing the maximum disparity map throughput from 2 000hz to 400hz requiring fewer chips to process the full image 9 truenorth chips we tested the maximum disparity map throughput achievable by executing a one chip model on a cropped input with no multiplexing one disparity map ejected per tick at a 0 5ms tick period achieving the 2 000hz disparity map throughput we also observe qualitatively good performance fig left right consistency constraints are typically present in the best performing fan sequence models but not so in the butterfly sequences distance and orientation do not have a significant effect on performance we have introduced an advanced neuromorphic 3d vision system uniting a pair of davis cameras with multiple truenorth processors to create an end to end scalable event based stereo system by using a spiking neural network with low precision weights we have shown that the system is capable of injecting event streams and ejecting disparity maps at high throughputs low latencies and low power the system is highly parameterized and can operate with other event based sensors such as atis or dvs table compares our approach with the literature on event based disparity comparative advantages are low power multi resolution disparity calculation scalability to live sensor feed with large input sizes and evaluation using synthetic as well as real world fast movements and depth gradients in neuromorphic non von neumann hardware the implemented neuromorphic stereo disparity system achieves these advantages while consuming less power per pixel per disparity map compared to the stateof the art the homogeneous computational substrate provides the first example of a fully end to end low power high throughput fully event based neuromorphic stereo system capable of running on live input event streams using a fully graph based computation model where no frames arrays or other such data structures are used", "1017": "the latter inference attempts to leverage the causal structure to compute quantitative claims about the effect of interventions and retrospective counterfactuals which are critical to assign credit understand blame and responsibility and perform judgement about fairness in decision making one of the most popular languages used to encode the invariances needed to reason about causal relations for both learning and inference is based on graphical models and appears under the rubric of causal graphs 16 21 a causal graph is a directed acyclic graph dag with latent variables where each edge encodes a causal relationship between its endpoints x is a direct cause of y i e x y if when the remaining factors are held constant forcing x to take a specific value affects the realization of y where x y are random variables representing some relevant features of the system the task of learning the causal structure entails a search over the space of causal graphs that are compatible with the observed data the collection of these graphs forms what is called an equivalence class based on the second rule of do calculus one can infer that there is an open backdoor path from x to y where the edge adjacent to x on this path has an arrowhead into x in our setting we do not have access to the true graph but we leverage this and the other do constraints to reverse engineer the process and try to learn the structure a pag which represents a markov equivalence class of a mag is learnable from the independence model over the observed variables and the fci algorithm is a standard sound and complete method to learn such an object related work learning causal graphs from a combination of observational and interventional data has been studied in the literature 3 11 7 20 8 12 for causally sufficient systems the notion and characterization of interventional markov equivalence has been introduced in 9 more recently showed that the same characterization can be used for both hard and soft interventions rule inducing paths if fk f is adjacent to a node y sk and sk 1 e g sk x then orient x y out of x i e x y the intuition for this rule is as follows if fk is adjacent to a node y sk in g then there is an inducing path p between fk and y in augi d where d is any causal graph in the equivalence class hence the edge between x and y is out of x and into y in mag augi d and consequently in g we give an example to illustrate the steps of the algorithm in figure 3 where i x figure 3a shows the augmented causal graph i e augi d and figure 3b shows the corresponding augmented mag i e mag augi d we investigate the problem of learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data we pursue this endeavor by noting that a generalization of the converse of pearl s do calculus thm 1 leads to new tests that can be evaluated against data these tests in turn translate into constraints over the structure itself we then define an interventional equivalence class based on such criteria def 1 and then derive a graphical characterization for the equivalence of two causal graphs thm finally we develop an algorithm to learn an interventional equivalence class from data which includes new orientation rules", "1018": "structured knowledge bases kb like wikidata and freebase can support answering questions kbqa over a diverse spectrum of structural complexity this includes queries with single hop obama s birthplace yao 2015 berant et al 2013 or multi hop who voiced meg in family guy bast and haumann 2015 yih et al 2015 xu et al 2016 guu et al 2015 mccallum et al 2017 das et al 2017 or complex queries such as how many countries have more rivers and lakes than brazil saha et al 2018 main contributions we present complex imperative program induction from terminal rewards cipitr 2 an advanced neural program induction npi system that is able to answer complex logical quantitative and comparative queries by inducing programs of length up to 7 using atomic operators and variable types on the other hand training the kvmnet model on the balanced data helps showcase the real performance of the model where cipitr outperforms kvmnet significantly on most of the harder query classes to summarize cipitr has the following advantages inducing programs more efficiently and pragmatically as illustrated by the sample outputs in table 5 generating syntactically correct programs because of the token by token decoding of the program nsm cannot restrict its search to only syntactically correct programs but rather only resorts to a post filtering step during training however at test time it could still generate programs with wrong syntax as shown in table for example for the logical question it invokes a genset with a wrong argument type none and for the quantitative count question it invokes the setunion operator on a non set argument on the other hand cipitr by design can never generate a syntactically incorrect program because at every step it implicitly samples only feasible actions generating semantically correct programs cipitr is capable of incorporating different generic programming styles as well as problemspecific constraints restricting its search space to only semantically correct programs on the other hand the nsmgenerated programs are often semantically wrong for instance both in the quantitative and quantitative count based questions the type of the answer is itself wrong rendering the program meaningless this arises once again owing to the token by token decoding of the program by nsm which makes it hard to incorporate high level rules to guide or constrain the search efficient search space exploration owing to the different strategies used to explore the program space more intelligently cipitr scales better to a wide variety of complex queries by using less than half of nsm s beam size we experimentally established that for programs of length these various techniques reduced the average program space from to 2 998 programs we presented cipitr an advanced npi framework that significantly pushes the frontier of complex program induction in absence of gold programs cipitr uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic pragmatic programming styles to constrain the combinatorial program space to only semantically correct programs as future directions of work cipitr can be further improved to handle the hardest question types by making the search more strategic and can be further generalized to a diverse set of goals when training on all question categories together other potential directions of research could be toward learning to discover sub goals to further decompose the most complex classes beyond just the two level phase transition proposed here additionally further improvements are required to induce complex programs without availability of gold program input variables", "1019": "the vast amounts of textual data end users need to consume motivates the need for automatic summarization an automatic summarizer gets as an input one or more documents and possibly also a limit on summary length e g maximum number of words the summarizer then needs to produce a textual summary that captures the most salient general and informative content parts within input documents moreover the summarizer may also be required to satisfy a specific user information need expressed by one or more queries while both saliency and focus goals should be considered within a query focused summarization setting these goals may be actually conflicting with each other higher saliency usually comes at the expense of lower focus and vice versa contact author haggaiil ibm com to illustrate the effect of summary length on this tradeoff using the duc dataset figure reports the summarization quality which was obtained by the cross entropy summarizer ces a state of the art unsupervised query focused multidocument extractive summarizer saliency was measured according to cosine similarity between the summary s bigram representation and that of the input documents aiming at better handling the saliency versus focus tradeoff in this work we propose dual ces an extended ces summarizer similar to ces dual ces is an unsupervised query focused multi document extractive summarizer to this end is incrementally learned using an importance sampling approach a sample of n sentence subsets sj is generated according to the selection policy t1 which was learned in the previous iteration t the likelihood of picking a sentence s d at iteration t is estimated via cross entropy minimization as follows t s def n j1 q sj q d tssj n j1 q sj q d t 1 here denotes the kronecker delta indicator function and t denotes the 1 quantile 0 1 of the sample performances q sj q d j 1 2 therefore the likelihood of picking a sentence s d will increase when it is being included in more subset samples whose performance is above the current minimum required quality target value t we further smooth t as follows t t1 1 t with 0 upon its termination the ce method is expected to converge to the global optimal selection policy we then produce a single summary s figure illustrates the average learning curve of its adaptive length parameter lt overall dual ces s summarization quality remains quite stable exhibiting low sensitivity to l similar stability was further observed for the two other duc benchmarks in addition figure depicts an interesting empirical outcome dual ces a converges more or less to the best hyperparameter l value i e l in table 3 dual ces a therefore serves as a robust alternative for flexibly estimating such hyperparameter value during runtime dual ces a can provide similar quality and may outperform dual ces we proposed dual ces an unsupervised query focused extractive multi document summarizer dual ces was shown to better handle the tradeoff between saliency and focus providing the best summarization quality compared to other alternative stateof the art unsupervised summarizers moreover in many cases dual ces even outperforms state of the art supervised summarizers as a future work we would like to learn to distill from additional pseudo feedback sources", "1020": "in recent years we are experiencing a dramatic improvement of the synthesized speech quality in tts systems with the introduction of systems that are based on neural networks nn a major improvement in quality was achieved by using attention based models such as tacotron and by replacing vocoders with a nn based waveform generators such as wavenet a useful feature of systems with trainable models is the ability to adapt the tts to an unseen voice using a small amount of training data from a few seconds to an hour of speech in our previous paper we introduced a nn based tts system with two trainable modules for prosody prediction and acoustic features prediction in this paper we show that we can get a considerable quality improvement by modifying a tts system that produced the world vocoder parameters to predict parameters for lpcnet as in the previous work 6 we conduct multiple adaptation experiments applied on multiple vctk voices and show that the new system has much better quality and similarity to the target voices but can still run much faster than real time in a single cpu mode we adopted the front end block which is used in the ibm watson tts engine and is described in detail in the front end performs a grapheme to phoneme conversion represents each word with a set of positional and categorical linguistic features and associates the features with the phonemes contained within the word each sub phoneme element represents either a heading a middle or a trailing part of a phoneme the input features derived from the tts front end are comprised of 1 hot coded categorical features and standard positional features in this architecture the prosody adaptation to unseen speaker is based on a variational auto encoder vae utterance prosody embedding averaged over all the speaker utterances 6 as presented on figure in the current work we used multi speaker baseline models for prosody adaptation to unseen voices as it resulted in better quality than the single speaker models the subject is asked to rate their voice similarity using a 4 point scale adopted from the voice conversion challenge vcc 16 and utilized in our previous experiments we performed two tests one with only male voices and the second with only female voices we can compare these results to those of the vcc hub task although the task setup and the listening tests conditions are a bit different we can see that our system mos and similarity score are comparable to those of the best vcc system n10 with quality of and similarity of where the corresponding scores for the original speech are and 95 we tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems the task of creating a high quality tts system out of a smaller set of audio data is even more challenging we have shown that our system can perform well even with datasets as small as 5 20 minutes of audio we demonstrated that when we reduce the size of the training data there is some graceful degradation to the quality but we are still able to maintain good similarity to the original speaker for future work we plan to allow voice modifications by adding control over voice parameters such as pitch breathiness and vocal tract j shen et al natural tts synthesis by conditioning wavenet on mel spectrogram predictions ieee international conference on acoustics speech and signal processing icassp calgary ab 2018 pp", "1021": "feature selection is an important problem in statistics and machine learning for interpretable predictive modeling and scientific discoveries our goal in this paper is to design a dependency measure that is interpretable and can be reliably used to control the false discovery rate in feature selection the mutual information between two random variables x and y is the most commonly used dependency measure the mutual information i x y is defined as the kullback leibler divergence between the joint distribution pxy of x y and the product of their marginals pxpy i x y kl pxy pxpy instead of the usual kl divergence the metric d with its witness function or critic f x y measures the distance between the joint pxy and the product of marginals pxpy with this generalized definition of mutual information the feature selection problem can be formalized as finding a sparse selector or gate w rdx such thatd pw x y pw xpy is maximal i e 42 we train a neural network to act as a generator of a features xj given the remaining features xj as inputs as a replacement for the conditional distributions p xj xj in all of our tasks one three layer neural network with relu units and conditional batch normalization bcn applied to all hidden layers serves as generator for all features j 1 p a sample from p xj xj is generated by giving as input to the network an index j indicating the feature to generate and a sample xj p xj represented as a sample from the full joint distribution x p x1 xp with feature j being masked out f 1 2 liang dataset liang dataset is a variant of the synthetic dataset proposed by the dataset prescribes a regression model with 500 dimensional correlated input features x where the 1 d regression target y depends on the first features only the last correlated features are ignored f 2 ccle dataset the cancer cell line encyclopedia ccle dataset provides data about of anti cancer drug response in cancer cell lines the dataset contains the phenotypic response measured as the area under the dose response curve auc for a variety of drugs that were tested against hundreds of cell lines the main goal in this task is to discover the genomic features associated with drug response 8 we also present quantitative results that show the effectiveness of these features when used to train regression models f 3 sic neural network descriptions and training details the first critic network used in the experiments with sinexp and hiv 1 datasets is a standard three layer relu dropout network with no biases i e when using this network the inputs x and y are first concatenated then given as input to the network the two first layers have size 100 while the last layer has size we train the network using adam optimizer with 0 5 0 999 weightdecay1e 4 learning rate 1e 3 and 0 1 and perform training iterations updates computed with batches of size all nns used in our experiments were implemented using pytorch smallcritic branchxy sequential 0 linear infeatures51 outfeatures100 biasfalse 1 relu 2 dropout p0 3 3 linear infeatures100 outfeatures100 biasfalse 4 relu 5 dropout p0 3 6 linear infeatures100 outfeatures1 biasfalse the critic network used in the experiments with liang and ccle datasets contains two different branches that separately process the inputs x branchx and y branchy then the output of these two branches are concatenated and processed by a final branch that contains three layer leakyrelu network branchxy"}