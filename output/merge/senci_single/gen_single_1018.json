{"1018": "neural program induction ( npi) is a pragmatic approach toward modularizing the reasoning process by translating complex natural language query into a multistep executable program. while npi has been commonly trained with the gold sketch or its payoff there, for realistic kbqa applications such gold programs are expensive to obtain and obtain the corresponding answers can be provided for training. the resulting combinatorial explosion in program space, along with extremely sparse rewards, makes npi for kbqa ambitious supervision and challenging. we present complex imperative program induction from terminal rewards ( cipitr) , an advanced neural programmer that propagates reward sparsity with auxiliary rewards, and restricts reward to correct programs using high-level constraints, kb schema, and inferred answer type. cipitr solves complex kbqa considerably more accurately than key-value memory networks and neural symbolic machines ( nsm) . for moderately complex queries requiring 2to 5-step programs, cipitr scores at least higher f1 than the competing systems. on one of the hardest class of programs ( comparative reasoning) with steps, cipitr outperforms nsm by a factor of and memory networks by times.1 the absence of gold programs extremely challenging. main contributions we present complex program induction from terminal rewards ( cipitr) , an advanced neural program induction ( npi) system that is able to answer complex logical, quantitative , and comparative queries by inducing programs of up to 7 using atomic operators and variable types. this, to our knowledge, is the first npi system to be trained with only the gold answer as ( very distant) supervision for inducing such complex programs. cipitr reduces the combinatorial program space to only semantically correct programs by ( i) incorporating symbolic constraints guided by kb schema and inferred answer type, and ( ii) adopting pragmatic programming techniques by the final goal into a hierarchy of subgoals, thereby mitigating the sparse reward problem by considering additional auxiliary rewards in a generic, task-independent way. the massive size of the kb involved ( 13 million entities and million tuples) poses a scalability challenge for prior npi techniques. availability of kb metadata helps standardize comparisons across techniques. the human-notated semantic parse of the questions provide the exact structure of the subgraph and the inference process on it to reach the final answer. as in this work, we are focusing on inducing programs where the gold entity relation annotations are known; for this data set as well, we use the human-annotations to collect all the entities and relations in the oracle subgraph associated with the query. the npi model has to understand the role of these gold program inputs in question-answering and learn to induce a program to reflect the same inferencing. cipitr is a program induction engine that samples from the feasible subset using generic constraints. the key and value embedding is obtained from a gru encoder as q. npi core: the query representation q is fed at the initial timestep to an environment encoding rnn, which gives out the environment state et at every timestep. this, along with the value embedding uvalt1 of the last output variable generated by the npi engine, is fed at every timestep into another rnn that finally outputs the program state ht. ht is then fed into the successive modules of the program induction engine as described below. the outvargen algorithm describes how to obtain uvalt1. procedure : npi core ( et1, ht1, uvalt1) et gru ( et1, u val t1, ht1) output: et, ht operator sampler: it takes the program state ht, a boolean vector p feas t denoting operator feasibility, and the number of operators to sample np. it passes ht through the lookup operation followed by feasibility sampling to obtain the top-np operations ( pt) . this paper introduces cipitr, an end - to - end training method to train a reinforcement learning model on complex real - word problems. cipitr takes a natural language query and generates an output program in a number of steps. a program is composed of actions, which are operators applied over variables, cipitr uses a beam search to obtain multiple candidate programs to provide feedback to the model from a single training instance. finally, in order to learn from the discrete action samples, the reinforce objective we next next describe several learning challenges that arise in the context of this overall architecture. handling complex queries by expanding the operator set and generating longer programs blows up the program space to a huge size of ( numop ( maxvar) m. additionally, whereas the relatively simple nsm architecture could explore a large beam size ( 50100), the complex architecture of cipitr entailed by the cpi problem entailed by the cpi problem could only afford to operate with a smaller beam size ( 20) which further exacerbates the sparsity of the reward space. for example, for integer answers, only a single point in the integer space returns a positive reward, without any notion of partial reward, without any notion of partial reward. this paper proposes a model for program induction that is able to predict the answer type of a human-annotated parse question. the model is able to predict the answer type of a human-annotated parse question with high accuracy. a comparative performance analysis of the proposed cipitr model, the rule-based model and the sparql decomposition approach is in table that take-away from these results is cipitr is indeed able to learn the rules behind the multistep inference process simply from the distance supervision provided by the questionanswer pairs and even perform slightly better in some of the query classes. we present cipitr, an advanced npi framework that significantly pushes the frontier of complex program induction in absence of gold programs. we show that it can generate at least meaningful programs having the desired answer-type or without repeating lines of code. on the other hand the nsmgenerated programs are often semantically wrong, for instance, both in the quantitative and quantitative count based questions, the type of the answer is itself wrong, rendering the program meaningless. this arises once again, owing to the token-by-token decoding of the program by nsm. while npi has been commonly trained with the gold sketch or its program to obtain the program space to only natural language queries and the corresponding answers can be provided for training. we present complex imperative program induction from terminal rewards ( cipitr) , an advanced neural programmer that mitigates sparsity with auxiliary rewards, and uses high-level constraints, kb schema, and inferred answer type. cipitr is an end - to - end program induction engine that is able to generate a large number of candidate programs from a single training instance. the program induction engine is able to generate a large number of candidate programs from a single training instance. in order to learn from the discrete action samples, cipitr uses a beam search to obtain multiple candidate programs to provide feedback to the model from a single training instance. the model is allowed to operate on all the generated variables in order to reach the answer. additionally, whereas the relatively simple nsm architecture could explore a large beam size ( 50100), the complex architecture of cipitr entailed by the cpi problem could only afford to operate with a smaller beam size ( 20), which further exacerbates the sparsity of the reward space. a problem as complex as ours requires not only generic constraints for producing semantically correct programs, but also incorporation of prior knowledge, if the model permits. cipitr is able to learn the rules behind the multistep inference process simply from the distance supervision and even perform the performance of cipitr slightly better in some of the query classes. the query types next in order of complexity are quantitative and quantitative count, which translate to an average of the hardest programs. for nsm and cipitr, seven models with different hyperparameters tuned on each of the seven question types. for the train and valid split, a rule-based query type classifier with accuracy was used to bucket queries into the classes listed in table for each of these three systems. for the train and valid split, a rule-based query type classifier with accuracy was used to bucket queries into the classes listed in table for each of these three systems, we also train and evaluate one single model over all question types.", "1000": "###", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1007": "###", "1008": "###", "1009": "###", "1010": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1019": "###", "1020": "###", "1021": "###"}