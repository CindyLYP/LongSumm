{"1000": "the paper presents a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data  but have translated text in a resource-rich language  to this end  a bilingual graph over word types is constructed to establish a connection between the two languages  and then label propagation is used to project syntactic information from english to the foreign language  given a corpus of parallel sentences  the task is to build a bilingual pos tagger that can project syntactic information from the english side to the foreign language  the vertices of the graph correspond to word types  while the vertices on the english side are individual word types  the edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  the paper proposes graph-based methods for semi-supervised learning  vertices from different languages are combined using edge weights  each feature concept is akin to a random variable and its occurrence in the text corresponds to a particular instantiation of that random variable  for each trigram type x2 x3  count how many times that trigram type co-occurs with the different instantiations of each concept  and compute the pointwise mutual information (pmi) between the two 5 similarity between two trigram types is given by summing over the pmi values over feature instantiations that they have in common  this is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them  finally  note that while most feature concepts are lexicalized  others  such as the suffix concept  are not lexicalized  given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and for all other vertices  to define a similarity function between the english and the foreign vertices  we rely on high-confidence word alignments  since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the english sentences de 5note that many combinations are impossible giving a pmi value of 0; e g   when the trigram type and the feature instantiation dont have words in common  and their foreign language translations df will provide coverage and high recall  and therefore we extract only intersected high-confidence alignments def based on these high-confidence alignments we can extract tuples of the form u v  where u is a foreign trigram type  whose middle word aligns to an english word type v  our bilingual similarity function then sets the edge weights in proportion to these tuples to initialize the graph for label propagation we use a supervised english tagger to label the english side of the bitext 7 we then simply count the individual labels of the english tokens and normalize the counts to produce tag distributions over english word types  these tag distributions are used to initialize the label distributions over the english vertices in the graph  (note that the middle words of the italian trigrams are too  which exhibits the fact that the similarity metric connects types having the same syntactic category ) the paper proposes a graph-based label propagation approach to project english pos labels to the foreign language  in the first stage  a single step of label propagation is run  transferring the label distributions from the english vertices to the connected foreign language vertices (say  v lf) at the periphery of the graph  note that because we extracted only high-confidence alignments  many foreign vertices will not be connected to any english vertices  the second stage consists of running traditional label propagation to propagate labels from these peripheral vertices v lf to all foreign language vertices in the graph  optimizing the following objective: c(q) vf \u2047 v lf ujn (ui) uivf \u2047 v lf  uivf \u2047 v lf   the authors propose a novel neural model for pos tagging  which is based on the feature-based hmm of berg-kirkpatrick et al  datasets svhn(url) and stl(url/) datasets as well as two baselines were used  languages not covered by the resources are indo-an english  danish  german  italian  portuguese  spanish  and greek  the universal pos tagset of petrov et al  - no lp  but extracts the constraint feature from the first stage of label propagation  hyperparameter tuning was done for each language  the no lp model does not outperform the unsupervised feature-hmm baseline  but it is statistically indistinguishable in terms of accuracy  the full model outperforms the no lp setting because it has better vocabulary coverage and allows the extraction of a larger set of constraint features  the paper proposes graph-based label propagation (gnp) for projecting part-of-speech (pos) information across languages", "1001": "the paper demonstrates that rnns can be effectively used in order to encode sequences and provide effective representations  the methodology is based on fisher vectors  where the rnns are the generative probabilistic models and the partial derivatives are computed using backpropagation  state of the art results are obtained in two central but distant tasks  which both rely on sequences: video action recognition and image annotation  this paper proposes a novel approach for fv representation of sequences using a recurrent neural network (rnn)  the rnn is trained to predict the next element of a sequence given the previous elements  the new representation is sensitive to ordering and therefore mitigates the disadvantage of using the standard fisher vector representation  it is applied to two different and challenging tasks: video action recognition and image annotation by sentences  the paper explores two different approaches for training the rnn for the image annotation and image search tasks  in the classification approach  the rnn is trained to predict the following word in the sentence  in the regression approach  the rnn is trained to predict the embedding of the following word (i e  it is treated as a regression task)  the large vocabulary size makes the regression approach more scalable and achieves better results than the classification approach  in the image annotation task  the rnn is used to extract features from the frames of the video  vgg c3d is used to extract features from the frames of the video  the rnn is trained to predict the next embedding of the video  the proposed method achieves state-of-the-art results on the tasks the model is composed of three main components: image representation  sentence representation  and image and sentence matching  the image is usually represented by applying a pre-trained cnn on the image and taking the activations from the last hidden layer  the sentence representation is represented by a recurrent neural network (rnn) that learns to compose the word embeddings of the fragments of the word  the notation of a multiset is used to clarify that the order of the words in a sentence does not affect the representation  and that a vector can appear more than once  for example  in the text encoding task  where each word is represented by its word2vec embedding  the location obtained in the semantic embedding space is somewhere in the convex hull of the words that belong to the multiset  the first improvement is to apply an element-wise power normalization function  the second improvement is to apply an l2 normalization on the fv after applying the power normalization function  the model is trained to predict the next element in the sequence  given the previous elements  at the end of the training  the output of the network is fed to a recurrent network which is trained to predict the next word in the sequence  the loss function for the training of the rnn is the cross-entropy loss  after the rnn is trained  it is ready to be used as a feature vector extractor for new sequences  also find this summary at davidstutz de(url/)  action recognition pipeline contains the underlying appearance features used to encode the video  the sequence encoding using the rnn-fv  and an svm classifier on top  each image is cropped into ten different ways  e g  the mean intensity is then subtracted in each color channel and the resulting images are encoded by the network  the average of the feature vectors obtained is then used as the single image representation  the cca algorithm is trained for an unrelated task of image to sentence matching  and its success suggests a new application of transfer learning: from image annotation to action recognition  their model consists of layers: leakyrelu activation layer  long-term memory (lstm) layer  and a 500d linear layer  the rnn is trained to predict the next element in the representation sequence given the previous elements  the partial derivatives with respect to the weights of other layers did not improve the performance  - dimensionality issues - word embeddings are not present in the vector space  - generalization issues - word embeddings are not present in the vector space  - regularization issues - no cosine similarity regularization  application domains action recognition  image textual annotation and search  results they test their model on ucf101 and hmdb51 action recognition datasets  the authors evaluate the proposed model on image annotation  image search  and sentence similarity tasks  this paper introduces a novel fv representation for sequences that is derived from rnns  the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs  the rnn-fv representation surpasses the state-of-the-art results for video action recognition on two challenging datasets  icri!", "1002": "we hypothesize that such talks constitute a coherent and concise description of the papers content  and can form the basis for good summaries  the rate of publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research  the paper proposes talksumm (acronym for talk-based summarization)  a method to automatically generate extractive content-based summaries for scientific papers based on video talks  the paper proposes to use the transcripts of talks  and treat them as the spoken summaries of pa-s  then  using unsupervised alignment- rithms  we map the transcripts to the corresponding text  and create extractive summaries  alignment between text and videos was studied in previous work  the paper focuses on nlp and ml conferences  and analyzed video talks from acl  naacl  emnlp  sig-2018)  and icml-2018)  during the talk  the speaker generates words for describing al bally sentences from the paper  one word at each time step  thus  at each time step  the speaker has a single sentence from the paper in mind  and produces a word that constitutes a part of its ver- bal description  then  at the next time-step  the speaker either stays with the same sentence  or moves on to describing another sentence  and so on  given the transcript  we aim to retrieve those source sentences and use them as the sum- mary  the number of words uttered to describe each sentence can serve as importance score  in dicating the amount of time the speaker spent de- scribing the sentence  this enables to control the summary length by considering only the most im- portant sentences up to some threshold  the hmm model is used to model the assumed generative process  each hidden state of the hmm corresponds to a single sentence  the transition probabilities between any two sentences in the paper are conditioned on the assumption that there is a con-protract between the sentences  evaluation the model is evaluated on the cl-scisumm shared task and on the talksum dataset  the authors evaluate the model on the following tasks: given the talks presented in the conference  evaluate the quality of the talk  given the sentences in the generated talk  the paper proposes a novel automatic method for gener- ate training data for scientific paper summarization based on conference talks given by authors  split-and-rephrase dataset split-and-rephrase dataset contains instances mapping a single complex sentence to a sequence of sentences that express the same meaning  together with rdf triples that describe their semantics  they consider two systems: text-to-text setup that does not use the accompanying rdf information  and a semantics-augmented and context-aware setup that does", "1003": "the paper proposes an ensemble approach to detect emotions in text using pre-trained word embeddings  the paper proposes a model for emotion detection in text based on word vectors obtained from pre-trained word embedding models  pre-trained word vectors are used as input to the neural network  classi er given some test sample  a classi er outputs the decision function value for each emotion that appears in the training data  the classes associated with the test sample are then taken to be the emotion with the highest decision value (for multi-class) or the set of emotions with a positive decision value (for multi-label)  nrc lexicon features (number of terms in a post associated with each label in the nrc lexicon) and presence of punctuation marks  question marks  links  happy emoticons  and sad emoticons  word embedding based vectors can be used to represent a document into a xed vector  cbow (continuous bag of words) is used to represent the document in the vector space  tfidf is the tfidf weight for each term ti in case ti was not present in the training data  smoothed its idf weight as if it appeared in one document (this yielded better performance than discarding the term)  classi er weight (classi) - calculated a weight function  w (t  e  t) for each term in the training data which indicates its importance in classifying a document as expressing emotion e we did this by rst representing the documents in a bow binary vector representation where we only extracted unigram features  then  for each e we trained an svm model with a linear kernel and took m(e  t) to be the weight associated with the model with each term t in the training data  motivated by guyon who showed that m(e  t) is an indicative feature selection criterion  we de ne: w (t  t) m(e  t) e  where e and e are the corresponding average and standard deviation of model weights in absolute value  ensemble methods tend to achieve better results when there is a signi cant diversity among the classi ers  datasets sentiment analysis datasets is a set of datasets where participants have reported experiences and reactions for seven emotions  semeval fairy tales contains newspaper headlines labeled with the six ekman emotions by six annotators  for blog posts  the most dominant emotion was considered as the headline label  bow is a state-of-the-art approach for emotion detection in short texts  this was used as a state-of-the-art approach for emotion detection in short texts in many cases  e g   19  and more  emotion detection datasets are labeled with multiple emotions and imbalanced  thus  the classi cation performance for all emotion classes is evaluated by using macro average f1-score results are not very detailed or exhaustive  results for all datasets and document representation methods  word vectors trained by glove achieved higher performance than using word2vec based vectors  combining both bow and embedded document representations improves f1-scores for all the models  pre-trained word vectors are used for emotion detection", "1004": "  we show that  in addition to text based turn features  dialogue features can significantly improve detection of emotions in social media customer service and help predict emotional techniques used by customer service agents  a recent study shows that one in five (23) customers in the u s  say they have used social media for customer service in 2014  up from of obviously  companies hope that such 1/url/news/docs/2014-x/2014-global-customer- uses are associated with a positive experience  yet there are limited tools for assessing this  emotions are a cardinal aspect of inter-personal communication: they are an implicit or explicit part of communication  and of particular importance in the setting of customer service  as they relate directly to customer satisfaction and experience (o  typical emotions expressed by customers in the context of social media service dialogues include anger and frustration  as well as gratitude and more)  however  it is important to note that emotions expressed by customer service agents are typically governed by company policies that specify which emotions should be expressed in which situation (rafaeli and sutton  1987)  this is why we talk  about agent emotional techniques rather than agent emotions  consider  for example  the real (anonymized) twitter dialogue in figure in figure in this dialogue  customer disappointment is expressed in the first turn (bummer/uh oh!) followed by customer support (uh oh!) then in the last two turns both right and left  this paper proposes a novel approach for predicting emotions in online dialogues  tldr; the authors propose an emotion-based dialogue system that predicts the user\u2019s next response based on the previous one  in addition  if the text is considered  then the texts are transcripts of calls that are very different from written text  twitter dialogue dataset we collected a twitter dialogue dataset corresponding to all conversations recorded by twitter between may and august of last year  companies that utilize the twitter platform as a channel for customer service use a dedicated twitter account which provides real-time support by monitoring tweets that customers address to it  at the same time corporate support agents reply to these tweets also through the twitter platform  a customer and an agent  can use the twitter reply mechanism to discuss until the issue is solved (e g   a solution is provided  or the customer is directed to another channel)  or until the customer is no longer active  in the present work  we define a dialogue to be a sequence of turns between a specific customer and an agent  where the customer initiates the first turn  consecutive posts of the same party ( customer or agent) uninterrupted by the other party  are considered as a single turn (even if there are several tweets)  given the nature of customer support services  we assume the last turn in the dialogue is an agent turn (e  youre very welcome  :) hit us back any time you need support  thus  we expect an even number of turns in the dialogue twitter accounts supporting north american customers were analysed  the paper proposes to classify the current turn to emotions expressed by a customer and to predict the emotional technique used by an agent in a dialogue  experiments on alexnet are performed  temporal features include features extracted from the timeline of the dialogue (time elapsed between the timestamp of the last customer/agent turn and the turn of the subsequent turn)  customer/agent response time (two local categorical features defined as the median of the customer/agent response times preceding the current turn) and customer emotion features (the emotions predicted for previous turns)  features unigrams  bigrams  nrc lexicon features (number of terms in a post associated with each label in nrc lexicon)  presence of exclamation marks  question marks  links  happy emoticons  and sad emoticons  for both of the agent and customer turn classification tasks  we implemented two different models which incorporate all of the feature sets we have detailed above  dialogue and textual features are extracted for ti if it is a customer turn  or for ti1 if it is an agent turn  temporal features are extracted using time values between previous turns  sequence classification method (svmhmm) - classifies a sequence into its most probable tag sequence  conditional random fields (crf) are used for the classification task  the paper presents a novel approach to classify dialogue transcripts based on their emotion content  both svm and svm-hmm were superior for all tasks and for all possible dialogue sizes  for both classification tasks  svm-hmm dialogue model outperformed baseline results for almost all emotions  where average macro and micro results are statistically significant compared to the baseline  baseline (textual features)  emotional features  temporal features and integral features  in the future  we plan to run experiments in which the predicted emotional technique is actually applied in the context of new dialogues to measure the effect of such predictions on real support dialogues", "1005": "tldr; the authors propose a differentiable neural architecture search (d-nas) approach to optimize the architecture for few-shot learning (fsl)  the paper presents a novel neural network architecture search (nas) technique for few-shot classification (fsc)  the method is called as neural network architecture search (neas)  the method is based on the idea of bi-adaption where different operations are adapted to the task at hand  the method is tested on a dataset of few-shot classification tasks  a few-shot learning problem is one where you train a model on a small number of examples  and you want it to be able to classify new instances of the same category according to their proximity to the examples in the same training set  task-adaptable block the task-adaptable block has a graph structure with connections that can modulate the architecture  adapting it to the few-shot task at hand  sub-models metadapt controllers predict the change in connectivity that is needed in the learned graph as a function of the current task  the architecture of the metadapt block is similar to that of a directed acyclic graph (dag) - feature maps are linked by mixtures of operations  task-adaptive block the task-adaptive block is accompanied by a set of metadapt controller modules  one per edge  they are responsible for predicting  given a few-shot task  the best way of adapting the mixing coefficients (i j) for the corresponding edge operations  let (i j) be the vector of all (i j) o let (i j) be the globally optimized coefficients (optimization process described below)  then metadapt controllers predict the task-specific residuals (i j)  that is the modification required to make to (i j) for the current task (few-shot episode)  finally  (i j) (i j) are the final task-adapted coefficients used for the mixed operation calculation as in the equation architecture for each metadapt controller  predicting (i j) is as follows  it receives the input feature maps of the corresponding edge xi  computed for all the support samples of the episode  it performs global average pooling to obtain a (s d) tensor  followed by a bottleneck linear layer (with relu activation) that operates on each sample individually  to get a (s d) size tensor  then  all support samples are concatenated to form a in conventional gradient descent training  replacing simple sequence of convolutional layers with the suggested dag  with its many layers and parameters  in conventional gradient descent training will result in a larger over-fitting  this is even worse for fsl  where it is harder to achieve generalization due to scarcity of the data and the domain differences between the training and test sets  they test on miniimagenet and fc100 few-shot image classification datasets  they use adam with learning rate 0 001  momentum and weight decay  architecture transferability it has been shown  in the case of architecture search  that it is possible to learn an architecture on a smaller dataset  e g  cifar-10  and then optimize the architecture is transferable to a larger dataset  this helps mitigating the costly architecture search process  the architecture learned on fc100 is transferred to miniimagenet and the weights of the transferred architecture are trained on miniimagenet  gumbel-like weights are used instead  propose met modifiers  a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks", "1006": "  we outline an approach to detecting such egregious conversations  using behavioral cues from the user  patterns in agent responses  and user-agent interaction    we study detecting egregious conversations that can arise in numerous ways  for example  incomplete or internally inconsistent training data can lead to false classification of user intent  failure to maintain adequate context can cause chatbots to miss anaphoric references  in the extreme case  malicious actors may provide heavily biased responses  the classifier model is trained on a corpus of conversations  in addition  some of these features are contextual  meaning that they are dependent on where in the conversation they appear  when the agent starts losing the context of a conversation  fails to understand the customer\u2019s intentions  or keeps repeating the same responses  the conversation may become extremely annoying  similarity between agent\u2019s responses is measured by averaging the pre-trained embeddings of each word in the sentence  we can expect that training would not cover all customer intents  if the classifier technology provides an estimate of classification confidence  the agent can respond with some variant of im not trained on that when confidence is low  in some cases  customers will accept that not all requests are supported  in other cases  unsupported intents can lead to customer dissatisfaction (sarikaya  and cascade to an egregious conversation)  an effective interaction requires the expenditure of relatively large effort from the customer with little return on the investment  when a customer repeats or rephrases an utterance  it usually indicates a problem with the agent\u2019s understanding of the customer\u2019s intent  this can be caused by different reasons as described in (sano et al ) to measure the similarity between subsequent customer turns to detect repetition or rephrasing  we used the same approach as described in (sano et al ) to measure the similarity between subsequent customer turns with a high similarity value  in order to analyze the emotions that customers exhibit in each turn  we utilized the ibm tone analyzer service  trained using customer care interactions  and infers emotions such as frustration  sadness  happiness  we focused on negative emotions (denoted as emo) to identify turns with a negative emotional peak (i e   single utterances that carried high negative emotional state)  as well as to estimate the aggregated negative emotion throughout the conversation (i  the averaged negative emotion intensity)  in order to get a more robust representation of the customer\u2019s negative emotional state  we summed the score of the negative emotions (such as frustration  anger etc ) into a single negative sentiment score (denoted as neg)  note that we used the positive emotions as a filter for other customer features  such as the rephrasing analysis in examining the conversation logs  we noticed that it is not unusual to find a customer asking to be transferred to a human agent  such a request might indicate that the virtual agent is not providing a satisfactory service  moreover  even if there are human agents  they might not be available at all times  and thus  a rejection of such a request is sometimes reasonable  but might still lead to customer frustration  customer turns that contain exactly one word are also detected  the authors also looked at features across utterance-response pairs in order to capture a more complete picture of the interac-tion between the customer and the virtual agent  here  we considered a pair to be customer utterance followed by an agent response  for example  a pair may contain a turn in which the customer expressed negative emotions and received a response of not trained by the agent  in this case  we would leverage the two analyses: emotional and unsupported intent  similarity between the customer\u2019s turn and the agent\u2019s response in cases of rephrasing  after training  test conversations are classified by the model  after being transformed to a feature vector in the same way a training sample is transformed  we extracted data from two commercial systems that provide customer support via conversational bots (hereafter denoted as company a and company b)  both agents are using similar underlying conversation engines  each embedded in a larger system with its own unique business logic  baseline models (rule-based and text-based) were also tested  for the egregious class  the text-based model performed significantly better than the egr model  to better understand the contributions of different sets of features to our egr model  we examined various features in an incremental fashion  based on the groups of feature sets that we defined in section 3  we tested the performance of different group combinations  added in the following order: agent  customer and customer-agent interactions  the authors also tested how robust the features of the egr model were to changes in the dataset  in addition to natural language understanding (nlu) error  the other rephrasing motivations considered were language generation limitation (lg) and unsupported intent error  egr outperforms the other models in terms of recall    we have shown how it is possible to detect egregious conversations using a combination of customer utterances  agent responses  and customer-agent interactional features  the goal of this work is to give developers of automated agents tools to detect and then solve problems cre-ated by exceptionally bad conversations", "1007": "the paper presents an analysis into the inner workings of convolutional neural networks (cnns) for processing text  cnns used for computer vision can be interpreted by projecting filters into image space  but for discrete sequence inputs cnns remain a mystery  we aim to understand the method by which the networks process and classify text  we examine common hypotheses to this problem: that filters  accompanied by global max-pooling  serve as ngram detectors  we show that filters may capture several different semantic classes of ngrams by using different activation patterns  and that global max-pooling induces behavior which separates important ngrams from the rest  interpretability - the ability to explain the learned functionality of a model to the untrained observer  each word in the input document is represented as an embedding vector  max-pooling is applied to the vectors followed by a relu activation  the result is then passed to a linear layer for the final classification  - sentiment analysis datasets - amazon reviews  yelp reviews and glove word embeddings  intuitively  ngrams which any filter scores highly (relative to how it scores other ngrams) are ngrams which are highly relevant for the classification of the text  the paper proposes a method for separating informative and uninformative contributions in a max-pooling network  common intuition suggests that each filter is homogeneous and specializes in detecting a specific classes of ngrams  for example  a filter may specialize in detecting ngrams such as had no issues  had zero issues  and had no problems  we challenge this view and show that filters often specialize in multiple distinctly different semantic classes by utilizing activation patterns which are not necessarily maximized  for each ngram  we can calculate the score by considering the inner products between every word embedding wi in u and every parallel slice in f  the slot activation vector captures how much each word in the ngram contributes to its activation  there is a big and consistent gap in scores between the top-scoring naturally occurring ngrams and top-scoring possible ngrams  in our elec model  when averaging over all filters  the top naturally-occurring ngrams score less than the top possible ngrams  interestingly  the model does not consider the filter bias  if one is used  this bias is a single number (per filter) which is added to the sum of slot activations to arrive at the ngram activation which is passed to the max-pooling layer  in particular  we reveal that filters are not necessarily homogeneous: a single filter may detect several different semantic patterns  each one of them relying on a different slot activation pattern  for example  ngrams with determiners and other filler tokens such as periods and commas do not pass the threshold for this filter  hypothesis (ii) suggests that this slot may receive a strong negative score for words such as not and nt  causing such negated patterns to drop below the threshold  we are interested in a more systematic method of identifying these cases  identifying negative slot activations would be very useful for understanding the semantics captured by a filter and the reasoning behind the bold activation of an ngram  also find this summary at davidstutz de(url/)  the paper proposes to associate each filter with the following items: the class which this filter\u2019s strong signals contribute to (in the sentiment task: positive or negative); the threshold value for the filter  together with its purity and coverage percentages (which essentially capture how informative this filter is); a list of semantic patterns identified by this filter  this improves the previous interpretation attempts by considering only ngrams that pass the threshold for their filter  - maxpooling over time induces a thresholding behavior on the output of the convolution layer  essentially separating between features that are relevant to the final classification and features that are not  - decompose the ngram score into word-level scores by treating the convolution of a filter as a sum of word-level convolutions  allowing us to examine the word-level composition of the activation  specifically  by maximizing the word-level activations by iterating over the vocabulary  we observed that filters do not maximize activations at the word-level  but instead form slot activation patterns that give different types of ngrams similar activation strengths  this provides empirical evidence that filters are not homogeneous  by clustering high-scoring ngrams according to their slot patterns we can identify the groups of linguistic patterns captured by a filter  - filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words", "1008": "cnn/dailymail dataset a mixed extractive-abstractive summarization approach is proposed  which is applied as a postprocessing step over a given sequence of extracted sentences  this paper presents a mixed abstract and extractive summarization approach for text  the output of the system is a summary whose sentences have been extracted from a given document  for each sentence si s (in order) the editor makes one of the three possible decisions: extract  abstract or reject  therefore  the editor may modify summary s by paraphrasing or rejecting some of its sentences  resulting in a mixed extractive-abstractive summary s  in each step  the editor considers both sentence representations esi and asi as its input  together with two additional auxiliary representations  the first auxiliary representation is that of the whole document d itself  such a representation provides a global context for decision making  the second auxiliary representation is that of the summary that was generated by the editor so far  denoted as gi1 rn  with g0 such a representation provides a local context for decision making  given the four representations as an input  the editors decision for sentence si s is implemented using two fully-connected layers: softmax tanh (esi  asi  gi1  d bc)  where denotes the vectors concatenation  v r3m  wc rm4n  bc rm and b are learnable parameters  the editor chooses the action i  ar with the highest likelihood (according to eq  upon decision  in case it is either e or s a the abstractor is basically an encoder-decoder with a copy mechanism  yet  instead of applying it directly on a single extracted sentence  we apply it on a chunk of three consecutive sentences  this in turn allows to generate an abstractive version of sei that benefits from a wider local context  s is replaced by s ai within the original document d  sai is treated as an ordinary sentence within d  where the rest of the document remains untouched  the soft-labeling loss is a combination of a temporal convolutional model followed by a bilstm encoder  training and validation methodology cnn/mail dataset non-annonymized version of the cnn/dailymail dataset abstractor extractor and abstractor are trained using the same hyperparameters  the authors evaluate their model on several tasks  where it outperforms state-of-the-art models  instead of solely applying extraction or abstraction  editnet mixes both together  moreover  editnet implements a novel sentence rejection decision  allowing to correct initial sentence selection decisions which are predicted to negatively effect summarization quality", "1009": "the paper proposes a high-dimensional encoding of chinese phonetic similarities (pinyin) based on annotated data  this paper proposes a supervised learning approach to address the problem of phonetic similarity in chinese languages  the key idea is to use a supervised learning approach to learn a relative encoding of words in a language  this is done by considering a language as a whole  rather than just a single language  the proposed approach is called dimsim  which stands for relative encoding of place  dimsim generates ranked candidate words with similar pronunciations to a seed word  similarity is measured by a phonetic distance metric based on n-dimensional encodings  initial  final and tone components can be independently phonetically compared in pinyin  an important characteristic of pinyin is that the three components  initial  final and tone  can be independently phonetically compared  for example  the phonetic similarity of the final ie and ue is identical in the pinyin pairs xie2 ue2 and lie2 lue2  in spite of the varying initials  english  by contrast  does not have this characteristic  dimsim dimsim represents a given word w as a list of characters ci1 i k where k is the number of characters and pci denotes the pinyin of ith character  the initial  final  and tone components of pci are denoted as p i ci  p f ci  and p t ci  respectively  manhattan distance is an appropriate metric since the three components are independent  any single change does not affect more than one component  and any change affecting several components is the result of multiple independent and additive changes  for example  u is written as u after j  q  x uo is written as o after b  p  m  f or w  there are a total of six rewritten rules in pinyin (iso)  since these rules are fixed  it is straightforward to preprocess the pinyins according to these rules to turn them into the original form of pinyins as an internal representation before conducting the comparison  for example  we represent ju as bo aso  after the preprocessing step  we independently compare ompon nts  this is done by grouping pinyin components into initial clusters and only annotating pairs within each cluster  and represent tive cluster pairs  the model is generic and can easily be extended to any n-dimensional space  there are five tones in chinese  represented by a tone number scale ranging from to it is simple to use tone numbers for tone encodings and the difference between the tones of two pinyins as the raw measure of distance  ranging from to (e g   st (xue2  xue4))  one exception is that we encode tone as the numerical value of tones since tone is more similar to tone compared to tone according to the relative pitch changes of the four tones (iso  however  this measure must first be scaled to be comparable to the pairwise phonetic difference of initials and finals  there is an additional constraint: any pairwise difference in or finals must have input word w  threshold th dict; output: words outws; begin pys getpinyins(w  th)  headpy getspinyins(py  headwords getpinyins(py  dict; end sim getsimilarity(cw w)  end sortascsim(outws)  return outws; end algorithm 1: generating phonetic candidates how to generate similar candidates given a word w  a similarity threshold th  and a chinese pinyin dictionary dict  generate a list of pinyins pys whose similarity to py falls within the threshold th  these are used to generate a list of words with the same pinyin in pys and the same number of characters as w  calculate the similarity of each candidate word with w using equation and filter out candidates that fall outside the similarity threshold th  thus  th is a parameter that affects the precision and recall of the generated candidates  a larger th generates more candidates  increasing recall while decreasing precision 3 finally  the candidates ranked in ascending order by similarity distance are output  the baselines are: double metaphone (dm)  aline  and minimum edit distance (med)  dimsim improves recall and mrr by factors of 1 5  1 2  and respectively  dimsim is robust to variations in the scoring and penalty functions  increasing the number of dimensions did not improve the results  dimsim matches more characters that are simi- lar to the first character of the given word  which in turn increases the number of candidates within the distance  thus  the probability of including the labeled gold standard words in the results increases  mmr is less sensitive to th  converging when th reaches  but is reduced too much for th 128  while generating more candidates improves the recall  presenting too many candidates to a downstream application is not desirable  dimsim works as long as the targeted words are in the dictionary  this shortcoming is generally alleviated by adding new terms to the dictionary  dimsim cannot derive phonetic candidates from dialects that are not encoded in our mapping table  for example  for (dong4)(suan4)  the targeted word (dong1)(x2) is obtained using the pronunciation of southern fujian  however  our approach can easily be extended to incorporate and capture such variants by learning mapping tables for each dialect and using them to generate corresponding candidates  however  this paper does not address the problem of the sparsity problem in korean language processing by breaking down each character into a small set of primitive units  the paper demonstrates that this approach improves mrr by 7 5x  recall by 1 5x and precision by 1 4x over existing approaches", "1010": "relation detection is a core component of many nlp applications including knowledge base question answering (kbqa)    we propose a hierarchical recurrent neural network enhanced by residual learning which detects kb relations given an input question  our method uses deep residual lstms to compare questions and relation names via different levels of abstraction  additionally  we propose a simple kbqa system that integrates entity linking and our proposed relation detector to make the two components enhance each other  our experimental results show that our approach not only achieves outstanding relation detection performance  but more importantly  it helps our kbqa system achieve state-of-the-art accuracy for both single-relation (simple) and multi-relation (webqsp) qa benchmarks  knowledge base question answering (kbqa) systems answer questions by obtaining information from kbs  the paper proposes to break the relation names into word sequences for question-relation matching  key points - given an input question and a set of candidate entities retrieved by an entity linker based on the question  our proposed relation detection model plays a key role in the kbqa process: re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model  this step is important to deal with the ambiguities normally present in entity linking results  finding the core relation (chains) for each topic entity selection from a much smaller candidate entity set after re-ranking  - optional constraint detection step  when the question cannot be answered by single relations (e g   multiple entities in the question)  - the highest scored query from the above 2 is the directed path from root to answer node relation extraction (re) is an important sub-field of information extraction  general research in this field usually works on a (small) pre-defined relation set  where given a text and two entities  the goal is to determine whether the text indicates any types of relations between the entities or not  the number of relations is usually not large: the widely used ace2005 has 11/32 coarse/fine-grained relations; pactac has kb relations although it considers open-domain relations  all are much fewer than thousands of relations in kbqa  thus  few work in this field focuses on dealing with large number of relations or unseen relations  kb relation detection is formulated as a sequence matching problem  however  while the questions are natural word sequences  how to represent relations as sequences remains a challenging problem  here we give an overview of two types of relation sequence representations commonly used in previous work  relation as a single token (relationlevel)  in this case  each relation name is treated as a unique token  the problem with this approach is that it suffers from low relation coverage due to limited amount of training data  thus cannot generalize well to large number of opendomain relations  relation as word sequence (word-level)  the relation is treated as a sequence of words from the tokenized relation name  it has better generalization  but suffers from the lack of global information from the original relation names  this is because the incorrect relation contains word plays  which are more similar to the question 3 entity information used in kbqa systems as features for the final answer re-rankers  (such word play) in the embedding space  on the other hand  if the target relation co-occurs with questions related to tv appearance in training  by treating the whole relation as a token (i e  relation id)  we could better learn the correspondence between this token and phrases like tv show and hierarchical sequence matching with residual learning approach for relation detection  each token is transformed to its word embedding and two bilstms are used to get its hidden representations bword1:m1 brel1:m2 (each row vector is the concatenation between forward/backward representations of i)  the lstms are initialized with the final state representations of the word sequence and used as a back-off for unseen relations  max-pooling is applied to these two sets of vectors and the final relation representation is obtained  bilstm works on the word embeddings of question words q1   qn and gets the hidden representations 1:n; ; n the second layer bilstm works on the embeddings of question words n1:n to get the second set of hidden representations 1:n  note that the first layer does not necessarily correspond to the word(relation)-level relation representations  instead it could potentially match to either level of relation representations the key idea is to add shortcut connections between layers of the same bilstm  relation detection - use the raw question text as input for a relation detector to score all relations in the kb that are associated to the entities in elk(q)  use the relation scores to re-rank elk(q) and generate a shorter list el0k0 containing the top-k0 entity candidates  query generation - combine the scores from step and 2  and select the top pair (e  r) constraint detection - compute similarity between q and any neighbor entity of the entities along r (by a relation rc) add the high scoring c and rc tuple to the query  for each question q  after generating a score using hr-bilstm  we use the top l best scoring relations (rlq) to re-rank the original entity candidates  in this step  for each candidate entity el0k(q)  we use the question text as the input to a relation detector to score all the relations r re that are associated to the entity el in the kb  finally  the system outputs the entity  relation (or core-chain) pair according to: s(e  r) max e2el0 k0 (q)  sre2re (q)  where is a hyperparameter  entity matching given the top-scored query generated by the previous steps  for each node v (answer node or cvt node)  we collect all the nodes c connecting to v (with relation rc) with any relation  and generate a sub-graph associated to the original query  entity-tuned on sub-graph nodes  we compute a matching score between each n-gram in the input question (without overlapping the topic entity) and the entity name of c (except for the node in the original query) by taking into account the maximum overlapping sequence of characters between them  if the matching score is larger than a threshold (on training set)  we will add the constraint entity c (and rc) to the query by attaching it to the corresponding node v on the core-chain  link to the paper(url) dataset simplequestions(url simplequestions) webqsp(url webqsp) multi-relation kbqa dataset  - hierarchical matching between questions and relation words improves results on webqsp and simplequestions  but does not improve on simplequestions  - weight embeddings do not improve results on webqsp  - does not use joint-inference or feature-based re-ranking step  kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks  we propose a novel kb relation detection model  hr-bilstm  that performs hierarchical matching between questions and kb relations  also find this summary at davidstutz de(url/)  tldr; the authors propose a method for constraint-based question answering with knowledge graph  semantic parsing on freebase from question-answer pairs  large-scale simple question answering on memory networks  zihang dai  lei li  and wei xu  classifying relations by ranking with convolutional neural networks  the results of a study on the effect of temperature on the dynamics of the coriolis force are reported and discussed  question answering with attention  improved relation extraction with feature-rich compositional embedding models  propose deep residual learning for image recognition  the paper proposes a neural machine translation (nmt) system for the task of translating between different corpora  the paper proposes to combine lexical resources with semantic resources in order to perform semantic classification  generating characteristic-rich question sets for qa evaluation  link to the paper(url) relation classification via multi-level attention cnns  the paper proposes to use reinforcement learning (rl) for language modeling  the paper proposes a novel approach for performing multi-perspective matching for natural language sentences  association for computational linguistics beijing  china  pages(url/)  freebase qa freebase qa: information extraction or semantic parsing? xuchen yao and benjamin van durme  semantic parsing via graph query generation semantic parsing via graph query generation semantic parsing via graph query generation semantic parsing via graph query generation semantic parsing via graph query generation semantic parsing via graph query generation semantic parsing via graph query generation semantic parsing for single-relation question answering  simple question answering by attentive convolutional neural network  embedding lexical features via lowrank tensors  relation classification via convolutional deep neural network  exploring various knowledge relation extraction in association with computational linguistics  tldr; the authors propose attention-based bidirectional short-term memory networks for relation classification", "1011": "  we propose a human-in-the-loop (huml) dictionary expansion approach that employs a lightweight neural language model coupled with tight huml supervision to assist the user in building and maintaining a domain-specific dictionary from an input text corpus  the approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus as well as predict new unseen terms not currently in the corpus using the accepted dictionary entries (exploit)  huml is a system that uses neural language models (e g   word2vec) to perform dictionary expansion  the system runs in two phases: explore phase: identify similar instances to the dictionary entries that are present in the input text corpus  using term vectors from the neural language model to calculate a similarity score  exploit phase: construct more complex multi-term phrases by replacing single terms in the input dictionary with terms that are related to the instances in the text corpus  the evaluation shows that the system performs nearly as good as the human evaluator on the exploit phase what they suggest a new approach for dictionary extraction from text  the approach runs in iterations  where each iteration runs first the explore phase  then the exploit phase  the explore phase uses the instances available in the input dictionary to identify similar candidates that are already present in the corpus vocabulary vtc  which are then accepted or rejected by the huml  the accepted candidates are then added to the input dictionary and are used in the exploit phase as well as the next explore iteration  similarity between words in the input dictionary and all the words in the corpus vocabulary are calculated using the cosine similarity between the vectors of the instances  in the exploit phase  we try to identify more complex phrases that dont exist in the corpus vocabulary by analyzing the structure of the instances in the input dictionary  this is critical to help future proof a lexicon against new text  for a surveillance application (e g  drug side effects mentioned on twitter) it reduces how frequently a human needs to tune up the lexicon to make sure it is catching all relevant entity instances  two phrase generation algorithms are used: the first approach breaks each instance in the vocabulary to a set of single terms t1  t2   tn  then for each term in tit we identify a set of similar terms tsti  ts1  ts2  tss in the vocabulary vtc using equation in the next step  we build new phrases by replacing ti with a term tsi from tsti the new phrases are sorted based on the similarity score and the top-n are selected as candidates evaluation the evaluation is conducted using data from the healtcare domain  specifically tackling the problem of identifying adverse drug reactions in user generated data  the results show that using explore and exploit alternately leads to the best performances  in this experiment  we show the importance of the promptness of the huml system on the number of newly discovered instances  i e   we evaluate if the user gives their feedback to the system sooner it will improve the performance of the system  to do so  we run the explore/exploit approach with different feedback intervals  the feedback interval indicates how many candidates the system needs to identify before the user gives their feedback to the system  for example  when using feedback interval of 10  the user gives their feedback after candidates are identified by the system  we evaluate feedback intervals of 10  50  100  and after each iteration we count the number of accepted candidates  and include them in the dictionary to be used for the next iteration  the results are shown in fig this paper proposes an interactive dictionary expansion tool using a lightweight neural language model", "1012": "in this work  we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes  hence  the method is able to consider significantly more proposals and doesnt rely on a successful first stage hypothesizing bounding box proposals  bounding box candidates are extracted from images and phrases  this paper proposes a method to explicitly ground natural language in images and videos  the paper proposes an approach for textual grounding which is formulated as an energy minimization over a large number of concepts bounding boxes  the search over a large number of bounding boxes allows us to retrieve an accurate bounding-box prediction for a given phrase and an image  the energy is based on a set of image concepts like semantic segmentations  detections or image priors  all those concepts come in the form of score maps which we combine linearly before searching for the bounding box containing the highest accumulated score over the combined score map  it is trivial to add additional information to our approach by adding additional score maps  moreover  linear combination of the score maps reveals importance of score maps for specific queries as well as similarity between queries such as tokener  hence the framework is easy to interpret and extend to other settings  word embeddings are used for both the word embeddings and word embeddings  in contrast to existing approaches which are generally based on a small set of bounding box proposals  we efficiently search over all possible bounding boxes", "1013": "the paper proposes a new method for learning implicit generative models by performing mean and covariance matching of features extracted from pretrained deep convnets  generative feature matching networks (gfmn) learns implicit generative models by performing mean and covariance matching of features extracted from all convolutional layers of pretrained deep convnets  some interesting properties of gfmns include: (a) the loss function is directly correlated to the generated image quality; (b) mode collapsing is not an issue; (c) the same pretrained feature extractor can be used across different datasets; (d) the loss function can be used across different datasets with different minibatch sizes; (e) the same pretrained feature extractor can be used across different datasets with different minibatch sizes; (f) the loss function can be used across different datasets with different minibatch sizes; (g) the same pretrained feature extractor can be used across different datasets with different minibatch sizes; (g) the loss function can be used across different datasets with different minibatch sizes; (f) the loss function can be used across different datasets with different minibatch sizes; (g) the same pretrained feature extractor can be used across different datasets with different minibatch sizes; (c) the loss function can be used across different datasets with different minibatch sizes; (d) the loss function can be used across different datasets with different mini g is trained by minimizing the l2-loss with precomputed estimates of jpdata and jpdata  multiple training iterations are performed where minibatch of generated (fake) data is sampled and optimized using stochastic gradient descent with backpropagation  in order to train with a mean and covariance feature matching loss  one needs large minibatches to obtain good mean and covariance estimates  with images larger than 3232  dcnns produce millions of features  resulting easily in memory issues  we propose to alleviate this problem by using moving averages of the difference of the means (covariances) of real and generated data  instead of computing the (memory) expensive feature matching loss in eq  1  we keep moving averages of the difference of feature means (covariances) at layer j between real and generated data  we can now rely on vj to get better estimates of the population feature means of real and generated data while using a small minibatch of size n for a similar result using the feature matching loss given in eq  one would need a minibatch with large size n  which is problematic for large number of features  mmd(k  p  q) mmd(k  q)) mmd(k  q) mmd( generative moment matching network autoencoder (gfmn) uses a gaussian kernel to perform mean and covariance matching in a pf space induced by a non-linear kernel function that is orders of magnitude larger than the latent code  gfmns are trained with an adam optimizer  they train on cifar-10  celeba  lsun bedrooms  svhn  stl  mnist  cifar-10  cifar-100  moving average (ma) outperforms simple moving average (ma) in terms of image quality  the generator is trained with a loss function that only performs feature matching  tldr; the authors propose gmmnae  a method for non-adversarial training of implicit gans by using moment matching on perceptual features from all layers of pretrained neural networks  a kernel is said universal if for any compact subset of z it is dense in c(z)  a feature map is universal if (y) is dense in c(z)  for all z compact subsets of x i  also find this summary at davidstutz de(url/)  results show that using mean covariance feature matching improves results for both is and fid  in both dcgan-like generator and discriminator  an extra layer is added when using images of size in vgg architecture  after each convolution  we apply batch normalization and relu  they use either mse or laplacian pyramid loss  the generated images do not look as realistic as the real images  results higher quality images when more layers are used  the discriminator  being pretrained on imagenet  can quickly learn to distinguish between real and fake images  this limits the reliability of the gradient information from the discriminator  which in turn renders the training of a proper generator extremely challenging or even impossible  this is a well-known issue with gan training where the training of the generator and discriminator must strike a balance  they use adam as their moving average  results better quality images for both gmmn and gfmn  results for both autoencoder and convnet are shown", "1014": "the authors propose a joint low-rank tensor completion and product category inter-purchase duration vector estimation approach to solve the positive-unlabeled recommendation problem  the authors propose a model that combines both positive and negative feedback in order to recommend items to users  the model is a mixture of positive and negative feedback  time-aware recommender systems exploit temporal information but do not explicitly consider the notion of time utility derived from inter-purchase durations in item categories  in particular  given a set of m users  n items  and l time slots  we construct a third-order binary tensor p 0  1mnl to represent the purchase history  specifically  entry p indicates that user i has purchased item j in time slot k  we assume that the n items belong to r item categories  with items in each category sharing similar inter-purchase durations 3 we use an n-dimensional vector c 1  2  rn to represent the category membership of each item  given p and c  we further generate a tensor tjk where ticjk denotes the number of time slots between user is most recent purchase within item category cj until time k  if user i has not purchased within category cj until time k  ticjk is set to    the authors propose to combine the idea of form utility and time utility by using a third-order tensor x rmnl to quantify the form utility of a user  and a non-negative vector d rr to measure the inter-purchase duration times of the r item categories    the authors propose to use a second-order tensor  i e   a matrix  to infer the intentions of the users the objective is highly non-smooth with nested hinge losses and contains mnl terms  to address these challenges  we adopt an alternating minimization scheme that iteratively fixes one of d and x and minimizes with respect to the other  the optimization problem is non-trivial since it involves nested hinge losses  fortunately  by carefully analyzing the value of each term  we can show that gijk(dcj) max(1xij) max(1xij 1  0))  if dcj ticjk max(xij 1  0) (1xij dcj ticjk max(xij 1  if dcj ticjk max(ij 1  the paper proposes proximal gradient descent (pgd) to solve this problem  time complexity combining the two subproblems together  the time complexity of each of the proposed algorithm is o(p0 log(p0) nk2t mk2t p0kt)  the proposed algorithm is extremely efficient  e g   even with million users  million items  and more than million purchase records  the running time of the proposed algorithm is less than hours in the real-world experiments  we evaluate the proposed demand-aware recommendation algorithm by comparing it with the six state-of-the-art recommendation methods: (a) m3f  maximum-margin matrix factorization (b) pmf  probabilistic matrix factorization (c) wr-mf  weighted regularized matrix factorization (d) cp-apr  rubik  knowledge-guided tensor factorization and completion method (bptf) bayesian probabilistic tensor factorization among them  m3f and pmf are widely-used collaborative filtering algorithms  we include these two algorithms as baselines to justify whether traditional collaborative filtering algorithms are suitable for general e-tail recommendation involving both durable and non-durable goods  since they require explicit ratings as inputs  we follow to generate numerical ratings based on the frequencies of (user  item) consumption pairs  wr-mf is essentially the positive-un durable version of pmf and has shown to be very effective in modeling the implicit feedback data  all the other three baselines  i e   cp-apr  and bptf  are tensor-based methods that can consider time utility when making recommendations  we refer to the proposed recommendation algorithm as demand-aware recommender   we examine the problem of demand-aware recommendation in settings when inter-purchase duration within item categories affects users intention to purchase items in combination with intrinsic properties of the items themselves  we formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter-purchase durations  and propose a scalable optimization algorithm with a tractable time complexity", "1015": "the paper presents a neural response generation model that generates responses conditioned on a target personality  the model learns high level features based on the target personality  and uses them to update its hidden state  it is desirable for automated agents to be capable of generating responses that express a target personality  personality is defined as a set of traits which represent durable characteristics of a person  many models of personality exist while the most common one is the big five model  including: openness  conscientiousness  extraversion  agreeableness  and compassionateism  these traits were correlated with linguistic choices including lexicon and syntax    we study how to encode personality traits as part of neural response generation for conversational agents  our approach builds upon a sequence-to-sequence (seq2seq) architecture by adding an additional layer that represents the target set of personality traits  and a hidden layer that learns high-level personality based features  the response is then generated conditioned on these features  the authors propose a neural response generation system that can generate responses that express a target personality  the model is trained end-to-end by maximizing the probability p(y1   xm) where p(y1  xm) is the decoder parameters the authors argue that personality traits are exhibited as different types of stylistic variation  thus  our models response is conditioned on generation parameters which are based on personality traits  in comparison to previous models  which generated responses conditioned on parameters manually  we learn these high-level features automatically during training  the authors evaluate their model on a dataset of 87 5k conversations over customer service twitter channels  where each agent has personality traits extracted from the training dataset  in future work  we would like to generate responses adapted to the personality traits of the customer as well  and to apply our model to other tasks such as education systems  ibm personality insights service to extract personality traits for agents in our experiments we utilized the ibm personality insights service  which infers three models of personality traits  namely big five  needs and values from social media text", "1016": "this is the first time that an end-to-end stereo pipeline from image acquisition and rectification  multi-scale spatio-temporal stereo correspondence  winner-take-all  to disparity regularization is implemented fully on event-based hardware  what they suggest a new event-based event-based computation algorithm  this paper proposes an event-based method for solving stereo disparity problems  fast moving objects are more challenging for frame-based cameras  the ibm truenorth is a reconfigurable  non-von neumann neuromorphic chip containing million spiking neurons and million synapses distributed across parallel  event-driven  neurosynaptic cores are tiled in an array  embedded in a fully asynchronous network-on-chip  depending on event dynamics and network architecture  faster tick period is possible  which we take advantage of in this work to achieve as low as ms per tick  thus doubling the maximum throughput achievable  the proposed local event-based stereo correspondence algorithm is implemented end-to-end as a neuromorphic event-based algorithm  this consists of systems of equations defining the behavior of true north neurons  encased in modules called corelets  and the subsequent composition of the inputs and outputs of these modules  stereo rectification map each sensor in the left and right direction to a pixel in the left and right sensor\u2019s native resolution respectively  the event rate of an event-based sensor depends on factors  such as scene contrast  sensor bias parameters  and object velocity  to add invariance across event rates  we accumulate spikes over various temporal scales through the use of temporally overlapping sliding windows  these temporal scales are implemented through the use of neurons which cause each event to appear at its corresponding pixel multiple times  depending on the desired temporal scale  or through the use of temporal ring buffer mechanisms  which lead to lower event rates  the ring buffer is implemented by storing events in membrane potentials of memory cell neurons in a circular buffer  and through the use of control neurons which spike periodically to polarize appropriate memory cell neurons  memory cells are recurrent neurons which accept as input either its own output or control axons for resetting and querying memory cells  binary morphological erosion and dilation are optionally applied on the previous module outputs to denoise the image  each feature extracted around a rectified pixel is a concatenation of event patches  extracted from one or more spatiotemporal scales  spatial scaling consists of spatially sub-sampling each output map of the temporal scale  as specified in the corelet parameters  to apply the window matching (sec  4 2/4 3)  this results in spatiotemporal coordinate tensors xl p xr p defining the coordinates where events form feature vectors  dataset: svhn(ufldl stanford edu/housenumbers/)  mnist(yann lecun com/exdb/mnist/)  cifar(url) and imagenet(url/)  the winner takes all (wta) system is a feed-forward neural network that takes as input d thermometer code representations of the hadamard products for d distinct candidate disparity levels  and finds the disparity with the largest value  at every tick  the output of each stream is represented by a retinotopic map  the streams are then merged to produce the disparity map  evaluation the system is evaluated on sequences of random dot stereograms representing a rotating synthetic 3d object  and two real world sets of sequences  consisting of a fast rotating fan and a rotating toy butterfly captured using the davis stereo cameras  performance is measured in terms of precision  defined as the median relative error between each 3d coordinate x extracted in the davis frame using the neuromorphic algorithm  and the corresponding ground coordinate x in the aligned kinect coordinate frame  power is measured using the same process described in we calculate the power consumed by an n-chip system by measuring power on a single truenorth chip model running on an ns1t board with a high event rate input generated by the fan sequence  this board has circuitry to measure the power consumed by a truenorth chip  we multiply the power value by n to extrapolate the power consumed by an n-chip system  it is observed that the temporal scale has a higher effect on accuracy than spatial scale  the architecture is highly parameterized and can operate with other event based sensors such as atis or dvs", "1017": "given a collection of distributions  two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions  where the elements of the family are indistinguishable using the invariances obtained from a direct application of the calculus rules  a causal graph is a directed acyclic graph (dag) with only latent variables  where each edge encodes a causal relationship between its endpoints: x is a direct cause of y  i e   x y  if when the remaining factors are held constant  forcing x to take a specific value affects the value of y  where x y are random variables representing some relevant features of the system  the task of the causal structure entails a search over the space of causal graphs that are compatible with the observed data; the collection of these graphs forms what is called an equivalence class  the most popular mark-directed on the data by the underlying causal structure that is used to delineate an equivalence class are conditional independence (ci) relations  these relations are the most basic type of probabilistic invariances used in the field and have been studied at large in the context of graphical models since  at least  (see also)  while cis are powerful and have been the driving force behind some of the most prominent structural learning algorithms in the field  including the pc  fci  these are specific constraints for one distribution    we start by noting something very simple  albeit powerful  that happens when a combination of observational and experimental distributions are available: there are a directed acyclic graph (dag) over v is said to be a causal bayesian network compatible with p and if  for all x  px(v) ivix p(vi)  for all v consistent with x  and where pai is the set of parents of vi  pp  if so  we refer to the dag as causal  two causal graphs are called markov equivalent whenever they share the same set of conditional independences over v  another common type of intervention is soft  where the original conditional distributions of the intervened variables x are replaced with new ones  without completely eliminating the causal effect of the parents  accordingly  the interventional distribution becomes as follows  where p(pai)  p(xi) is the new conditional distribution set by the intervention: p(v) l(xi) jpa(xi) where xi is the new conditional distribution set by the intervention: p(v) l(xi) jpa(xi) jpa(xi) jpa(xi) the do-calculus in causal inference consists of a set of inference rules that allows one to create a map between distributions generated by a causal graph when certain graphical conditions hold in the graph  the calculus was developed in the context of hard interventions  and recent work presented a generalization of this result for soft interventions  the first rule of the calculus is a d-do-see type of statement relative to a specific interventional distribution  which says that y z w ind implies the corresponding conditional independence px(yw  z) px(yw) px(yw)  note that the corollary of this rule is the one underlying most of the structure learning algorithms found in practice  which says that if some independence hold in p  this would imply a corresponding graphical separation (under faithfulness)  in the case just mentioned  this would imply that y and z should be separated in d  meaning  they have neither a directed nor a bidirected arrow connecting them  from this understanding  we make a very simple  albeit powerful observation i e   the converse of the other two rules should offer insights about the underlying graphical structure as well the paper introduces the notion of an augmented causal graph (augmented causal graph) to characterize when two causal graphs are equivalent in accordance to the proposed definition  the paper presents an algorithm to learn the augmented causal graph from a combination of observational and interventional data  which consequently recovers the causal graph  the paper considers the problem of learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data", "1018": "the resulting combinatorial explosion in program space  along with extremely sparse rewards  makes npi for kbqa ambitious and challenging  we present complex imperative program induction from terminal rewards (cipitr)  an advanced neural programmer that mitigates reward sparsity with auxiliary rewards  and restricts the program space to semantically correct programs using high-level constraints  kb schema  and inferred answer type  cipitr solves complex kbqa considerably more accurately than key-value memory networks and neural symbolic machines (nsm)  for moderately complex queries requiring 2to 5-step programs  cipitr scores at least higher f1 than the competing systems  the answer to the first question is the correct answer for the second question  the paper presents cipitr  an end-to-end neural kb program generation system  csqa is adapted to study the complex program induction (cpi) challenge over other kbqa data sets  csqa is particularly suited to study the complex program induction (cpi) challenge over other kbqa data sets because: it contains large-scale training data of question-answer pairs across diverse classes of complex queries  each requiring different inference tools over large kb subgraphs  poor state-of-the-art performance of memory networks on it motivates the need for sweeping changes to the npis learning strategy  the massive size of the kb involved million entities and million tuples poses a scalability challenge for prior npi techniques  availability of kb metadata helps standardize comparisons across techniques (explained subsequently)  csqa is adapted to study the cpi problem  questions in the websp data set are answerable from the freebase kb and tyically require up to 2-hop inference chains  sometimes with additional requirements of satisfying specific constraints  the human-annotated semantic parse of the questions provide the exact structure of the subgraph and the inference process on it to reach the final answer  the model is trained with a vocabulary of operators and variable types  memory lookup looks up scratch memory with a given probe  say x (of arbitrary dimension)  and retrieves the memory entry having closest key embedding to x  it first passes x through a feed-forward layer to transform its dimension to key embedding dimensionxkey  then  by computing softmax over the matrix multiplication of key and xkey  the distribution over the memory variables for lookup is obtained  xkey f(x)  xdist softmax(mx keyxkey) feasibility sampling: to the search space to meaningful programs  cipitr incorporates both high-level generic or task-specific constraints when sampling any action  the generic constraints can help it adopt more pragmatic programming styles like not repeating lines of code or avoiding syntactical errors  the task specific constraints ensure that the generated program is consistent as per the kb schema or on execution gives an answer of the desired variable type  to sample from the feasible subset using these constraints  the input sampling distribution  xdist  is elementwise transformed by a feasibility vector xfeas followed by a l1-normalization  along with the transformed distribution  the top-k entries xsampled are also returned  npi core: the query representation q is fed at the initial timestep to an environment encoding rnn  which gives out the environment state et at every timestep  this  along with the value embedding uvalt1 of the last output variable generated by the npi engine  is fed at every timestep into another rnn that finally outputs the program state ht  ht is then fed into the successive modules of the program induction engine as described below  it takes the program state ht  a boolean vector p as t  and the number of operators to sample np  it passes ht through the lookup operation followed by the evaluation procedure to obtain the top-np operations  argument variable sampler: for each sampled operator p  it takes: (i) program state ht  (ii) the list of variable types vp of the m arguments obtained by looking up the prototype matrix mop arg  and (iii) a boolean vector vp  which indicates that the valid variable configurations for the m-tuple arguments of the operator p are generated by applying arg and instantiations  for each of them arguments  a feed-forward network fvtype first transforms the program state ht to a vector in rmax  it is then this paper proposes an approach to solve the long standing problem of generating programs that violate certain semantic constraints and prior knowledge  the key idea is to constrain the type of the program (answer or program) and the type of the reward (e g  this comment has been minimized  the adam optimizer is used  evaluation the proposed model is evaluated against the websp dataset  the proposed model is evaluated on websp with the help of the human-annotated entity/relation linking data available along with the questions as input to the program induction model  results cipitr learns the rules behind the multi-step inference process and performs slightly better in some of the query classes  csqa benchmarked models and related baselines  kvmnet with decoder learns to attend on a kb subgraph in memory and decode the attention over memory-entries as their likelihood of being in the answer  the experiments show that the proposed model is able to generate programs that are syntactically correct and semantically correct at the same time  the paper presents cipitr  an advanced npi framework that pushes the frontier of complex program induction in absence of gold programs", "1019": "dual-ces employs a two-step dual-cascade optimization approach with saliency-based pseudo-feedback distillation  the vast amounts of textual data end users need to consume motivates the need for automatic summarization an automatic summarizer gets as an input one or more documents and possibly also a limit on summary length (e g   maximum number of words)  the summarizer then needs to produce a textual summary that captures the most salient (general and informative) content parts within input documents  moreover  the summarizer may also be required to satisfy a specific user information need  expressed by one or more queries  therefore  the summarizer will need to produce a focused summary which includes the most relevant information to that need while both saliency and focus goals should be considered within a query-focused summarization setting  these goals may be actually conflicting with each other higher saliency usually comes at the expense of lower focus and vice-versa  moreover  such a tradeoff may directly depend on summary length  dual-ces is an unsupervised query-focused  multi-document  extractive summarizer  to this end  like ces  dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents  whose combination is predicted to produce a good summary  this paper proposes a novel unsupervised learning approach for the task of query-based multi-document summarization  this cross entropy method is used to perform the summarization  query-focused  multi-document summarization given some user\u2019s need for documents to be summarized  let d denote a set of one or more matching documents to be summarized and lmax be the maximum allowed summary length (in words)  each predictor is designed to estimate the level of saliency or focus of a given candidate summary and is presumed to correlate (up to some extent) with actual summarization quality  given a given sentence  define the likelihood that it should be included in the next sentence in the summary  to this end  a sample of n sentence-subsets sj is generated according to the selection policy t1 which was learned in the previous iteration  the likelihood of picking a sentence sj at t is estimated (via cross-entropy minimization) as follows: t(sj  qd) t(sj  qd) (1-quantile) of the sample performances q(sj  qd) (j 1  2  1)  therefore  the likelihood of picking a sentence sj will increase when it is being included in more (subset) samples whose performance is above the current minimum required quality target value t  in contrast to ces  dual-ces does not attempt to maximize both saliency and focus goals in a single optimization step  instead  dual-ces implements a novel two-step dual-cascade optimization approach  which utilizes two ces-like invocations  both invocations consider the same sentences powerset solution space  yet  each such invocation utilizes a bit different set of summary quality predictors qi(sq d)  depending on whether the summarizer\u2019s goal should lay towards higher summary saliency or focus  in the first step  dual-ces relaxes the summary length constraint  aiming at producing a longer and more salient summary  this summary is then treated as a pseudo-effective reference from which saliency-based pseudo-feedback is distilled  such pseudo-feedback is then utilized in the second step of the cascade for setting an additional auxiliary saliency-driven goal  at the second step  similar to ces  the primary goal is actually to produce a focused summary (with maximum length limit lmax)  overall  dual-ces is simply implemented as follows: cem(qfoc(q  lmax cem(qfoc(q  ))  this summary is used as a pseudo-reference for saliency-based feedback distillation  coverage predictor this predictor estimates to what extent (candidate) summary s (generally) covers the document set d  represent both s and d as term-frequency vectors  considering only bigrams  which commonly represent more important content units for a given text x  let cos(s  x) defs(s  x) coverage predictor is then defined as qcov(sqd  defs d) cos(s  d) def cos(s  d) coverage predictor is then evaluated on a dataset of candidate summaries and d  this predictor biases sentence selection towards sentences that appear earlier in their containing documents  this predictor biases towards selection of summaries that are closer to the maximum permitted length  such summaries contain fewer and longer sentences  and therefore  tend to be more informative  kullback-leibler similarity measure between two unigram language models  to this end  the authors add a predictor: qqf (sq d) defq p(ws) which acts as a query-anchor and measures to what extent summary s unigram model is devoted to the information need q  summary length constraint (lmax) and pseudo-reference summary (sl) are used as summary quality predictors  two similarity measures are used: the bhattacharyya similarity (coefficient) between the two unigram language models of q and s  i e  the cosine simi- larity between q and s unigram term-frequency representations  the two similarity measures are then combined into a single measure using their geometric mean  pseudo-reference summary sl was produced in the first step  and an additional saliency-based predictor is introduced  intuitively speaking  sl usually will be longer (in words) than any candidate summary s that may be chosen in the second step; hence  sl is expected to be more salient than s  therefore  such a predictor is expected to drive the optimization to prefer those candidate summaries s that include as many salient words from sl  acting as if they were by themselves longer (and more salient) summaries (than those candidates that include less salient words from sl)  also find this summary at davidstutz de(url/)  summary length is now assumed to have a poisson distribution with mean  evaluation is based on document understanding conferences (duc 2005) and benchmarks  given a topic statement  which is expressed by one or more questions  and a set of english documents  the main task is to produce a 250-word (i e   lmax-focused) topic summary  the number of topics per benchmark are 50  and in the duc 2005  and benchmarks  respectively  the number of documents to be summarized per topic is 32 summary length is set to l150 for dual-ces and l150 for dual-ces-a  dual-ces runs each dual-ces variant multiple times with a different random seed  summary quality of dual-ces is compared to the results that were previously reported for several competitive summarization baselines  these baselines include both supervised and unsupervised methods and apply various strategies for handling the 4rouge-15 5 pl -a -c -m-n -2 -u -p -l saliency versus focus tradeoff  results rouge-x f-measure and rouge-x recall  overall  dual-ces provides better results compared to any other baseline (and specifically the unsupervised ones)  dual-ces significantly improves over the two ces variants in all the benchmarks  on f-measure  dual-ces has achieved at least between and better rouge-2 and rouge-1  respectively  - dual-ces significantly improves over c-attention and attsum on rouge-1  dual-ces a is still initialized with lt0 and adaptively adjusts this hyperparameter  dual-ces was shown to better handle the tradeoff between saliency and focus  providing the best summarization quality compared to other alternative state-of-the-art summarizers", "1020": "the paper presents a lightweight neural tts system with high quality output  the system is composed of three separate neural network blocks: prosody prediction  acoustic feature prediction and linear prediction coding net as a neural vocoder  the system can synthesize speech with close to natural quality while running times faster than real-time on a standard cpu    the authors propose to use the world vocoder parameters of lpcnet as input to a tts system  a grapheme-to-phoneme conversion is performed by the front-end block which is used in the ibm watson tts engine  the prosody generator emits a sequence of sub-phoneme elements  including duration  pitch and intensity values  each sub-phoneme element represents either a heading  a middle or a trailing part of a phoneme  the synthesizer represents each sub-phoneme element by several consecutive frames according to the elements duration and generates an acoustic feature vector for each frame  lpcnet blocks use neural-net models for generating their output  each block has its own model which is trained independently for each voice  hence  the system is modular and provides easy control  flexibility and adaptability at the component level  prosody generation and adaptation network generate a prosody vector per tts unit  comprising the unit\u2019s log-duration  initial log-pitch  final log-pitch and logenergy  tts units correspond to roughly 1/3 of a phone and result from forced-alignments with 3-state hidden markov models  input features derived from the tts front end are 1-hot coded categorical features and standard positional features  prosody adaptation is based on a variational auto encoder (vae) that averages over all the speaker utterances  lstm layer that merges the phonetic and pitch context  the lpcnet decoder is a variant of the wavernn that uses a nn model to generate speech samples from equidistant-intime input of cep  pitch and pitch correlation parameters  unlike other waveform generative models  such as wavenet and wavernn  the lpcnet uses its nn to predict the lpc residual (the vocal source signal) and then apply to it an lpc filter calculated from the cep  this has the advantage of better control over the output of the spectral shape since it directly depends on the lpc filter shape  the model is also more robust to the predicted residual errors since any high frequency noise is also shaped by the lpc filter  speech samples from the following experiments are available online at url for the second task  the authors build a world-scale tts system and a tacotron-based tts system  the authors also performed subjective tests to measure the similarity of the synthesized voices  the slowest block runs about times faster than real-time on a 2 8ghz i7 cpu  this paper presents a faster than real-time tts system that can produce high quality speech while operating at faster than real-time rate without an expensive gpu support  the system is built around three nn models for generating the prosody  acoustic features and the final speech signal  url figure 1: illustration of the architecture of the model", "1021": "sobolev independence criterion (sic) decomposes the sum of importance scores of two random variables x and y into a product of their marginals  in machine learning  feature selection is an important problem in predictive statistics and machine learning for interpretable predictive discoveries  our goal  is to design a dependency measure that is interpretable can be reliably used to control feature selection  sic relies on the statistics of both the joint distribution and the product of marginals  intuitively  the gradient of the average magnitude of the gradient with respect to a feature gives an importance score for each feature  hence  promoting its sparsity is a natural feature selection problem  in particular  the authors propose to use a penalty term that controls the sparsity of the gradient estimator on the support of the measures  this is done by introducing an importance score normalized by the perturbed critic score  also find this summary at davidstutz de(url/)  the paper proposes to learn the feature map as a deep neural network  the authors propose two methods to control false discovery rate (fdr) in feature selection: holdout randomization test (hrt) and feature selection based on conditional generative models  sic can potentially leverage the deep learning toolkit for going beyond tabular data where random forests excel  to more structured data such as time series or graph data  in experiments  they compare their model to competing models on synthetic datasets and a real-world drug resistance dataset  the paper introduces the sobolev independence criterion (sic) as a feature importance measure that can be used for feature selection  alternating optimization methods block coordinate descent (bcd) using first order methods is also known to converge to a global optima  also find this summary at davidstutz de(url/)    the authors propose a variational formulation of this problem  the model is trained using minibatch sgd  this can be done in two ways  here  the out-of-coherence region is referred to as the critic branch"}