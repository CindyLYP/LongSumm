<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Cloud Service Resilience using Brownout-Aware Load-Balancing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Klein</surname></persName>
							<email>cklein@cs.umu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">Umeå University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">Vittorio</forename><surname>Papadopoulos</surname></persName>
							<email>alessandro@control.lth.se</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Automatic Control</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Dellkrantz</surname></persName>
							<email>manfred@control.lth.se</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Automatic Control</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Dürango</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Automatic Control</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martina</forename><surname>Maggio</surname></persName>
							<email>martina@control.lth.se</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Automatic Control</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Hernández-Rodriguez</surname></persName>
							<email>francisco@cs.umu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">Umeå University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Automatic Control</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Elmroth</surname></persName>
							<email>elmroth@cs.umu.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Science</orgName>
								<orgName type="institution">Umeå University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Cloud Service Resilience using Brownout-Aware Load-Balancing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We focus on improving resilience of cloud services (e.g., e-commerce website), when correlated or cascading failures lead to computing capacity shortage. We study how to extend the classical cloud service architecture composed of a loadbalancer and replicas with a recently proposed self-adaptive paradigm called brownout. Such services are able to reduce their capacity requirements by degrading user experience (e.g., disabling recommendations). Combining resilience with the brownout paradigm is to date an open practical problem. The issue is to ensure that replica self-adaptivity would not confuse the load-balancing algorithm, overloading replicas that are already struggling with capacity shortage. For example, load-balancing strategies based on response times are not able to decide which replicas should be selected, since the response times are already controlled by the brownout paradigm. In this paper we propose two novel brownout-aware loadbalancing algorithms. To test their practical applicability, we extended the popular lighttpd web server and load-balancer, thus obtaining a production-ready implementation. Experimental evaluation shows that the approach enables cloud services to remain responsive despite cascading failures. Moreover, when compared to Shortest Queue First (SQF), believed to be nearoptimal in the non-adaptive case, our algorithms improve user experience by 5%, with high statistical significance, while preserving response time predictability.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Due to their ever-increasing scale and complexity, hardware failures in cloud computing infrastructures are the norm rather than the exception <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. This is why Internet-scale interactive applications -also called services -such as e-commerce websites, include replication early in their design <ref type="bibr" target="#b2">[3]</ref>. This makes the service not only more scalable, i.e., more users can be served by adding more replicas, but also more resilient to failures: In case a replica fails, other replicas can take over. In a replicated setup, a single or replicated load-balancer is responsible for monitoring replicas' health and directing requests as appropriate. Indeed, this practice is well established and can successfully deal with failures as long as computing capacity is sufficient <ref type="bibr" target="#b2">[3]</ref>.</p><p>However, failures in cloud infrastructures are often correlated in time and space <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Therefore, it may be economically inefficient for the service provider to provision enough spare capacity for dealing with all failures in a satisfactory manner. This means that, in case correlated failures occur, the service may saturate, i.e., it can no longer serve users in a timely manner. This in turn leads to dissatisfied users, that may abandon the service, thus incurring long-term revenue loss to the service provider. Note that the saturated service causes infrastructure overload, which by itself may trigger additional failures <ref type="bibr" target="#b5">[6]</ref>, thus aggravating the initial situation. Hence, a mechanism is required to deal with rare, cascading failures, that feature temporary capacity shortage.</p><p>A promising self-adaptation technique that would allow dealing with this issue is brownout <ref type="bibr" target="#b6">[7]</ref>. In essence, a service is extended to serve requests in two modes: with mandatory content only, such as product description in an e-commerce website, and with both mandatory and optional content, such as recommendations of similar products. Serving more requests with optional content, increases the revenue of the provider <ref type="bibr" target="#b7">[8]</ref>, but also the capacity requirements of the service. A carefully designed controller decides the ratio of requests to serve with optional content, so as to keep the response time below the user's tolerable waiting time <ref type="bibr" target="#b8">[9]</ref>. From the data-center's point-of-view, the service modulates its capacity requirements to match available capacity.</p><p>Brownout has been successfully applied to services featuring a single replica. Extending it to multiple replicas needs to be done carefully: The self-adaptation of each replica may confuse commonly used load-balancing algorithms (Section II).</p><p>In this paper we enhance the resilience of replicated services through brownout. In other words, the service performs better at hiding failures from the user, as measured in the number of timeouts a user would observe. As a first step, a commonlyused load-balancing algorithm, Shortest Queue First (SQF), proved adequate for most scenarios. However, we found a few corner cases where the performance of the load-balancer could be improved using two novel, queue-length-based, brownoutaware algorithms that are fully event-driven.</p><p>Our contribution is three-fold: 1) We present two novel load-balancing algorithms, specifically designed for brownout services (Section III-A). 2) We provide a production-ready brownout-aware loadbalancer (Section III-B). 3) We compare fault-tolerance without and with brownout, and existing load-balancing algorithms to our novel ones (Section IV).</p><p>Results show that the resulting service can tolerate more replica failures and that the novel load-balancing algorithms improve the number of requests served with optional content, and thus the revenue of the provider by up to 5%, with high statistical significance. Note that SQF is thought to be nearoptimal, in the sense that it minimizes average response time for non-adaptive services <ref type="bibr" target="#b9">[10]</ref>.</p><p>To make our results reproducible and foster further research on improved resilience through brownout, we make all source code available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION</head><p>In this section we provide the relevant background and define the challenge to address with respect to previous contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Single Replica Brownout Services</head><p>To provide predictable performance in cloud services, the brownout paradigm <ref type="bibr" target="#b6">[7]</ref> relies on a few, minimally intrusive code changes (e.g., 8 lines of code) and an online adaptation strategy that controls the response time of a single-replica based service. The service programmer builds a brownoutcompliant cloud service breaking the service code into two distinct subsets: Some functions are marked as mandatory, while others as optional. For example, in an e-commerce website, retrieving the characteristics of a product from the database can be seen as mandatory -a user would not consider the response useful without this information -while obtaining comments and recommendations of similar products can be seen as optional -this information enhances the quality of experience of the user, but the response is useful without them.</p><p>For a brownout-compliant service, whenever a request is received, the mandatory part of the response is always computed, whereas the optional part of the response is produced only with a certain probability given by a control variable, called the dimmer value. Not executing the optional code reduces the computing capacity requirements of the service, but also degrades user experience. Clearly, the user would have a better experience seeing optional content, such as related products and comments from other users. However, in case of overload and transient failure conditions, it is better to obtain partial information than to have increased response times or no response, due to insufficient capacity.</p><p>Keeping the service responsive is done by adjusting the probability of executing the optional components <ref type="bibr" target="#b6">[7]</ref>. Specifically, a controller monitors response times and adjusts the dimmer value to keep the 95th percentile response time observed by the users around a certain setpoint. Focusing on 95th percentile instead of average, allows more users to receive a timely response, hence improve their satisfaction <ref type="bibr" target="#b10">[11]</ref>. A setpoint of 1 second can be used, to leave a safety margin to the user's tolerable waiting time, estimated to be around 4 seconds <ref type="bibr" target="#b8">[9]</ref>. While the initial purpose of the brownout control was to enhance the service's tolerance to a sudden increase <ref type="bibr" target="#b0">1</ref> https://github.com/cloud-control/brownout-lb-lighttpd in popularity, it also significantly improves responsiveness during infrastructure overload phases, when the service is not allocated enough capacity to manage the amount of incoming requests without degrading the user experience. However, the brownout approach was used only in services composed of a single replica, thus the service could not tolerate hardware failures.</p><p>Let us briefly describe the design of the controller. Denoting the dimmer value with Θ and using a simple and useful model, we assume that the 95th percentile response time of the service, measured at regular time intervals, follows the equation</p><formula xml:id="formula_0">t(k + 1) = α(k) • Θ(k) + δt(k),<label>(1)</label></formula><p>i.e., the 95th percentile response time t(k + 1) of all the requests that are served between time index k and time index k + 1 depends on a time varying unknown parameter α(k) and can have some disturbance δt(k) that is a priori unmeasurable. α(k) takes into account how the dimmer Θ affects the response time, while δt(k) is an additive correction term that models variations that do not depend on the dimmer choice -for example, variation in retrieval time of data due to cache hit or miss. Notice that the used model ignores the time needed to compute the mandatory part of the response, but it captures the service behavior enough for the control action to be useful. The controller design aims for canceling the disturbance δt(k) and selecting the value of Θ(k) so that the 95th percentile response time would be equal to the setpoint value. With a control-theoretical analysis <ref type="bibr" target="#b6">[7]</ref>, it is possible to select the dimmer value to provide some guarantees on the service behavior. The selection is based on the adaptive proportional and integral controller</p><formula xml:id="formula_1">Θ(k + 1) = Θ(k) + 1 − p 1 α(k) • e(k),<label>(2)</label></formula><p>where the valueα(k) is an estimate of the unknown parameter α(k) computed with a Recursive Least Square (RLS) filter.</p><p>The error e(k) is the difference measured at time index k between the setpoint for the response time and its measured value, p 1 is a parameter of the controller, that allows to trade reactivity for robustness. A formal analysis of the guarantees provided by the controller and the effect of the value of p 1 can be found in <ref type="bibr" target="#b6">[7]</ref>. Besides computing a new dimmer value, the model parameter α is re-estimated asα(k), which is computed using the last estimationα(k − 1), the measured response time t(k) and the current dimmer θ (k), as illustrated in the following RLS filter equations</p><formula xml:id="formula_2">ε(k) =t(k) − Θ(k)α(k − 1) g(k) =P(k − 1)Θ(k) f + Θ(k) 2 P(k − 1) −1 P(k) = f −1 [P(k − 1) − g(k)Θ(k)P(k − 1)] α(k) =α(k − 1) + ε(k)g(k),<label>(3)</label></formula><p>where ε is the so called "prediction error", g is a gain factor, f is a "forgetting factor" and P is the covariance matrix of the prediction error. Through empirical testing on two popular cloud applications, RUBiS <ref type="bibr" target="#b11">[12]</ref> and RUBBoS, we found the following values to give a good trade-off between reactivity and stability: p 1 = 0.9 and f = 0.95. In the end, making a single-replica cloud service brownout-compliant improves its robustness to sudden increases in popularity and infrastructure overload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multiple Replica Brownout-Compliant Services</head><p>For fault tolerance, cloud services should feature multiple replicas. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the software architecture that is deployed to execute a brownout-compliant service composed of multiple replicas. Besides the addition of replica controllers to make it brownout-compliant, the architecture is widely accepted as the reference one for replicated cloud services <ref type="bibr" target="#b0">[1]</ref>.</p><p>In the given cloud service architecture, access can only happen through the load-balancer. The client requests are assumed to arrive at an unknown but measurable rate λ . Each client request is received by the load-balancer, that forwards it to one of the n replicas. Each replica independently decides if the request should be served with or without the optional part. The chosen replica produces the response and sends it back to the load-balancer, which forwards it to the original client. Since all responses of the replicas go through the loadbalancer, it is possible to piggy-back the current value of the dimmer Θ i of each replica i through the response, so that this value can be observed by the load-balancer.</p><p>For better decoupling and redundancy, the load-balancer does not have any knowledge on how each replica controller adjusts Θ i . Hence, the load-balancer only stores soft state, reducing impact in case of failover to a backup load-balancer. Also, operators can deploy our solution incrementally, first adding brownout to replicas, then upgrading the load-balancer.</p><p>In the end, each replica i receives a fraction λ i of the incoming traffic and serves requests with a 95th percentile response time around the same setpoint of 1 second. Each replica i chooses a dimmer Θ i that depends on the amount of traffic it receives and the computing capacity available to it. Noteworthy is the fact that by directing too many requests to a certain replica the load-balancer may indirectly decrease the amount of optional requests served by that replica.</p><p>Preliminary simulation results <ref type="bibr" target="#b12">[13]</ref> compared different loadbalancing algorithms for this architecture, such as round-robin, fastest replica first, random and two random choices. The main result of this comparison is that load-balancing algorithms that are based on measurements of the response times of the single replicas are not suited to be used with brownout-compliant services, since the replica controllers already keep the response times close to the setpoint. The only existing algorithm that proved to work adequately with brownout-compliant services is Shortest Queue First (SQF) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>. It works by tracking the number of queued requests q i on each replica and directing the next request to the replica with the lowest q i .</p><p>However, SQF proved to be inadequate for maximizing the optional content served, such as recommendations, hence producing lower revenues for the service provider <ref type="bibr" target="#b7">[8]</ref>. Brownoutaware load-balancers do better in maximizing the optional component served. However, to date, only weight-based algorithms were considered, where each replica gets a fraction of the incoming traffic proportional to a dynamic weight. A controller periodically adjusts the weights based on the dimmer values of each replica <ref type="bibr" target="#b12">[13]</ref>. Results suggested that deciding periodically gives good results in steady-state, however, the resulting service is not reactive enough to sudden capacity changes, as would be the case when a replica fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Problem Statement</head><p>The main objective is to improve resilience of cloud services. On one hand, the service should serve requests with a 95th percentile response time as close as possible to the setpoint. On the other hand, the service should maximize the optional content served.</p><p>In this paper we propose novel brownout-aware loadbalancers that are event-based, for better reactivity. We limit the comparison to SQF, since it was shown to be the only reasonable choice to maximize optional content in brownoutcompliant services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DESIGN AND IMPLEMENTATION</head><p>This section describes the core of our contribution, two loadbalancing algorithms and a production-ready implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Brownout-Compliant Load-Balancing Algorithms</head><p>Here we discuss two brownout-compliant control-based load-balancing algorithms. Those are based on some ideas presented in <ref type="bibr" target="#b12">[13]</ref>, but with two major modifications. First, all the techniques proposed in <ref type="bibr" target="#b12">[13]</ref> are trying to maximize the optional content served by acting on the fraction of incoming traffic sent to a specific replica, while here the algorithms are acting in an SQF-like way but with queue-offsets that are dynamically changed in time. The queue-offsets u i take into account the measured performance of each replica i in terms of dimmers, and are subtracted from the actual value of the queue length q i so as to send the request to the replica with the lowest q i − u i .</p><p>The second and most important modification is that in <ref type="bibr" target="#b12">[13]</ref> all the algorithms run periodically, independently of the incoming traffic, while in this paper we are considering algorithms that are fully event-driven, updating the queue-offsets and taking a decision for each request. Therefore all gains in the two following algorithms need to be scaled by the time elapsed since the last queue-offsets update.</p><p>These two modifications highly improve the achieved performance, both in terms of optional content served and response time, rendering the service more reactive to sudden capacity changes, as is the case with failures. Let us now present two algorithms for computing the queue-offsets u i . 1) PI-Based Heuristic (PIBH): Our first algorithm is based on a variant of the PI (Proportional and Integral) controller on incremental form, which is typical in digital control theory <ref type="bibr" target="#b13">[14]</ref>. In principle, the PI control action in incremental form is based both on the variation of the dimmers value (which is related to the proportional part), and their actual values (which is related to the integral part).</p><p>As presented above, the values of the queue offsets u i are updated every time a new request is received by the service, according to the last values of the dimmers Θ i , piggy-backed by each replica i through a previous response, and on the queue lengths q i , using the formula</p><formula xml:id="formula_3">u i (k + 1) = (1 − γ) [u i (k) + γ P ∆Θ i (k) + γ I Θ i (k)] + γq i (k), (4)</formula><p>where γ ∈ (0, 1) is a filtering constant, γ P and γ I are constant gains related to the proportional and integral action of the classical PI controller.</p><p>We selected γ = 0.01 and γ P = 0.5 based on empirical testing. Once γ and γ P are fixed to a selected value, increasing the integral gain γ I calls for a stronger action on the loadbalancing side, which means that the load-balancer would take decisions very much influenced by the current values of Θ i , therefore greatly improving performance at the cost of a more aggressive control action. On the contrary, decreasing γ I would smooths the control action, possibly resulting in performance loss due to a slower reaction time. The choice of the integral gain allows to exploit the trade-off between performance and robustness. For the experiments we chose γ I = 5.0.</p><p>2) Equality Principle-Based Heuristic (EPBH): The second algorithm is based on the heuristic that the system will perform well in a situation when all replicas have the same dimmer value. By comparing Θ i for each replica i with the mean dimmer of all replicas, a carefully designed update rule can deduce which replica should receive more load, in order to drive all dimmer to equality. The queue offsets can thus be updated as</p><formula xml:id="formula_4">u i (k + 1) = u i (k) + γ e Θ i (k) − 1 n n ∑ j=1 Θ j (k) ,<label>(5)</label></formula><p>where γ e is a constant gain. The gain decides how fast the controller should act. Based on empirical tuning we chose γ e = 0.1.</p><p>Since the implementation only updates the dimmer measurements in the load balancer when responses are sent, EPBH risks ending up in a situation where a replica gets completely starved. To remedy this, the algorithm first chooses a random empty replica (q i = 0) if there are any, otherwise chooses the replica with the lowest q i − u i , as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation</head><p>In order to show the practical applicability of the two algorithms and evaluate their performance, we decided to implement them in an existing load-balancing software. We chose lighttpd <ref type="bibr" target="#b1">2</ref> , a popular open-source web server and load-balancing software, that features good scalability, thanks to an event-driven design. lighttpd already included all necessary prerequisites, such as HTTP request forwarding, HTTP response header parsing, replica failure detection and the state-of-the-art queue-length-based SQF algorithm. HTTP response header parsing allowed us to easily implement dimmer piggy-backing through the custom X-Dimmer HTTP response header, with a small overhead of only 20 bytes. In the end, we obtained a production-ready brownout-aware loadbalancer implementation featuring the two algorithms, with less than 180 source lines of C code 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EMPIRICAL EVALUATION</head><p>In this section we show through real experiments the benefits in terms of resilience that can be obtained through our contribution. First, we describe our experimental setup. Next, we show the benefits that brownout can add to a replicated cloud service which uses the state-of-the-art loadbalancing algorithm, SQF. Finally, we show the improvements that can be made using our brownout-specific load-balancing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>Experiments were conducted on a single physical machine equipped with two AMD Opteron TM 6272 processors <ref type="bibr" target="#b3">4</ref> and 56 GB of memory. To simulate a typical cloud environment and allow us to easily fail and restart replicas, we use the Xen hypervisor <ref type="bibr" target="#b14">[15]</ref>. Each replica is deployed with all its tiers -web server and database server -inside its own Virtual Machine (VM), as is commonly done in practice <ref type="bibr" target="#b15">[16]</ref>, e.g., using a LAMP stack <ref type="bibr" target="#b16">[17]</ref>. Each VM was configured with a static amount of memory, 6 GB, enough to hold all processes and the database in-memory, and a number of virtual cores depending on the experiment.</p><p>Inside each replica we deployed an identical copy of RUBiS <ref type="bibr" target="#b11">[12]</ref>, an eBay-like e-commerce prototype, that is widely-used for cloud benchmarking <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b23">[24]</ref>. RUBiS was already brownout-compliant, thanks to a previous contribution <ref type="bibr" target="#b6">[7]</ref> and adding piggy-backing of the dimmer value was trivial <ref type="bibr" target="#b4">5</ref> . The replica controllers are configured the same, with a target 95th percentile response time of 1 second. To avoid having to deal with synchronization or consistency issues, we only used a read-only workload. However, adding consistency to replicated services is well-understood <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref> and, in case of RUBiS, would only require an engineering effort. The loadbalancer, i.e., lighttpd extended with our brownout-aware algorithms, was deployed inside the privileged VM in Xen, i.e., Dom0, pinned to a dedicated core.</p><p>To generate the workload, we had to choose between three system models: open, closed or partly-open <ref type="bibr" target="#b27">[28]</ref>. In an open system model, typically modeled as Poisson process, requests are issued with an exponentially-random inter-arrival time, characterized by a rate parameter, without waiting for requests https://github.com/cloud-control/brownout-lb-lighttpd to actually complete. In contrast, in a closed system model, a number of users access the service, each executing the following loop: issue a request, wait for the request to complete, "think" for a random time interval, repeat. The resulting average request inter-arrival time is the sum of the average think-time and the average response time of the service, hence dependent on the performance of the evaluated service.</p><p>A partly-open system model is a mixture between the two: Users arrive according to a Poisson process and leave after some time, but behave closed while in the system. As with the closed model, the inter-arrival time depends on the performance of the evaluated system.</p><p>We chose to use an open system model workload generator. Since its behavior does not depend on the performance of the service, this allows us to eliminate a factor potentially contributing to noise when comparing our contribution to competing approaches. We extended this model to include timeouts, as required to emulated users' tolerable waiting time of 4 seconds <ref type="bibr" target="#b8">[9]</ref>. Given our chosen model and the need to measure brownoutspecific behavior, the workload generator provided with RUBiS was insufficient for three reasons. First, RUBiS's workload generator uses a closed system model, without timeouts. Second, it only reports statistics for the whole experiment and does not export the time series data, preventing us from observing the service's behavior during transient phases. Finally, the tool cannot measure the number of requests served with optional content, which represents the quality of the userexperience and the revenue of the service provider. Therefore, we extended our own workload generator, httpmon 6 , as required.</p><p>We made sure that the results are reliable and unbiased as follows:</p><p>• replicas were warmed up before each experiment, i.e., all virtual disk content was cached in the VM's kernel;</p><p>• replicas were isolated performance-wise by pinning each virtual core to its own physical core;</p><p>• experiments were terminated after the workload generator issued the same number of requests;</p><p>• httpmon and the lighttpd were each executed on a dedicated core;</p><p>• no non-essential processes nor cron scripts were running at the time of the experiments. To qualify the resilience of the service, we chose two metrics that measure how well the service is performing in hiding failures, or, otherwise put, how strongly the user is affected by failures. The timeout rate represents the number of requests per second that were not served by the service within 4 seconds, due to overload. In production, a request that timed out will make a user unhappy. She may leave the service to join other competitors, thus incurring long-term losses to the service provider. The optional content ratio represents the percentage of requests served with optional content. Serving a request with optional content, such as recommendations of similar products, may increase the service provider's revenue by 50% <ref type="bibr" target="#b7">[8]</ref>. Therefore, a request served without optional content also represents a revenue loss to the provider, albeit, a smaller one than the long-term loss incurred by a timeout. Ideally, the service should strive to maximize the optional content ratio, without causing timeouts. Finally, to give insight into the system's behavior, we also report the response time, i.e., the time it took to serve a request from the user's perspective, including the time required to traverse the load-balancer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Resilience without and with Brownout</head><p>In this section, we show through experiments how brownout can increase resilience, even if used with a brownout-unaware load-balancing algorithm, such as SQF. To this end, we expose both a non-brownout and a brownout service to cascading failures and their recovery. The experiment starts with 5 replicas, each being allocated 4 cores, i.e., the service is allocated a total computing capacity of 20 cores. Every 100 seconds a replica crashes until only a single one is active. Then, every 100 seconds a replica is restored. Crashing and restoring replicas are done by respectively killing and restarting both the web server and the database server of the replica.</p><p>We plot the timeout ratio and the optional content ratio. Note that, for the service without brownout, the ratio of optional content is fixed at 100%, whereas the service featuring brownout this quantity is adapted based on the available capacity, i.e., the number of available replicas. To focus on the behavior of the service due to failure, we kept the requestrate constant at 200 requests per second. Note that, the replicas were configured with enough soft resources (file descriptors, sockets, etc.) to deal with 2500 simultaneous requests. We ran several experiments in different conditions and always obtained similar results. Therefore, to better highlight the behavior of the service as a function of time, we present the results of a single experiment instance as time series. <ref type="figure">Fig. 2</ref> show the results. One can observe that the nonbrownout service performs well even with 2 failed replicas, from time 0 to 300. Indeed, there are no timeouts and all requests are served with optional content. lighttpd already includes code to retry a failing requests on a different replica, hence hiding the failure from the user. During this time interval, the brownout service performs almost identically, except negligible reductions in optional content ratio at startup and when a replica fails, until the replica controller adapts to the new conditions.</p><p>However, starting with time 300, when the third replica fails, the non-brownout service behaves poorly. Computing capacity is insufficient to serve the incoming requests fast enough and response time starts increasing. A few seconds later the service is saturated and almost all incoming requests time out. The small oscillations and spikes on the timeout per second plot are due to the randomness of the request inter-arrival time in the open client model. Even worse, when enough replicas are restored to make capacity sufficient, the non-brownout service still does not recover. This finding may seem counter-intuitive, but repeating the experiments also in different conditions (number of allocated cores, different workloads, etc.) gave similar results.</p><p>In our experiments, as common practice in production environments, user timeouts are not propagating to the service, i.e., they do not cancel pending web requests or database transactions. Thus, the database server is essentially filled with transactions that will time out, or that may have already timed out on the user-side. Hence, all computing capacity is wasted on "rotten" requests, instead of striving to serve new requests. The database server continues to waste computing capacity on "rotten" requests, even after enough replicas are restored. The non-brownout service does recover eventually, but this takes significant time, at least 10 minutes in our experiments. Of course, in production environments the service operator or a self-healing mechanism would likely disable the service, kill all pending transactions on the database servers and re-enable the service. Nevertheless, this behavior is still undesirable. In contrast, the brownout service performs well even with few active replicas. At time 300, when the third replica fails leading the service into capacity insufficiency, the replica controllers detect the increase in response time and quickly reacts by reducing the optional content ratio to around 55%. As a results, the service does not saturate and users can continue enjoying a responsive service. At time 400 when the fourth replica fails, capacity available to the service is barely sufficient to serve any requests, even with zero optional content ratio. However, even in this case, the brownout service significantly reduces the number of timeouts by keeping the optional content ratio low, around 10%. Finally, when replicas are restored, the service recovers fairly quickly. Thanks to the action of the replica controllers, the database servers do not fill up with "rotten" requests.</p><p>On the downside, the brownout service features some oscillations of optional content while dealing with capacity shortage. This is due to the fact that the replica controllers attempt to maximize the number of optional content served, risking short increases in response time. These increases in response time are detected by the controllers, which adapt by reducing the number of optional content served. This process repeats, thus causing the oscillations. Except when capacity is close to being insufficient even with optional content completely disabled, these oscillations are harmless. Nevertheless, we are currently investigating several research directions to mitigate them, so as to allow brownout services to function well even in extreme capacity shortage situations. In addition to the 4-core scenario above, we devised two other experimental scenarios to confirm our findings, as summarized in Table I. In the 2-core scenario, we configured each replica with 2 cores, while in the heterogeneous scenario the number of cores for each replica is 8, 8, 1, 1, 1, respectively. In both scenarios, we scaled down the request-rate to maintain the same request-rate per core as in the 4-core scenario. Noteworthy is that in the heterogeneous scenario, the nonbrownout service recovered faster than in the 4-core and 2-core scenarios. This can be observed by comparing the difference between the percentage of requests served by the brownout service and the non-brownout service among the three scenarios. Nevertheless, the key findings still hold.</p><p>In summary, adding brownout to a replicated service improves its resilience, even when using a brownout-unaware load-balancing algorithm. The increase in resilience that can be obtained is specific to each service and depends on the ratio between the maximum throughput with optional content disabled and the one with optional content enabled. Hence, by measuring these two values a cloud service provider can either estimate the increase in resilience during capacity shortages given the current version of the service, or may decide to develop a new version of the service, with more content marked as optional, so as to reach the desired level of resilience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SQF vs. Brownout-Aware Load-Balancers</head><p>In this section, we compare the two brownout-aware loadbalancing algorithms proposed herein, i.e., PIBH and EPBH, to the best brownout-unaware one, SQF <ref type="bibr" target="#b12">[13]</ref>. We shall use the word better in the sense that we have statistical evidence that the average performance is significantly higher with a pvalue smaller than 0.01, by performing a Welch two sample ttest <ref type="bibr" target="#b28">[29]</ref> on the optional component served and on the response time. In other words, the probability that the difference is due to chance is less than 1%. Analogously, we use the word similarly to denote that the difference is not statistically significant.</p><p>For thorough comparison, we tested the three algorithms using a series of scenarios, each having a certain pattern of request rate over time and amount of cores allocated to each replica. Each scenario was executed several times, to collect enough results to draw statistically significant conclusions. We were unable to find any scenario in which SQF would perform better, which supports the hypothesis that our algorithms are at least as good as SQF. In fact, in most scenarios, such as those featuring high request rate variability or many replicas failing at once, SQF performed similarly to our brownout-aware load-balancers (not shown for briefness). However, we observed that in scenarios featuring capacity heterogeneity, our algorithms performed better than SQF with respect to the optional content ratio.</p><p>As a matter of fact, in cloud computing environments, replicas may end up being allocated heterogeneous capacity, e.g., one replica is allocated 2 cores, while another replica is allocated 8 cores. This may happen due to several factors. For example, the cloud infrastructure provider may practice overbooking and the machine on which a replica is hosted becomes overloaded <ref type="bibr" target="#b29">[30]</ref>. As another example, previous elasticity (autoscaling) decisions may have resulted in heterogeneously sized replicas <ref type="bibr" target="#b30">[31]</ref>. Hence, it is of uttermost importance that a loadbalancing algorithm is able to deal efficiently with such cases. As illustrated below on two scenarios, both PIBH and EPBH perform better than SQF.</p><p>1) "2×1+3×8 cores" Scenario: The first scenario consists of a constant request rate of 400 requests per second. The service consists of 5 replicas, two of which are allocated 1 core, while the other three are allocated 8 cores. This scenario leaves the service with insufficient capacity to serve all requests with optional content. Furthermore, the constant workload and capacity allows us to eliminate sources of noise and obtain statistically significant results with 30 experiments for each algorithm, a total of 90 experiments. <ref type="figure" target="#fig_1">Fig. 3</ref> presents the results of the first scenario as scatter plots: The x-axis represents response time (average and 95th percentile respectively in the top and the bottom graph), while the y-axis represents optional content ratio, each experiment being associated with a point. The results of the paired t-test comparing the optional content ratio of the three algorithms are presented in <ref type="table" target="#tab_1">Table II</ref>. As can be observed, when compared to SQF, the novel brownout-aware algorithms PIBH and EPBH improve optional content ratio by 5.34% and 4.52%, respectively, with a high significance (low p-value). This is due to the fact that the brownout-aware algorithms are able to exploit the replicas with a higher optional content ratio, at the expense of somewhat higher response times. Slightly increasing the average response time <ref type="figure" target="#fig_1">(Fig. 3</ref> top) yet improving the optional content served to the end user is an acceptable tradeoff, also considering that we have control on the target 95th percentile of the response time <ref type="figure" target="#fig_1">(Fig. 3 bottom)</ref>.</p><p>Recall that the replica controllers are configured with a target response time of 1 second. Furthermore, improved optional content ratio does not interfere with the self-adaptation of the replicas. As can be seen in <ref type="figure" target="#fig_1">Fig. 3</ref>, all three algorithms obtain a similar distribution of response times. In <ref type="table" target="#tab_1">Table III</ref> the paired t-test is applied also to the 95th percentile of the response time. The results confirm that PIBH behaves in a similar way with respect to the SQF, but producing better performance in terms of optional content served. When comparing EPBH to SQF, the average 95th percentile is 42ms higher in the former with quite a low p-value. However, it is to be noticed that the setpoint for the 95th percentile is set to 1 second, which is way higher than all of the presented results. Thus, the higher 95th percentile response time is not a concern.</p><p>2) "3×1+2×8 cores" Scenario: For the second scenario, we maintain the same request rate, but configure three replicas with 1 core and two replicas with 8 cores. This means that the service has even less capacity available than in the first scenario, thus being forced to further reduce the optional content ratio. Scatter plots of response time and optional content ratio are presented in <ref type="figure" target="#fig_2">Fig. 4</ref>, analogously to the previous scenario, while pair-wise comparison of algorithms is presented in <ref type="table" target="#tab_1">Table IV</ref>. PIBH and EPBH outperform SQF with respect to optional content ratio by 5.17% and 3.13%, respectively.</p><p>Again, this is achieved without interfering with the selfadaptation of the replicas: 95th percentile response times are distributed similarly for all three algorithms close to the target. This is also proven by the paired t-test presented in <ref type="table">Table V</ref>, where both PIBH and EPBH appear to be comparable with SQF in terms of 95th percentile of the response time. In this case, since the capacity of the system is reduced, this  quantity is increased, but on average still lower than the setpoint (set to 1 second). The same holds for the average response time, which is slightly increased with respect to the previous scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>To sum up, our novel brownout-aware load-balancing algorithms perform at least as well as or outperform SQF by up to 5% in terms of optional content served, with a high statistical significance. This improvement translates into better  quality of experience for users and increased revenue for the service provider. Hence, our contribution helps cloud services to better hide failures leading to capacity shortages, in other words, services are more resilient. Noteworthy is that the competitor, SQF has been found to be near-optimal with respect to response time for non-adaptive services <ref type="bibr" target="#b9">[10]</ref>. Thus, besides improving resilience of cloud services, our contribution may be of interest to other communities, to discover the limits of SQF, and sketch a possible way to design new dynamic load-balancing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RELATED WORK</head><p>The challenge of building reliable distributed systems consists in providing various safety and liveness guarantees while the system is subject to certain classes of failures. Our contribution closely relates to multi-graceful degradation <ref type="bibr" target="#b31">[32]</ref>, in which the requirements that the service guarantees vary depending on the magnitude of the failure. However, due to the conflicting nature of requirements -maintaining maximum response time and maximizing optional content served, in the presence of noisy request servicing times -brownout does not provide formal guarantees. Instead, thanks to controltheoretical tools, the service is driven to a state to increase likelihood of meeting its requirements.</p><p>Brownout can be seen as a model revision, i.e., an existing service is extended to provide new guarantees. Specifically, we deal with crashes but also with limplocks <ref type="bibr" target="#b32">[33]</ref>, the latter implying that a machine is working, but slower than expected.</p><p>In the context of self-stabilization, a new metric has been proposed to measure the recovery performance of an algorithm, the expected number of recovery steps <ref type="bibr" target="#b33">[34]</ref>. An equivalent metric, the number of control decisions to recovery, could be used by a service operator for tuning the service to the expected capacity drop and the request servicing time of the replicas.</p><p>Our contribution is designed to deal with failures reactively. Failure prediction <ref type="bibr" target="#b1">[2]</ref>, if accurate enough, could be used as a feed-forward signal to improve reactivity and reduce the number of timeouts after a sudden drop in computing capacity.</p><p>Since the service's data has to be replicated an important issue is ensuring consistency. Various algorithms have been proposed, each offering a different trade-off between performance and guarantees <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b26">[27]</ref>. Our contribution is orthogonal to consistency issues, hence our methodology can readily be applied no matter what consistency the service requires. However, a future extension of brownout could consist in avoiding service saturation by reducing consistency.</p><p>In replicated cloud services, load-balancers have a crucial role for ensuring resilience but also maintain performance <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Load-balancing algorithm can either be global (inter-datacenter) or local (intra-data-center or cluster-level). Global loadbalancing decides what data-center to direct a user to, depending on geographic proximity <ref type="bibr" target="#b34">[35]</ref> or price of energy <ref type="bibr" target="#b35">[36]</ref>. Once a data-center has been selected a local algorithm directs the request to a machine in the data-center. Our contribution is of the local type.</p><p>Various local load-balancing algorithms have been proposed. For non-adapting replicas, Shortest Queue First (SQF) has shown to be very close to optimal, despite it using little information about the state of the replicas <ref type="bibr" target="#b9">[10]</ref>. Our previous simulation results <ref type="bibr" target="#b12">[13]</ref> show that for self-adaptive, brownout replicas, SQF performs quite well, but can be outperformed by weight-based, brownout-aware solutions. In this article, we combine the two approaches and produce queue-length-based, brownout-aware load-balancing algorithms and show that they are practically applicable for improving resilience in the case of failures leading to service capacity shortage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>We present a novel approach for improving resilience, the ability to hide failures, in cloud services using a combination of brownout and load-balancing algorithms. The adoption of the brownout paradigm allows the service to autonomously reduce computing capacity requirements by degrading user experience in order to guarantee that response times are bounded. Thus, it provides a natural candidate for resilience improvement when failures lead to capacity shortages. However, state-of-the-art load-balancers are generally not designed for self-adaptive cloud services. The self-adaptivity embedded in the brownout service interferes with the actions of loadbalancers that route requests based on measurements of the response times of the replicas.</p><p>In order to investigate how brownout can be used for improving resilience, we extended the popular lighttpd web server with two new brownout-aware load-balancers. A first set of experiments showed that brownout provides substantial advantages in terms of resilience to cascading failures, even when employing SQF, a state-of-the-art, yet brownoutunaware, load-balancer. A second set of experiments compared SQF to the novel brownout-aware load-balancers, specifically designed to act on a per-request basis. The obtained results indicate that, with high statistical significance, our proposed solutions consistently outperform the current standards: They reduce the user experience degradation, thus perform better at hiding failures. While designed with brownout in mind, PIBH and EPBH may be useful to load-balance other selfadaptive cloud services, whose performance is not reflected in the response time or queue length.</p><p>During this investigation, we highlighted the difference between load-balancers that act whenever a new request is received and algorithms that periodically update the routing weights, finding out that the formers are far more effective than the latter ones. However, the brownout paradigm periodically updates the dimmer values to match specific requirements. A future improvement is to react faster also to events happening at the replica level, therefore redesigning the local replica controller to be event based. In the future, we would also like to design a holistic approach to replica control and loadbalancing, extending our replica controllers with auto-scaling features <ref type="bibr" target="#b36">[37]</ref>, that would allow to autonomously manage the number of replicas, together with the traffic routing, to obtain a cloud service that is both resilient and cost-effective. Finally, some control parameters were chosen empirically based on the many tests we have conducted. Ongoing work will quality the robustness of the system given the chosen parameters in a more systematic way and for a larger scenario space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Architecture of a brownout cloud service featuring multiple replicas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Comparison of SQF and brownout-aware load-balancing algorithms when two replicas have 1 core and three replicas have 8 cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Comparison of SQF and brownout-aware load-balancing algorithms when three replicas have 1 core and two replicas have 8 cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF NON-BROWNOUT VS. BROWNOUT RESULTS.</figDesc><table><row><cell>Scenario</cell><cell>Metric</cell><cell>Non-brownout</cell><cell>Brownout</cell></row><row><cell>4 cores</cell><cell>Requests served</cell><cell>31.2%</cell><cell>99.3%</cell></row><row><cell cols="2">200 requests/s With optional content</cell><cell>31.2%</cell><cell>81.0%</cell></row><row><cell>cores</cell><cell>Requests served</cell><cell>31.6%</cell><cell>99.3%</cell></row><row><cell cols="2">100 requests/s With optional content</cell><cell>31.6%</cell><cell>82.0%</cell></row><row><cell>heterogeneous</cell><cell>Requests served</cell><cell>68.8%</cell><cell>99.5%</cell></row><row><cell cols="2">166 requests/s With optional content</cell><cell>68.8%</cell><cell>90.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II IMPROVEMENT</head><label>II</label><figDesc>IN AMOUNT OF OPTIONAL CONTENT SERVED, AFTER 120000 REQUESTS (SUMMARYOF FIG. 3, "2×1+3×8 CORES" SCENARIO).</figDesc><table><row><cell cols="2">Algorithms (# Optional Content)</cell><cell>Impr.</cell><cell>Statistical Conclusion</cell></row><row><cell>PIBH (105646)</cell><cell>SQF (100273)</cell><cell>5.34%</cell><cell>PIBH significantly better (p &lt; 10 −15 )</cell></row><row><cell>EPBH (104816)</cell><cell>SQF (100273)</cell><cell>4.52%</cell><cell>EPBH significantly better (p &lt; 10 −15 )</cell></row><row><cell cols="4">TABLE III IMPROVEMENT IN AMOUNT OF 95TH PERCENTILE OF THE RESPONSE TIME (SUMMARY OF FIG. 3, "2×1+3×8 CORES" SCENARIO).</cell></row><row><cell cols="2">Algorithms (95th perc. [ms])</cell><cell>Impr.</cell><cell>Statistical Conclusion</cell></row><row><cell>PIBH (637ms)</cell><cell>SQF (648ms)</cell><cell>-1.7%</cell><cell>PIBH and SQF similar (p = 0.992)</cell></row><row><cell>EPBH (690ms)</cell><cell>SQF (648ms)</cell><cell>6.4%</cell><cell>SQF significantly better (p &lt; 10 −9 )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV IMPROVEMENT</head><label>IV</label><figDesc>IN AMOUNT OF OPTIONAL CONTENT SERVED, AFTER 120000 REQUESTS (SUMMARYOF FIG. 4, "3×1+2×8 CORES" SCENARIO).</figDesc><table><row><cell cols="2">Algorithms (# Optional Content)</cell><cell>Impr.</cell><cell>Statistical Conclusion</cell></row><row><cell>PIBH (83360)</cell><cell>SQF (79244)</cell><cell>5.17%</cell><cell>PIBH significantly better (p &lt; 10 −15 )</cell></row><row><cell>EPBH (81735)</cell><cell>SQF (79244)</cell><cell>3.13%</cell><cell>EPBH significantly better (p &lt; 10 −15 )</cell></row><row><cell cols="4">TABLE V IMPROVEMENT IN AMOUNT OF 95TH PERCENTILE OF THE RESPONSE TIME (SUMMARY OF FIG. 4, "3×1+2×8 CORES" SCENARIO).</cell></row><row><cell cols="2">Algorithms (95th perc. [ms])</cell><cell>Impr.</cell><cell>Statistical Conclusion</cell></row><row><cell>PIBH (963ms)</cell><cell>SQF (959ms)</cell><cell>0.4%</cell><cell>PIBH and SQF similar (p = 0.3778)</cell></row><row><cell>EPBH (969ms)</cell><cell>SQF (959ms)</cell><cell>1.0%</cell><cell>EPBH and SQF similar (p = 0.2265)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.lighttpd.net/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">2100 MHz, 16 cores per processor, no hyper-threading.<ref type="bibr" target="#b4">5</ref> https://github.com/cloud-control/brownout-lb-rubis</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/cloud-control/httpmon</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was partially supported by the Swedish Research Council (VR) for the projects "Cloud Control" and "Power and temperature control for large-scale computing infrastructures", and through the LCCC Linnaeus and ELLIIT Excellence Centers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hölzle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan &amp; Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive anomaly identification by exploring metric subspace in cloud computing infrastructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1109/SRDS.2013.29</idno>
	</analytic>
	<monogr>
		<title level="m">SRDS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On designing and deploying internet-scale services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LISA, USENIX</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A model for space-correlated failures in large-scale distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gallet</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15277-110</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>in Euro-Par</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis and modeling of timecorrelated failures in large-scale distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yigitbasi</surname></persName>
		</author>
		<idno type="DOI">10.1109/GRID.2010.5697961</idno>
	</analytic>
	<monogr>
		<title level="m">GRID</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Linking resource usage anomalies with system failures from cluster log data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chuah</surname></persName>
		</author>
		<idno type="DOI">10.1109/SRDS.2013.20</idno>
	</analytic>
	<monogr>
		<title level="m">SRDS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brownout: building more robust cloud applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1145/2568225.2568227</idno>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recommender systems and their effects on consumers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleder</surname></persName>
		</author>
		<idno type="DOI">10.1145/1807342.1807378</idno>
	</analytic>
	<monogr>
		<title level="m">Electronic Commerce</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A study on tolerable waiting time: how long are web users willing to wait</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-H</forename><surname>Nah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour and Information Technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of join-the-shortest-queue routing for web server farms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.peva.2007.06.012</idno>
	</analytic>
	<monogr>
		<title level="j">Perform. Eval</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s highly available key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Decandia</surname></persName>
		</author>
		<idno type="DOI">10.1145/1323293.1294281</idno>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rice university bidding system</title>
		<ptr target="http://rubis.ow2.org" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Control-theoretical load-balancing for cloud applications with brownout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dürango</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Decision and Control (CDC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Digital control systems: design, identification and implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Landau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<idno type="DOI">10.1145/945445.945462</idno>
	</analytic>
	<monogr>
		<title level="m">SOSP, ACM</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are clouds ready for large distributed applications?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sripanidkulchai</surname></persName>
		</author>
		<idno type="DOI">10.1145/1773912.1773918</idno>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tutorial: installing a LAMP web server</title>
		<ptr target="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-LAMP.html" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PRESS: predictive elastic resource scaling for cloud systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">10.1109/CNSM.2010.5691343</idno>
	</analytic>
	<monogr>
		<title level="m">CNSM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CloudScale: elastic resource scaling for multi-tenant cloud systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2038916.2038921</idno>
		<editor>SoCC, ACM</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">JustRunIt: experiment-based management of virtualized data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ATC</title>
		<imprint>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance modeling and system management for multi-component online services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI, USENIX</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DejaVu: accelerating resource allocation in virtualized environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasić</surname></persName>
		</author>
		<idno type="DOI">10.1145/2189750.2151021</idno>
	</analytic>
	<monogr>
		<title level="j">ASPLOS</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploiting nonstationarity for performance prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stewart</surname></persName>
		</author>
		<idno type="DOI">10.1145/1272998.1273002</idno>
	</analytic>
	<monogr>
		<title level="j">EuroSys</title>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SLA decomposition: translating service level objectives to system level thresholds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICAC.2007.36</idno>
	</analytic>
	<monogr>
		<title level="j">ICAC</title>
		<imprint>
			<date type="published" when="2007" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bumper: sheltering transactions from conflicts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Diegues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Romano</surname></persName>
		</author>
		<idno type="DOI">10.1109/SRDS.2013.27</idno>
	</analytic>
	<monogr>
		<title level="m">SRDS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with YCSB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<idno>DOI: 10. 1145/1807128.1807152</idno>
	</analytic>
	<monogr>
		<title level="j">SoCC</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-monotonic snapshot isolation: scalable and strong consistency for geo-replicated transactional systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ardekani</surname></persName>
		</author>
		<idno>DOI: 10. 1109/SRDS.2013.25</idno>
	</analytic>
	<monogr>
		<title level="m">SRDS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Open versus closed: a cautionary tale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>NSDI, USENIX</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The generalization of &apos;student&apos;s&apos; problem when several different population variances are involved</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Welch</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/34.1-2.28</idno>
		<ptr target="DOI:10.1093/biomet/34.1-2.28" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving cloud infrastructure utilization through overbooking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tomás</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tordsson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2494621.2494627</idno>
	</analytic>
	<monogr>
		<title level="m">CAC, ACM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A virtual machine re-packing approach to the horizontal vs. vertical elasticity tradeoff for cloud autoscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sedaghat</surname></persName>
		</author>
		<idno type="DOI">10.1145/2494621.2494628</idno>
	</analytic>
	<monogr>
		<title level="m">CAC, ACM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated multi-graceful degradation: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<idno>DOI: 10 . 1109/SRDS.2013.17</idno>
	</analytic>
	<monogr>
		<title level="m">SRDS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Limplock: understanding the impact of limpware on scale-out cloud systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<idno type="DOI">10.1145/2523616.2523627</idno>
		<editor>SoCC</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rigorous performance evaluation of self-stabilization using probabilistic model checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fallahi</surname></persName>
		</author>
		<idno type="DOI">10.1109/SRDS.2013.24</idno>
	</analytic>
	<monogr>
		<title level="m">SRDS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online algorithms for geographical load balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/IGCC.2012.6322266</idno>
	</analytic>
	<monogr>
		<title level="j">IGCC</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stratus: load balancing the cloud for carbon emissions control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Doyle</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCC.2013.4</idno>
	</analytic>
	<monogr>
		<title level="j">TCC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An adaptive hybrid elasticity controller for cloud infrastructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ali-Eldin</surname></persName>
		</author>
		<idno type="DOI">10.1109/NOMS.2012.6211900</idno>
	</analytic>
	<monogr>
		<title level="j">NOMS</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
