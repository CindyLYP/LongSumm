<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-05">5 Nov 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Zahavy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of ECE</orgName>
								<orgName type="department" key="dep2">Department of EE</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sivak</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of ECE</orgName>
								<orgName type="department" key="dep2">Department of EE</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of ISE</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
						</author>
						<title level="a" type="main">Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-05">5 Nov 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1602.02389v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>The question why deep learning algorithms generalize so well has attracted increasing research interest. However, most of the well-established approaches, such as hypothesis capacity, stability or sparseness, have not provided complete explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus on the robustness approach (Xu &amp; Mannor, 2012), i.e., if the error of a hypothesis will not change much due to perturbations of its training examples, then it will also generalize well. As most deep learning algorithms are stochastic (e.g., Stochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness arguments of Xu &amp; Mannor, and introduce a new approach-ensemble robustness-that concerns the robustness of a population of hypotheses. Through the lens of ensemble robustness, we reveal that a stochastic learning algorithm can generalize well as long as its sensitiveness to adversarial perturbations is bounded in average over training examples. Moreover, an algorithm may be sensitive to some adversarial examples (Goodfellow et al., 2015) but still generalize well. To support our claims, we provide extensive simulations for different deep learning algorithms and different network architectures exhibiting a strong correlation between ensemble robustness and the ability to generalize.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Neural Networks (DNNs) have been successfully applied in many artificial intelligence tasks, providing state-of-the-art performance and a remarkably small generalization error. On the other hand, DNNs often have far more trainable model parameters than the number of samples they are trained on and were shown to have a large enough capacity to memorize the training data <ref type="bibr">(Zhang et al., 2016)</ref>. Thus, most statistical learning theory that explains generalization via hypothesis capacity struggle to explain the generalization ability of large artificial neural networks.</p><p>In this work, we focus on a different approach to study generalization of DNNs, i.e., the connection between the robustness of a deep learning algorithm and its ability to generalize. <ref type="bibr">Xu &amp; Mannor have</ref> shown that if an algorithm is robust (i.e., its empirical loss does not change dramatically for perturbed samples), its generalization performance can also be guaranteed. However, in the context of DNNs, practitioners observe contradicting evidence between these two attributes. On the one hand, DNNs generalize well, and on the other, they are fragile to adversarial perturbation on the inputs <ref type="bibr" target="#b16">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015)</ref>. Nevertheless, adversarial training (methods based on generating adversarial examples to training examples and using them during training) have been shown to improve the generalization of deep neural network models <ref type="bibr" target="#b16">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b13">Shaham et al., 2015)</ref>, indicating an implicit connection between the robustness of a neural net and its ability to generalize. Moreover, it was observed that dropout, coupled with adversarial training, is best at hindering memorization without reducing the model's ability to learn <ref type="bibr" target="#b0">(Arpit et al., 2017)</ref>.</p><p>In order to solve this contradiction, we revisit the robustness argument in <ref type="bibr" target="#b18">(Xu &amp; Mannor, 2012)</ref> and present ensemble robustness, to characterize the generalization performance of deep learning algorithms. Our proposed approach is not intended to give tight performance guarantees for general deep learning algorithms, but rather to pave a way for addressing the question: how can deep learning perform so well while being fragile to adversarial examples? Answering this question is difficult, yet we present evidence in both theory and simulation strongly suggesting that ensemble robustness is crucial to the generalization performance of deep learning algorithms.</p><p>Ensemble robustness concerns the fact that a randomized algorithm (e.g., Stochastic Gradient Descent (SGD), Dropout <ref type="bibr" target="#b14">(Srivastava et al., 2014)</ref>, Bayes-by-backprop <ref type="bibr" target="#b2">(Blundell et al., 2015)</ref>, etc.) produces a distribution of hypotheses instead of a deterministic one. Therefore, ensemble robustness takes into consideration robustness of the population of the hypotheses: even though some hypotheses may be sensitive to perturbation on inputs, an algorithm can still generalize well as long as most of the hypotheses sampled from the distribution are robust on average. <ref type="bibr" target="#b9">Kawaguchi et al. (2017)</ref> took a different approach and claimed that deep neural networks can generalize well despite nonrobustness. However, our definition of ensemble robustness together with our empirical findings suggest that deep learning methods are typically robust although being fragile to adversarial examples.</p><p>Through ensemble robustness, we prove that the following holds with a high probability: randomized learning algorithms can generalize well as long as its output hypothesis has bounded sensitiveness to perturbation in average (see Theorem 1). Specified for deep learning algorithms, we reveal that if hypotheses from different runs of a deep learning method perform consistently well in terms of robustness, the performance of such deep learning method can be confidently expected. Moreover, each hypothesis may be sensitive to some adversarial examples as long as it is robust on average.</p><p>Although ensemble robustness may be difficult to compute analytically, we demonstrate an empirical estimate of ensemble robustness and investigate the role of ensemble robustness via extensive simulations. The results provide supporting evidence for our claim: ensemble robustness consistently explains the generalization performance of deep neural networks. Furthermore, ensemble robustness is measured solely on training data, potentially allowing one to use the testing examples for training and selecting the best model based on its ensemble robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Xu et al.( 2012) proposed to consider model robustness for estimating generalization performance for deterministic algorithms, such as for SVM <ref type="bibr">(Xu et al., 2009b)</ref> and Lasso <ref type="bibr" target="#b19">(Xu et al., 2009a)</ref>. They suggest using robust optimization to construct learning algorithms, i.e., minimizing the empirical loss with respect to the adversarial perturbed training examples.</p><p>Introducing stochasticity to deep learning algorithms has achieved great success in practice and also receives theoretical investigation. <ref type="bibr" target="#b6">Hardt et al. (2015)</ref> analyzed the stability property of SGD methods, and Dropout <ref type="bibr" target="#b14">(Srivastava et al., 2014)</ref> was introduced as a way to control over-fitting by randomly omitting subsets of features at each iteration of a training procedure. Different explanations for the empirical success of dropout have been proposed, including, avoiding over-fitting as a regularization method <ref type="bibr" target="#b1">(Baldi &amp; Sadowski, 2013;</ref><ref type="bibr" target="#b17">Wager et al., 2013;</ref><ref type="bibr" target="#b8">Jain et al., 2015)</ref> and explaining dropout as a Bayesian approximation for a Gaussian process <ref type="bibr" target="#b3">(Gal &amp; Ghahramani, 2015)</ref>. Different from those works, this work will extend the results in <ref type="bibr" target="#b18">(Xu &amp; Mannor, 2012)</ref> to randomized algorithms, in order to analyze them from an ensemble robustness perspective.</p><p>Adversarial examples for deep neural networks were first introduced in <ref type="bibr" target="#b16">(Szegedy et al., 2014)</ref>, while some recent works propose to utilize them as a regularization technique for training deep models <ref type="bibr" target="#b4">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b5">Gu &amp; Rigazio, 2014;</ref><ref type="bibr" target="#b13">Shaham et al., 2015)</ref>. However, all of those works attempt to find the "worst case" examples in a local neighborhood of the original training data and are not focused on measuring the global robustness of an algorithm nor on studying the connection between robustness and generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>In this work, we investigate the generalization property of stochastic learning algorithms in deep neural networks, by establishing their PAC bounds. In this section, we provide some preliminary facts that are necessary for developing the approach of ensemble robustness. After introducing the problem setup we are interested in, we in particular highlight the inherent randomness of deep learning algorithms and give a formal description of randomized learning algorithms. Then, we briefly review the relationship between robustness and generalization performance established in <ref type="bibr" target="#b18">(Xu &amp; Mannor, 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem setup</head><p>We now introduce the learning setup for deep neural networks, which follows a standard one for supervised learning. More concretely, we have Z and H as the sample set and the hypothesis set respectively. The training sample set s = {s 1 , . . . , s n } consists of n i.i.d. samples generated by an unknown distribution µ, and the target of learning is to obtain a neural network that minimizes expected classification error over the i.i.d. samples from µ. Throughout the paper, we consider the training set s with a fixed size of n.</p><p>We denote the learning algorithm as A, which is a mapping from Z n to H. We use A : s → h s to denote the learned hypothesis given the training set s. We consider the loss function (h, z) whose value is nonnegative and upper bounded by M . Let L(•) and emp (•) denote the expected error and the training error for a learned hypothesis h s , i.e.,</p><formula xml:id="formula_0">L(h s ) E z∼µ (h s , z), and emp (h s ) 1 n si∈s (h s , s i ).<label>(1)</label></formula><p>We are going to characterize the generalization error |L(h s ) − emp (h s )| of deep learning algorithms in the following section.</p><p>Randomized algorithms Most of modern deep learning algorithms are in essence randomized ones, which map a training set s to a distribution of hypotheses ∆(H) instead of a single hypothesis. For example, running a deep learning algorithm A with dropout for multiple times will produce different hypotheses which can be deemed as samples from the distribution ∆(H). This is an important observation we make for deep learning analysis in this work, and we will point out such randomness actually plays an important role for deep learning algorithms to perform well. Therefore, before proceeding to analyze the performance of deep learning, we provide a formal definition of randomized learning algorithms here.</p><p>Definition 1 (Randomized Algorithms). A randomized learning algorithm A is a function from Z n to a set of distributions of hypotheses ∆(H), which outputs a hypothesis h s ∼ ∆(H) with a probability π s (h).</p><p>When learning with a randomized algorithm, the target is to minimize the expected empirical loss for a specific output hypothesis h s , similar to the ones in (1). Here is the loss incurred by a specific output hypothesis by one instantiation of the randomized algorithm A.</p><p>Examples of the internal randomness of a deep learning algorithm A include dropout rate (the parameter for a Bernoulli distribution for randomly masking certain neurons), random shuffle among training samples in SGD, the initialization of weights for different layers, to name a few. <ref type="bibr" target="#b18">Xu &amp; Mannor (2012)</ref> established the relation between algorithmic robustness and generalization for the first time. An algorithm is robust if the following holds: if two samples are close to each other, their associated losses are also close. For being self-contained, we here briefly review the algorithmic robustness and its induced generalization guarantee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness and generalization</head><formula xml:id="formula_1">Definition 2 (Robustness, Xu &amp; Mannor (2012)). Algorithm A is (K, (•)) robust, for K ∈ N and (•) : Z n → R, if Z can be partitioned into K disjoint sets, denoted by {C i } K i=1</formula><p>, such that the following holds for all s ∈ Z n :</p><formula xml:id="formula_2">∀s ∈ s, ∀z ∈ Z, ∀i = 1, . . . , K : if s, z ∈ C i , then | (A s , s) − (A s , z)| ≤ (n).</formula><p>Based on the above robustness property of algorithms, Xu et al. <ref type="bibr" target="#b18">(Xu &amp; Mannor, 2012)</ref> prove that a robust algorithm also generalizes well. Motivated by their results, Shaham et al. <ref type="bibr" target="#b13">(Shaham et al., 2015)</ref> proposed adversarial training algorithm to minimize the empirical loss over synthesized adversarial examples. However, those results cannot be applied for characterizing the performance of modern deep learning models well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ensemble Robustness</head><p>In order to explain the good performance of deep learning, one needs to understand the internal randomness of deep learning algorithms and the population performance of the multiple possible hypotheses. Intuitively, a single output hypothesis cannot be robust to adversarial perturbation on training samples and the deterministic robustness argument in <ref type="bibr" target="#b18">(Xu &amp; Mannor, 2012</ref>) cannot be applied here. Fortunately, deep learning algorithms generally output the hypothesis sampled from a distribution of hypotheses. Therefore, even if some samples are not "nice" for one specific hypothesis, they aren't likely to fail most of the hypothesis from the produced distribution. Thus, deep learning algorithms are able to generalize well. Such intuition motivates us to introduce the concept of ensemble robustness that is defined over the distribution of output hypotheses of a deep learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3 (Ensemble Robustness).</head><p>A randomized algorithm A is (K,¯ (n)) ensemble robust, for K ∈ N and (n), if Z can be partitioned into K disjoint sets, denoted by</p><formula xml:id="formula_3">{C i } K i=1</formula><p>, such that the following holds for all s ∈ Z n :</p><formula xml:id="formula_4">∀s ∈ s, ∀i = 1, . . . , K : if s ∈ C i , then E A max z∈Ci | (A s , s) − (A s , z)| ≤¯ (n).</formula><p>Here the expectation is taken w.r.t. the internal randomness of the algorithm A.</p><p>Ensemble robustness is a "weaker" requirement for the model compared with the robustness proposed in <ref type="bibr" target="#b18">(Xu &amp; Mannor, 2012)</ref> and it fits better for explaining deep learning. In the following section, we demonstrate through simulations that a deep learning model is not robust but it is indeed ensemble robust. So in practice the deep model can still achieve good generalization performance.</p><p>An algorithm with strong ensemble robustness can provide good generalization performance in expectation w.r.t. the generated hypothesis, as stated in the following theorem. We note that the proofs for all the theorems that we present in this section can be found supplementary material. In addition, the supplementary material holds an additional proof for the special case of Dropout.</p><p>Theorem 1. Let A be a randomized algorithm with (K,¯ (n)) ensemble robustness over the training set s, with |s| = n. Let ∆(H) ← A : s denote the output hypothesis distribution of A. Then for any δ &gt; 0, with probability at least 1 − δ with respect to the random draw of the s and h ∼ ∆(H), the following holds:</p><formula xml:id="formula_5">|L(h) − emp (h)| ≤ nM¯ (n) + 2M 2 δn .</formula><p>Note that in the above theorem, we hide the dependency of the generalization bound on K in ensemble robustness measure (n). Due to space limitations, all the technical lemmas and details of the proofs throughout the paper are deferred to supplementary material. Theorem 1 leads to following corollary which gives a way to minimize expected loss directly.</p><p>Corollary 1. Let A be a randomized algorithm with (K,¯ (n)) ensemble robustness. Let C 1 , . . . , C K be a partition of Z, and write z 1 ∼ z 2 if z 1 , z 2 fall into the same C k . If the training sample s is generated by i.i.d. draws from µ, then with probability at least 1 − δ, the following holds over h ∈ H</p><formula xml:id="formula_6">L(h) ≤ 1 n n i=1 max zi∼si (h, z i ) + nM¯ (n) + 2M 2 δn .</formula><p>Corollary 1 suggests that one can minimize the expected error of a deep learning algorithm effectively through minimizing the empirical error over the training samples s i perturbed in an adversarial way. In fact, such an adversarial training strategy has been exploited in <ref type="bibr" target="#b4">(Goodfellow et al., 2015;</ref><ref type="bibr" target="#b13">Shaham et al., 2015)</ref>.</p><p>Theorem 2. Let A be a randomized algorithm with (K,¯ (n)) ensemble robustness over the training set s, where |s| = n. Let ∆(H) denote the output hypothesis distribution of the algorithm A on the training set s. Suppose following variance bound holds:</p><formula xml:id="formula_7">var A max z∼si | (A s , s i ) − (A s , z)| ≤ α</formula><p>Then for any δ &gt; 0, with probability at least 1 − δ with respect to the random draw of the s and h ∼ ∆(H), we have</p><formula xml:id="formula_8">|L(A s ) − emp (A s )| ≤¯ (n) + 1 √ 2δ α + M 2K ln 2 + 2 ln(1/δ) n</formula><p>Theorem 2 suggests that controlling the variance of the deep learning model can substantially improve the generalization performance. Note that here we need to consider the trade-off between the expectation and variance of ensemble robustness. To see this, consider following two extreme examples. When α = 0, we do not allow any variance in the output of the algorithm A. Thus, A reduces to a deterministic one. To achieve the above upper bound, it is required that the output hypothesis satisfies max z∈Ci | (h, s i ) − (h, z)| ≤ (n). However, due to the intriguing property of deep neural networks <ref type="bibr" target="#b16">(Szegedy et al., 2014)</ref>, the deterministic model robustness measure (n) (ref. Definition 2) is usually large. In contrast, when the hypotheses variance α can be large enough, there are multiple possible output hypotheses from the distribution ∆(H). We fix the partition of Z as C 1 , . . . , C K . Then,</p><formula xml:id="formula_9">E A [ max z∼s∈s∩Ci | (h, s) − (h, z)|] = j∈∆(H) P{h = h j } max z∼s∈s∩Ci | (h j , s) − (h j , z)| ≤ j∈∆(H) P{h = h j } max z∼s∈s∩Ci max h∈∆H | (h, s) − (h, z)| ≤ max z∼s∈s∩Ci max h∈∆(H) | (h, s) − (h, z)|.</formula><p>Therefore, allowing certain variance on produced hypotheses, a randomized algorithm can tolerate the non-robustness of some hypotheses to certain samples. As long as the ensemble robustness is small, the algorithm can still perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Simulations</head><p>This section is devoted to simulations for quantitatively and qualitatively demonstrating how ensemble robustness of a deep learning method explains its performance. We first introduce our experiment settings and implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>Data sets We conduct simulations on two benchmarks. MNIST, a dataset of handwritten digit images (28x28) with 50,000 training samples and 10,000 test samples <ref type="bibr" target="#b10">(LeCun et al., 1998)</ref>. NotMNIST 1 , a "mnist like database" containing font glyphs for the letters A through J (10 classes). The training set contains 367,440 samples and 18,724 testing examples. The images (for both data sets) were scaled such that each pixel is in the range [0, 1]. We note that we did not use the cross-validation data.</p><p>Network architecture and parameter setting Without explicit explanation, we use multi-layer perceptrons throughout the simulations. All networks we examined are composed of three fully connected layers, each of which is followed by a rectified linear unit on top. The output of the last fully-connected layer is fed to a 10-way softmax. In order to avoid the bias brought by specific network architecture on our observations, we sample at random the number of units in each layer (uniformly over {400, 800, 1200} units) and the learning rate (uniformly over [0.005, 0.05]). Finally, we used a mini-batch of 128 training examples at a time.</p><p>Compared algorithms We evaluate and compare ensemble robustness as well as the generalization performance for following 4 deep learning algorithms. (1) Explicit ensembles, i.e., using a stochastic algorithm to train different members in the ensemble by running the algorithm multiple times with different seeds. In practice, this was implemented using SGD as the stochastic algorithm, trained to minimize the cross-entropy loss. (2) Implicit ensembles, i.e., learning a probability distribution on the weights of a neural network and sampling ensemble members from it. This was implemented with the Bayes-by-backprop <ref type="bibr" target="#b2">(Blundell et al., 2015)</ref> algorithm, a recent approach for training Bayesian Neural http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html Networks. It uses backpropagation to learn a probability distribution on the weights of a neural network by minimising the expected lower bound on the marginal likelihood (or the variational free energy). Methods 3 and 4 correspond for adding adversarial training <ref type="bibr" target="#b16">(Szegedy et al., 2014;</ref><ref type="bibr" target="#b4">Goodfellow et al., 2015;</ref><ref type="bibr" target="#b13">Shaham et al., 2015)</ref> to the ensemble methods, where the magnitude of perturbation is measured by its</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Empirical Ensemble Robustness and Generalization</head><p>We now present simulations that empirically validate Theorem 1, i.e., that the ensemble robustness of a DNN (measured on the training set) is highly correlated with its generalization performance. As ensemble robustness involves taking an expectation over all the possible output hypothesis, it is computationally intractable to exactly measure ensemble robustness for different deep learning algorithms. In this simulation, we take the empirical average of robustness to adversarial perturbation from 5 different hypotheses of the same learning algorithm as its ensemble robustness. In the case of the SGD variants, for each configuration, we collect an ensemble of output hypotheses by repeating the training procedures using the same configuration while using different random seeds. In the case of the Bayes-by-backprop methods, the algorithm explicitly outputs a distribution over output hypothesis, so we simply sample the networks from the learned weight distribution. In particular, we aim to empirically demonstrate that a deep learning algorithm with stronger ensemble robustness presents better generalization performance (Theorem 1). Recall the definition of ensemble robustness in Definition 3, another obstacle in calculating ensemble robustness is to find the most adversarial perturbation ∆s (or equivalently the most adversarial example z = s + ∆s) for a specific training sample s ∈ s within a partition set C i . We therefore employ an approximate search strategy for finding the adversarial examples. More concretely, we optimize the following first-order Taylor expansion of the loss function as a surrogate for finding the adversarial example:</p><formula xml:id="formula_10">∆s i ∈ arg max ∆si ≤r (s i ) + ∇ si (s), ∆s i ,<label>(2)</label></formula><p>with a pre-defined magnitude constraint r on the perturbation ∆s i . In the simulations, we vary the magnitude r in order to calculate the empirical ensemble robustness at different perturbation levels.</p><p>We then calculate the empirical ensemble robustness by averaging the difference between the loss of the algorithm on the training samples and the adversarial samples output by the method in (2):</p><formula xml:id="formula_11">emp = 1 T T t=1 max i∈{1,...,n} | (A (t) s , s i ) − (A (t) s , s i + ∆s i )|,<label>(3)</label></formula><p>with T = 5 denoting the size of the ensemble.</p><p>We emphasize that¯ (n) (Theorem 1) and the empirical approximation¯ (n) emp measure the non robustness of an algorithm, i.e., an algorithm is more robust if¯ (n) is smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>The generalization performance of different learning algorithms and different networks compared with the empirical ensemble robustness on MNIST is given in <ref type="figure" target="#fig_0">Figure 1</ref>. Notice that the x-axis corresponds to the empirical ensemble robustness <ref type="formula" target="#formula_11">Equation 3</ref>, and the y-axis corresponds to the test error. Examining <ref type="figure" target="#fig_0">Figure 1</ref> we observe a high correlation between ensemble robustness and generalization for all learning algorithms, i.e., algorithms that are more robust (have lower¯ (n)) generalize better on this data set.</p><p>In addition, adversarial training methods consistently present stronger ensemble robustness (smaller ) than pure SGD or Bayes-by-backprop. <ref type="figure" target="#fig_1">Figure 2</ref> presents similar results on the notMNIST dataset, although we observe lower (yet positive) correlation for the Bayes-by-backprop algorithm in this case. These observations support our claim on the relation between ensemble robustness and algorithm generalization performance in Theorem 1.</p><p>We also compare ensemble robustness with robustness on MNIST in <ref type="table">Table 1</ref>, where robustness is measured similarly to ensemble robustness using Equation 3 but with T = 1 (while T = 5 for ensemble robustness). Indeed, we observe that averaging over instances of the same algorithm, exhibits a higher correlation between generalization and robustness, i.e., ensemble robustness is a better estimation for the generalization performance than standard robustness.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we investigated the generalization ability of stochastic deep learning algorithm based on their ensemble robustness; i.e., the property that if a testing sample is "similar" to a training sample, then its loss is close to the training error. We established both theoretically and experimentally evidence that ensemble robustness of an algorithm, measured on the training set, indicates its generalization performance well. Moreover, our theory and experiments suggest that DNNs may be robust (and generalize) while being fragile to specific adversarial examples. Measuring ensemble robustness of stochastic deep learning algorithms may be computationally prohibitive as one needs to sample several output hypotheses of the algorithm. Thus, we demonstrated that by learning the probability distribution of the weights of a neural network explicitly, e.g., via variational methods such as Bayes-by-backprop, we can still observe a positive correlation between robustness and generalization while using fewer computations, making ensemble robustness feasible to measure.</p><p>As a direct consequence, one can potentially measure the generalization error of an algorithm without using testing examples. In future work, we plan to further investigate if ensemble robustness can be used for model selection instead of cross-validation (and hence, increasing the training set size), in particular in problems that have a small training set. A different direction is to study the resilience of deep learning methods to adversarial attacks <ref type="bibr" target="#b12">(Papernot et al., 2016)</ref>. <ref type="bibr" target="#b15">Strauss et al. (2017)</ref> recently showed that ensemble methods are useful as a mean to defense against adversarial attacks. However, they only considered implicit ensemble methods which are computationally prohibitive. As our simulations show that explicit ensembles are robust as well, we believe that they are likely to be a useful defense strategy while reducing computational cost. Finally, Theorem 2 suggests that a randomized algorithm can tolerate the non-robustness of some hypotheses to certain samples; this may help to explain Proposition 1 in <ref type="bibr" target="#b9">Kawaguchi et al. (2017)</ref>: "For any dataset, there exist arbitrarily unstable non-robust algorithms such that has a small generalization gap". We leave this intuition for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material 7 Understanding Dropout via Ensemble Robustness</head><p>In this section, we illustrate how ensemble robustness can well characterize the performance of various training strategies of deep learning. In particular, we take the dropout as a concrete example. Dropout is a widely used technique for optimizing deep neural network models. We demonstrate that dropout is a random scheme to perturb the algorithm. During dropout, at each step, a random fraction of the units are masked out in a round of parameter updating.</p><p>Assumption 1. We assume the randomness of the algorithm A is parametrized by r = (r 1 , . . . , r L ) ∈ R where r l , l = 1, . . . , L are random elements drawn independently.</p><p>For a deep neural network consisting of L layers, the random variable r l is the dropout randomness for the l-th layer. The next theorem establishes the generalization performance for the neural network with dropout training.</p><p>Theorem 3 (Generalization of Dropout Training). Consider an L-layer neural network trained by dropout. Let A be an algorithm with (K,¯ (n)) ensemble robustness. Let ∆(H) denote the output hypothesis distribution of the randomized algorithm A on a training set s. Assume there exists a β &gt; 0 such that,</p><formula xml:id="formula_12">sup r,t sup z∈Z | (A s,r , z) − (A s,t , z)| ≤ β ≤ L −3/4 ,</formula><p>with r and t only differing in one element. Then for any δ &gt; 0, with probability at least 1 − δ with respect to the random draw of the s and h ∼ ∆(H),</p><formula xml:id="formula_13">L(h s,r ) − emp (h s,r ) ≤¯ (n) + 2 log(1/δ)/L + 2K ln 2 + 2 ln(2/δ) n .</formula><p>Theorem 3 also establishes the relation between the depth of a neural network model and the generalization performance. It suggests that when using dropout training, controlling the variance β of the empirical performance over different runs is important: when β converges at the rate of L −3/4 , increasing the layer number L will improve the performance of a deep neural network model. However, simply making L larger without controlling β does not help. Therefore, in practice, we usually use voting from multiple models to reduce the variance and thus decrease the generalization error . Also, when dropout training is applied for more layers in a neural network model, smaller variance of the model performance is preferred. This can be compensated by increasing the size of training examples or ensemble of multiple models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Technical Lemmas</head><p>Lemma 1. For a randomized learning algorithm A with (K,¯ (n)) uniform ensemble robustness, and loss function such that 0 ≤ (h, z) ≤ M , we have,</p><formula xml:id="formula_14">P s E A |L(h) − emp (h)| ≤¯ (n) + M 2K ln 2 + 2 ln(1/δ) n ≥ 1 − δ,</formula><p>where we use P s to denote the probability w.r.t. the choice of s, and |s| = n.</p><p>Proof. Given a random choice of training set s with cardinality of n, let N i be the set of index of points of s that fall into the C i . Note that (|N 1 |, . . . , |N K |) is an i.i.d. multinomial random variable with parameters n and (µ(C 1 ), . . . , µ(C K )). The following holds by the Breteganolle-Huber-Carol inequality:</p><formula xml:id="formula_15">P s K i=1 |N i | n − µ(C i ) ≥ λ ≤ 2 K exp −nλ 2 2 .</formula><p>We have</p><formula xml:id="formula_16">E A |L(A s ) − emp (A s )| = E A K i=1 E z∼µ ( (A s , z)|z ∈ C i )µ(C i ) − 1 n n i=1 (A s , s i ) (a) ≤ E A K i=1 E z∼µ ( (A s , z)|z ∈ C i ) |N i | n − 1 n n i=1 (A s , s i ) + E A K i=1 E z∼µ ( (A s , z)|z ∈ C i )µ(C i ) − K i=1 E z∼µ ( (A s , z)|z ∈ C i ) |N i | n (b) ≤ K i=1 E A E z∼µ ( (A s , z)|z ∈ C i ) |N i | n − 1 n j∈Ni (A s , s j ) + E A K i=1 E z∼µ ( (A s , z)|z ∈ C i )µ(C i ) − K i=1 E z∼µ ( (A s , z)|z ∈ C i ) |N i | n ≤ 1 n K i=1 j∈Ni E A max z∈Ci | (A s , s j ) − (A s , z)| + max z∈Z | (A s , z)| K i=1 |N i | n − µ(C i ) (4) (c) ≤¯ (n) + M K i=1 |N i | n − µ(C i ) (d) ≤¯ (n) + M 2K ln 2 + 2 ln(1/δ) n<label>(5)</label></formula><p>Here the inequalities (a) and (b) are due to triangle inequality, (c) is from the definition of ensemble robustness and the fact that the loss function is upper bounded by M , and (d) holds with a probability greater than 1 − δ.</p><p>Lemma 2. For a randomized learning algorithm A with (K,¯ (n)) uniform ensemble robustness, and loss function such that 0 ≤ (h, z) ≤ M , we have,</p><formula xml:id="formula_17">E s |L(h) − emp (h)| 2 ≤ M¯ (n) + 2M 2 n .</formula><p>Proof. Let N i be the set of index of points of s that fall into the C i . Note that (|N 1 |, . . . , |N K |) is an i.i.d. multinomial random variable with parameters n and (µ(</p><formula xml:id="formula_18">C 1 ), . . . , µ(C K ). Then E s |N k | = n • µ(C k ) for k = 1, . . . , K. E s |L(h) − emp (h)| 2 = E s E z∈Z (h, z) − 1 n n i=1 (h, s i ) 2 = E s K k=1 E z∈Z (h, z|z ∈ C k )µ(C k ) − 1 n n i=1 (h, s i ) 2 = K k=1 E z∈Z (h, z|z ∈ C k )µ(C k ) 2 + 1 n 2 E s n i=1 (h, s i ) 2 − 2 K k=1 E z∈Z (h, z|z ∈ C k )µ(C k ) E s 1 n n i=1 (h, s i ) ≤ K k=1 E z∈Z (h, z|z ∈ C k )µ(C k ) K k=1 E z∈Z (h, z|z ∈ C k )µ(C k ) − E s 1 n n i=1 (h, s i ) + 1 n 2 E s n i=1 (h, s i ) 2 − K k=1 E z∈Z (h, z|z ∈ C k )µ(C k ) E s 1 n n i=1 (h, s i ) ≤ M K k=1 E z∈Z (h, z|z ∈ C k )µ(C k ) − E s 1 n n i=1 (h, s i ) H + 2M 2 n</formula><p>We then bound the term H as follows.</p><formula xml:id="formula_19">H = K k=1 E z∈Z (h, z|z ∈ C k )E s N k n − E s 1 n n i=1 (h, s i ) ≤ 1 n E s K k=1 E z∈Z (h, z|z ∈ C k )N k − j∈C k (h, s j ) ≤ 1 n E s K k=1 N k max sj ∈C k ,z∈C k | (h, z) − (h, s j )| = 1 n K k=1 N k E s max sj ∈C k ,z∈C k | (h, z) − (h, s j )| ≤¯ (n).</formula><p>Then we have,</p><formula xml:id="formula_20">E s |L(h) − emp (h)| 2 ≤ M¯ (n) + 2M 2 n .</formula><p>To analyze the generalization performance of deep learning with dropout, following lemma is central.</p><p>Lemma 3 (Bounded difference inequality <ref type="bibr" target="#b11">(McDiarmid, 1989)</ref>). Let r = (r 1 , . . . , r L ) ∈ R be L independent random variables (r l can be vectors or scalars) with r l ∈ {0, 1} m l . Assume that the function f :</p><formula xml:id="formula_21">R L → R satisfies: sup r (l) , r (l) f (r (l) ) − f ( r (l) ) ≤ c l , ∀l = 1, . . . , L,</formula><p>whenever r (l) and r <ref type="bibr">(l)</ref> differ only in the l-th element. Here, c l is a nonnegative function of l. Then, for every &gt; 0,</p><formula xml:id="formula_22">P r {f (r 1 , . . . , r L ) − E r f (r 1 , . . . , r L ) ≥ } ≤ exp −2 2 / L l=1 c 2 l .</formula><p>9 Proof of Theorem 1</p><p>Proof of Theorem 1. Now we proceed to prove Theorem 1. Using Chebyshev's inequality, Lemma 2 leads to the following inequality:</p><formula xml:id="formula_23">Pr s {|L(h) − emp (h)| ≥ |h} ≤ nM E s max s∈s,z∼s | (h, s) − (h, z)| + 2M 2 n 2 .</formula><p>By integrating with respect to h, we can derive the following bound on the generalization error:</p><formula xml:id="formula_24">Pr s,A {|L(h) − emp (h)| ≥ } ≤ nM E A,s max s∈s,z∼s | (h, s) − (h, z)| + 2M 2 n .</formula><p>This is equivalent to:</p><p>|L(h) − emp (h)| ≤ nM¯ (n) + 2M 2 δn holds with a probability greater than 1 − δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Proof of Theorem 2</head><p>Proof. To simplify the notations, we use X(h) to denote the random variable max z∼s | (h, s) − (h, z)|. According to the definition of ensemble robustness, we have E A X(h) ≤ (n). Also, the assumption gives var[X(h)] ≤ α. According to Chebyshev's inequality, we have,</p><formula xml:id="formula_25">P X(h) ≤ (n) + α √ δ ≥ 1 − δ.</formula><p>Now, we proceed to bound |L(h) − emp (h)| for any h ∼ ∆(H) output by A s . Following the proof of Lemma 2, we also divide the set Z into K disjoint set C 1 , . . . , C K and let N i be the set of index of points in ∫ that fall into C i . Then we have, holds with probability at least 1 − δ. This gives the first inequality in the theorem. The second inequality can be straightforwardly derived from the fact that var(X</p><formula xml:id="formula_26">|L(h) − emp (h)| ≤ 1 n K i=1 j∈Ni max z∈Ci | (h,<label>s</label></formula><formula xml:id="formula_27">) = E[X 2 ] − (E[X]) 2 ≤ M E[X] − (E[X]) 2 .</formula><p>11 Proof of Theorem 3 holds with probability greater than 1 − δ. Observe that the above two inequalities hold simultaneously with probability at least 1 − 2δ. Combining those inequalities and setting δ = δ/2 gives R(s, r) ≤ β 2L log(1/δ) +¯ (n) + 2K ln 2 + 2 ln(2/δ) n .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Results for MNIST. Empirical ensemble robustness¯ emp (x-axis) vs generalization error (y-axis). Results are given for four different deep learning algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Results for notMNIST. Empirical ensemble robustness¯ emp (x-axis) vs generalization error (y-axis). Results are given for four different deep learning algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>at least 1 − 2δ. Let δ be 2δ, we have,|L(h) − emp (h)| ≤ (n) + α √ 2δ + 2K ln 2 + 2 ln(1/2δ) n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proof.</head><label></label><figDesc>Let R(s, r) = L(A s,r ) − emp (A s,r ) denote the random variable that we are going to bound. For every r, t ∈ R L , and L ∈ N, we have|R(s, r) − R(s, t)| = E z∈Z [ (A s,r , z) − (A s,t , z)] − 1 n n i=1 ( (A s,r , z i ) − (A s,t , z i )) ≤ E z∈Z | (A s,r , z) − (A s,t , z)| + 1 n n i=1 | (A s,r , z i ) − (A s,t , z i )| .According to the definition of β:sup r,t |R(s, r) − R(s, t)| ≤ 2β,and applying Lemma 3 we obtain (note that s is independent of r)P r {R(s, r) − E r R(s, r) ≥ |s} ≤ exp − 2 2Lβ 2 .We also haveE s P r {R(s, r) − E r R(s, r) ≥ } = E s P r {R(s, r) − E r R(s, r) ≥ |s} ≤ exp − 2 2Lβ 2 .Setting the r.h.s. equal to δ and writing as a function of δ, we have that with probability at least 1 − δ w.r.t. the random sampling of s and r:R(s, r) − E r R(s, r) ≤ β 2L log(1/δ).Then according to Lemma 1: E r R(s, r) ≤¯ (n) + 2K ln 2 + 2 ln(1/δ) n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Huan Xu, Constantine Caramanis, and Shie Mannor. Robustness and regularization of support vector machines. The Journal of Machine Learning Research, 10:1485-1510, 2009b. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. International Conference on Learning Representations, 2016.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">norm and is sampled uniformly over {0.1, 0.3, 0.5} to avoid sampling bias. From now on, a specific configuration will refer to a unique set of these parameters (algorithm type, network width, learning rate and perturbation norm).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzkebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<title level="m">Weight uncertainty in neural networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Insights and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning Workshop, ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Rigazio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5068</idno>
		<title level="m">Towards deep neural network architectures robust to adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01240</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">To drop or not to drop: Robustness, consistency and differential privacy properties of dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhradeep</forename><surname>Thakurta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Williams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02031</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the method of bounded differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Mcdiarmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="148" to="188" />
		</imprint>
	</monogr>
	<note>Surveys in combinatorics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2016 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Understanding adversarial training: Increasing local stability of neural nets through robust optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Negahban</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05432</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ensemble methods as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thilo</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hanselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Junginger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Ulmer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03423</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robustness and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="423" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust regression and lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Caramanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
