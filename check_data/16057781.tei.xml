<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Model Primitive Hierarchical Lifelong Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayesh</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykel</forename><forename type="middle">J</forename><surname>Kochenderfer</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Model Primitive Hierarchical Lifelong Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reinforcement learning</term>
					<term>Task decomposition</term>
					<term>Transfer</term>
					<term>Lifelong learning</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the lifelong learning setting, we want our agent to solve a series of related tasks drawn from some task distribution rather than a single, isolated task. Agents must be able to transfer knowledge gained in previous tasks to improve performance on future tasks. This setting is different from multi-task reinforcement learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref> and various meta-reinforcement learning settings <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, where the agent jointly trains on multiple task environments. Not only do such nonincremental settings make the problem of discovering common structures between tasks easier, they allow the methods to ignore the problem of catastrophic forgetting <ref type="bibr" target="#b15">[16]</ref>, which is the inability to solve previous tasks after learning to solve new tasks in a sequential learning setting.</p><p>Our work takes a step towards solutions for such incremental settings. We draw on the idea of modularity <ref type="bibr" target="#b16">[17]</ref>. While learning to perform a complex task, we force the agent to break its solution down into simpler subpolicies instead of learning a single monolithic policy. This decomposition allows our agent to rapidly learn another related task by transferring these subpolicies. We hypothesize that many complex tasks are heavily structured and hierarchical in nature. The likelihood of transfer of an agent's solution increases if it can capture such shared structure.</p><p>A key ingredient of our proposal is the idea of world models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref> -transition models that can predict future sensory data given the agent's current actions. The world however is complex, and learning models that are consistent enough to plan with is not only hard <ref type="bibr" target="#b23">[24]</ref>, but planning with such one-step models is suboptimal <ref type="bibr" target="#b10">[11]</ref>. We posit that the requirement that these world models be good predictors of the world state is unnecessary, provided we have a multiplicity of such models. We use the term model primitives to refer to these suboptimal world models. Since each model primitive is only relatively better at predicting the next states within a certain region of the environment space, we call this area the model primitive's region of specialization.</p><p>Model primitives allow the agent to decompose the task being performed into subtasks according to their regions of specialization and learn a specialized subpolicy for each subtask. The same model primitives are used to learn a gating controller to select, improve, adapt, and sequence the various subpolicies to solve a given task in a manner very similar to a mixture of experts framework <ref type="bibr" target="#b14">[15]</ref>.</p><p>Our framework assumes that at least a subset of model primitives are useful across a range of tasks and environments. This assumption is less restrictive than that of successor representations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. Even though successor representations decouple the state transitions from the rewards (representing the task or goals), the transitions learned are policy dependent and can only transfer across tasks with the same environment dynamics.</p><p>There are alternative approaches to learning hierarchical spatiotemporal decompositions from the rewards seen while interacting with the environment. These approaches include meta-learning algorithms like Meta-learning Shared Hierarchies (MLSH) <ref type="bibr" target="#b7">[8]</ref>, which require a multiplicity of pretrained subpolicies and joint training on related tasks. Other approaches include the option-critic architecture <ref type="bibr" target="#b0">[1]</ref> that allows learning such decompositions in a single task environment. However, this method requires regularization hyperparameters that are tricky to set. As observed by Vezhnevets et al. <ref type="bibr" target="#b29">[30]</ref>, its learning often collapses to a single subpolicy. Moreover, we posit that capturing the shared structure across task-environments can be more useful in the context of transfer for lifelong learning than reward-based task specific structures.</p><p>To summarize our contributions:</p><p>• Given diverse suboptimal world models, we propose a method to leverage them for task decomposition. • We propose an architecture to jointly train decomposed subpolicies and a gating controller to solve a given task. • We demonstrate the effectiveness of this approach at both single-task and lifelong learning in complex domains with high-dimensional observations and continuous actions. arXiv:1903.01567v1 [cs.</p><p>LG] 4 Mar 2019</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We assume the standard reinforcement learning (RL) formulation: an agent interacts with an environment to maximize the expected reward <ref type="bibr" target="#b22">[23]</ref>. The environment is modeled as a Markov decision process (MDP), which is defined by ⟨S, A, R, T , γ ⟩ with a state space S, an action space A, a reward function R : S × A → R, a dynamics model T : S×A → Π(S), and a discount factor γ ∈ [0, 1).</p><p>Here, Π(•) defines a probability distribution over a set. The agent acts according to stationary stochastic policies π : S → Π(A), which specify action choice probabilities for each state. Each policy π has a corresponding Q π : S × A → R function that defines the expected discounted cumulative reward for taking an action a from state s and following the policy π from that point onward. Lifelong Reinforcement Learning: In a lifelong learning setting, the agent must interact with multiple tasks and successfully solve each of them. Adopting the framework from Brunskill and Li <ref type="bibr" target="#b3">[4]</ref>, in lifelong RL, the agent receives S, A, initial state distribution ρ 0 ∈ Π(S), horizon H , discount factor γ , and an unknown distribution over reward-transition function pairs, D. The agent samples (R i , T i ) ∼ D and interacts with the MDP ⟨S, A, R i , T i , γ ⟩ for a maximum of H timesteps, starting according to the initial state distribution ρ 0 . After solving the given MDP or after H timesteps, whichever occurs first, the agent resamples from D and repeats.</p><p>The fundamental question in lifelong learning is to determine what knowledge should be captured by the agent from the tasks it has already solved so that it can improve its performance on future tasks. When learning with functional approximation, this translates to learning the right representation -the one with the right inductive bias for the tasks in the distribution. Given the assumption that the set of related tasks for lifelong learning share a lot of structure, the ideal representation should be able to capture this shared structure.</p><p>Thrun and Pratt <ref type="bibr" target="#b27">[28]</ref> summarized various representation decomposition methods into two major categories. Modern approaches to avoiding catastrophic forgetting during transfer tend to fall into either category. The first category partitions the parameter space into task-specific parameters and general parameters <ref type="bibr" target="#b18">[19]</ref>. The second category learns constraints that can be superimposed when learning a new function <ref type="bibr" target="#b12">[13]</ref>.</p><p>A popular approach within the first category is to use what Thrun and Pratt <ref type="bibr" target="#b27">[28]</ref> term as recursive functional decomposition. This approach assumes that solution to tasks can be decomposed into a function of the form f i = h i • д, where h i is task-specific whereas д is the same for all f i . This scheme has been particularly effective in computer vision where early convolutional layers in deep convolutional networks trained on ImageNet <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref> become a very effective д for a variety of tasks. However, this approach to decomposition often fails in DeepRL because of two main reasons. First, the gradients used to train such networks are noisier as a result of Monte Carlo sampling. Second, the i.i.d. assumption for training data often fails.</p><p>We instead focus on devising an effective piecewise functional decomposition of the parameter space, as defined by Thrun and Pratt <ref type="bibr" target="#b27">[28]</ref>. The assumption behind this decomposition is that each function f i can be represented by a collection of functions where m ≪ N , and N is the number of tasks to learn. Our hypothesis is that this decomposition is much more effective and easier to learn in RL.</p><formula xml:id="formula_0">h 1 , . . . , h m , π K π 3 π 2 π k T K T 3 T 2 T k × Gating controller Environment P (M k | s t ) a t s t , r t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL PRIMITIVE HIERARCHICAL RL</head><p>This section outlines the Model Primitive Hierarchical Reinforcement Learning (MPHRL) framework ( <ref type="figure" target="#fig_1">Figure 1</ref>) to address the problem of effective piecewise functional decomposition for transfer across a distribution of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Primitives and Gating</head><p>The key assumption in MPHRL is access to several diverse world models of the environment dynamics. These models can be seen as instances of learned approximations to the true environment dynamics T . In reality, these dynamics can even be non-stationary. Therefore, the task of learning a complete model of the environment dynamics might be too difficult. Instead, it can be much easier to train multiple approximate models that specialize in different parts of the environment. We use the term model primitives to refer to these approximate world models.</p><p>Suppose we have access to K model primitives:T k : S × A → Π(S). For simplicity, we can assign a label M k to eachT k , such that their predictions of the environment's transition probabilities can be denoted byT</p><formula xml:id="formula_1">(s t +1 | s t , a t , M k ).</formula><p>3.1.1 Subpolicies. The goal of the MPHRL framework is to use these suboptimal predictions from different model primitives to decompose the task space into their regions of specialization, and learn different subpolicies π k : S → Π(A) that can focus on these regions. In the function approximation regime, each subpolicy π k belongs to a fixed class of smoothly parameterized stochastic policies {π θ k | θ k ∈ Θ}, where Θ is a set of valid parameter vectors.</p><p>Model primitives are suboptimal and make incorrect predictions about the next state. Therefore we do not use them for planning or model-based learning of subpolicies directly. Instead, model primitives give rise to useful functional decompositions and allow subpolicies to be learned in a model-free way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Gating Controller.</head><p>Taking inspiration from the mixture-ofexperts literature <ref type="bibr" target="#b14">[15]</ref>, where the output from multiple experts can be combined using probabilistic gating functions, MPHRL decomposes the solution for a given task into multiple "expert" subpolicies and a gating controller that can compose them to solve the task. We want this switching behavior to be probabilistic and continuous to avoid abrupt transitions. During learning, we want this controller to help assign the reward signal to the correct blend of subpolicies to ensure effective learning as well as decomposition.</p><p>Since the gating controller's goal is to choose the subpolicy whose corresponding model primitive makes the best prediction for a given transition, using Bayes' rule we can write:</p><formula xml:id="formula_2">P(M k | s t , a t , s t +1 ) ∝ P(M k | s t )π k (a t | s t )T (s t +1 | s t , a t , M k ) (1) because π k (a t | s t ) = π (a t | s t , M k ).</formula><p>The agent only has access to the current state s t during execution. Therefore, the agent needs to marginalize out s t +1 and a t such that the model choice only depends on the current state s t :</p><formula xml:id="formula_3">P(M k | s t ) = ∫ s t +1 ∈S ∫ a t ∈A P(M k | s t , a t , s t +1 ) P(s t +1 , a t )da t ds t +1 (2)</formula><p>This is equivalent to:</p><formula xml:id="formula_4">P(M k | s t ) = E s t +1 ,a t ∼P (s t +1 ,a t ) [P(M k | s t , a t , s t +1 )]<label>(3)</label></formula><p>Unfortunately, computing these integrals requires expensive Monte Carlo methods. However, we can use an approximate method to achieve the same objective with discriminative learning <ref type="bibr" target="#b17">[18]</ref>.</p><p>We parameterize the gating controller (GC) as a categorical dis-</p><formula xml:id="formula_5">tribution P ϕ (M k | s t ) = P(M k | s t ; ϕ) and minimize the conditional cross entropy loss between E s t +1 ,a t ∼P (s t +1 ,a t ) [P(M k | s t , a t , s t +1 )] and P ϕ (M k | s t ) for all sampled transitions (s t , a t , s t +1 ) in a rollout: minimize ϕ L GC<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">L GC = s t k − s t +1 a t P(M k | s t , a t , s t +1 ) × log P(M k | s t ; ϕ) (5)</formula><p>This is equivalent to an implicit Monte Carlo integration to compute the marginal if s t +1 , a t ∼ P(s t +1 , a t ). Although we cannot query or sample from P(s t +1 , a t ) directly, s t , a t , and s t +1 can be sampled according to their respective distributions while we perform rollouts in the environment. Despite the introduced bias in our estimates, we find Eq. 4 sufficient for achieving task decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Subpolicy Composition.</head><p>Taking inspiration from mixtureof-experts, the gating controller composes the subpolicies into a mixture policy:</p><formula xml:id="formula_7">π (a t | s t ) = K k =1 P ϕ (M k | s t )π k (a t | s t )<label>(6)</label></formula><p>3.1.4 Decoupling Cross Entropy from Action Distribution. During a rollout, the agent samples as follows:</p><formula xml:id="formula_8">a t ∼ π (a t | s t )<label>(7)</label></formula><formula xml:id="formula_9">s t +1 ∼ T (s t +1 | s t , a t )<label>(8)</label></formula><p>The π k from Eq. 1 gets coupled with this sampling distribution, making the target distribution in Eq. 5 no longer stationary and the approximation process difficult. We alleviate this issue by ignoring π k , effectively treating it as a distribution independent of k. This transforms Eq. 1 into:</p><formula xml:id="formula_10">P(M k | s t , a t , s t +1 ) ∝ P(M k | s t )T (s t +1 | s t , a t , M k )<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning</head><p>Since the focus of this work is on difficult continuous action problems, we mostly concentrate on the issue of policy optimization and how it integrates with the gating controller. The standard policy (SP) optimization objective is:</p><formula xml:id="formula_11">maximize θ L S P = E ρ 0 , π θ [π θ (a t | s t )Q π θ (s t , a t )]<label>(10)</label></formula><p>With baseline subtraction for variance reduction, this turns into <ref type="bibr" target="#b19">[20]</ref>:</p><formula xml:id="formula_12">maximize θ L PG = E ρ 0 , π θ [π θ (a t | s t )Â t ]<label>(11)</label></formula><p>whereÂ t is an estimator of the advantage function <ref type="bibr" target="#b1">[2]</ref>.</p><p>In MPHRL, we directly use the mixture policy as defined by Eq. 6. The standard policy gradients (PG) get weighted by the probability outputs of the gating controller, enforcing the required specialization by factorizing into:</p><formula xml:id="formula_13">д k = E ρ 0 , π θ k P ϕ (M k | s t )∇ θ k log π θ k (a t | s t )Â t<label>(12)</label></formula><p>In practice, we use the Clipped PPO objective <ref type="bibr" target="#b20">[21]</ref> instead to perform stable updates by limiting the step size. This includes adding a baseline estimator (BL) parameterized by ψ for value prediction and variance reduction. We optimize ψ according to the following loss:</p><formula xml:id="formula_14">L BL = E V ψ − V π θ 2 (13)</formula><p>We summarize this single-task learning algorithm in Algorithm 1, which results in a set of decomposed subpolicies, π θ 1 , . . . π θ K , and a gating controller P ϕ that can modulate between them to solve the task under consideration. Algorithm 1 MPHRL: single-task learning</p><formula xml:id="formula_15">1: Initialize P ϕ , π θ = {π θ 1 , . . . , π θ K }, V ψ 2: while not converged do 3: Rollout trajectories τ ∼ π θ,ϕ 4: Compute advantage estimatesÂ τ 5: Optimize L PG wrt θ 1 , . . . , θ K</formula><p>with expectations taken over τ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Optimize L BL wrt ψ with expectations taken over τ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Optimize L GC wrt ϕ with expectations taken over τ</p><p>Lifelong learning: We have shown how MPHRL can decompose a single complex task solution into different functional components. Complex tasks often share structure and can be decomposed into similar sets of subtasks. Different tasks however require different recomposition of similar subtasks. Therefore, we transfer the subpolicies to learn target tasks, but not the gating controller or the baseline estimator. We summarize the lifelong learning algorithm in Algorithm 2, with the global variable RESET set to true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 MPHRL: lifelong learning</head><formula xml:id="formula_16">1: Initialize P ϕ , π θ = {π θ 1 , . . . , π θ K }, V ψ 2: for Tasks (R i , T i ) ∼ D do 3:</formula><p>if RESET then 4:</p><formula xml:id="formula_17">Initialize P ϕ , V ψ 5:</formula><p>while not converged do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Rollout trajectories τ ∼ π θ,ϕ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Compute advantage estimatesÂ τ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Optimize L PG wrt θ 1 , . . . , θ K with expectations taken over τ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Optimize L BL wrt ψ with expectations taken over τ 10:</p><p>Optimize L GC wrt ϕ with expectations taken over τ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIMENTS</head><p>Our experiments aim to answer two questions: (a) can model primitives ensure task decomposition? (b) does such decomposition improve transfer for lifelong learning?</p><p>We evaluate our approach in two challenging domains: a Mu-JoCo <ref type="bibr" target="#b28">[29]</ref> ant navigating different mazes and a Stacker <ref type="bibr" target="#b25">[26]</ref> arm picking up and placing different boxes. In our experiments, we use subpolicies that have Gaussian action distributions, with mean given by a multi-layer perceptron taking observations as input and standard deviations given by a different set of parameters. MPHRL's gating controller outputs a categorical distribution and is parameterized by another multi-layer perceptron. We also use a separate multi-layer perceptron for the baseline estimator. We use the standard PPO algorithm as a baseline to compare against MPHRL. Transferring network weights empirically led to worse performance for standard PPO. Hence, we re-initialize its weights for every task. For fair comparison, we also shrink the hidden layer size of MPHRL's subpolicy networks from 64 to 16. We conduct each experiment across 5 different seeds. Error bars represent the standard deviation from the mean.</p><p>The focus of this work is on understanding the usefulness of model primitives for task decomposition and the resulting improvement in sample efficiency from transfer. To conduct controlled experiments with interpretable results, we hand-designed model primitives using the true next state provided by the environment simulator. Concretely, we apply distinct multivariate Gaussian noise models with covariance σ Σ to the true next state. We then sample from this distribution to obtain the mean of the probability distribution of a model primitive's next state prediction, using Σ as its covariance. Here, σ is the noise scaling factor that distinguishes model primitives, while Σ refers to the empirical covariance of the sampled next states:</p><formula xml:id="formula_18">µ ∼ N (s t +1 , σ k Σ)<label>(14)</label></formula><p>T</p><formula xml:id="formula_19">(s t +1 | s t , a t , M k ) = N (µ, σ k Σ)<label>(15)</label></formula><p>Using Σ as opposed to a constant covariance is essential for controlled experiments because different elements of the observation space have different orders of magnitude. Sampling µ from a distribution effectively adds random bias to the model primitive's next state probability distribution.</p><p>Hyperparameter details are in <ref type="table" target="#tab_0">Table 1</ref>, and our code is freely available at http://github.com/sisl/MPHRL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Single-task Learning</head><p>First, we focus on two single-task learning experiments where MPHRL learns a number of interpretable subpolicies to solve a single task. Both the L-Maze and D-Maze <ref type="figure" target="#fig_3">(Figure 2a</ref>) tasks require the ant to learn to walk and reach the green goal within a finite  horizon. For both tasks, both the goal and the initial ant locations are fixed. For the L-Maze, the agent has access to two model primitives, one specializing in the horizontal (E, W) corridor and the other specializing in the vertical (N, S) corridor of the maze. Similarly for the D-Maze, the agent has access to four model primitives, one specializing in each N, S, E, W corridor of the maze. In their specialized corridors, the noise scaling factor σ = 0. Outside of their regions of specialization, σ = 0.5. The observation space includes the standard joint angles and positions, lidar information tracking distances from walls on each side, and the Manhattan distance to the goal. <ref type="figure" target="#fig_3">Figure 2b</ref> shows the experimental results on these environments. Notice that using model primitives can make the learning problem more difficult and increase the sample complexity on a single task. This is expected, since we are forcing the agent to decompose the solution, which could be unnecessary for easy tasks. However, we will observe in the following section that this decomposition can lead to remarkable improvements in transfer performance during lifelong learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lifelong Learning</head><p>To evaluate our framework's performance at lifelong learning, we introduce two tasksets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">10-Maze.</head><p>To evaluate MPHRL's performance in lifelong learning, we generate a family of 10 random mazes for the MuJoCo Ant environment, referred to as the 10-Maze taskset <ref type="figure">(Figure 4</ref>) hereafter. The goal, the observation space, the Gaussian noise models, and the model primitives remain the same as in D-Maze. The agent has a maximum of 3 × 10 7 timesteps to reach 80% success rate in each of the 10 tasks. As shown in <ref type="figure" target="#fig_4">Figure 3a</ref>, MPHRL requires nearly double the number of timesteps to learn the decomposed subpolicies in the first task. However, this cost gets heavily amortized over the entire taskset, with MPHRL taking half the total number of timesteps of the baseline PPO, exhibiting strong subpolicy transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">8-Pickup&amp;Place</head><p>. We modify the Stacker task <ref type="bibr" target="#b25">[26]</ref> to create the 8-Pickup&amp;Place taskset. As shown in <ref type="figure" target="#fig_7">Figure 5</ref>, a robotic arm is tasked to bring 2 boxes to their respective goal locations in a certain order. Marked by colors red, green, and blue, the goal locations reside within two short walls forming a "stack".</p><p>Each of the 8 tasks has a maximum of 3 goal locations. The observation space of the agent includes joint angles and positions, box and goal locations, their relative distances to each other, and the current stage of the task encoded as one-hot vectors. The agent has access to six model primitives for each box that specialize in reaching above, lowering to, grasping, picking up, carrying, and dropping a certain box. Similar to 10-Maze, model primitives have σ of 0 within their specialized stages and σ of 0.5 otherwise. <ref type="figure" target="#fig_4">Figure 3b</ref> shows MPHRL's experimental performance by learning twelve useful subpolicies for this taskset. We notice again the strong transfer performance due to the decomposition forced by the model primitives. Note that this taskset is much more complex than 10-Maze such that MPHRL even accelerates the learning of the first task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation</head><p>We conduct ablation experiments to answer the following questions:</p><p>(1) How much gain in sample efficiency is achieved by transferring subpolicies? (2) Can MPHRL learn the task decomposition even when the model primitives are quite noisy or when the source task does not cover all "cases"? (3) When does MPHRL fail to decompose the solution? (4) What kind of diversity in the model primitives is essential for performance? (5) When does MPHRL lead to negative transfer? (6) Is MPHRL's gain in sample efficiency a result of hand-crafted model primitives and how does it perform with actual learned model primitives?</p><p>4.3.1 Model Noise. MPHRL has the ability to decompose the solution even given bad model primitives. Since the learning is done model-free, these suboptimal model primitives should not strongly affect the learning performance so long as they remain sufficiently distinct. To investigate the limitations to this claim, we conduct five experiments using various sets of noisy model primitives. Below, the first value corresponds to the noise scaling factor σ within their individual regions of specialization, while the second value corresponds to σ outside of their regions of specialization.     0±4.6 million timesteps to solve the first task and 2.8 ± 1.6 million timesteps to solve the second task, but failed to solve the third task within 30 million timesteps. This is because the model primitives are identical and provide no information about task decomposition. In summary, MPHRL is robust against bad model primitives so long as the they maintain some relative distinction. Similar observations hold true for the 8-Pickup&amp;Place taskset where noise models with distinctive models with large noise of σ = 5 and σ = 20 show little deterioration in performance, taking 15.8 ± 5.5 million timesteps to reach 75% average success rate.</p><formula xml:id="formula_20">(a) B1 → T1 B2 → T2 (b) B2 → T1 B1 → T2 B1 → T3 (c) B2 → T1 B2 → T2 (d) B1 → T1 B1 → T2 B2 → T3 (e) B1 → T1 B2 → T2 B2 → T3 (f) B1 → T1 B1 → T2 B1 → T3 (g) B2 → T1 B1 → T2 (h) B2 → T1 B1 → T2 B1 → T3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Overlapping Model Primitives.</head><p>We next test the condition when there is substantial overlap in regions of specialization between different model primitives. For the 10-Maze taskset, the most plausible region for this confusion is at the corners. In this experiment, within each corner, the two model primitives whose specialized corridors share the corner have σ = 0 while the other two have σ = 0.5. <ref type="figure">Figure 6b</ref> shows the performance for model primitive confusion against the standard set of model primitives with no confusion. We observe that despite some performance degradation, MPHRL continues to outperform the PPO baseline.  <ref type="table" target="#tab_1">Table 2</ref> shows MPHRL is susceptible to performance degradation given undesirable sets of model primitives. However, MPHRL still outperforms baseline PPO when given an extra, undesirable model primitive. This indicates that for best transfer, the model primitives need to approximately capture the structure present in the taskset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Negative Transfer and Catastrophic Forgetting.</head><p>Lifelong learning agents with neural network function approximators face the problem of negative transfer and catastrophic forgetting. Ideally, they should find the solution quickly if the task has already been seen. More generally, given two sets of tasks T and T ′ such that T ⊂ T ′ , after being exposed to T ′ the agent should perform no worse, and preferably better, than had it been exposed to T only.</p><p>In this experiment, we restore the subpolicy checkpoints after solving the 10 tasks and evaluate MPHRL's learning performance for the first 9 tasks. Similarly, we restore the subpolicy checkpoints after solving 6 tasks and evaluate MPHRL's performance on the    first 5 tasks. The gating controller is reset for each task as in earlier experiments. We summarize the results in <ref type="table" target="#tab_2">Table 3</ref>. Subpolicies trained sequentially on 6 or 10 tasks quickly relearn the required behavior for all previously seen tasks, implying no catastrophic forgetting. Moreover, if we compare the 10-task result to the 6-task result, we see remarkable improvements at transfer. This implies negative transfer is limited with this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Oracle Gating</head><p>Controller. One might suspect that all gains in sample efficiency come from hand-crafted model primitives because they allow the agent to learn a perfect gating controller. However, <ref type="figure" target="#fig_10">Figure 7</ref> shows the reward curves for an experiment where the gating controller is already perfectly known. This setup is unable to <ref type="figure">Figure 9</ref>: 10-Maze-v2 lifelong learning taskset <ref type="figure" target="#fig_1">Figure 10</ref>: Three corridor environments for learning the "N" model primitive learn any 10-Maze task. Since the 10-Maze taskset is composed of sequential subtasks, only one subpolicy will be learned in the first corridor when the gating controller is perfect. When transitioning to the second corridor, the second policy needs to be learned from scratch, making the ant's survival rate very low. This discourages the first subpolicy from entering the second corridor and activating the second subpolicy. Eventually, the ant stops moving forward close to the intersection between the first two corridors. In contrast, MPHRL's natural curriculum for gradual specialization allows multiple subpolicies to learn the basic skills for survival initially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6">Partial Decomposition.</head><p>To confirm that the ordering of tasks does not significantly affect MPHRL's performance, we modified 10-Maze to create the 10-Maze-v2 taskset <ref type="figure">(Figure 9)</ref>, in which the source task does not allow for complete decomposition into all useful subpolicies for the subsequent tasks. Again, we observe large improvement in sample efficiency over standard PPO <ref type="figure" target="#fig_12">(Figure 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.7">Learned Model</head><p>Primitives. This paper focuses on evaluating suboptimal models for task decomposition in controlled experiments using hand-designed model primitives. Here, we show one way to obtain each model primitive for 10-Maze-v2 using three corridor environments demonstrated in <ref type="figure" target="#fig_1">Figure 10</ref>. Concretely, we parameterize each model primitive using a multivariate Gaussian distribution. We learn the mean of this distribution via a multilayer perceptron using a weighted mean square error in dynamics prediction as the loss. The standard deviation is still derived from the empirical covariance Σ as described earlier. Even though the diversity in these learned model primitives is much more difficult to quantify and control, their sample efficiency substantially outperforms standard PPO and slightly underperforms hand-designed model primitives with 0 and 0.5 model noises <ref type="figure" target="#fig_12">(Figure 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.8">Gating Controller</head><p>Transfer. To explore factors that lead to negative transfer, we tested MPHRL without re-initializing the gating controller in target tasks, as shown in <ref type="figure">Figure 6c</ref>. Although the mean sample efficiency remains stable, its standard deviation increases dramatically, indicating volatility due to negative transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.9">Subpolicy</head><p>Transfer. To measure how much gain in sample efficiency MPHRL has achieved by transferring subpolicies alone, we conducted a 10-Maze experiment by re-initializing all network weights for every new task. As shown in <ref type="figure">Figure 6c</ref>, sample complexity more than quintuples when subpolicies are re-initialized (in green).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.10">Coupling between Cross Entropy and Action Distribution.</head><p>To validate usingP(M k | s t , a t , s t +1 ) in Eq. 9 as opposed to P(M k | s t , a t , s t +1 ) from Eq. 1, we tested MPHRL with Eq. 1 on 10-Maze. All runs with different seeds failed to solve the first 5 tasks <ref type="table" target="#tab_3">(Table 4</ref>). As the gating controller is re-initialized during transfer, most actions were chosen incorrectly. The gating controller is thus presented with the incorrect cross entropy target, which worsens the action distribution. The resulting vicious cycle forces the gating controller to converge to a suboptimal equilibrium against the incorrect target. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS</head><p>We showed how imperfect world models can be used to decompose a complex task into simpler ones. We introduced a framework that uses these model primitives to learn piecewise functional decompositions of solutions to complex tasks. The learned decomposed subpolicies can then be used to transfer to a variety of related tasks, reducing the overall sample complexity required to learn complex behaviors. Our experiments showed that such structured decomposition avoids negative transfer and catastrophic interference, a major concern for lifelong learning systems. Our approach does not require access to accurate world models. Neither does it need a well-designed task distribution or the incremental introduction of individual tasks. So long as the set of model primitives are useful across the task distribution, MPHRL is robust to other imperfections.</p><p>Nevertheless, learning useful and diverse model primitives, subpolicies and task decomposition all simultaneously is left for future work. The recently introduced Neural Processes [9] can potentially be an efficient approach to build upon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), N. Agmon, M. E. Taylor, E. Elkind, M. Veloso (eds.), May 13-17, 2019, Montreal, Canada. © 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Diagram of MPHRL Architecture. Solid arrows are active during both learning and execution. Dotted arrows are active only during learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Single-task learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>MPHRL vs. PPO for lifelong learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 : 10 -</head><label>410</label><figDesc>(a) 0.4 and 0.5: good models with limited distinction (b) 0.5 and 1.0: good models with reasonable distinction Maze lifelong learning taskset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 : 8 - 6 )Figure 6 :</head><label>5866</label><figDesc>Pickup&amp;Place lifelong learning taskset. B1 and B2 refer to Box1 (black) and Box2 (white); T1, T2, and T3 refer to Target 1 (red), Target 2 (green), and Target 3 (blue) Gating Controller and Subpolicy Transfer Subpolicy Transfer Only No Gating Controller or Subpolicy Transfer (c) Effect of gating controller and subpolicy transfer (target tasks only) 10-Maze: MPHRL ablation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(c) 5 .</head><label>5</label><figDesc>0 and 10.0: bad models with reasonable distinction (d) 9.0 and 10.0: bad models with limited distinction (e) 0.5 and 0.5: good models with no distinction Shown in Figure 6a, while (a), (b), (c), and (d) exhibit limited degradation in performance, (d) experiences the most performance degradation on average. On the other hand, in (e) MPHRL took 22.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4. 3 . 3</head><label>33</label><figDesc>Model Diversity. Having tested MPHRL against noises, we experimented with undesirable model primitives for 10-Maze: (a) Extra: a fifth model primitive that specializes in states where the ant is moving horizontally; (b) H-V corridors: 2 model primitives specializing in horizontal (E, W) and vertical (N, S) corridors respectively; (c) Velocity: 2 model primitives specializing in states where the ant is moving horizontally or vertically; and for 8-Pickup&amp;Place: (a) Box-only: 2 model primitives for all actions on 2 boxes; (b) Action-only: 6 model primitives for 6 actions performed on boxes: reach above, lower to, grasp, pick up, carry, and drop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Average rewards of MPHRL when using an oracle gating controller. The reward threshold for reaching 80% success rate for the first task is approximately 800.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>10-Maze-v2: partial decomposition and learned model primitives. Success threshold is at 70%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters: MPHRL and baseline PPO</figDesc><table><row><cell>Category</cell><cell>Hyper-parameter</cell><cell>Value</cell></row><row><cell cols="2">Num. model primitives: L-Maze</cell><cell>2</cell></row><row><cell>Maze</cell><cell>D-Maze</cell><cell>4</cell></row><row><cell></cell><cell>Standard 10-Maze</cell><cell>4</cell></row><row><cell></cell><cell>H-V Corridors</cell><cell>2</cell></row><row><cell></cell><cell>Velocity</cell><cell>2</cell></row><row><cell></cell><cell>Extra</cell><cell>5</cell></row><row><cell cols="2">Num. model primitives: Standard 8-P&amp;P</cell><cell>12</cell></row><row><cell>8-P&amp;P 1</cell><cell>Box-only</cell><cell>2</cell></row><row><cell></cell><cell>Action-only</cell><cell>6</cell></row><row><cell>Gating controller:</cell><cell>Hidden layers</cell><cell>2</cell></row><row><cell>Network</cell><cell>Hidden dimension</cell><cell>64</cell></row><row><cell>Gating controller:</cell><cell>Single / Source 2 (Maze)</cell><cell>1 × 10 −3</cell></row><row><cell>Base learning rate</cell><cell>Single / Source (8-P&amp;P)</cell><cell>3 × 10 −2</cell></row><row><cell></cell><cell>Target tasks</cell><cell>3 × 10 −3</cell></row><row><cell>Gating controller:</cell><cell>Single / Source</cell><cell>1</cell></row><row><cell>Num. epoches / batch</cell><cell>Target tasks</cell><cell>10</cell></row><row><cell>Baseline and model</cell><cell>Hidden layers</cell><cell>2</cell></row><row><cell>primitive networks 3</cell><cell>Hidden dimension</cell><cell>64</cell></row><row><cell></cell><cell>Base learning rate</cell><cell>3 × 10 −4</cell></row><row><cell></cell><cell>Hidden layers</cell><cell>2</cell></row><row><cell>Subpolicy networks 4</cell><cell>Hidden dimension (MPHRL) Hidden dimension (PPO)</cell><cell>16 64</cell></row><row><cell></cell><cell>Base learning rate</cell><cell>3 × 10 −4</cell></row><row><cell></cell><cell>Num. actors (Maze)</cell><cell>16</cell></row><row><cell></cell><cell>Num. actors (8-P&amp;P)</cell><cell>24</cell></row><row><cell></cell><cell>Batch size / actor (Maze)</cell><cell>2048</cell></row><row><cell></cell><cell>Batch size / actor (8-P&amp;P)</cell><cell>1536</cell></row><row><cell></cell><cell>Max. timesteps / task</cell><cell>3 × 10 7</cell></row><row><cell></cell><cell>Minibatch size / actor</cell><cell>256</cell></row><row><cell>Optimization</cell><cell>Num. epoches / batch 5</cell><cell>10</cell></row><row><cell></cell><cell>Discount (γ )</cell><cell>0.99</cell></row><row><cell></cell><cell>GAE parameter (λ)</cell><cell>0.95</cell></row><row><cell></cell><cell>PPO clipping coeff. (ϵ)</cell><cell>0.2</cell></row><row><cell></cell><cell>Gradient clipping</cell><cell>None</cell></row><row><cell></cell><cell>VF coeff. (c 1 )</cell><cell>1.0</cell></row><row><cell></cell><cell>Entropy coeff. (c 2 )</cell><cell>0</cell></row><row><cell></cell><cell>Optimizer</cell><cell>Adam</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effect of suboptimal model primitive types (N/A indicates failure to solve the task within 3 × 10 7 timesteps)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="8">Timesteps to reach target average success rate of 80% (10-Maze) and 75% (8-Pickup&amp;Place) (×10 6 )</cell><cell></cell><cell></cell></row><row><cell>Taskset</cell><cell>Model Primitives</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>Total</cell></row><row><cell>10-Maze</cell><cell>Extra</cell><cell>21.8±1.5</cell><cell>4.3±0.8</cell><cell>3.5±0.9</cell><cell>5.2±1.2</cell><cell>0.7±0.5</cell><cell>3.6±1.8</cell><cell>1.2±1.1</cell><cell>1.5±0.9</cell><cell>3.1±0.7</cell><cell cols="2">5.8±3.3 50.7±4.3</cell></row><row><cell>10-Maze</cell><cell>H-V Corridors</cell><cell>28.1±4.2</cell><cell>2.4±0.6</cell><cell>18.9±11.0</cell><cell>28.8±2.8</cell><cell>N/A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10-Maze</cell><cell>Velocity</cell><cell>14.7±2.4</cell><cell>1.4±1.2</cell><cell>17.9±11.2</cell><cell>20.5±13.2</cell><cell>N/A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8-Pickup&amp;Place</cell><cell>Action-only</cell><cell>5.1±0.6</cell><cell>4.8±1.8</cell><cell>16.8±12.6</cell><cell>26.9±6.9</cell><cell>N/A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>8-Pickup&amp;Place</cell><cell>Box-only</cell><cell>18.6±10.9</cell><cell>N/A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">: 10-Maze: effect of experience</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Timesteps to reach 80% average success rate (×10 6 )</cell><cell></cell><cell></cell></row><row><cell>Experience</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row><row><cell>10 tasks</cell><cell>0.6±0.1</cell><cell>0.4±0.0</cell><cell>0.6±0.0</cell><cell>0.5±0.1</cell><cell>0.4±0.0</cell><cell>2.3±0.7</cell><cell>0.7±0.2</cell><cell>2.6±0.3</cell><cell>0.7±0.1</cell></row><row><cell>6 tasks</cell><cell>3.1±0.4</cell><cell>2.0±0.2</cell><cell>3.5±0.7</cell><cell>2.2±0.7</cell><cell>1.8±0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>10-Maze: effect of coupling between cross entropy and action distribution</figDesc><table><row><cell></cell><cell cols="4">Timesteps to reach 80% average success rate (×10 6 )</cell><cell></cell></row><row><cell>Task</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell>Timesteps (×10 6 )</cell><cell>15.5±1.2</cell><cell>3.4±1.1</cell><cell>19.3±8.0</cell><cell cols="2">28.9±2.5 N/A</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Single task refers to L-Maze and D-Maze; source and target tasks refer to the first task and all subsequent tasks in a lifelong learning taskset, respectively.<ref type="bibr" target="#b2">3</ref> Baseline network hyperparameters apply to both MPHRL and baseline PPO; model primitive networks are for experiments with learned model primitives only.<ref type="bibr" target="#b3">4</ref> The baseline PPO has no subpolicies, so the subpolicy network is the policy network.<ref type="bibr" target="#b4">5</ref> Baseline and subpolicy networks only.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We are thankful to Kunal Menda and everyone at SISL for useful comments and suggestions. This work is supported in part by DARPA under agreement number D17AP00032. The content is solely the responsibility of the authors and does not necessarily represent the official views of DARPA. We are also grateful for the support from Google Cloud in scaling our experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Option-critic Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1726" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reinforcement Learning in Continuous Time: Advantage Updating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L C</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Neural Networks (ICNN)</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2448" to="2453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Successor Features for Transfer in Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hado P Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4055" to="4065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PAC-inspired Option Discovery in Lifelong Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="316" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving Generalization for Temporal Difference Learning: The Successor Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="613" to="624" />
			<date type="published" when="1993-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A Large-scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meta Learning Shared Hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SyX0IeWAW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Danilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S M Ali</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno>arXiv:cs.LG/1807.01622</idno>
		<title level="m">Neural Processes. CoRR</title>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>arXiv:cs.AI/1803.10122</idno>
		<ptr target="https://worldmodels.github.io" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zacharias</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bowling</surname></persName>
		</author>
		<idno>arXiv:cs.AI/1806.01825</idno>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sensorimotor Mismatch Signals in Primary Visual Cortex of the Behaving Mouse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><forename type="middle">B</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Bonhoeffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hãĳbener</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2012.03.040</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2012.03.040" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="809" to="815" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overcoming Catastrophic Forgetting in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Leinweber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">M</forename><surname>Sobczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Attinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><forename type="middle">B</forename><surname>Keller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2017.08.036</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2017.08.036" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="1420" to="1432" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mixture of Experts: A Literature Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Masoudnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of Learning and Motivation</title>
		<editor>Gordon H Bower</editor>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Modular Policies for Robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
	<note>Alexandros Paraschos, Andras Kupcsik</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Return of the Gating Network: Combining Generative Models and Discriminative Training in Natural Image Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2683" to="2691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
		<idno>arXiv:cs.LG/1606.04671</idno>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">High-dimensional Continuous Control Using Generalized Advantage Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>arXiv:cs.LG/1506.02438</idno>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Proximal Policy Optimization Algorithms. CoRR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Revisiting Unreasonable Effectiveness of Data in Deep Learning Era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-Correcting Models for Model-Based Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Talvitie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multitask Reinforcement Learning on the Distribution of MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumihide</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Yamamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Computational Intelligence in Robotics and Automation</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1108" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">DeepMind Control Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00690</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distral: Robust Multitask Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4496" to="4506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning to Learn: Introduction and Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4615-5529-2_1</idno>
		<ptr target="https://doi.org/10.1007/978-1-4615-5529-2_1" />
		<editor>Learning to Learn, Sebastian Thrun and Lorien Pratt</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="17" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MuJoCo: A Physics Engine for Model-based Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2012.6386109</idno>
		<ptr target="https://doi.org/10.1109/IROS.2012.6386109" />
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="5026" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">FeUdal Networks for Hierarchical Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3540" to="3549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-task Reinforcement Learning: A Hierarchical Bayesian Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
