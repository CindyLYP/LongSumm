<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Firefly Algorithms for Multimodal Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-03-07">7 Mar 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-She</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<addrLine>Trumpington Street</addrLine>
									<postCode>CB2 1PZ</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Firefly Algorithms for Multimodal Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-03-07">7 Mar 2010</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1003.1466v1[math.OC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Nature-inspired algorithms are among the most powerful algorithms for optimization. This paper intends to provide a detailed description of a new Firefly Algorithm (FA) for multimodal optimization applications. We will compare the proposed firefly algorithm with other metaheuristic algorithms such as particle swarm optimization (PSO). Simulations and results indicate that the proposed firefly algorithm is superior to existing metaheuristic algorithms. Finally we will discuss its applications and implications for further research.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Biologically inspired algorithms are becoming powerful in modern numerical optimization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, especially for the NP-hard problems such as the travelling salesman problem. Among these biology-derived algorithms, the multi-agent metaheuristic algorithms such as particle swarm optimization form hot research topics in the start-of-the-art algorithm development in optimization and other applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Particle swarm optimization (PSO) was developed by Kennedy and Eberhart in 1995 <ref type="bibr" target="#b4">[5]</ref>, based on the swarm behaviour such as fish and bird schooling in nature, the so-called swarm intelligence. Though particle swarm optimization has many similarities with genetic algorithms, but it is much simpler because it does not use mutation/crossover operators. Instead, it uses the real-number randomness and the global communication among the swarming particles. In this sense, it is also easier to implement as it uses mainly real numbers.</p><p>This paper aims to introduce the new Firefly Algorithm and to provide the comparison study of the FA with PSO and other relevant algorithms. We will first outline the particle swarm optimization, then formulate the firefly algorithms and finally give the comparison about the performance of these algorithms. The FA optimization seems more promising than particle swarm optimization in the sense that FA can deal with multimodal functions more naturally and efficiently. In addition, particle swarm optimization is just a special class of the firefly algorithms as we will demonstrate this in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Particle Swarm Optimization 2.1 Standard PSO</head><p>The PSO algorithm searches the space of the objective functions by adjusting the trajectories of individual agents, called particles, as the piecewise paths formed by positional vectors in a quasi-stochastic manner <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. There are now as many as about 20 different variants of PSO. Here we only describe the simplest and yet popular standard PSO.</p><p>The particle movement has two major components: a stochastic component and a deterministic component. A particle is attracted toward the position of the current global best g * and its own best location x * i in history, while at the same time it has a tendency to move randomly. When a particle finds a location that is better than any previously found locations, then it updates it as the new current best for particle i. There is a current global best for all n particles. The aim is to find the global best among all the current best solutions until the objective no longer improves or after a certain number of iterations.</p><p>For the particle movement, we use x * i to denote the current best for particle i, and g * ≈ min or max{f (x i )}(i = 1, 2, ..., n) to denote the current global best. Let x i and v i be the position vector and velocity for particle i, respectively. The new velocity vector is determined by the following formula</p><formula xml:id="formula_0">v t+1 i = v t i + αǫ 1 ⊙ (g * − x t i ) + βǫ 2 ⊙ (x * i − x t i ).<label>(1)</label></formula><p>where ǫ 1 and ǫ 2 are two random vectors, and each entry taking the values between 0 and 1. The Hadamard product of two matrices u ⊙ v is defined as the entrywise product, that is [u ⊙ v] ij = u ij v ij . The parameters α and β are the learning parameters or acceleration constants, which can typically be taken as, say, α ≈ β ≈ 2. The initial values of x t=0 i can be taken as the bounds or limits a = min(x j ), b = max(x j ) and v t=0 i = 0. The new position can then be updated by</p><formula xml:id="formula_1">x t+1 i = x t i + v t+1 i .<label>(2)</label></formula><p>Although v i can be any values, it is usually bounded in some range [0, v max ]. There are many variants which extend the standard PSO algorithm, and the most noticeable improvement is probably to use inertia function θ(t) so that v t i is replaced by θ(t)v t i where θ takes the values between 0 and 1. In the simplest case, the inertia function can be taken as a constant, typically θ ≈ 0.5 ∼ 0.9. This is equivalent to introducing a virtual mass to stabilize the motion of the particles, and thus the algorithm is expected to converge more quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Firefly Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Behaviour of Fireflies</head><p>The flashing light of fireflies is an amazing sight in the summer sky in the tropical and temperate regions. There are about two thousand firefly species, and most fireflies produce short and rhythmic flashes. The pattern of flashes is often unique for a particular species. The flashing light is produced by a process of bioluminescence, and the true functions of such signaling systems are still debating. However, two fundamental functions of such flashes are to attract mating partners (communication), and to attract potential prey. In addition, flashing may also serve as a protective warning mechanism. The rhythmic flash, the rate of flashing and the amount of time form part of the signal system that brings both sexes together. Females respond to a male's unique pattern of flashing in the same species, while in some species such as photuris, female fireflies can mimic the mating flashing pattern of other species so as to lure and eat the male fireflies who may mistake the flashes as a potential suitable mate.</p><p>We know that the light intensity at a particular distance r from the light source obeys the inverse square law. That is to say, the light intensity I decreases as the distance r increases in terms of I ∝ 1/r 2 . Furthermore, the air absorbs light which becomes weaker and weaker as the distance increases. These two combined factors make most fireflies visible only to a limited distance, usually several hundred meters at night, which is usually good enough for fireflies to communicate.</p><p>The flashing light can be formulated in such a way that it is associated with the objective function to be optimized, which makes it possible to formulate new optimization algorithms. In the rest of this paper, we will first outline the basic formulation of the Firefly Algorithm (FA) and then discuss the implementation as well as its analysis in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Firefly Algorithm</head><p>Now we can idealize some of the flashing characteristics of fireflies so as to develop firefly-inspired algorithms. For simplicity in describing our new Fireflire Algorithm (FA), we now use the following three idealized rules: 1) all fireflies are unisex so that one firefly will be attracted to other fireflies regardless of their sex; 2) Attractiveness is proportional to their brightness, thus for any two flashing fireflies, the less brighter one will move towards the brighter one. The attractiveness is proportional to the brightness and they both decrease as their distance increases. If there is no brighter one than a particular firefly, it will move randomly; 3) The brightness of a firefly is affected or determined by the landscape of the objective function. For a maximization problem, the brightness can simply be proportional to the value of the objective function. Other forms of brightness can be defined in a similar way to the fitness function in genetic algorithms.</p><p>Based on these three rules, the basic steps of the firefly algorithm (FA) can be summarized as the pseudo code shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Firefly Algorithm</head><p>Objective function f (x),  In certain sense, there is some conceptual similarity between the firefly algorithms and the bacterial foraging algorithm (BFA) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref>. In BFA, the attraction among bacteria is based partly on their fitness and partly on their distance, while in FA, the attractiveness is linked to their objective function and monotonic decay of the attractiveness with distance. However, the agents in FA have adjustable visibility and more versatile in attractiveness variations, which usually leads to higher mobility and thus the search space is explored more efficiently.</p><formula xml:id="formula_2">x = (x 1 , ..., x d ) T Generate initial population of fireflies x i (i = 1, 2, ..., n) Light intensity I i at x i is determined by f (x i ) Define light absorption coefficient γ while (t &lt;MaxGeneration) for i = 1 : n all n fireflies for j = 1 : i all n fireflies if (I j &gt; I i ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attractiveness</head><p>In the firefly algorithm, there are two important issues: the variation of light intensity and formulation of the attractiveness. For simplicity, we can always assume that the attractiveness of a firefly is determined by its brightness which in turn is associated with the encoded objective function.</p><p>In the simplest case for maximum optimization problems, the brightness I of a firefly at a particular location x can be chosen as I(x) ∝ f (x). However, the attractiveness β is relative, it should be seen in the eyes of the beholder or judged by the other fireflies. Thus, it will vary with the distance r ij between firefly i and firefly j. In addition, light intensity decreases with the distance from its source, and light is also absorbed in the media, so we should allow the attractiveness to vary with the degree of absorption. In the simplest form, the light intensity I(r) varies according to the inverse square law I(r) = I s /r 2 where I s is the intensity at the source. For a given medium with a fixed light absorption coefficient γ, the light intensity I varies with the distance r. That is I = I 0 e −γr , where I 0 is the original light intensity. In order to avoid the singularity at r = 0 in the expression I s /r 2 , the combined effect of both the inverse square law and absorption can be approximated using the following Gaussian form I(r) = I 0 e −γr 2 .</p><p>Sometimes, we may need a function which decreases monotonically at a slower rate. In this case, we can use the following approximation</p><formula xml:id="formula_4">I(r) = I 0 1 + γr 2 .<label>(4)</label></formula><p>At a shorter distance, the above two forms are essentially the same. This is because the series expansions about r = 0</p><formula xml:id="formula_5">e −γr 2 ≈ 1 − γr 2 + 1 2 γ 2 r 4 + ..., 1 1 + γr 2 ≈ 1 − γr 2 + γ 2 r 4 + ...,<label>(5)</label></formula><p>are equivalent to each other up to the order of O(r 3 ). As a firefly's attractiveness is proportional to the light intensity seen by adjacent fireflies, we can now define the attractiveness β of a firefly by</p><formula xml:id="formula_6">β(r) = β 0 e −γr 2 ,<label>(6)</label></formula><p>where β 0 is the attractiveness at r = 0. As it is often faster to calculate 1/(1 + r 2 ) than an exponential function, the above function, if necessary, can conveniently be replaced by β = β 0 1+γr 2 . Equation (6) defines a characteristic distance Γ = 1/ √ γ over which the attractiveness changes significantly from β 0 to β 0 e −1 . In the implementation, the actual form of attractiveness function β(r) can be any monotonically decreasing functions such as the following generalized form β(r) = β 0 e −γr m , (m ≥ 1).</p><p>For a fixed γ, the characteristic length becomes Γ = γ −1/m → 1 as m → ∞.</p><p>Conversely, for a given length scale Γ in an optimization problem, the parameter γ can be used as a typical initial value. That is γ = 1 Γ m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Distance and Movement</head><p>The distance between any two fireflies i and j at x i and x j , respectively, is the Cartesian distance</p><formula xml:id="formula_8">r ij = ||x i − x j || = d k=1 (x i,k − x j,k ) 2 ,<label>(8)</label></formula><p>where x i,k is the kth component of the spatial coordinate x i of ith firefly. In</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-D case, we have</head><formula xml:id="formula_9">r ij = (x i − x j ) 2 + (y i − y j ) 2 .</formula><p>The movement of a firefly i is attracted to another more attractive (brighter) firefly j is determined by</p><formula xml:id="formula_10">x i = x i + β 0 e −γr 2 ij (x j − x i ) + α (rand − 1 2 ),<label>(9)</label></formula><p>where the second term is due to the attraction while the third term is randomization with α being the randomization parameter. rand is a random number generator uniformly distributed in [0, 1]. For most cases in our implementation, we can take β 0 = 1 and α ∈ [0, 1]. Furthermore, the randomization term can easily be extended to a normal distribution N (0, 1) or other distributions. In addition, if the scales vary significantly in different dimensions such as −10 5 to 10 5 in one dimension while, say, −0.001 to 0.01 along the other, it is a good idea to replace α by αS k where the scaling parameters S k (k = 1, ..., d) in the d dimensions should be determined by the actual scales of the problem of interest. The parameter γ now characterizes the variation of the attractiveness, and its value is crucially important in determining the speed of the convergence and how the FA algorithm behaves. In theory, γ ∈ [0, ∞), but in practice, γ = O(1) is determined by the characteristic length Γ of the system to be optimized. Thus, in most applications, it typically varies from 0.01 to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Scaling and Asymptotic Cases</head><p>It is worth pointing out that the distance r defined above is not limited to the Euclidean distance. We can define many other forms of distance r in the n-dimensional hyperspace, depending on the type of problem of our interest. For example, for job scheduling problems, r can be defined as the time lag or time interval. For complicated networks such as the Internet and social networks, the distance r can be defined as the combination of the degree of local clustering and the average proximity of vertices. In fact, any measure that can effectively characterize the quantities of interest in the optimization problem can be used as the 'distance' r. The typical scale Γ should be associated with the scale in the optimization problem of interest. If Γ is the typical scale for a given optimization problem, for a very large number of fireflies n ≫ m where m is the number of local optima, then the initial locations of these n fireflies should distribute relatively uniformly over the entire search space in a similar manner as the initialization of quasi-Monte Carlo simulations. As the iterations proceed, the fireflies would converge into all the local optima (including the global ones) in a stochastic manner. By comparing the best solutions among all these optima, the global optima can easily be achieved. At the moment, we are trying to formally prove that the firefly algorithm will approach global optima when n → ∞ and t ≫ 1. In reality, it converges very quickly, typically with less than 50 to 100 generations, and this will be demonstrated using various standard test functions later in this paper.</p><p>There are two important limiting cases when γ → 0 and γ → ∞. For γ → 0, the attractiveness is constant β = β 0 and Γ → ∞, this is equivalent to say that the light intensity does not decrease in an idealized sky. Thus, a flashing firefly can be seen anywhere in the domain. Thus, a single (usually global) optimum can easily be reached. This corresponds to a special case of particle swarm optimization (PSO) discussed earlier. Subsequently, the efficiency of this special case is the same as that of PSO.</p><p>On the other hand, the limiting case γ → ∞ leads to Γ → 0 and β(r) → δ(r) (the Dirac delta function), which means that the attractiveness is almost zero in the sight of other fireflies or the fireflies are short-sighted. This is equivalent to the case where the fireflies fly in a very foggy region randomly. No other fireflies can be seen, and each firefly roams in a completely random way. Therefore, this corresponds to the completely random search method. As the firefly algorithm is usually in somewhere between these two extremes, it is possible to adjust the parameter γ and α so that it can outperform both the random search and PSO. In fact, FA can find the global optima as well as all the local optima simultaneously in a very effective manner. This advantage will be demonstrated in detail later in the implementation. A further advantage of FA is that different fireflies will work almost independently, it is thus particularly suitable for parallel implementation. It is even better than genetic algorithms and PSO because fireflies aggregate more closely around each optimum (without jumping around as in the case of genetic algorithms). The interactions between different subregions are minimal in parallel implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multimodal Optimization with Multiple Optima 4.1 Validation</head><p>In order to demonstrate how the firefly algorithm works, we have implemented it in Matlab. We will use various test functions to validate the new algorithm. As an example, we now use the FA to find the global optimum of the Michalewicz function</p><formula xml:id="formula_11">f (x) = − d i=1 sin(x i )[sin( ix 2 i π )] 2m ,<label>(10)</label></formula><p>where m = 10 and d = 1, 2, .... The global minimum f * ≈ −1.801 in 2-D occurs at (2.20319, 1.57049), which can be found after about 400 evaluations for 40 fireflies after 10 iterations (see <ref type="figure" target="#fig_1">Fig. 2</ref> and <ref type="figure" target="#fig_2">Fig. 3</ref>). Now let us use the FA to find the optima of some tougher test functions. This is much more efficient than most of existing metaheuristic algorithms. In the above simulations, the values of the parameters are α = 0.2, γ = 1 and β 0 = 1.</p><p>We have also used much tougher test functions. For example, Yang de- scribed a multimodal function which looks like a standing-wave pattern <ref type="bibr" target="#b10">[11]</ref> f</p><formula xml:id="formula_12">(x) = e − d i=1 (x i /a) 2m − 2e − d i=1 x 2 i • d i=1 cos 2 x i , m = 5,<label>(11)</label></formula><p>is multimodal with many local peaks and valleys, and it has a unique global minimum f * = −1 at (0, 0, ..., 0) in the region −20 ≤ x i ≤ 20 where i = 1, 2, ..., d</p><p>and a = 15. The 2D landscape of Yang's function is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison of FA with PSO and GA</head><p>Various studies show that PSO algorithms can outperform genetic algorithms (GA) <ref type="bibr" target="#b3">[4]</ref> and other conventional algorithms for solving many optimization problems. This is partially due to that fact that the broadcasting ability of the current best estimates gives better and quicker convergence towards the optimality. A general framework for evaluating statistical performance of evolutionary algorithms has been discussed in detail by Shilane et al. <ref type="bibr" target="#b7">[8]</ref>. Now we will compare the Firefly Algorithms with PSO, and genetic algorithms for various standard test functions. For genetic algorithms, we have used the standard version with no elitism with a mutation probability of p m = 0.05 and a crossover probability of 0.95. For the particle swarm optimization, we have also used the standard version with the learning parameters α ≈ β ≈ 2 without the inertia correction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. We have used various population sizes from n = 15 to 200, and found that for most problems, it is sufficient to use n = 15 to 50. Therefore, we have used a fixed population size of n = 40 in all our simulations for comparison. After implementing these algorithms using Matlab, we have carried out extensive simulations and each algorithm has been run at least 100 times so as to carry out meaningful statistical analysis. The algorithms stop when the variations of function values are less than a given tolerance ǫ ≤ 10 −5 . The results are summarized in the following table (see <ref type="table" target="#tab_1">Table 1</ref>) where the global optima are reached. The numbers are in the format: average number of evaluations (success rate), so 3752 ± 725(99%) means that the average number (mean) of function evaluations is 3752 with a standard deviation of 725. The success rate of finding the global optima for this algorithm is 99%.</p><p>We can see that the FA is much more efficient in finding the global optima with higher success rates. Each function evaluation is virtually instantaneous  on modern personal computer. For example, the computing time for 10,000 evaluations on a 3GHz desktop is about 5 seconds. Even with graphics for displaying the locations of the particles and fireflies, it usually takes less than a few minutes. It is worth pointing out that more formal statistical hypothesis testing can be used to verify such significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have formulated a new firefly algorithm and analyzed its similarities and differences with particle swarm optimization. We then implemented and compared these algorithms. Our simulation results for finding the global optima of various test functions suggest that particle swarm often outperforms traditional algorithms such as genetic algorithms, while the new firefly algorithm is superior to both PSO and GA in terms of both efficiency and success rate. This implies that FA is potentially more powerful in solving NP-hard problems which will be investigated further in future studies. The basic firefly algorithm is very efficient, but we can see that the solutions are still changing as the optima are approaching. It is possible to improve the solution quality by reducing the randomness gradually. A further improvement on the convergence of the algorithm is to vary the randomization parameter α so that it decreases gradually as the optima are approaching. These could form important topics for further research. Furthermore, as a relatively straightforward extension, the Firefly Algorithm can be modified to solve multiobjective optimization problems. In addition, the application of firefly algorithms in combination with other algorithms may form an exciting area for further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Pseudo code of the firefly algorithm (FA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Michalewicz's function for two independent variables with a global minimum f * ≈ −1.801 at (2.20319, 1.57049).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The initial 40 fireflies (left) and their locations after 10 iterations (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Yang's function in 2D with a global minimum f * = −1 at (0, 0) where a = 15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Move firefly i towards j in d-dimension; end if Attractiveness varies with distance r via exp[−γr]</figDesc><table><row><cell>Evaluate new solutions and update light intensity</cell></row><row><cell>end for j</cell></row><row><cell>end for i</cell></row><row><cell>Rank the fireflies and find the current best</cell></row><row><cell>end while</cell></row><row><cell>Postprocess results and visualization</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Comparison of algorithm performance</cell><cell></cell></row><row><cell>Functions/Algorithms</cell><cell>GA</cell><cell>PSO</cell><cell>FA</cell></row><row><cell cols="4">Michalewicz's (d=16) Rosenbrock's (d=16) De Jong's (d=256) Schwefel's (d=128) Ackley's (d=128) Rastrigin's Easom's Griewank's Shubert's (18 minima) 54077 ± 4997(89%) 23992 ± 3755(92%) 12577 ± 2356(100%) 89325 ± 7914(95%) 6922 ± 537(98%) 3752 ± 725(99%) 55723 ± 8901(90%) 32756 ± 5325(98%) 7792 ± 2923(99%) 25412 ± 1237(100%) 17040 ± 1123(100%) 7217 ± 730(100%) 227329 ± 7572(95%) 14522 ± 1275(97%) 9902 ± 592(100%) 32720 ± 3327(90%) 23407 ± 4325(92%) 5293 ± 4920(100%) 110523 ± 5199(77%) 79491 ± 3715(90%) 15573 ± 4399(100%) 19239 ± 3307(92%) 17273 ± 2929(90%) 7925 ± 1799(100%) 70925 ± 7652(90%) 55970 ± 4223(92%) 12592 ± 3715(100%) Yang's (d = 16) 27923 ± 3025(83%) 14116 ± 2949(90%) 7390 ± 2189(100%)</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Swarm Intelligence: From Natural to Artificial Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bonabeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Theraulaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optimisation for Engineering Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><forename type="middle">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Prentice-Hall</publisher>
			<pubPlace>New Delhi</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stability analysis of social foraging swarms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Passino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Sys. Man. Cyber. Part B -Cybernetics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="539" to="557" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Genetic Algorithms in Search, Optimisation and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Addison Wesley</publisher>
			<pubPlace>Reading, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
		<title level="m">Particle swarm optimization. Proc. of IEEE International Conference on Neural Networks</title>
		<meeting><address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<title level="m">Swarm intelligence</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Biomimicrt of Bacterial Foraging for Distributed Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Passino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>University Press</publisher>
			<pubPlace>Princeton, New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A general framework for statistical performance comparison of evolutionary computation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shilane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dudoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ovaska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences: an Int. Journal</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="2870" to="2879" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature-Inspired Metaheuristic Algorithms</title>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Luniver Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Biology-derived algorithms in engineering optimizaton (Chapter 32</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Bioinspired Algorithms and Applications</title>
		<imprint>
			<publisher>Chapman &amp; Hall / CRC</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Olarius &amp; Zomaya)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Engineering Optimization: An Introduction with Metaheuristic Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
