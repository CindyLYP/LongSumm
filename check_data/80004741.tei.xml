<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Introspective Generative Modeling: Decide Discriminatively</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-04-25">25 Apr 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Lazarow</surname></persName>
							<email>jlazarow@ucsd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
							<email>lojin@ucsd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dept. of CSE UCSD</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of CSE UCSD</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Introspective Generative Modeling: Decide Discriminatively</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-04-25">25 Apr 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1704.07820v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We study unsupervised learning by developing introspective generative modeling (IGM) that attains a generator using progressively learned deep convolutional neural networks. The generator is itself a discriminator, capable of introspection: being able to self-evaluate the difference between its generated samples and the given training data. When followed by repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. IGM learns a cascade of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and semisupervised learning. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Supervised learning techniques have made substantial impact on tasks that can be formulated as a classification/regression problem; some well-known classifiers include SVM <ref type="bibr" target="#b41">[42]</ref>, boosting <ref type="bibr" target="#b11">[12]</ref>, random forests <ref type="bibr" target="#b4">[5]</ref>, and convolutional neural networks <ref type="bibr" target="#b24">[25]</ref>. Unsupervised learning, where no task-specific labeling/feedback is provided on top of the input data, still remains one of the most difficult problems in machine learning but holds a bright future since a large number of tasks have little to no supervision.</p><p>Popular unsupervised learning methods include mixture models <ref type="bibr" target="#b9">[10]</ref>, principal component analysis (PCA) <ref type="bibr" target="#b21">[22]</ref>, spectral clustering <ref type="bibr" target="#b36">[37]</ref>, topic modeling <ref type="bibr" target="#b3">[4]</ref>, and autoencoders <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>. In a nutshell, unsupervised learning techniques are mostly guided by the minimum description length principle (MDL) <ref type="bibr" target="#b33">[34]</ref> to best reconstruct the data whereas supervised learning methods are primarily driven by minimizing error metrics to best fit the input labeling. Unsupervised learning models are often generative and supervised classifiers are often discriminative; generative model learning has been traditionally considered to be 1 * equal contribution. a much harder task than discriminative learning <ref type="bibr" target="#b12">[13]</ref>, due to its intrinsic learning complexity, as well many assumptions and simplifications made about the underlying models. <ref type="figure" target="#fig_3">Figure 1</ref>. The first row shows the development of two × 64 pseudonegative samples (patches) over the course of the training process on the "tree bark" texture at selected rounds. We can see the initial "scaffold" created and then refined by the network in later rounds. The input "tree bark" texture and a synthesized image by our IGM algorithm are shown in the second row.</p><p>Generative and discriminative models have traditionally been considered distinct and complementary to each other. In the past, connections have been built to combine the two families <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b19">20]</ref>. In the presence of supervised information with a large amount of data, a discriminative classifier <ref type="bibr" target="#b23">[24]</ref> exhibits superior capability in making robust classification by learning rich and informative representations; unsupervised generative models do not require supervision but at a price of relying on assumptions that are often too ideal in dealing with problems of real-world complexity. Attempts have previously been made to learn generative models directly using discriminative classifiers for density estimation <ref type="bibr" target="#b43">[44]</ref> and image modeling <ref type="bibr" target="#b38">[39]</ref>. There is also a wave of recent development in generative adversarial networks (GAN) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b0">1]</ref> in which a discriminator helps a generator try not to be fooled by "fake" samples. We will discuss in detail the relations and connections of our IGM with these existing literature in the next section.</p><p>In <ref type="bibr" target="#b43">[44]</ref>, a self supervised boosting algorithm was proposed to train a boosting algorithm by sequentially adding features as weak classifiers on additionally self-generated negative samples; the generative discriminative modeling work (GDL) in <ref type="bibr" target="#b38">[39]</ref> generalizes the concept that a generative model can be successfully modeled by learning a sequence of discriminative classifiers via self-generated pseudo-negatives.</p><p>Inspired by the prior work on generative modeling <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b38">39]</ref> and development of convolutional neural networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14]</ref>, we develop an image modeling algorithm, introspective generative modeling (IGM) that is simultaneously a generator and a discriminator, consisting of two critical stages during training: (1) a pseudo-negative sampling stage (synthesis) for self-generation, (2) and a CNN classifier learning stage (classification) for self-evaluation and model updating. There are a number of interesting properties about IGM that are worth highlighting:</p><p>• CNN classifier as generator: No special condition on CNN architecture is needed in IGM and many existing CNN classifiers can be directly made into generators, if trained properly.</p><p>• End-to-end self-evaluation and learning: Perform end-toend "introspective learning" to self-classify between synthesized samples (pseudo-negatives) and the training data, followed by direct discriminative learning, to approach the target distribution.</p><p>• Integrated unsupervised/supervised learning: Unsupervised and supervised learning can be carried out under similar pipelines, differing in the absence or presence of initial negative samples.</p><p>• All backpropagation: Our synthesis-by-classification algorithm performs efficient training using backpropagation in both stages: the sampling stage for the input images and the classification stage for the CNN parameters.</p><p>• Model-based anysize-image-generation: Since our generative modeling models the input image, we are able to train on images of a size and generate an image of a larger size while maintaining the coherence for the entire image.</p><p>• Agnosticity to various vision applications: Due to its intrinsic modeling power being at the same time generative and discriminative, IGM can be adopted to many applications; under the same pipeline, we show a number of vision tasks in the experiments, including texture modeling, artistic style transference, face modeling, and semi-supervised image classification in this paper. Supervised classification cases of an algorithm in the same introspective learning family can been seen in <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Significance and related work</head><p>Our introspective generative modeling (IGM) algorithm has connections to many existing approaches including the MinMax entropy work for texture modeling <ref type="bibr" target="#b50">[51]</ref>, the hybrid modeling work <ref type="bibr" target="#b12">[13]</ref>, and the self-supervised boosting algorithm <ref type="bibr" target="#b43">[44]</ref>. It builds on top of convolutional neural networks <ref type="bibr" target="#b24">[25]</ref> and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) <ref type="bibr" target="#b38">[39]</ref>, and the DeepDream code <ref type="bibr" target="#b29">[30]</ref> and the neural artistic style work <ref type="bibr" target="#b13">[14]</ref>. The general pipeline of IGM is similar to that of GDL <ref type="bibr" target="#b38">[39]</ref>, with the boosting algorithm used in <ref type="bibr" target="#b38">[39]</ref> is replaced by a CNN in IGM. More importantly, the work of <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b13">14]</ref> motives us to significantly improve the time-consuming sampling process in <ref type="bibr" target="#b38">[39]</ref> by an efficient SGD process via backpropagation (the reason for us to say "all backpropagation"). Next, we review some existing generative image modeling work, followed by detailed discussions about the two most related algorithms: GDL <ref type="bibr" target="#b38">[39]</ref> and the recent development of generative adversarial networks (GAN) <ref type="bibr" target="#b14">[15]</ref>.</p><p>The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory <ref type="bibr" target="#b15">[16]</ref>, deformable models <ref type="bibr" target="#b47">[48]</ref>, inducing features <ref type="bibr" target="#b8">[9]</ref>, wake-sleep <ref type="bibr" target="#b17">[18]</ref>, the MiniMax entropy theory <ref type="bibr" target="#b50">[51]</ref>, the field of experts <ref type="bibr" target="#b34">[35]</ref>, Bayesian models <ref type="bibr" target="#b48">[49]</ref>, and deep belief nets <ref type="bibr" target="#b18">[19]</ref>. Each of these pioneering works points to some promising direction to unsupervised generative modeling. However the modeling power of these existing frameworks is still somewhat limited in computational and/or representational aspects. In addition, not too many of them sufficiently explore the power of discriminative modeling. Recent works that adopt convolutional neural networks for generative modeling <ref type="bibr" target="#b46">[47]</ref> either use CNNs as a feature extractor or create separate paths <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b40">41]</ref>. The neural artistic transferring work <ref type="bibr" target="#b13">[14]</ref> has demonstrated impressive results on the image transferring and texture synthesis tasks but it is focused <ref type="bibr" target="#b13">[14]</ref> on a careful study of channels attributed to artistic texture patterns, instead of aiming to build a generic image modeling framework. The self-supervised boosting work <ref type="bibr" target="#b43">[44]</ref> sequentially learns weak classifiers under boosting <ref type="bibr" target="#b11">[12]</ref> for density estimation, but its modeling power was not adequately demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship with GDL [39]</head><p>The generative via discriminative learning framework (GDL) <ref type="bibr" target="#b38">[39]</ref> learns a generator through a sequence of boosting classifiers <ref type="bibr" target="#b11">[12]</ref> using repeatedly self-generated samples, called pseudo-negatives, to approach the target distribution. Our IGM algorithm takes inspiration from GDL, but we also observe a number of limitations in GDL that will be overcome by IGM: GDL uses manually specified feature types (histograms and Haar filters), which are fairly limited by today's standard; the sampling process in GDL, based on Markov chain Monte Carlo (MCMC), is a big computational bottleneck; the experimental results for image modeling and classification were not satisfactory. To summarize, the main differences between GDL and IGM include:</p><p>• The adoption of convolutional networks in IGM results in a significant boost to feature learning.</p><p>• Introducing backpropagation to the synthesis/sampling process in IGM makes a fundamental improvement to the sam-pling process in GDL that is otherwise slow and impractical.</p><p>• An alternative algorithm, namely IGM-single (see <ref type="figure" target="#fig_6">Fig. 4</ref>), is additionally proposed to maintain a single classifier for IGM.</p><p>• Higher quality results for image modeling are demonstrated in IGM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with GAN [15]</head><p>The recent development of generative adversarial neural networks <ref type="bibr" target="#b14">[15]</ref> is very interesting and also highly related to IGM. We summarize the key differences between IGM and GAN. Other recent algorithms alongside GAN <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">38]</ref> share similar properties with it.</p><p>• Unified generator/discriminator vs. separate generator and discriminator. IGM maintains a single model that is simultaneously a generator and a discriminator. The IGM generator therefore has self-awareness -being able to self-evaluate the difference between its generated samples (called pseudonegatives) w.r.t. the training data, followed by a direct CNN classifier training. GAN instead creates two convolutional networks, a generator and a discriminator.</p><p>• Training. Due to the internal competition between the generator and the discriminator, GAN is known to be hard to train <ref type="bibr" target="#b0">[1]</ref>. IGM carries out a straightforward use of backpropagation in both the sampling and the classifier training stage, making the learning process direct. For example, for the textures shown in the experiments <ref type="figure">Fig. 2</ref> and <ref type="figure">Fig. 6</ref>, all results by IGM are obtained under the identical setting without hyper-parameter tuning.</p><p>• Modeling. The generator in GAN is a mapping from the features to the images. IGM directly models the underlying statistics of an image with an efficient sampling/inference process, which makes IGM flexible. For example, we are able to conduct model-based-anysize-generation in the texture modeling task by directly maintaining the underlying statistics of the entire image.</p><p>• Speed. GAN performs a forward pass to reconstruct an image, which is generally faster than IGM where synthesis is carried out using backpropagation. IGM is still practically feasible since it takes only about 1 − 2 seconds to synthesize an image of size 64 × 64 and around 30 seconds to synthesize a texture image of size 320 × 200 excluding the time to load the models.</p><p>• Model size. Since a cascade of CNN classifiers (60 − 200) are included in a single IGM model, IGM has a much larger model complexity than GAN. This is an advantage of GAN over IGM. Our alternative IGM-single model maintains a single CNN classifier but its generative power is worse than those of IGM and GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship with ICL [21]</head><p>The Introspective Classifier Learning work (ICL) <ref type="bibr" target="#b20">[21]</ref> is a sister paper to IGM, with ICL focusing on the discriminator side emphasizing its classification power. a). IGM focuses on the generator side studying its image construction capability. b). IGM consists of a sequence of cascading classifiers, whereas ICL is only composed of a single classifier. Essentially, ICL is similar to IGM-single, with a small difference in the absence/presence of given negative samples. The generative modeling aspect of ICL/IGMsingle is not as competitive as IGM though. c). In ICL, a formulation for training a softmax multi-class classification was proposed, which is not in IGM. d). In addition, ICL focuses on single image patch, whereas IGM is able to model/synthesize an arbitrary size of image. A number of important image modeling tasks, including texture modeling, style transferring, face modeling, and semi-supervised learning are demonstrated here, which are not covered in ICL <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We describe below the introspective generative modeling (IGM) algorithm. We discuss the main formulation first, which bears some level of similarity to GDL <ref type="bibr" target="#b38">[39]</ref>. However, with the replacement of the boosting algorithm <ref type="bibr" target="#b11">[12]</ref> by convolutional neural networks <ref type="bibr" target="#b24">[25]</ref>, IGM demonstrates significant improvement over GDL in terms of both modeling and computational power. The enhanced modeling power comes mainly from CNNs due to its end-to-end learning with automatic feature learning and tuning when backpropagating on the network parameters; enhanced computational power also largely from CNNs due to a natural implementation of sampling by backpropagating on the input image. GDL is similar to IGM (see <ref type="figure" target="#fig_1">Fig. 3</ref>), but IGM-single (see <ref type="figure" target="#fig_6">Fig. 4</ref>) maintains a single CNN as opposed to having a sequence of classifiers in both GDL and IGM. We motivate the formulation of IGM from the Bayes theory, similar to GDL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>We start the discussion by borrowing notation from <ref type="bibr" target="#b38">[39]</ref>. Suppose we are given a set of training images (patches): S = {x i , i = 1..n}. We focus on patch-based input first and let x ∈ R m be a data sample (an image patch of size say m = 64 × 64). We adopt the pseudo-negative concept defined in <ref type="bibr" target="#b38">[39]</ref> and define class labels y ∈ {−1, +1}, indicating x being a negative or a positive sample. Here we assume the positive samples with label y = +1 are the patterns/targets we want to study. A generative model computes for p(y, x) = p(x|y)p(y), which captures the underlying generation process of x for class y. A discriminative classifier instead computes p(y|x). Under the Bayes rule, similar to the motivation in <ref type="bibr" target="#b38">[39]</ref>:</p><formula xml:id="formula_0">p(x|y = +1) = p(y = +1|x)p(y = −1) p(y = −1|x)p(y = +1) p(x|y = −1),<label>(1)</label></formula><p>which can be further simplified when assuming equal priors p(y = +1) = p(y = −1):  </p><formula xml:id="formula_1">p(x|y = +1) = p(y = +1|x) − p(y = +1|x) p(x|y = −1).<label>(2)</label></formula><formula xml:id="formula_2">p − 0 (x) = U (x) p − t (x) = 1 Z t q t (y = +1|x) q t (y = −1|x) • p − t−1 (x), t = 1..T<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">Z t = qt(y=+1|x) qt(y=−1|x) p − t−1 (x)dx.</formula><p>Our hope is to gradually learn p − t (x) by following this iterative process of Eq. 3:</p><formula xml:id="formula_4">p − t (x) t=∞ → p(x|y = +1),<label>(4)</label></formula><p>such that the samples drawn x ∼ p − t (x) become indistinguishable from the given training samples. The samples drawn from x ∼ p − t (x) are called pseudo-negatives, following a definition in <ref type="bibr" target="#b38">[39]</ref>. Next, we present the realization of Eq. 3, namely IGM (consisting of a sequence of CNN classifiers and see <ref type="figure" target="#fig_1">Fig. 3</ref>) and additionally IGM-single (maintaining a single CNN classifier and see <ref type="figure" target="#fig_6">Fig. 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">IGM Training</head><p>Next, we present our introspective generative modeling algorithm using a sequence of classifiers, called IGM. The given (unlabeled) training set is defined as S = {x i , i = 1..n}, which is turned into S + = {(x i , y i = +1), i = 1..n} within the discriminative setting. We start from an initial pseudo-negative set  where</p><formula xml:id="formula_5">S 0 − = {(x i , −1), i = 1, ..., l}</formula><formula xml:id="formula_6">x i ∼ p − 0 (x) = U (x) which is a Gaussian distribu- tion. A working set for t = 1..T S t−1 − = {(x i , −1), i = 1, ..., l}.</formula><p>then includes the pseudo-negative samples self-generated from each round. l indicates the number of pseudonegatives generated at each round. We carry out learning with t = 1...T to iteratively obtain</p><formula xml:id="formula_7">q t (y = +1|x), q t (y = −1|x)<label>(5)</label></formula><p>by updating classifier C t on S + ∪ S t−1 − . The reason for using q is because it is an approximation to the true p due Algorithm 1 Outline of the IGM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Given a set of training data S+ = {(xi, yi = +1), i = 1..n} with x ∈ m . Initialization: obtain an initial distribution e.g. Gaussian for the pseudo-negative samples:</p><formula xml:id="formula_8">p − 0 (x) = U (x). Create S 0 − = {(xi, −1), i = 1, ..., l} with xi ∼ p − 0 (x) For t=1..T 1. Classification-step: Train CNN classifier C t on S+ ∪ S t−1</formula><p>− , resulting in qt(y = +1|x).</p><p>2. Update the model: </p><formula xml:id="formula_9">p − t (x) = 1 Z t q t (y=+1|x) q t (y=−1|x) p − t−1 (x).</formula><formula xml:id="formula_10">p − t (x) = 1 Z t q t (y = +1|x) q t (y = −1|x) p − t−1 (x),<label>(6)</label></formula><p>where</p><formula xml:id="formula_11">Z t = qt(y=+1|x) qt(y=−1|x) p − t−1 (x)dx.</formula><p>We draw new samples</p><p>x i ∼ p − t (x) to have the pseudo-negative set:</p><formula xml:id="formula_12">S t − = {(x i , −1), i = 1, ..., l}.<label>(7)</label></formula><p>Algorithm 1 describes the learning process. The pipeline of IGM is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, which consists of (1) a synthesis step and (2) a classification step. A sequence of CNN classifiers is progressively learned. With the pseudo-negatives being gradually generated, the classification boundary gets tightened and approaches the target distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Classification-step</head><p>The classification-step can be viewed as training a normal classifier on the training set S</p><formula xml:id="formula_13">+ ∪ S t − where S + = {(x i , y i = +1), i = 1..n}. S t − = {(x i , −1), i = 1, ..., l} for t ≥ 1.</formula><p>We use a CNN as our base classifier. When training a classifier C t on S + ∪ S t − , we denote the parameters to be learned in C t by a high-dimensional vector</p><formula xml:id="formula_14">W t = (w (0) t , w (1)</formula><p>t ) which might consist of millions of parameters. w  t can be learned by the standard stochastic gradient descent algorithm via backpropagation to minimize a cross-entropy loss with an additional term on the pseudo-negatives:</p><formula xml:id="formula_15">L(Wt) = − i=1..n (x i ,+1)∈S + ln qt(+1|x i ; Wt)− i=1..l (x i ,−1)∈S t − ln qt(−1|x i ; Wt)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Synthesis-step</head><p>In the classification step, we obtain q t (y|x; W t ) which is then used to update p − t (x) according to Eq. <ref type="formula" target="#formula_10">6</ref>:</p><formula xml:id="formula_16">p − t (x) = t a=1 1 Z a q a (y = +1|x; W a ) q a (y = −1|x; W a ) p − 0 (x).<label>(9)</label></formula><p>In the synthesis-step, our goal is to draw fair samples from p − t (x). The sampling process is carried out by backpropagation, but now we need to go through a sequence classifiers by using qt(y=−1|x;Wt) . Therefore, generating pseudo-negative samples when training IGM does not need a large overhead. Additional Gaussian noise can be added to the stochastic gradient as in <ref type="bibr" target="#b42">[43]</ref> but we did not observe a big difference in the quality of samples in practice. This is probably due to the equivalent class <ref type="bibr" target="#b44">[45]</ref> where the probability mass is widely distributed over an extremely large image space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling strategies</head><p>In <ref type="bibr" target="#b38">[39]</ref>, various Markov chain Monte Carlo techniques <ref type="bibr" target="#b26">[27]</ref> including Gibbs sampling and Iterated Conditional Modes (ICM) have been adopted, which are often slow. Motivated by the DeepDream code <ref type="bibr" target="#b29">[30]</ref> and Neural Artistic Style work <ref type="bibr" target="#b13">[14]</ref>, we perform stochastic gradient descent via backpropagation in synthesis. Recent works show the connection and equivalence between stochastic gradient descent/ascent and Markov chain Monte Carlo sampling <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29]</ref>. When conducting experiments, some alternative sampling schemes using SGD can be applied: i) earlystopping once x becomes positive (or after a small fixed number of steps); or ii) sampling with equilibrium after long steps. We found early-stopping effective and efficient, which can be viewed as contrastive divergence <ref type="bibr" target="#b6">[7]</ref> where a short Markov chain is simulated.</p><p>Note that the partition function (normalization) Z a is a constant that is not dependent on the sample x. Let</p><formula xml:id="formula_17">g t (x) = q t (y = +1|x; W t ) q t (y = −1|x; W t ) = exp{&lt; w (1) t , φ(x; w (0) t ) &gt;},<label>(10)</label></formula><p>and take its ln, which is nicely turned into the logit of q t (y = +1|x; W t )</p><formula xml:id="formula_18">ln g t (x) =&lt; w (1) t , φ(x; w (0) t ) &gt; .<label>(11)</label></formula><p>Starting from a x drawn from p − t−1 (x), we directly increase &lt; w <ref type="bibr" target="#b0">(1)</ref> t , φ(x; w (0) subject to Eq. <ref type="bibr" target="#b8">(9)</ref>. A noise can be injected as in <ref type="bibr" target="#b42">[43]</ref> when performing SGD sampling. Overall model The overall IGM model after T stages of training becomes:</p><formula xml:id="formula_19">p − T (x) = Z T t=1 q t (y = +1|x; W t ) q t (y = −1|x; W t ) p − 0 (x) = 1 Z T t=1 exp{&lt; w (1) t , φ(x; w (0) t ) &gt;}p − 0 (x),<label>(12)</label></formula><p>where</p><formula xml:id="formula_20">Z = T t=1 exp{&lt; w (1) t , φ(x; w (0) t ) &gt;}p − 0 (x)dx.</formula><p>IGM shares a similar cascade aspect with GDL <ref type="bibr" target="#b38">[39]</ref> where the convergence of this iterative learning process to the target distribution was shown by the following theorem in <ref type="bibr" target="#b38">[39]</ref>.</p><formula xml:id="formula_21">Theorem 1 KL[p(x|y = +1)||p − t+1 (x)] ≤ KL[p(x|y = +1)||p − t (x)]</formula><p>where KL denotes the Kullback-Leibler divergences, and p(x|y = +1) ≡ p + (x).  We briefly present the IGM-single algorithm, which is similar to the introspective classifier learning algorithm <ref type="bibr" target="#b20">[21]</ref> with the difference without the presence of input negative samples. The pipeline of IGM-single is shown in <ref type="figure" target="#fig_6">Fig. 4</ref>. A key aspect here is that we maintain a single CNN classifier throughout the entire learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">An alternative: IGM-single</head><p>In the classification step, we obtain q t (y|x; W t ) (similar as Eq. 8) which is then used to update p − t (x) according to Eq. (13):</p><formula xml:id="formula_22">p − t (x) = 1 Z t q t (y = +1|x; W t ) q t (y = −1|x; W t ) p − 0 (x).<label>(13)</label></formula><p>In the synthesis-step, we draw samples from p − t (x). Overall model The overall IGM-single model after T stages of training becomes:  Given a particularly sized image, anysize-imagegeneration within IGM allows one to generate/synthesize an image much larger than the given one. Patches extracted from the training images are used in the training of the discriminator. However, their position within the training (or pseudo-negative) image is not lost. In particular, when performing synthesis using backpropagation, updates to the pixel values are made by considering the average loss of all patches that overlap a given pixel. Thus, up to round T , in order to consider the updates to the patch of x(i, j) centered at position (i, j) for image I of size m 1 × m 2 , we perform backpropagation on the patches to increase the probability:</p><formula xml:id="formula_23">p − T (x) = 1 Z T exp{&lt; w (1) T , φ(x; w (0) T ) &gt;}p − 0 (x), (14) where Z T = exp{&lt; w (1) T , φ(x; w (0) T ) &gt;}p − 0 (x)dx.</formula><formula xml:id="formula_24">p T (I) ∝ T t=1 m1 i=1 m2 j=1 g t (x(i, j))p − 0 (x(i, j))<label>(15)</label></formula><p>where g t (x(i, j)) (see Eq. 10) denotes the score of the patch of size e.g. 64 × 64 for x(i, j) under the discriminator at round t. <ref type="figure" target="#fig_8">Fig. 5</ref> gives an illustration for one round of sampling. This allows us to synthesize much larger images by being able to enforce the coherence and interactions surrounding a particular pixel. In practice, we add stochasticity and efficiency to the synthesis process by randomly sampling these set of patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate both IGM and IGM-single. In each method, we adopt the discriminator architecture of <ref type="bibr" target="#b32">[33]</ref> which involves an input size of 64x64x3 in the RGB colorspace, four convolutional layers using 5 × 5 kernel sizes with the layers using 64, 128, 256 and 512 channels, respectively. We include batch normalization after each convolutional layer (excluding the first) and use leaky ReLU activations with leak slope 0.2. The classification layer flattens the input and finally feeds it into a sigmoid activation.</p><p>This serves as the discriminator for the 64 × 64 patches we extract from the training image(s). Note that is is a gen-  <ref type="figure">Figure 6</ref>. More texture synthesis results. Gatys et al. <ref type="bibr" target="#b13">[14]</ref> and Texture</p><p>Nets <ref type="bibr" target="#b40">[41]</ref> results are from <ref type="bibr" target="#b40">[41]</ref>.</p><p>eral purpose architecture with no modifications made for a specific task in mind.</p><p>In texture synthesis and artistic style, we make use of the "anysize-image-generation" architecture by adding a "head" to the network that, at each forward pass of the network, randomly selects some number (equal to the desired batch size) of 64 × 64 random patches (possibly overlapping) from the full sized images and passes them to the discriminator. This allows us to retain the whole space of patches within a training image rather than select some subset of them in advance to use during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Texture synthesis</head><p>Texture modeling/rendering is a long standing problem in computer vision and graphics <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32]</ref>. Here we are interested in statistical texture modeling <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b45">46]</ref>, instead of just texture rendering <ref type="bibr" target="#b10">[11]</ref>. We train similar textures to <ref type="bibr" target="#b40">[41]</ref>. Each source texture is resized to 256 × 256, used as the single "positive" example in the training set and a set of 200 negative examples are initially sampled from a normal distribution with σ = 0.3 of size 320 × 320 after adding padding of 32 pixels to each spatial dimension of the image to ensure each pixel of the 256×256 center has equal probability of being extracted in some patch. 1000 patches are extracted randomly across the training images and fed to the discriminator at each forward pass of the network (during training and synthesis stages) from a batch size of 100 images -50 random positives and negatives when training and 100 pseudo-negatives during synthesis. At each round, our classifier is finetuned using stochastic gradient descent with learning rate 0.01 from the previous round's classifier after the augmentation of the negative set with the 200 320 × 320 synthesized pseudo-negatives. Pseudo-negatives from more recent rounds are chosen in mini-batches with higher probability than those of earlier rounds in order to ensure the discriminator learns from its most recent mistakes as well as provide for more efficient training when the set of accumulated negatives has grown large in later rounds. During the synthesis stage, pseudo-negatives are synthesized using the previous round's pseudo-negatives as their initialization. Adam is used with a learning rate of 0.1 and β = 0.5 and stops early when the average probability of the patches under the discriminator is more likely than not to be a positive across some window of previous steps, usually <ref type="bibr" target="#b19">20</ref>, in order to reduce variance. This allows us to, on average, cross the decision boundary of the current iteration of the discriminator. We find this sampling strategy to attain a good balance in effectiveness and efficiency. Empirically, we find training the networks for 70 rounds to provide good results in terms of synthesis and distillation of the model's knowledge.</p><p>New textures are synthesized under IGM by: sampling from the same distribution used initially during training (in our case, normally distributed with σ = 0.3), performing backpropagation of the synthesis using the saved parameters of the networks for each round, and feeding the resulting partial synthesis to the next round. The same early stopping criterion is used as outlined during training, however, the number of patches is dialed down to match the number being synthesized. We use about 10 patches per image when synthesizing a 256 × 256 image since this matches the average number of patches extracted per image during training. Making the number of patches much larger than corresponding ratio used in the training process has shown to generate images of lower quality and diversity.</p><p>Under IGM-single there is a single network, and thus only a single round of synthesis takes place to transform the initial noise to a high probability texture.</p><p>Considering the results in <ref type="figure">Fig. 2 and 6</ref>, we see that IGM generates images of similar quality to <ref type="bibr" target="#b40">[41]</ref>, however, it is usually more faithful to the structure of the input images. <ref type="figure">In Fig. 2</ref>, the "bricks" texture synthesized by IGM is very strict about the grout lines being straight to ensure the bricks are rectilinear. Similarly, in <ref type="figure">Fig. 6</ref>, the "forest" texture preserves continuity but allows for some of the variation in angle and path that the tree trunks take. The "diamond" texture is reflective of the grid-like pattern seen from the input image and does not allow for overlap or differently sized diamonds. In the bottom row of "pebbles", the resulting synthesis captures the size of the pebbles seen in the input image as well as the variation in color and shading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Artistic style transfer</head><p>We also attempt to transfer artistic style as shown in <ref type="bibr" target="#b13">[14]</ref>. However, our architecture makes no use of additional networks for content and texture transferring task uses a loss </p><formula xml:id="formula_25">| I) ∝ α•||I style −I|| 2 −(1−α)•ln p − style (I style ),</formula><p>where I is an input image and I style is its stylized version, and p − style (I) denotes the model learned from the training style image. We include a L 2 fidelity term during synthesis, weighted by a parameter α, making I style not too far away from the input image I. We choose α = 0.3 and average the L 2 difference between the original content image and the current stylized image at each step of synthesis. Two examples of the artistic style transfer are shown in <ref type="figure" target="#fig_10">Fig. 7</ref>. <ref type="figure">Figure 8</ref>. Generated images learned on the CelebA dataset. The first, the second, and the third column are respectively results by DCGAN <ref type="bibr" target="#b32">[33]</ref> using tensorflow implementation <ref type="bibr" target="#b22">[23]</ref>, IGM-single, and IGM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face modeling</head><p>The CelebA dataset <ref type="bibr" target="#b27">[28]</ref> is used in our face modeling experiment, which consists of 202, 599 face images. We crop the center 64 × 64 patches in these images as our positive examples. For the classification step, we use stochastic gradient descent with learning rate 0.01 and a batch size of 100 images, which contains 50 random positives and 50 random negatives. For the synthesis step, we use the Adam optimizer with learning rate 0.01 and β = 0.5 and stop early when the pseudo-negatives cross the decision boundary. In <ref type="figure">Fig. 8</ref>, we show some face examples generated by our model and the DCGAN model. <ref type="figure">Figure 9</ref>. Generated images learned on the SVHN dataset. The first, the second, and the third column are respectively results by DCGAN <ref type="bibr" target="#b32">[33]</ref> using tensorflow implementation <ref type="bibr" target="#b22">[23]</ref>, IGM-single, and IGM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">SVHN unsupervised learning</head><p>The SVHN <ref type="bibr" target="#b30">[31]</ref> dataset consists of color images of house numbers collected by Google Street View. The training set consists of 73, 257 images, the extra set consists of 531, 131 images, and the test set has 26, 032 images. The images are of the size 32 × 32. We combine the training and extra set as our positive examples for unsupervised learning. Following the same settings in the face modeling experiments, our IGM model can generate examples as shown in <ref type="figure">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">SVHN semi-supervised classification</head><p>We perform the semi-supervised classification experiment by following the procedure outlined in <ref type="bibr" target="#b32">[33]</ref>. We first train a model on the SVHN training and extra set in an unsupervised way, as in Section 4.4. Then, we train an L2-SVM on the learned representations of this model. The features from the last three convolutional layers are concatenated to form a 14336-dimensional feature vector. A 10, 000 example held-out validation set is taken from the training set and is used for model selection. The SVM classifier is trained on 1000 examples taken at random from the remainder of the training set. The test error rate is averaged over different SVMs trained on random 1000-example training sets. Within the same setting, our IGM model achieves the test error rate of 36.44 ± 0.72% and the DCGAN model achieves 33.13 ± 0.83% (we ran the DCGAN code <ref type="bibr" target="#b22">[23]</ref> in an identical setting as IGM for a fair comparison since the result reported in <ref type="bibr" target="#b32">[33]</ref> was achieved by training on the Ima-geNet dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Introspective generative modeling points to an encouraging direction for unsupervised image modeling that capitalizes on the power of discriminative deep convolutional neural networks. It can be adopted for a wide range of problems in computer vision and machine learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Based on Eq. (2), a generative model for the positive samples (patterns of interest) p(x|y = +1) can be fully represented by a generative model for the negatives p(x|y = −1) and a discriminative classifier p(y = +1|x), if both p(x|y = −1) and p(y = +1|x) can be accurately obtained/learned. However, this seemingly intriguing property is, in a way, a chicken-and-egg problem. To faithfully learn the positive patterns p(x|y = +1), we need to have a representative p(x|y = −1), which is equally difficult, if not more. For clarity, we now use p − (x) to represent p(x|y = −1). In the GDL algorithm [39], a solution was given to learning p(x|y = +1) by using an iterative process starting from an initial reference distribution of the negatives p − 0 (x), e.g. a Gaussian distribution U (x) on the entire space of x ∈ R m :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Schematic illustration of the pipeline of IGM. The top figure shows the input training samples shown in red circles. The bottom figure shows the pseudo-negative samples drawn by the learned final model. The left panel displays pseudo-negative samples drawn at each time stamp t. The right panel shows the classification by the CNN on the training samples and pseudo-negatives at each time stamp t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 .</head><label>3</label><figDesc>Synthesis-step: sample l pseudo-negative samples xi ∼ p − t (x), i = 1, ..., l from the current model p − t (x) using a variational sampling procedure (backpropagation on the input) to obtain S t − = {(xi, −1), i = 1, ..., l}. 4. t ← t + 1 and go back to step 1 until convergence (e.g. indistinguishable to the given training samples). End to limited samples drawn in m . At each time t, we then compute</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 1 )</head><label>1</label><figDesc>t denotes the weights on the top layer combining the features φ(x; w internal representations. Without loss of generality, we assume a sigmoid function for the discriminative probability q t (y|x; W t ) = 1/(1 + exp{−y &lt; w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 1 − by taking 1</head><label>111</label><figDesc>Za qa(y=+1|x;Wa) qa(y=−1|x;Wa) , a = 1..t. This can be timeconsuming. In practice, we can simply perform backpropagation on the previous set S t−Zt qt(y=+1|x;Wt)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Schematic illustration of the pipeline of IGM-single.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3. 4 .</head><label>4</label><figDesc>Model-based anysize-image-generation IGM generator/discriminator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of model-based anysize-image-generation strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>input</head><label></label><figDesc>Gatys et al.TextureNets IGM-single (ours) IGM (ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Artistic style transfer results using the "Starry Night" and "Scream" style on the image from Amsterdam. functions during synthesis to minimize − ln p(I style</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 2. Texture synthesis algorithm comparison. Gatys et al.<ref type="bibr" target="#b13">[14]</ref>, Texture Nets<ref type="bibr" target="#b40">[41]</ref>, Portilla &amp; Simoncelli<ref type="bibr" target="#b31">[32]</ref>, and DCGAN<ref type="bibr" target="#b32">[33]</ref> results are from<ref type="bibr" target="#b40">[41]</ref>.</figDesc><table><row><cell>input</cell><cell>Gatys et al.</cell><cell>TextureNets</cell><cell>Portilla &amp; Simoncelli</cell><cell>DCGAN</cell><cell>IGM-single (ours)</cell><cell>IGM (ours)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t ) &gt; using stochastic gradient ascent on x via backpropagation which allows us to obtain fair samples</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>This work is supported by NSF IIS-1618477 and a Northrop Grumman Contextual Robotics grant. We thank Saining Xie, Jun-Yan Zhu, Jiajun Wu, Stella Yu, and Alexei Efros for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Autoencoders, unsupervised learning, and deep architectures. ICML unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Scaling learning algorithms towards ai. Large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<title level="m">Random Forests. Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07093</idno>
		<title level="m">Neural photo editing with introspective adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On contrastive divergence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Carreira-Perpinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic gradient hamiltonian monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inducing features of random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="380" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Texture synthesis by nonparametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Comp. and Sys. Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>series in statistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
		<title level="m">General pattern theory-A mathematical study of regular structures</title>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pyramid-based texture analysis/synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The&quot; wake-sleep&quot; algorithm for unsupervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">5214</biblScope>
			<biblScope unit="page">1158</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Machine learning: discriminative and generative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Introspective classifier learning: Empower generatively</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lazarow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<editor>Wiley Online Library</editor>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dcgan-Tensorflow</surname></persName>
		</author>
		<ptr target="https://github.com/carpedm20/DCGAN-tensorflow" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Monte Carlo strategies in scientific computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04289</idno>
		<title level="m">Stochastic gradient descent as approximate bayesian inference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepdream -a code example for visualizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tyka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Google Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reading Digits in Natural Images with Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients. Int&apos;l j. of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling by shortest data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02386</idno>
		<title level="m">Adagan: Boosting generative models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning generative models via discriminative approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Brain anatomical structure segmentation by hybrid discriminative/generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Narr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Toga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tran. on Medical Imag</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer-Verlag New York, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self supervised boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Equivalence of julesz ensembles and frame models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Cooperative training of descriptor and generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09408</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Feature extraction from faces using deformable templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Vision as bayesian inference: analysis by synthesis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="301" to="308" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<title level="m">Energy-based generative adversarial network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Minimax entropy principle and its application to texture modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
