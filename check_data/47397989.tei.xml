<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Arguing Machines: Human Supervision of Black Box AI Systems That Make Life-Critical Decisions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-09-24">24 Sep 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lex</forename><surname>Fridman</surname></persName>
							<email>fridman@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Massachusetts Institute of Technology (MIT) Black Box</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Massachusetts Institute of Technology (MIT) Black Box</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Jenik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Massachusetts Institute of Technology (MIT) Black Box</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Reimer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Massachusetts Institute of Technology (MIT) Black Box</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Arguing Machines: Human Supervision of Black Box AI Systems That Make Life-Critical Decisions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-24">24 Sep 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1710.04459v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We consider the paradigm of a black box AI system that makes life-critical decisions. We propose an "arguing machines" framework that pairs the primary AI system with a secondary one that is independently trained to perform the same task. We show that disagreement between the two systems, without any knowledge of underlying system design or operation, is su cient to arbitrarily improve the accuracy of the overall decision pipeline given human supervision over disagreements. We demonstrate this system in two applications: (1) an illustrative example of image classi cation and (2) on large-scale real-world semi-autonomous driving data. For the rst application, we apply this framework to image classi cation achieving a reduction from 8.0% to 2.8% top-5 error on ImageNet. For the second application, we apply this framework to Tesla Autopilot and demonstrate the ability to predict 90.4% of system disengagements that were labeled by human annotators as challenging and needing human supervision.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Successful operation of intelligent automated systems in realworld applications where errors are assigned extremely high costs, such as when the systems are tasked with making life-critical decisions, is one of the grand challenges facing the AI community. The di culty is not within the task itself, but rather in the small margin of allowable error given the human life at stake and the large number of edge cases that have to be accounted for in real-world operation. This challenge has two categories of approaches: (1) improve the accuracy of the system such that it reaches the acceptable level of performance, or (2) integrate the system with a human supervisor that aids its operation such that the combined system of human and machine reach the acceptable level of performance. The former set of approaches has been the focus of the machine learning community. The latter is the focus of this paper.</p><p>We consider the real-world operating paradigm of a black box AI system (termed "primary system") that is tasked with making life-critical decisions. The proposed method integrates the human being into the critical role of resolving uncertainty and disagreement in decisions whose errors are associated with high negative utility values. We demonstrate this system in two applications: <ref type="bibr" target="#b0">(1)</ref> an illustrative example of image classi cation and (2) on large-scale real-world semiautonomous driving data. For the rst application, we show</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arguing Machines</head><p>Disagreement Detected <ref type="figure">Figure 2</ref>: Concept diagram of the arguing machines framework applied to the automated steering task where the primary system is Tesla Autopilot and the secondary system is an end-to-end neural network. When disagreement between the two exceed a threshold, the human supervisor is noti ed and may elect to take control of the vehicle.</p><p>this framework applied to image classi cation achieving an improvement from 8.0% to 2.8% top-5 error on ImageNet over ResNet-50 network (treated as a black box). For the second application, we apply the arguing machines framework to monocular-vision-based automated steering systems. The rst is a proprietary Tesla Autopilot system equipped in the rst generation of Autopilot-capable vehicles. The second is an end-to-end neural network trained on a largescale naturalistic dataset of 420 hours or 45 million frames of autonomous driving in Tesla vehicles. We demonstrate the ability of the overall arguing machines to predict 90.4% of system disengagements that were deemed as "tricky" by human annotators and thus likely to be associated with highprobability of driver injury if not handled by the driver. This paper demonstrates the surprising and impactful nding that the disagreement between two systems, without any knowledge of the design of either system, may have sucient information to signi cantly improve the performance of the overall framework when combined with human supervision. This result has serious implications for the design of e ective and safe human-computer interaction experiences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arguing Machines Concept</head><p>The "black box" nature of AI systems is the property of some machine learning approaches that make it di cult to "see inside" the model inference process that makes a particular decision. This is both due to the inherent di cult of engineering explainable AI systems <ref type="bibr" target="#b13">[13]</ref> and the natural reluctance by companies that provide the AI system to visualize the inner workings of the system and to reveal uncertainty of predictions and system errors. The motivation for this work is that there are applications in which such errors can lead to loss of human life. Errors are inherently part of supervised machine learning systems that seek to generalize from patterns of the past to pattern of the present. It is very di cult to engineer such errors out completely. We propose to instead manage them by integrating the human being as a supervisor. This is important for both creating a safe interaction with an AI system, but also a more e ective human-computer interaction experience that develops an appropriate amount of trust and understanding. <ref type="figure">Fig. 1</ref> shows the arguing machines framework. Consider that there is a primary AI system trained to perform a speci c task. A task is de ned as making a decision based on a well-de ned input. For image classi cation (see ยง3), the task is to take an image as input and make a prediction of likelihood that the image is one of a number of categories. For autonomous steering (see ยง4), the task is to take a sequence of video frames of the forward roadway and make a steering decision. The output of this system is a decision, discrete in the former case and continuous in the latter case. The arguing machines framework introduces a secondary system trained to perform the same task without any interaction with the primary system. The disagreement between the two systems is measured by the arbitrator and passed to a human supervisor if the disagreement exceeds a constant prede ned threshold. This threshold controls the tradeo between the relative amount of human supervision and overall system error as illustrated in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-World Application: Autonomous Driving</head><p>We use image classi cation in ยง3 as an illustrative case study to demonstrate the concept of arguing machines. However, in this work, the central case study of applying the arguing machines framework in the real world is semi-autonomous driving (detailed in ยง4). We chose this application because it is a domain where AI systems are already making hundreds of thousands of life-critical decisions every day in Tesla vehicles equipped with Autopilot <ref type="bibr" target="#b8">[9]</ref> and many other cars equipped with various degrees of automation <ref type="bibr" target="#b9">[10]</ref>. These perception-control systems are black box AI systems that provide very limited communication of system limits, uncertainty, and errors to the driver. Therefore, we believe applying the arguing machines framework in this context may help integrate the human driver in a way that may help save their life.</p><p>As shown in <ref type="figure">Fig. 2</ref>, for the semi-autonomous driving case study, the role of the primary machine is served by the rst generation of Tesla Autopilot software with the perception and steering predictions performed by the integrated Mobileye system <ref type="bibr" target="#b27">[27]</ref>. The role of the secondary machine in this paper is served by an end-to-end convolutional neural network similar to that described and evaluated in <ref type="bibr" target="#b0">[1]</ref> except that our model considers the temporal dynamics of the driving scene by taking as input some aspects of the visual change in the forward-facing video for up to 1 second back in time (see ยง4). The output of both systems is a steering angle. The di erences in those outputs is what constitutes the argument based on which disengagement suggestions and edge case proposals are made. The network model is trained on a balanced dataset constructed through sampling from 420 hours of real-world on-road automated driving by a eet of 16 Tesla vehicles <ref type="bibr" target="#b9">[10]</ref> (see ยง4).</p><p>The central idea proposed in this work is that robustness of the arti cial intelligence system behind the perception and planning necessary for automated driving can be achieved by supplementing the training dataset with edge cases automatically discovered through monitoring the disagreement between multiple machine learning models.</p><p>We implement and deploy the system described in this work to show its capabilities and performance in real-world conditions. Its successful operation is exhibited in an extensive, on-road video demonstration that is made publicly available at https://hcai.mit.edu/arguing-machines. As <ref type="figure" target="#fig_5">Fig. 8</ref> shows, we instrumented a Tesla Model S vehicle with an NVIDIA Jetson TX2 running the neural network based perceptioncontrol system and disagreement function in real-time. The input to the system is a forward-facing monocular camera and the output are steering commands. The large display shows steering commands both from the primary system (Tesla) and secondary system (neural network), and noti es the driver when a disagreement is detected.</p><p>The case studies presented in this paper have associated data, source code, and demonstration videos that are made available on https://hcai.mit.edu/arguing-machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Life-critical and safety-critical systems are those whose failure may result in loss of human life <ref type="bibr" target="#b20">[20]</ref>. Naturally, many domains of real-world human-machine interaction involve risk of injury and loss of life through a long sequence of cause and e ect that is far removed from the initial decisions made by the machine. In this work, we are focusing on applications where a single erroneous decision by an AI system has a high-likelihood of causing direct harm to a human being in a way that does not separate the initial decision from the nal negative result via a chaos of unintended consequences. This latter paradigm is less amenable to analysis <ref type="bibr" target="#b15">[15]</ref>.</p><p>The real-world application data analyzed in this work is from the domain of autonomous vehicle perception-control systems. Other application domains where AI systems make life-critical decision include medicine, nuclear engineering, aviation, and autonomous weapon systems. Medical diagnosis is the process in medicine that is clearly amenable to assistance by AI systems, assuming the speci c diagnosis task can be formalized and digitally grounded in human measurement data. In many cases, this process is life-critical in that a misdiagnosis (incorrect diagnosis) can lead to bodily harm and loss of life <ref type="bibr" target="#b18">[18]</ref>. Such a diagnosis task can be directly formed into an exam classi cation problem, allowing for supervised deep learning methods to be e ectively applied. In exam classi cation, one or multiple images (an exam sample) as input is matched with a single diagnostic variable as output (e.g., disease present or not). <ref type="bibr" target="#b12">[12]</ref> applies deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs. <ref type="bibr" target="#b6">[7]</ref> demonstrates classi cation of skin lesions using a single CNN, trained end-to-end from images directly to predict disease labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble of Neural Networks</head><p>The idea of multiple networks collaborating or competing against each other to optimize an objective have been implemented in various contexts. For example, multiple networks have been combined together in order to improve accuracy <ref type="bibr" target="#b22">[22]</ref> as have traditionally been explored in machine learning as ensembles of classi ers. For deep neural networks, <ref type="bibr" target="#b32">[32]</ref> propose a technique that provides a way of approximately combining exponentially many di erent network architectures. Recent work <ref type="bibr" target="#b14">[14]</ref> combine six models of di erent depth to form an ensemble. <ref type="bibr" target="#b34">[34]</ref> independently trained seven versions of the same network with same initialization, which only di er in sampling methodologies and the randomized input image order. In these approaches, decision-level fusion is performed across many classi ers in order to increase accuracy and robustness of the overall system. Besides, ensemble can also be done on the dataset-level. Early statistical sampling methods such as <ref type="bibr" target="#b5">[6]</ref> can be used to improve the performance and get the con dence interval of a model. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">30]</ref> use the method to test whether the performance of di erent networks is statistically signi cantly di erent, and obtain the con dence interval of error rate. Moreover for computer vision speci cally, various ensemble methods can be done on input-level, such as averaging prediction of ve di erent crops and their horizontal re ections <ref type="bibr" target="#b21">[21]</ref>, multi-scale multi-crop prediction <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b34">34]</ref>, are commonly used to increase accuracy and robustness of the whole system during testing. However, <ref type="bibr" target="#b34">[34]</ref> also note that such terminology may not be necessary in real-world applications, as the bene t of which becomes marginal after a reasonable number.</p><p>Alternatively, generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11]</ref> have two di erent networks working against each other for representation learning and subsequent generation of samples from those learned representations, including generation of steering commands <ref type="bibr" target="#b23">[23]</ref>. Neural networks have also been used in di erent environments at the same time <ref type="bibr" target="#b24">[24]</ref> to learn from them in parallel, or, as in our work, to look at what the disagreement to other systems reveals about the underlying state of the world the networks operate in. Although not directly referred in our work, the above research share the similar idea of using the disagreement between different systems, and indicates that there is much information contained in such disagreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-End Approaches to Driving</head><p>In contrast to modular engineering approaches to self-driving systems, where deep learning only plays a role for the initial scene interpretation step <ref type="bibr" target="#b17">[17]</ref>, it is also possible to approach driving as a more holistic task that can possibly be solved in a data-driven way by a single learner: an end-to-end neural network. First attempts were made almost 30 years ago <ref type="bibr" target="#b28">[28]</ref>, long before the recent GPU-enabled performance breakthroughs in deep learning <ref type="bibr" target="#b21">[21]</ref>.</p><p>A similar, but more modern approach using deeper, convolutional nets has been deployed in an experimental vehicle by NVIDIA <ref type="bibr" target="#b0">[1]</ref>, and further improvements to that were made using various forms of data augmentation <ref type="bibr" target="#b29">[29]</ref> and adapted to the driving context by <ref type="bibr" target="#b37">[37]</ref>. A more advanced approach <ref type="bibr" target="#b36">[36]</ref> formulates autonomous driving as a vehicle egomotion prediction problem, and uses an end-to-end sequence model built upon a scene perception model. They also show that by training scene perception alone as a side task further improves the whole system. Recently, <ref type="bibr" target="#b19">[19]</ref> studies the visual explanations and network's behavior in end-to-end driving, by using a visual attention model to train a convolutional network from images to steering angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ARGUING MACHINES FOR IMAGE CLASSIFICATION ON IMAGENET</head><p>The ImageNet Dataset <ref type="bibr" target="#b4">[5]</ref> and Challenge <ref type="bibr" target="#b30">[30]</ref> has become the standard benchmark for large-scale object recognition, allowing signi cant algorithmic advances in large-scale image recognition and retrieval. Most of the state-of-the-art approaches <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b31">31]</ref> are variants of deep convolutional neural network architectures. However, although signicant strides toward solving the image classi cation problem have been taken, the systems are still far from perfection. We chose image classi cation as the illustrative case study because it is one of the best studied problems in arti cial intelligence, and yet even in this well-studied problem space, we can demonstrate improvement by integrating human supervision via the arguing machines framework.</p><p>If we consider the general process of decision making, aggregating ideas from multiple sources strengthens the generalizability of the decision. A single source is likely to be biased due to factors of data selection or underlying model speci cs. This concept is widely used in machine learning algorithms to improve performance. Despite the fact that deep neural networks models themselves are ensembles of linear functions with non-linear activations, unsupervised ensemble methods such as bootstrap <ref type="bibr" target="#b5">[6]</ref>, bagging <ref type="bibr" target="#b2">[3]</ref>, dropout <ref type="bibr" target="#b32">[32]</ref> and supervised ones such as stacking <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">35]</ref> can be utilized to improve the generalization accuracy of the overall system.</p><p>In this paper we consider the idea that in collaborative decision making, disagreement may contain as much if not more critical information than agreement, especially when the individual decision makers are very good at the task in question. We explore this kind of disagreement in a machine learning scenario, and seek to leverage the information behind such disagreement in order to improve the overall performance of the system..</p><p>In this section, we illustrate the idea of arguing machines with a toy experiment on ImageNet Dataset. The arguing machines framework is proposed as follows. Suppose, there exists a state-of-the-art black-box AI system (primary system) whose accuracy is great but not perfect. In order to safely use or test the system, we propose to have a secondary system that can argue with the primary system. When disagreement arises between two systems, we regard it as a di cult case and mark it as needing human supervision. The purpose of arguing machines is to improve the system performance with minimal human e ort, especially when the primary system is a black-box and gives no other information except the nal output.</p><p>The experiment in this section is a common image classication task. We take two popular image recognition models, VGG <ref type="bibr" target="#b31">[31]</ref> and ResNet <ref type="bibr" target="#b14">[14]</ref>. Speci cally, we treat a single ResNet-50 model as the black-box and a VGG-16 model as an end-to-end deep learning model. The models are pretrained and we obtain the prediction results from single center-cropped images in the ImageNet validation set.</p><p>The arguing machines arbitrator detects the disagreement when the top predictions of two systems di er. In this experiment, ResNet and VGG disagree on images, which is 23.3% of the whole validation set. For the results of arguing machines, we assume with human taking look at the disagreement cases, the classi cation is always correct. We also propose a baseline method that with the same amount of images send to human veri cation (always correct), but randomly selected. We evaluate both the top-1 error and the top-5 error. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. The results show that with the arguing machines framework, the performance of a state-of-the-art image recognition system can be signi cantly improved, even when we treat it as a black-box system. <ref type="table" target="#tab_1">Table 2</ref> shows the analysis of arguing machines in this context. With less than a quarter of images veri ed by a human supervisor, the arguing machines framework is able to detect more than half of the failure cases in both top-1 and top-5 tasks, even given the fact that both systems already have very strong performance. Such results also indicate that although two deep convolutional neural networks are trained on the same dataset, with similar architectures featuring a combination of convolutional layers, fully connected layers, dropout layers, etc., the behavior of the two trained systems is quite di erent, as they do not fail the same way during testing. This is a surprising and fascinating result that reveals the predictive power of disagreement between arti cial intelligence systems.</p><p>The precision of top-5 classi cation is much lower than top-1, because the two systems can be both correct even if they disagree on the top prediction. However the recall for both top-1 and top-5 tasks are consistently high, indicating that even with the simpler classi cation task, where systems fail less often, the arguing machines framework can still detect many of the failure cases with disagreements and in so doing signi cantly reduce the error. Examples of disagreements between the primary and secondary systems on the image classi cation task are shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. More examples, including disagreement over object detection and classi cation in video, are available online at https://hcai.mit.edu/arguing-machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ARGUING MACHINES FOR SEMI-AUTONOMOUS DRIVING</head><p>Software is taking on greater operational control in modern vehicles and in so doing is opening the door to machine learning. These approaches are fundamentally hungry for data, based on which, they aim to take on the higher level perception and planning tasks. As an example, over 15 million vehicles worldwide are equipped with Mobileye computer vision technology <ref type="bibr" target="#b33">[33]</ref>, including the rst generation Autopilot system that serves as the "primary machine" in this work. Given the requirement of extremely low error rates and need to generalize over countless edge cases, large-scale annotated data is essential to making these approaches work in real-world conditions. In fact, for driving, training data representative of all driving situations may be more important than incremental improvements in perception, control, and planning algorithms. Tesla, as an example, is acknowledging this need by asking its owners to share data with the company for the explicit purpose of training the underlying machine learning models. Our work does precisely this, applying end-to-end neural network approaches to training on large-scale, semi-autonomous, real-world driving data. The resulting model serves as an observer and critic of the primary system with the goals of (1) discovering edge cases in the o ine context and (2) bringing the human back into the loop when needed in the online context.</p><p>We perform two evaluations in our application of arguing machines to semi-autonomous driving. First, we evaluate the ability of the end-to-end network to predict steering angles commensurate with real-world steering angles that were used to keep the car in its lane. For this, we use distinct periods of automated lane-keeping during Autopilot engagement as the training and evaluation datasets. Second, we evaluate the ability of an argument arbitrator (termed "disagreement function") to estimate, based on a short time window, the likelihood that a transfer of control is initiated, whether by the human driver (termed "human-initiated") or the Autopilot system itself (termed "machine-initiated"). We have 6,500 total disengagements in our dataset. All disengagements (whether human-initiated or machine-initiated) are considered to be representative of cases where the visual characteristics of the scene (e.g., poor lane markings, complex lane mergers, light variations) were better handled by a human operator. Therefore, we chose to evaluate the disagreement function by its ability to predict these disengagements, which it is able to do with 90.4% accuracy (see <ref type="figure" target="#fig_3">Fig. 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naturalistic Driving Dataset</head><p>The dataset used for the training and evaluation of the end-toend steering network model comprising the "secondary machine" is taken from a large-scale naturalistic driving study of semi-autonomous vehicle technology <ref type="bibr" target="#b9">[10]</ref>. Speci cally, we used 420 hours of driving data where a Tesla Autopilot system was controlling both the longitudinal and lateral movement of the vehicle.</p><p>This subset of the full naturalistic driving dataset served as ground truth for automated lane keeping. In other words, given the operational characteristics of Autopilot, we know that the vehicle only leaves the lane in two situations: <ref type="bibr" target="#b0">(1)</ref> during automated lane changes and (2) as part of a "disengagement" where the driver elects or is forced to take back control of the vehicle. We have the full enumeration of both scenarios. The latter is of particular interest to the task of arguing machines, as one indication of a valuable disagreement is one that is associated with a human driver feeling su ciently uncomfortable to elect to take back control of the vehicle. There are 6,500 such instances of disengagement that are used for evaluating the ability of the disagreement function to discover edge cases and challenging driving scenarios as discussed in ยง4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-End Learning of the Steering Task</head><p>Our model, which is inspired by <ref type="bibr" target="#b0">[1]</ref> uses 5 convolutional layers, the rst 3 with a stride of 2 ร 2 and 5 ร 5 kernels and the remaining 2 keeping the same stride, while switching to smaller 3 ร 3 kernels. On top of that, we add 4 fully connected layers going down to output sizes of 100, 50, 10, and 1, respectively. Throughout the net ReLU activations <ref type="bibr" target="#b25">[25]</ref> are used on the layers. In addition, we use Dropout <ref type="bibr" target="#b21">[21]</ref> as regularization technique on the fully connected layers. The net is trained using an RMSprop <ref type="bibr" target="#b16">[16]</ref> optimizer minimizing the mean squared error between predicted and actual steering angle.</p><p>Since a large part of driving -and therefore also our dataset -consists of going straight, we had to speci cally select input images to remove that imbalance, and resulting bias towards lower steering angle values the net would learn otherwise.  To accomplish this dataset balancing task, we calculate a threshold using the minimum number of available frames in steering angle ranges of one degree. This threshold is then used within the range of interest of [โ10 โข , 10 โข ] steering angle to allow at max threshold frames get selected to achieve a balance. This results in about 100,000 training and 50,000 validation frames. For the input to the neural network we considered 5 different preprocessing methods (see <ref type="figure" target="#fig_1">Fig. 4</ref>) -referenced as M1 -M5 in the following sections -each producing a 256 ร image with 3 channels. M5 uses the method proposed in <ref type="bibr" target="#b0">[1]</ref> as a comparison, consisting of the RGB channels of a single frame. M4 uses the same single frame, but precomputes edges on each color channel.</p><p>To improve the accuracy beyond that, for input methods M1 to M3 we use a temporal component, meaning multiple frames, to improve situation awareness. M3, in addition to the current frame, also looks 10 and 20 frames back and provides the grayscale version of them as the input image channels. M2 goes beyond that and, in addition to using multiple frames as input, also subtracts them from each other, which helps with an implicit input normalization, as well as automatically highlighting the important moving parts like lane markings. The exact mathematical formulation of the input is:</p><formula xml:id="formula_0">I t = {(F t โ F t โ10 ), (F t โ F t โ5 ), (F t โ F t โ1 )}<label>(1)</label></formula><p>where I t and F t are the input to the neural network and the video frame at time t. The unit of time is 1 video frame or 33.3 milliseconds given the 30 fps video used in this work. In <ref type="bibr" target="#b0">(1)</ref>, each channel is based on the current frame, but also incorporates a " ashback" to another frame further back. M1 does not use " ashbacks", but instead looks at the changes that happened over a series of time segments -each 10 frames long, as follows:</p><formula xml:id="formula_1">I t = {(F iโ20 โ F iโ30 ), (F t โ10 โ F t โ20 ), (F t โ F t โ10 )} (2)</formula><p>To evaluate the network using the di erent preprocessing methods, we compute the mean absolute steering angle error over the validation set. The results are shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. Precomputing edges (M4) already leads to improved performance over just supplying the RGB image (M5), and providing temporal context (M1-M3) does even better, with " ashbacks" (M2) performing better than just providing multiple frames (M3), and comparing time segments (M1) performing best. For the evaluation of the disagreement function in ยง4, we use M1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disagreement Function and Edge Case Discovery</head><p>The goal for the disagreement function is to compare the steering angle suggested by the "primary machine" (Autopilot) and the "secondary machine" (neural network) and based on this comparison to make a binary classi cation of whether : Implementation and evaluation of the system presented in this paper. The primary perception-control system is Tesla Autopilot. The secondary perception-control system is an end-to-end neural network. We equipped a Tesla Model S vehicle with a monocular camera, an NVIDIA Jetson TX2, and an LCD display that shows the steering commands from both systems, the temporal di erence input to the neural network, and (in red text) a notice to the driver when a disagreement is detected.</p><p>the current situation is a challenging driving situation or not.</p><p>The disagreement function can take many forms including modeling the underlying entropy of the disagreement, but the function computed and evaluated in this work purposefully took on a simple form through the following process:</p><p>(1) Normalize the steering angle for both the primary and secondary machines to be in [โ1, 1] normalized by the range [โ10, 10] and all angles exceeding the range are set to the range limits. (2) Compute the di erence between the normalized steering suggestions and sum them over a window of 1 second (or 30 samples). (3) Make the binary classi cation decision based on a disagreement threshold ฮด .</p><p>The metrics used for evaluating the performance of the disagreement system are false accept rate (FAR) and false reject rate (FRR). Where the detection event of interest is the Autopilot disengagement. In other words, an "accept" is a prediction that this moment in time is likely to be associated with a disengagement and can thus be considered an edge case for the machine learning system. A "reject" is a prediction that this moment in time is not likely to be associated with a disengagement. In order to compute FAR and FRR measure for a given value of ฮด , we use classi cation windows evenly sampled from disengagement periods and non-disengagement periods. A disengagement period is de ned as the 5 seconds leading up to a disengagement and 1 second following it.</p><p>The illustrative example in <ref type="figure" target="#fig_4">Fig. 7</ref> shows the temporal dynamics of the two steering suggestions, the resulting disagreement, and the role of ฮด in marking that moment leading up to the disengagement as an edge case. The ROC curve in <ref type="figure" target="#fig_3">Fig. 6</ref> shows, by varying ฮด , that the optimal mean error rate is 0.096, and is achieved when ฮด = 10. This means that given any 1 second period of Autopilot driving in our test dataset, the di erence function can predict whether a disengagement will happen in the next 5 seconds with 90.4% accuracy. This is a promising result that motivates further evaluation of the predictive power of the disagreement function both on a larger dataset of Autopilot driving and in real-world on-road testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On-Road Deployment</head><p>As part of exploring and validating the concept of arguing machines we also built a version that runs real time inside a car. This system consists of a NVIDIA Jetson TX2 to run the model, a 23 inch high resolution screen for the human interface attached over the center stack of a Tesla Model S with Autopilot version 1, a custom interface to connect to the vehicle CAN bus to get its current steering angle and a dashboard-mounted Logitech C920 camera capturing the forward roadway scene at 720p resolution at 30fps.</p><p>The system uses OpenCV's camera capture module <ref type="bibr" target="#b1">[2]</ref> to get a live, real-time video stream of the road scene from the C920 camera. The captured image is stored in a shortterm, dynamic, temporally-sorted bu er structure and uses that bu er structure to assemble the right combination of frames. In this case, we used our best network model (M1), where frames that are 10, 20 and 30 frames back in time are combined with the current frame to compute the input for the neural network. The in-car neural network uses the same network layout as described above, running an optimized PyTorch <ref type="bibr" target="#b26">[26]</ref> implementation on the Jetson TX2's Tegra Parker SoC with a Pascal GPU compute chip. The steering angle computed by the neural network and the one captured from Tesla's autopilot system are then fed into the actual disagreement measurement routine and additionally displayed on the center stack mounted screen. In addition, in case of a severe disagreement, the system also displays a "disagreement detected" warning on the same screen.</p><p>Even though this system is a proof of concept, it achieves a latency from camera input to screen GUI update of less than 200 milliseconds, while performing neural network inference in real time. During an on-road demonstration during evening rush hour it appears to work reliably and help to warn the driver of oncoming di cult situations in multiple instances. The video of the demonstration is available online at https://hcai.mit.edu/arguing-machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This work proposes a framework for integrating a human supervisor into the decision making process of a black box AI system that is tasked with making life critical decision. We demonstrate this framework in two applications: (1) an illustrative example of image classi cation and (2) on largescale real-world semi-autonomous driving data. For the rst application, we apply this framework to image classi cation achieving a reduction from 8.0% to 2.8% top-5 error on Ima-geNet. For the second application, we apply this framework to Tesla Autopilot and demonstrate the ability to predict 90.4% of system disengagements that were labeled by human annotators as challenging and needing human supervision. Finally, we implement, deploy, and demonstrate our system in a Tesla Model S vehicle operating in real-world conditions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>ImageNet examples where the primary system (ResNet) and secondary system (VGG) disagree on the image classication task. The ground truth and correct classi cations are shown in blue. Incorrect classi cations are shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Visualization on one illustrative example of each of the 5 neural network preprocessing models evaluated in this paper. SeeFig. 5for mean absolute error achieved by each model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The mean absolute error achieved by each of the 5 models illustrated inFig. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>The tradeo between false accept rate (FAR) and false reject rate (FRR) achieved by varying the constant threshold used to make the binary disagreement classi cation. The red circle designates a threshold of 10 that is visualization on an illustrative example inFig. 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Illustrative example showing snapshots of the forward roadway, plots of the steering angles suggested by the primary machine (black line) and secondary machine (blue line), and a plot of the disagreement function along with a threshold value of 10 that corresponds to the red circle inFig. 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8</head><label>8</label><figDesc>Figure 8: Implementation and evaluation of the system presented in this paper. The primary perception-control system is Tesla Autopilot. The secondary perception-control system is an end-to-end neural network. We equipped a Tesla Model S vehicle with a monocular camera, an NVIDIA Jetson TX2, and an LCD display that shows the steering commands from both systems, the temporal di erence input to the neural network, and (in red text) a notice to the driver when a disagreement is detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on ImageNet-val set.</figDesc><table><row><cell>Method</cell><cell>Top-1 Error (%)</cell><cell>Top-5 Error (%)</cell></row><row><cell>ResNet-50 (primary system)</cell><cell>25.2</cell><cell>8.0</cell></row><row><cell>VGG-16 (secondary system)</cell><cell>29.0</cell><cell>10.1</cell></row><row><cell cols="2">Ensemble: ResNet-50, VGG-16 24.4</cell><cell>7.8</cell></row><row><cell>Random Arbitrator</cell><cell>19.3</cell><cell>6.2</cell></row><row><cell>Arguing Machines</cell><cell>10.7</cell><cell>2.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance analysis of arguing machines.</figDesc><table><row><cell>Task</cell><cell cols="2">Precision (%) Recall (%)</cell></row><row><cell cols="2">Top-1 Classi cation 62.4</cell><cell>57.6</cell></row><row><cell cols="2">Top-5 Classi cation 22.2</cell><cell>64.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariusz</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><forename type="middle">Del</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasoon</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiakai</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The OpenCV Library. Dr. Dobb&apos;s Journal: Software Tools for the Professional Programmer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="120" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stacked regressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="49" to="64" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bootstrap methods: another look at the jackknife</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bradley Efron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="569" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dermatologistlevel classi cation of skin cancer with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lex</forename><surname>Fridman</surname></persName>
		</author>
		<ptr target="https://hcai.mit.edu/tesla-autopilot-miles/" />
		<title level="m">Tesla Vehicle Deliveries and Autopilot Mileage Statistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">MIT Autonomous Vehicle Technology Study: Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with Automation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lex</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Angell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spencer</forename><surname>Dodd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Jenik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Terwilliger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kindelsberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Seaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hillary</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alea</forename><surname>Mehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Sipperley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Pettinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Angell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Mehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Reimer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06976</idno>
		<ptr target="https://arxiv.org/abs/1711.06976" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lily</forename><surname>Varun Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arunachalam</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kasumi</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Widner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Madams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuadros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Defense Advanced Research Projects Agency (DARPA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gunning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Explainable arti cial intelligence (xai). nd Web</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sea gulls, butter ies, and grasshoppers: A brief history of the butter y e ect in nonlinear dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hilborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="425" to="427" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural Networks for Machine Learning Lecture 6a Overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Rey Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirsh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameep</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Je</forename><surname>Kiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Pazhayampallil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toki</forename><surname>Migimatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Royce</forename><surname>Cheng-Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01716</idno>
		<title level="m">An empirical evaluation of deep learning on highway driving</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Misdiagnosis of lung cancer in a 2000 consecutive autopsy study in Budapest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kendrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lapis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hargitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fj Roe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">General &amp; diagnostic pathology</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="169" to="178" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Safety critical systems: challenges and directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24rd International Conference on</title>
		<meeting>the 24rd International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="547" to="550" />
		</imprint>
	</monogr>
	<note>Software Engineering</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet Classi cation with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo Rey E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural network ensembles, cross validation, and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Vedelsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="231" to="238" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imitating driver behavior with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kue Er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykel</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recti ed linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Rey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
		<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Tesla AutoPilot -An In-Depth Look At The Technology Behind the Engineering Marvel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usman</forename><surname>Pirzada</surname></persName>
		</author>
		<ptr target="http://wccftech.com/tesla-autopilot-story-in-depth-technology" />
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ALVINN: An Autonomous Land Vehicle in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pomerleau</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>1, D. S. Touretzky</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="305" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stรฉphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from over tting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Rey Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Mano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<title level="m">Collision warning system</title>
		<imprint>
			<date type="published" when="2015-10-27" />
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page">868</biblScope>
		</imprint>
	</monogr>
	<note>US Patent 9</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.4842" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-To-End Learning of Driving Models From Large-Scale Video Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Query-E cient Imitation Learning for End-to-End Simulated Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiakai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14590" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Arti cial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Arti cial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="2891" to="2897" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
