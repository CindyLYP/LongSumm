<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">nocaps: novel object captioning at scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal˚1</surname></persName>
							<email>hagrawal9@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
							<email>mark.johnson@mq.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai˚1</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<email>dbatra@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
							<email>yufei.wang@mq.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Macquarie University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<email>parikh@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
							<email>xinleic@fb.com</email>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Oregon State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Jain</surname></persName>
							<email>rishabhjain@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
							<email>peter.anderson@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">nocaps: novel object captioning at scale</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Image captioning models have achieved impressive results on datasets containing limited visual concepts and large amounts of paired image-caption training data. However, if these models are to ever function in the wild, a much larger variety of visual concepts must be learned, ideally from less supervision. To encourage the development of image captioning models that can learn visual concepts from alternative data sources, such as object detection datasets, we present the first large-scale benchmark for this task. Dubbed &apos;nocaps&apos;, for novel object captioning at scale, our benchmark consists of 166,100 human-generated captions describing 15,100 images from the Open Images validation and test sets. The associated training data consists of COCO image-caption pairs, plus Open Images imagelevel labels and object bounding boxes. Since Open Images contains many more classes than COCO, nearly 400 object classes seen in test images have no or very few associated training captions (hence, nocaps). We extend existing novel object captioning models to establish strong baselines for this benchmark and provide analysis to guide future work.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent progress in image captioning, the task of generating natural language descriptions of visual content <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref>, can be largely attributed to the publicly available large-scale datasets of image-caption pairs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b49">50]</ref> as well as steady modeling improvements <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b47">48]</ref>. However, these models generalize poorly to images in the wild <ref type="bibr">[39]</ref> despite impressive benchmark performance, because they are trained on datasets which cover a tiny fraction of the long-tailed distribution of visual concepts in the real world. For example, models trained on COCO Captions <ref type="bibr" target="#b5">[6]</ref> can typically describe images containing dogs, people and umbrellas, but not accordions or dolphins. This  limits the usefulness of these models in real-world applications, such as providing assistance for people with impaired vision, or for improving natural language query-based image retrieval.</p><p>To generalize better 'in the wild', we argue that captioning models should be able to leverage alternative data sources -such as object detection datasets -in order to describe objects not present in the caption corpora on which they are trained. Such objects which have detection annotations but are not present in caption corpora are referred to as novel objects and the task of describing images containing novel objects is termed novel object captioning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr">49]</ref>. Until now, novel object captioning approaches have been evaluated using a proof-ofconcept dataset introduced in <ref type="bibr" target="#b13">[14]</ref>. This dataset has restric-1 arXiv:1812.08658v3 [cs.CV] 30 Sep 2019 tive assumptions -it contains only 8 novel object classes held out from the COCO dataset <ref type="bibr" target="#b14">[15]</ref>, deliberately selected to be highly similar to existing ones (e.g. horse is seen, zebra is novel). This has left the large-scale performance of these methods open to question. Given the emerging interest and practical necessity of this task, we introduce nocaps, the first large-scale benchmark for novel object captioning, containing nearly 400 novel object classes.</p><p>In detail, the nocaps benchmark consists of a validation and test set comprised of 4,500 and 10,600 images, respectively, sourced from the Open Images object detection dataset <ref type="bibr" target="#b19">[20]</ref> and annotated with 11 human-generated captions per image (10 reference captions for automatic evaluation plus a human baseline). Crucially, we provide no additional paired image-caption data for training. Instead, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, training data for the nocaps benchmark is image-caption pairs from the COCO Captions 2017 <ref type="bibr" target="#b5">[6]</ref> training set (118K images containing 80 object classes), plus the Open Images V4 object detection training set (1.7M images annotated with bounding boxes for 600 object classes and image labels for 20K categories).</p><p>To be successful, image captioning models may utilize COCO paired image-caption data to learn to generate syntactically correct captions, while leveraging the massive Open Images detection dataset to learn many more visual concepts. Our key scientific goal is to disentangle 'how to recognize an object' from 'how to talk about it'. After learning the name of a novel object, a human can immediately talk about its attributes and relationships. It is therefore intellectually dissatisfying that existing models, having already internalized a huge number of caption examples, can't also be taught new objects. As with previous work, this task setting is also motivated by the observation that collecting human-annotated captions is resource intensive and scales poorly as object diversity grows, while on the other hand, large-scale object classification and detection datasets already exist <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref> and their collection can be massively scaled, often semi-automatically <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>To establish the state-of-the-art on our challenging benchmark, we evaluate two of the best performing existing approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref> and report their performance based on well-established evaluation metrics -CIDEr <ref type="bibr" target="#b40">[41]</ref> and SPICE <ref type="bibr" target="#b0">[1]</ref>. To provide finer-grained analysis, we further break performance down over three subsetsin-domain, near-domain and out-of-domain-corresponding to the similarity of depicted objects to COCO classes. While these models do improve over a baseline model trained only on COCO Captions, they still fall well short of human performance on this task -indicating there is still work to be done to scale to 'in-the-wild' image captioning. In summary, we make three main contributions: -We collect nocaps -the first large-scale benchmark for novel object captioning, containing "400 novel objects.</p><p>-We undertake a detailed investigation of the performance and limitations of two existing state-of-the-art models on this task and contrast them against human performance. -We make improvements and suggest simple heuristics that improve the performance of constrained beam search significantly on our benchmark. We believe that improvements on nocaps will accelerate progress towards image captioning in the wild. We are hosting a public evaluation server on EvalAI <ref type="bibr" target="#b46">[47]</ref> to benchmark progress on nocaps. For reproducibility and to spur innovation, we have also released code to replicate our experiments at: https://github.com/nocaps-org.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Novel Object Captioning Novel object captioning includes aspects of both transfer learning and domain adaptation <ref type="bibr" target="#b7">[8]</ref>. Test images contain previously unseen, or 'novel' objects that are drawn from a target distribution (in this case, Open Images <ref type="bibr" target="#b19">[20]</ref>) that differs from the source/training distribution (COCO <ref type="bibr" target="#b5">[6]</ref>). To obtain a captioning model that performs well in the target domain, the Deep Compositional Captioner <ref type="bibr" target="#b14">[15]</ref> and its extension, the Novel Object Captioner <ref type="bibr" target="#b41">[42]</ref>, both attempt to transfer knowledge by leveraging object detection datasets and external text corpora by decomposing the captioning model into visual and textual components that can be trained with separate loss functions as well as jointly using the available image-caption data.</p><p>Several alternative approaches elect to use the output of object detectors more explicitly. Two concurrent works, Neural Baby Talk <ref type="bibr" target="#b26">[27]</ref> and the Decoupled Novel Object Captioner <ref type="bibr" target="#b44">[45]</ref>, take inspiration from Baby Talk <ref type="bibr" target="#b20">[21]</ref> and propose neural approaches to generate slotted caption templates, which are then filled using visual concepts identified by modern state-of-the-art object detectors. Related to Neural Baby Talk, the LSTM-C [49] model augments a standard recurrent neural network sentence decoder with a copying mechanism which may select words corresponding to object detector predictions to appear in the output sentence.</p><p>In contrast to these works, several approaches to novel object captioning are architecture agnostic. Constrained Beam Search <ref type="bibr" target="#b1">[2]</ref> is a decoding algorithm that can be used to enforce the inclusion of selected words in captions during inference, such as novel object classes predicted by an object detector. Building on this approach, partiallyspecified sequence supervision (PS3) <ref type="bibr" target="#b2">[3]</ref> uses Constrained Beam Search as a subroutine to estimate complete captions for images containing novel objects. These complete captions are then used as training targets in an iterative algorithm inspired by expectation maximization (EM) <ref type="bibr" target="#b8">[9]</ref>.</p><p>In this work, we investigate two different approaches: Neural Baby Talk (NBT) <ref type="bibr" target="#b26">[27]</ref> and Constrained Beam Search (CBS) <ref type="bibr" target="#b1">[2]</ref> on our challenging benchmark -both of which recently claimed state-of-the-art on the proof-of- <ref type="figure">Figure 2</ref>: Compared to COCO Captions <ref type="bibr" target="#b5">[6]</ref>, on average nocaps images have more object classes per image (4.0 vs. 2.9), more object instances per image (8.0 vs. 7.4), and longer captions (11 words vs. 10 words). These differences reflect both the increased diversity of the underlying Open Images data <ref type="bibr" target="#b19">[20]</ref>, and our image subset selection strategy (refer Section 3.1).</p><p>concept novel object captioning dataset <ref type="bibr" target="#b14">[15]</ref>.</p><p>Image Caption Datasets In the past, two paradigms for collecting image-caption datasets have emerged: direct annotation and filtering. Direct-annotated datasets, such as Flickr 8K <ref type="bibr" target="#b15">[16]</ref>, Flickr 30K <ref type="bibr" target="#b49">[50]</ref> and COCO Captions <ref type="bibr" target="#b5">[6]</ref> are collected using crowd workers who are given instructions to control the quality and style of the resulting captions. To improve the reliability of automatic evaluation metrics, these datasets typically contain five or more captions per image. However, even the largest of these, COCO Captions, is based only on a relatively small set of 80 object classes. In contrast, filtered datasets, such as Im2Text <ref type="bibr" target="#b28">[29]</ref>, Pinterest40M <ref type="bibr" target="#b27">[28]</ref> and Conceptual Captions <ref type="bibr" target="#b37">[38]</ref>, contain large numbers of image-caption pairs harvested from the web. These datasets contain many diverse visual concepts, but are also more likely to contain non-visual content in the description due to the automated nature of the collection pipelines. Furthermore, these datasets lack human baselines, and may not include enough captions per image for good correlation between automatic evaluation metrics and human judgments <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Our benchmark, nocaps, aims to fill the gap between these datasets, by providing a high-quality benchmark with 10 reference captions per image and many more visual concepts than COCO. To the best of our knowledge, nocaps is the only image captioning benchmark in which humans outperform state-of-the-art models in automatic evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">nocaps</head><p>In this section, we detail the nocaps collection process, constrast it with COCO Captions <ref type="bibr" target="#b5">[6]</ref>, and introduce the evaluation protocol and benchmark guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Caption Collection</head><p>The images in nocaps are sourced from the Open Images V4 <ref type="bibr" target="#b19">[20]</ref> validation and test sets. <ref type="bibr" target="#b0">1</ref> Open Images is currently the largest available human-annotated object detection dataset, containing 1.9M images of complex scenes annotated with object bounding boxes for 600 classes (with an average of 8.4 object instances per image in the training set). Moreover, out of the 500 classes that are not overly broad (e.g. 'clothing') or infrequent (e.g. 'paper cutter'), nearly 400 are never or rarely mentioned in COCO Captions <ref type="bibr" target="#b5">[6]</ref> (which we select as image-caption training data), making these images an ideal basis for our benchmark.</p><p>Image Subset Selection Since Open Images is primarily an object detection dataset, a large fraction of images contain well-framed iconic perspectives of single objects. Furthermore, the distribution of object classes is highly unbalanced, with a long-tail of object classes that appear relatively infrequently. However, for image captioning, images containing multiple objects and rare object co-occurrences are more interesting and challenging. Therefore, we select subsets of images from the Open Images validation and test splits by applying the following sampling procedure.</p><p>First, we exclude all images for which the correct image rotation is non-zero or unknown. Next, based on the ground-truth object detection annotations, we exclude all images that contain only instances from a single object category. Then, to capture as many visually complex images as possible, we include all images containing more than 6 unique object classes. Finally, we iteratively select from the remaining images using a sampling procedure that encourages even representation both in terms of object classes and image complexity (based on the number of unique classes per image). Concretely, we divide the remaining images into 5 pools based on the number of unique classes present in the image (from 2-6 inclusive). Then, taking each pool in turn, we randomly sample n images and among these, we select the image that when added to our benchmark results in the highest entropy over object classes. This prevents nocaps from being overly dominated by frequently occurring object classes such as person, car or plant. In total, we select 4,500 validation images (from a total of 41,620 images in Open Images validation set) and 10,600 test images (from a total of 125,436 images in Open Images test set). On average, the selected images contain 4.0 object classes and 8.0 object instances each (see <ref type="figure">Figure 2</ref>). Collecting Image Captions from Humans To evaluate model-generated image captions, we collected 11 English captions for each image from a large pool of crowd-workers Priming: Some people enjoying a nice ride on a gondola with a tree behind them.</p><p>Priming: A red panda is sitting in grass next to a tree. on Amazon Mechanical Turk (AMT). Out of 11 captions, we randomly sample one caption per image to establish human performance on nocaps and use the remaining 10 captions as reference captions for automatic evaluation. Prior work suggests that automatic caption evaluation metrics correlate better with human judgment when more reference captions are provided <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref>, motivating us to collect more reference captions than COCO (only 5 per image).</p><p>Our image caption collection interface closely resembles the interface used for collection of the COCO Captions dataset, albeit with one important difference. Since the nocaps dataset contains more rare and fine-grained classes than COCO, in initial pilot studies we found that human annotators could not always correctly identify the objects in the image. For example, as illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, a red panda was incorrectly described as a brown rodent. We therefore experimented with priming workers by displaying the list of ground-truth object classes present in the image. To minimize the potential for this priming to reduce the language diversity of the resulting captions, the object classes were presented as 'keywords', and workers were explicitly instructed that it was not necessary to mention all the displayed keywords. To reduce clutter, we did not display object classes which are classified in Open Images as parts, e.g. human hand, tire, door handle. Pilot studies comparing captions collected with and without priming demonstrated that primed workers produced more qualitative accurate and descriptive captions (see <ref type="figure" target="#fig_3">Figure 3)</ref>. Therefore, all nocaps captions, including our human baselines, were collected using this priming-modified COCO collection interface.</p><p>To help maintain the quality of the collected captions, we used only US-based workers who had completed at least 5K previous tasks on AMT with more than 95% approval rate. We also spot-checked the captions written by each worker and blocked workers providing low-quality captions. Captions written by these workers were then discarded and replaced with captions written by high-quality workers. Over-  all, 727 workers participated, writing 228 captions each on average for a grand total of 166,100 captions of nocaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset Analysis</head><p>In this section, we compare our nocaps benchmark to COCO Captions <ref type="bibr" target="#b5">[6]</ref> in terms of both image content and caption diversity. Based on ground-truth object detection annotations, nocaps contains images spanning object classes, while COCO contains only 80. Consistent with this greater visual diversity, nocaps contains more object classes per image (4.0 vs 2.9), and slightly more object instances per image (8.0 vs 7.4) as shown in <ref type="figure">Figure 2</ref>. Further, nocaps contains no iconic images containing just one object class, whereas 20% of the COCO dataset consists of such images. Similarly, less than 10% of COCO images contain more than 6 object classes, while such images constitute almost 22% of nocaps.</p><p>Although priming the workers with object classes as keywords during data collection has the potential to reduce language diversity, nocaps captions are nonetheless more diverse than COCO. Since nocaps images are visually more complex than COCO, on average the captions collected to describe these images tend to be slightly longer (11 words vs. 10 words) and more diverse than the captions in the COCO dataset. As illustrated in <ref type="table" target="#tab_1">Table 1</ref>, taking uniformly random samples over the same number of images and captions in each dataset, we show that not only do nocaps captions utilize a larger vocabulary than COCO captions reflecting the increased number of visual concepts present. The number of unique 2, 3 and 4-grams is also significantly higher for nocaps-suggesting a greater variety of unique language compositions as well.</p><p>Additionally, we compare visual and linguistic similarity between COCO, in-domain and out-of-domain in <ref type="figure" target="#fig_4">Figure 4</ref>. We observe that in-domain classes shows high visual similarity to equivalent COCO classes (e.g. cat, book) while many out-of-domain classes are visually and linguistically different from in-domain classes (e.g. jellyfish, beetle, cello). out-of-domain also covers many visually and linguistically similar concepts to COCO but rarely described in COCO (e.g. tiger, lemon)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation</head><p>The aim of nocaps is to benchmark progress towards models that can describe images containing visually novel concepts in the wild by leveraging other data sources. To facilitate evaluation and avoid exposing the novel object captions, we host an evaluation server for nocaps on EvalAI <ref type="bibr" target="#b46">[47]</ref> -as such, we put forth these guidelines for using nocaps: -Do not use additional paired image-caption data collected from humans. Improving evaluation scores by leveraging additional human-generated paired imagecaption data is antithetical to this benchmark -the only paired image-caption dataset that should be used is the COCO Captions 2017 training split. However, external text corpora, knowledge bases, and object detection datasets may be used during training or inference. -Do not leverage ground truth object annotations. We note that ground-truth object detection annotations are available for Open Images validation and test splits (and hence, for nocaps). While ground-truth annotations may be used to establish performance upper bounds on the validation set, they should never be used in a submission to the evaluation server unless this is clearly disclosed. We anticipate that researchers may wish to investigate the limits of performance on nocaps without any restraints on the training datasets. We therefore maintain a separate leaderboard for this purpose "nocaps (XD)" 2 leaderboard. Metrics As with existing captioning benchmarks, we rely on automatic metrics to evaluate the quality of modelgenerated captions. We focus primarily on CIDEr <ref type="bibr" target="#b40">[41]</ref> and SPICE <ref type="bibr" target="#b0">[1]</ref>, which have been shown to have the strongest correlation with human judgments <ref type="bibr" target="#b24">[25]</ref> and have been used in prior novel object captioning work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27</ref>], but we also report Bleu <ref type="bibr" target="#b31">[32]</ref>, Meteor <ref type="bibr" target="#b21">[22]</ref> and ROUGE <ref type="bibr" target="#b23">[24]</ref>. These metrics test whether models mention novel objects accurately <ref type="bibr" target="#b42">[43]</ref> as well as describe them fluently <ref type="bibr" target="#b21">[22]</ref>.It is worth XD stands for "extra data" near-domain out-of-domain 6. An army tank is parked at a gas station. 7. The standing camel is near a sitting one with a man on its back.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.</head><p>A land vehicle is parked in a gas station fueling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Someone is sitting on a camel</head><p>and is in front of another camel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8.</head><p>A large military vehicle at the gas pump of a gas station. 9. Two camels in the dessert and a man sitting on the sitting one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9.</head><p>A tanker parked outside of an old gas station 10. Two camels are featured in the sand with a man sitting on one of the seated camels.</p><p>10. Multiple military vehicles getting gasoline at a civilian gas station. noting that the absolute scale of these metrics is not comparable across datasets due to the differing number of reference captions and corpus-wide statistics. Evaluation Subsets We further break down performance on nocaps over three subsets of the validation and test splits corresponding to varied 'nearness' to COCO. To determine these subsets, we manually map the 80 COCO classes to Open Images classes. We then select an additional 39 Open Images classes that are not COCO classes, but are nonetheless mentioned more than 1,000 times in the COCO captions training set (e.g. 'table', 'plate' and 'tree'). We classify these 119 classes as in-domain relative to COCO. There are 87 Open Images classes that are not present in nocaps <ref type="bibr" target="#b2">3</ref> . The remaining 394 classes are outof-domain. Image subsets are then determined as follows: in-domain images contain only objects belonging to indomain classes. Since these objects have been described in the paired image-caption training data, we expect caption models trained only on COCO to perform reasonably well on this subset, albeit with some negative impact due </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To provide an initial measure of the state-of-the-art on nocaps, we extend and present results for two contemporary approaches to novel object captioning -Neural Baby Talk (NBT) <ref type="bibr" target="#b26">[27]</ref> and Constrained Beam Search (CBS) <ref type="bibr" target="#b1">[2]</ref> inference method which we apply both to NBT and to the popular UpDown captioner <ref type="bibr" target="#b3">[4]</ref>. We briefly recap these approaches for completeness but encourage readers to seek the original works for further details. Bottom-Up Top-Down Captioner (UpDown) <ref type="bibr" target="#b3">[4]</ref> reasons over visual features extracted using object detectors trained on a large numbers of object and attribute classes and produces near state-of-the-art for single model captioning performance on COCO. For visual features, we use the publicly available Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> detector trained on Visual Genome by <ref type="bibr" target="#b3">[4]</ref> to establish a strong baseline trained exclusively on paired image-caption data. Neural Baby Talk (NBT) <ref type="bibr" target="#b26">[27]</ref> first generates a hybrid textual template with slots explicitly tied to specific image regions, and then fill these slots with words associated with visual concepts identified by an object detector. This gives NBT the capability to caption novel objects when combined with an appropriate pretrained object detector. To adapt NBT to the nocaps setting, we incorporate the Open Images detector and train the language model using Visual Genome image features. We use fixed GloVe embeddings <ref type="bibr" target="#b32">[33]</ref> in the visual feature representation for an object region for better contextualization of words corresponding to novel objects. Open Images Object Detection. Both CBS and NBT make use of object detections; we use the same pretrained Faster R-CNN model trained on Open Images for both. Specifically, we use a model 4 from the Tensorflow model zoo <ref type="bibr" target="#b16">[17]</ref> which achieves a detection mean average precision at 0.5 IoU (mAP@0.5) of 54%. Constrained Beam Search (CBS) <ref type="bibr" target="#b1">[2]</ref> CBS is an inferencetime procedure that can force language models to include specific words referred to as constraints -achieving this by tf_faster_rcnn_inception_resnet_v2_atrous_oidv4 casting the decoding problem as a finite state machine with transitions corresponding to constraint satisfaction. We apply CBS to both the baseline UpDown model and NBT based on detected objects. Following <ref type="bibr" target="#b1">[2]</ref>, we use a Finite State Machine (FSM) with 24 states to incorporate up to three selected objects as constraints, including two and three word phrases. After decoding, we select the highest logprobability caption that satisfies at least two constraints. Constraint Filtering Although the original work <ref type="bibr" target="#b1">[2]</ref> selected constraints from detections randomly, in preliminary experiments in the nocaps setting we find that a simple heuristic significantly improves the performance of CBS.To generate caption constraints from object detections, we refine the raw object detection labels by removing 39 Open Images classes that are 'parts' (e.g. human eyes) or rarely mentioned (e.g. mammal). Specifically, we resolve overlapping detections (IoU ě 0.85) by removing the higher-order of the two objects (e.g. , a 'dog' would suppress a 'mammal') based on the Open Images class hierarchy (keeping both if equal). Finally, we take the top-3 objects based on detection confidence as constraints. Language Embeddings To handle novel vocabulary, CBS requires word embeddings or a language model to estimate the likelihood of word transitions. We extend the original model -which incorporated GloVe <ref type="bibr" target="#b32">[33]</ref> and dependency embeddings <ref type="bibr" target="#b22">[23]</ref> -to incorporate the recently proposed ELMo <ref type="bibr" target="#b33">[34]</ref> model, which increased performance in our preliminary experiments. As captions are decoded left-to-right, we can only use the forward representation of ELMo as input encodings rather than the full bidirectional model as in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b43">44]</ref>. We also initialize the softmax layer of our caption decoder with that of ELMo and fix it during training to improve the model's generalization to unseen or rare words. Training and Implementation Details. We train all models on the COCO training set and tune parameters on the nocaps validation set. All models are trained with crossentropy loss, i.e. we do not use RL fine-tuning to optimize for evaluation metrics <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Analysis</head><p>We report results on the nocaps test set in <ref type="table" target="#tab_5">Table 2</ref>. While our best approach (UpDown + ELMo + CBS, which is explained further below) outperforms the COCO-trained Up-Down baseline captioner significantly ("19 CIDEr), it still under-performs humans by a large margin ("12 CIDEr). As expected the most sizable gap occurs for out-of-domain instances ("25 CIDEr). This shows that while existing novel object captioning techniques do improve over standard models, captioning in-the-wild still presents a considerable open challenge.</p><p>In the remainder of this section, we discuss detailed results on the nocaps and COCO validation sets (   <ref type="table" target="#tab_4">Table 3</ref>: Single model image captioning performance on the COCO and nocaps validation sets. We begin with a strong baseline in the form of the UpDown <ref type="bibr" target="#b3">[4]</ref> trained on COCO captions. We then investigate decoding using Constrained Beam Search <ref type="bibr" target="#b1">[2]</ref> based on object detections from the Open Images detector (+ CBS), as well as the impact of incorporating a pretrained language model (+ ELMo) and ground-truth object detections (+ GT), respectively. In panel 2, we review the performance of Neural Baby Talk (NBT) <ref type="bibr" target="#b26">[27]</ref>, illustrating similar performance trends. Even when using ground-truth object detections, all approaches lag well behind the human baseline on nocaps. Note: Scores on COCO and nocaps should not be directly compared (see Section 3.3). COCO human scores refer to the test split.</p><p>further progress can be made through stronger object detectors and stronger language models, but open questions remain -such as the best way to combine these elements, and the extent to which that solution should involve learning vs. inference techniques like CBS. We align these discussions in the context of a series of specific questions below.</p><p>-Do models optimized for nocaps maintain their performance on COCO? We find significant gains in nocaps performance correspond to large losses on COCO (rows 2-3 vs 1 -dropping "20 CIDEr and "3 SPICE). Given the similarity of the collection methodology, we do not expect to see significant differences in linguistic structure between COCO and nocaps. However, recent work has observed significant performance degradation when transferring models across datasets even when the new target dataset is an exact recreation of the old dataset <ref type="bibr" target="#b34">[35]</ref>. Limiting this degradation in the captioning setting is a potential focus for future work.</p><p>-How important is constraint filtering? Applying CBS greatly improves performance for both UpDown and NBT (particularly on the out-of-domain captions), but success depends heavily on the quality of the constraints. Without our 39-class blacklist and overlap filtering, we find overall nocaps validation performance falls "8 CIDEr and "3 SPICE for our UpDown + ELMo + CBS model -with most of the losses coming from the blacklisted classes. It seems likely that more sophisticated constraint selection techniques that consider image context could improve performance further. -Do better language models help in CBS? To handle novel vocabulary, CBS requires representations for the novel words. We compare using ELMo encoding (row 3) as described in Section 4 with the setting in which word embeddings are only learned during COCO training (row 2). Note that in this setting the embedding for any word not found in COCO is randomly initialized. Surprisingly, the trained embeddings perform on par with the in-domain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>near-domain out-of-domain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method UpDown</head><p>A beach with chairs and umbrellas on it. A man in a red shirt holding a baseball bat. A bird on the ocean in the ocean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ ELMo</head><p>A beach with chairs and umbrellas on it. A man in a red shirt holding a baseball bat. A bird that is floating on the water. + ELMo + CBS A beach with chairs and umbrellas and kites.</p><p>A man in a red hat holding a baseball rifle. A dolphin swimming in the ocean on a sunny day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ ELMo + CBS + GT</head><p>A beach with chairs and umbrellas on it.</p><p>A man in a red hat holding a baseball rifle. A whale dolphin swimming in the ocean on the ocean.</p><p>NBT A beach with a bunch of lawn chairs and umbrellas.</p><p>A baseball player holding a baseball bat in the field.</p><p>A dolphin sitting in the water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ CBS</head><p>A beach with a bunch of umbrellas on a beach.</p><p>A baseball player holding a baseball rifle in the field.</p><p>A marine mammal sitting on a dolphin in the ocean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ CBS + GT</head><p>A beach with many umbrellas on a beach. A baseball player holding a baseball rifle in the field.</p><p>A black dolphin swimming in the ocean on a sunny day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head><p>A couple of chairs that are sitting on a beach.</p><p>A man in a red hat is holding a shotgun in the air.</p><p>A dolphin fin is up in the water.. <ref type="figure">Figure 6</ref>: Some challenging images from nocaps and the corresponding captions generated by our baseline models. The constraints given to the CBS are shown in blue, and the grounded visual words associated with NBT are shown in red. Models perform reasonably well on in-domain images but confuse objects in near-domain and out-of-domain images with visually similar in-domain objects, such as rifle (with baseball bat) and fin (with bird). On the difficult out-of-domain images, the models generate captions with repetitions, such as "in the ocean on the ocean", and produce incoherent captions, such as "marine animal" and "dolphin" referring to the same entity in the image.</p><p>ELMo embeddings for the in-domain and near-domain subsets, although the model with ELMo performs much better on the out-of-domain subset. It appears that even relatively rare occurrences of nocaps object names in COCO are sufficient to learn useful linguistic models, but not visual grounding as shown by the COCO-only model's poor scores (row 1). -Do better object detectors help? To evaluate reliance on object detections, we supply ground truth detections sorted by decreasing area to our full models (rows 4 and 7). These ground truth detections undergo the same constraint filtering as predicted ones. Comparing to prediction-reliant models (rows 3 and 6), we see large gains on all splits (rows 4 vs 3 -"9 CIDEr and "0.6 SPICE gain for UpDown). As detectors improve, we expect to see commensurate gains on nocaps benchmark. To qualitatively assess some of the differences between the various approaches, in <ref type="figure">Figure 6</ref> we illustrate some examples of the captions generated using various model configurations. As expected, all our baseline models are able to generate accurate captions for in-domain images. For near-domain and out-of-domain, our UpDown model trained only on COCO fails to identify novel objects such as rifle and dolphin, and confuses them with known objects such as baseball bat or bird. The remaining models leverage the Open Images training data, enabling them to potentially describe these novel object classes. While they do produce more reasonable descriptions, there remains much room for improvement in both grounding and grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we motivate the need for a stronger and more rigorous benchmark to assess progress on the task of novel object captioning. We introduce nocaps, a largescale benchmark consisting of 166,100 human-generated captions describing 15,100 images containing more than 500 unique object classes and many more visual concepts. Compared to the existing proof-of-concept dataset for novel object captioning <ref type="bibr" target="#b13">[14]</ref>, our benchmark contains a fifty-fold increase in the number of novel object classes that are rare or absent in training captions (394 vs 8). Further, we collected twice the number of evaluation captions per image to improve the fidelity of automatic evaluation metrics.</p><p>We extend two recent approaches for novel object captioning to provide strong baselines for the nocaps benchmark. While our final models improve significantly over a direct transfer from COCO, they still perform well below the human baseline -indicating there is significant room for improvement on this task. We provide further analysis to help guide future efforts, showing that it helps to leverage large language corpora via pretrained word embeddings and language models, that better object detectors help (and can be a source of further improvements), and that simple heuristics for determining which object detections to men-tion in a caption have a significant impact.   8. Chinese lanterns that are red are floating into the sky. 9. The tan dog happily accompanies the human on the grass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9.</head><p>A woman is sitting behind a camera with tripod.</p><p>9. This town has many lit Chinese lanterns hanging between the buildings. 10. A dog is on the grass is looking to a person 10. A woman with a camera in front of her. 10. The street is filled with light from hanging lanterns.</p><p>1. people are standing on the side of a food truck 1. A room with a hot tub and sauna.   Note that not all captions mention the ground-truth object classes consistent with the instructions provided on the data collection interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Additional Details about nocaps Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation Subsets</head><p>As outlined in Section 3.3 of the main paper, to determine the in-domain, near-domain and out-of-domain subsets of nocaps, we first classify Open Images classes as either in-domain or out-of-domain with respect to COCO. To identify the in-domain Open Images classes, we manually map the 80 COCO classes to Open Images classes. We then select an additional 39 Open Images classes that are not COCO classes, but are nonetheless mentioned more than 1,000 times in the COCO captions training set (e.g. 'table', 'plate' and 'tree'), and we classify all 119 of these classes as in-domain. The remaining classes are considered to be out-of-domain.</p><p>To put this in perspective, in <ref type="figure">Figure we</ref> plot the number of mentions of both the in-domain classes (in orange) and the out-of-domain classes (in blue) in the COCO Captions training set using a log scale. As intended, the in-domain object classes occur much more frequently in COCO Captions compared to out-of-domain object classes. However, it is worth noting that the out-of-domain are not necessarily absent from COCO Captions, but they are relatively infrequent which makes these concepts hard to learn from COCO.</p><p>Open Images classes ignored during image subset selection: We also note that 87 Open Images classes were not considered during the image subset selection procedure to create nocaps, for one of the following reasons:</p><p>• Parts: In our image subset selection strategy (refer Section 3.1 of the main paper), we ignored 'part' categories such as 'vehicle registration plate', 'wheel', 'human-eye', which always occur with parent categories such car, person; • Super-categories: Our image subset selection strategy also ignored super-categories such as 'sports equipment', 'home appliance', 'auto part' which are often too broad and subsumes both COCO and Open Images categories; • Solo categories: Certain categories such as 'chime' and 'stapler' did not appear in images alongside any other classes, and so were filtered out by our image subset selection strategy; and • Rare categories: Some rare categories such as 'armadillo', 'pencil sharpener' and 'pizza cutter' do not actually occur in the underlying Open Images val and test splits.    <ref type="figure" target="#fig_1">Figure 11</ref>: T-SNE <ref type="bibr" target="#b39">[40]</ref> plot comparing the visual similarity between object classes in COCO, in-domain and out-of-domain splits of nocaps. For each object class in a particular split, we extract bottom-up image features from the Faster-RCNN detector made publicly available by <ref type="bibr" target="#b3">[4]</ref> and mean pool them to form a 2048-dimensional vector. We further apply PCA on the feature vectors for all object classes and pick the first 128 principal components. Using these feature vectors of reduced dimension, we compute the exact form T-SNE with perplexity 30. We observe that: (a) in-domain shows high visual similarity to COCO-green and brown points of same object class are close to each other. (b) Many out-of-domain classes are visually different from in-domain classes -large clusters of blue, far away from green and brown. (c) out-of-domain also covers many visually similar concepts to COCO-blue points filling the gaps between sparse clusters green/brown points. <ref type="figure" target="#fig_1">Figure 12</ref>: T-SNE <ref type="bibr" target="#b39">[40]</ref> plot comparing the linguistic similarity between object classes in in-domain and out-of-domain splits of nocaps. For each object class in a particular split, we obtain 300-dimensional GloVe <ref type="bibr" target="#b32">[33]</ref>. We further apply PCA on these GloVe vectors vectors for all object classes and pick the first 128 principal components. Using these feature vectors of reduced dimension, we compute the exact form T-SNE with perplexity 30. We observe that: (a) Many out-of-domain classes are linguistically different from in-domain classeslarge clusters of blue points far away from brown points. (b) out-of-domain also covers many linguistically similar, fine-grained classes not present in in-domain-blue points filling gaps in sparse clusters of brown points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">T-SNE Visualization in Visual Feature Embedding Space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">T-SNE Visualization in Linguistic Feature Embedding Space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Linguistic Similarity to COCO</head><p>Overall, our collection methodology closely follows COCO. However, we do introduce keyword priming to the collection interface (refer <ref type="figure">Figure 7)</ref> which has the potential to introduce some linguistic differences between nocaps and COCO. To quantitatively assess linguistic differences between the two datasets, we review the performance of COCO-trained models on the nocaps validation set while controlling for visual similarity to COCO. As a proxy for visual similarity to COCO, we use the average cosine distance in FC7 CNN feature space between each nocaps image and the 10 closest COCO images.</p><p>As illustrated in <ref type="table" target="#tab_11">Table 4</ref>, the baseline UpDown model (trained using COCO) exceeds human performance on the decile of nocaps images which are most similar to COCO images (decile=1, avg. cosine distance=0.15), consistent with the trends seen in the COCO dataset. This suggests that the linguistic structure of COCO and nocaps captions is extremely similar. As the nocaps images become visually more distinct from COCO images, the performance of UpDown drops consistently. This suggests that no linguistic variations have been introduced between COCO and nocaps due to priming and the degradation in the performance is due to visual differences. Similar trends are observed for our best model (UpDown + ELMo + CBS) although the performance degradation with increasing visual dissimilarity to COCO is much less.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Additional Implementation Details for Baseline Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Neural Baby Talk (NBT)</head><p>In this section, we describe our modifications to the original authors' implementation of Neural Baby Talk (NBT) <ref type="bibr" target="#b26">[27]</ref> to enable the model to produce captions for images containing novel objects present in nocaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grounding Regions for Visual Words</head><p>Given an image, NBT leverages an object detector to obtain a set of candidate image region proposals, and further produces a caption template, with slots explicitly tied to specific image regions. In order to accurately caption nocaps images, the object detector providing candidate region proposals must be able to detect the object classes present in nocaps (and broadly, Open Images). Hence, we use a Faster-RCNN <ref type="bibr" target="#b35">[36]</ref> model pre-trained using Open Images V4 <ref type="bibr" target="#b19">[20]</ref> (referred as OI detector henceforth), to obtain candidate region proposals as described in Section 4 of the main paper. This model can detect 601 object classes of Open Images, which includes the novel object classes of nocaps. In contrast, the authors' implementation uses a Faster-RCNN trained using COCO.</p><p>For every image in COCO train 2017 split, we extract image region proposals after the second stage of detection, with an IoU threshold of 0.5 to avoid highly overlapping region proposals, and a class detection confidence threshold of 0.5 to reduce false positive detections. This results in number of region proposals per image varies up to a maximum of 18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-Up Visual Features</head><p>The language model in NBT (Refer <ref type="figure" target="#fig_4">Figure 4</ref> in <ref type="bibr" target="#b26">[27]</ref>) has two separate attention layers, and takes visual features as input in three different manners: All the three listed visual features are extracted using ResNet-101, with the first being specific to visual words, while the second and third provide the holistic context of the image. We replace the ResNet-101 feature extractor with the publicly available Faster-RCNN model pre-trained using Visual Genome (referred as VG detector henceforth), same as <ref type="bibr" target="#b3">[4]</ref>. Given a set of candidate region proposals obtained from OI detector, we extract 2048-dimensional bottom-up features using the VG detector and use them as input to first attention layer (and also for input to the Pointer Network). For input to the second attention layer, we extract top-36 bottom-up features (class agnostic) using the VG detector. Similarly, we perform mean-pooling of these 36 features for input to the language model at every time-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-grained Class Mapping</head><p>NBT fills the slots in each caption template using words corresponding to the object classes detected in the corresponding image regions. However, object classes are coarse labels (e.g. 'cake'), whereas captions typically refer entities in a fine-grained fashion (e.g. 'cheesecake', 'cupcake', 'coffeecake' etc.). To account for these linguistic variations, NBT predicts a fine-grained class for each object class using a separate MLP classifier. To determine the output vocabulary for this fine-grained classifier we extend the fine-grained class mapping used for COCO (Refer <ref type="table">Table 5</ref> in <ref type="bibr" target="#b26">[27]</ref>), adding Open Images object classes. Several fine-grained classes in original mapping are already present in Open Images (e.g. 'man', 'woman' -fine-grained classes of 'person'), we drop them as fine-grained classes from original mapping and retain them as Open Images object classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Word Prediction Criterion</head><p>In order to ensure correctness in visual grounding, the authors' implementation uses three criteria to decide whether a particular region proposal should be tied with a "slot" in the caption template. At any time during decoding, when the Pointer Network attends to a visual feature (instead of the visual sentinel), the corresponding region proposal is tied with the "slot" if:</p><p>-The class prediction threshold of this region proposal is higher than 0.5. -The IoU of this region proposal with at least one of the ground truth bounding boxes is greater than 0.5. -The predicted class is same as the object class of ground truth bounding box having highest IoU with this region proposal.  <ref type="table">Table 5</ref>: Blacklisted object class names for constraint filtering (CBS) and visual word prediction (NBT).</p><p>To quantify the impact of this simple constraint filtering heuristic, in <ref type="table">Table we</ref> report the results of the following ablation studies:</p><p>-Using all the object classes for constraints (w/o class), -Using overlapping objects for constraints (w/o overlap), and -Using no filtering heuristic at all (w/o both).</p><p>Note that in all cases we rank objects based on confident score for detected objects and pick the top-3 as the constraints. We report results for three models, the baseline model (UpDown), the baseline model using Glove <ref type="bibr" target="#b32">[33]</ref> and dependency-based <ref type="bibr" target="#b22">[23]</ref> word embeddings (UpDown + GD) and our ELMo-based model (UpDown + ELMo +CBS). Table shows that removing the above 39 classes significantly improves the performance of constrained beam search and removing overlapping objects can also slightly improve the performance. This conclusion is consistent across the three models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finite State Machine</head><p>Constrained Beam Search implements constraints in the decoding process using a Finite State Machine (FSM). In all experiments we use a 24 state FSM. We use 8 states for standard three single word constraints D1, D2 and D3. As shown in <ref type="figure" target="#fig_1">Figure 15</ref>, the outputs of this FSM are the captions that mention at least two constraints out of three. Each Di (i = 1,2,3) represents a set of alternative constraint words (e.g., bike, bikes). Di can also be multi-word expressions. Our FSM can dynamically support two-word or three-word phrases in Di by extending additional one states (see <ref type="figure" target="#fig_1">Figure 13</ref>) or two states (see <ref type="figure" target="#fig_1">Figure 14)</ref> for two-word or three-word phrases respectively. Since D1, D2 and D3 are all used 4 times in the base eight-state FSM, we need to allocate 4 states for a single two-word expression and 8 states for a single three-word expression. <ref type="figure" target="#fig_1">Figure 13</ref>: FSM for a two-word phrase ta1, a2u constraint  <ref type="table">Table 6</ref>: We investigate the effect of different object filtering strategies in Constrained Beam Search and report the model performance in nocaps val. We find that using both strategies with the ELMo model performs best. <ref type="figure" target="#fig_1">Figure 14</ref>: FSM for a three-word phrase ta1, a2, a3u constraint <ref type="figure" target="#fig_1">Figure 15</ref>: FSM for D1, D2, D3 constraints. States q3, q5, q6 and q7 are desired states to satisfy at least two out of three constraints.</p><formula xml:id="formula_0">q 0 q 1 q 2 V´a 1 a 1 V´a 2 a 2 V</formula><formula xml:id="formula_1">q 0 q 1 q 2 q 3 V´a 1 a 1 V´a 2 V´a 3 a 2 a 2 V</formula><formula xml:id="formula_2">q 0 q q 2 q 3 q 4 q 5 q 6 q 7 D 1 D 1 D 1 D 1 D 2 D 2 D 3 D 3 D 3 D 3 D 2 D 2</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>‹</head><label></label><figDesc>First two authors contributed equally, listed in alphabetical order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The nocaps task setup: Image captioning models must exploit the Open Images object detection dataset (bottom left) to successfully describe novel objects not covered by the COCO Captions dataset (top left). The nocaps benchmark (right) evaluates performance over in-domain, near-domain and out-of-domain subsets of images containing only COCO classes, both COCO and novel classes, and only novel classes, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Labels:</head><label></label><figDesc>Gondola, Tree, Vehicle Labels: Red Panda, Tree No Priming: A man and a woman being transported in a boat by a sailor through canals No Priming: A brown rodent climbing up a tree in the woods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>We conducted pilot studies to evaluate caption collection interfaces. Since Open Images contains rare and fine-grained classes (such as red panda, top right) we found that priming workers with the correct object categories resulted in more accurate and descriptive captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>: Visual similarity (Average bottom-up features from GT bounding boxes of classes). T-SNE [40] plots comparing visual (left) and linguistic (right) similarity in COCO, in-domain and out-of-domain classes. We observe that: (a) in-domain shows high visual similarity to COCO (e.g. cat, book (left)). (b) Many out-of-domain classes are visually and linguistically different from in-domain classes (e.g. jellyfish, beetle, cello). (c) out-of-domain also covers many visually and linguistically similar concepts to COCO, which are not well-covered in COCO (e.g. tiger, lemon).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Examples of near-domain and out-of-domain images from the nocaps validation set. The image on the left belongs to the near-domain subset (COCO and Open Images categories), while the image on the right belongs to out-of-domain subset (only Open Images categories).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>9 . 9 . 10 .Figure 8 :</head><label>99108</label><figDesc>Two people in an room competing in a fencing competition. Pandas enjoy the outside and especially with a friend. 10. a row of horses and jockeys running in the same direction in a line 10. Two people in all white holding swords and fencing. Two black and white panda bears eating leaf stems Examples of images belonging to the in-domain, near-domain and out-of-domain subsets of the nocaps validation set. Each image is annotated with 10 reference captions, capturing more of the salient content of the image and improving the accuracy of automatic evaluations<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref>. Categories in orange are in-domain object classes while categories in blue are out-of-domain classes. Note that not all captions mention the ground-truth object classes consistent with the instructions provided on the data collection interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 .</head><label>3</label><figDesc>A jacuzzi sitting on rocks inside of a patio. 3. A brewery with big, silver, metal containers and a sign. 4. The food truck has a line of people in front of the window.4.A hot tub sits in the middle of the room. 4. A brew station inside of a restaurant.5. A woman standing in front of a food truck. 5. A jacuzzi sitting near some rocks and a sauna 5. Large steel breweries sit behind a chalkboard displaying different food and drink deals. 6. A food truck outside of a small business with several people eating 6. A hot tub in a room with wooden flooring. 6. A cabinetry with big tin cans and a chalkboard on the top 7. people stand in line to get food from a food truck. 7. A room is shown with a hot tub, decorative plants and some paintings ont he wall. 7. A man works on machinery inside a brewery. 8. A large metal truck serving food to people in a parking lot. 8. A room with a large hot tub and a sauna. 8. The many silver tanks are used for beverage making. 9. men and women speaking in front of a grey food truck that is open for business. 9. A water filled jacuzzi surrounded by smooth river rocks and a wooden deck. 9. A menu is hanging above a craft brewery.10. woman wearing jeans in front of the truck 10. A white and grey jacuzzi around rock building 10. A man peers at a brewing tank while standing on a step ladder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>More examples of images belonging to the in-domain, near-domain and out-of-domain subsets of the nocaps validation set. Each image is annotated with 10 reference captions, capturing more of the salient content of the image and improving the accuracy of automatic evaluations<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">41]</ref>. Categories in orange are in-domain object classes while categories in blue are out-of-domain classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Histogram of mentions in the COCO Captions training set for various Open Images object classes. In nocaps, classes in orange are considered to be in-domain while classes in blue are classified as out-of-domain. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>-</head><label></label><figDesc>The first attention layer learns an attention distribution over region features, extracted using ResNet-101 + RoI Align layer. -The second attention layer learns an attention distribution over spatial CNN features from the last convolutional layer of ResNet-101 (7 x 7 grid, 2048 channels). -The word embedding input is concatenated with FC7 features from ResNet-101 at every time-step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Unique n-grams in equally-sized (4,500 images / 22,500 captions) uniformly randomly selected subset from the COCO and nocaps validation sets. The increased visual variety in nocaps demands a larger vocabulary compared to COCO (1-grams), but also more diverse language compositions (2-, 3-and 4-grams).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>These classes are not included either because they are not present in the underlying Open Images val and test splits, or because they got filtered out by our image subset selection strategy favoring more complex images.to image domain shift. This subset contains 1,311 test images (13K captions).near-domain images contain both in-domain and out-ofdomain object classes. These images are more challenging for COCO trained models, especially when the most salient objects in the image are novel. This is the largest subset containing 7,406 test images (74K captions). out-of-domain images do not contain any in-domain classes, and are visually very distinct from COCO images. We expect this subset to be the most challenging and models trained only on COCO data are likely to make 'embarrassing errors'<ref type="bibr" target="#b24">[25]</ref> on this subset, reflecting the current performance of COCO trained models in the wild. This subset contains 1,883 test images (19K captions).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 )</head><label>3</label><figDesc>to help guide future work. Overall, the evidence suggests that</figDesc><table><row><cell>nocaps test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Single model image captioning performance on the nocaps test split. We evaluate four models, including the UpDown model<ref type="bibr" target="#b3">[4]</ref> trained only on COCO, as well as three model variations based on constrained beam search (CBS)<ref type="bibr" target="#b1">[2]</ref> and Neural Baby Talk (NBT)<ref type="bibr" target="#b26">[27]</ref> that leverage the Open Images training set.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">COCO val 2017</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">nocaps val</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Overall</cell><cell></cell><cell></cell><cell cols="2">in-domain</cell><cell cols="4">near-domain out-of-domain</cell><cell cols="2">Overall</cell></row><row><cell>Method</cell><cell cols="5">Bleu-1 Bleu-4 Meteor CIDEr SPICE</cell><cell cols="8">CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE</cell></row><row><cell>(1) UpDown</cell><cell>77.0</cell><cell>37.2</cell><cell>27.8</cell><cell>116.2</cell><cell>21.0</cell><cell>78.1</cell><cell>11.6</cell><cell>57.7</cell><cell>10.3</cell><cell>31.3</cell><cell>8.3</cell><cell>55.3</cell><cell>10.1</cell></row><row><cell>(2) UpDown + CBS</cell><cell>73.3</cell><cell>32.4</cell><cell>25.8</cell><cell>97.7</cell><cell>18.7</cell><cell>80.0</cell><cell>12.0</cell><cell>73.6</cell><cell>11.3</cell><cell>66.4</cell><cell>9.7</cell><cell>73.1</cell><cell>11.1</cell></row><row><cell>(3) UpDown + ELMo + CBS</cell><cell>72.4</cell><cell>31.5</cell><cell>25.7</cell><cell>95.4</cell><cell>18.2</cell><cell>79.3</cell><cell>12.4</cell><cell>73.8</cell><cell>11.4</cell><cell>71.7</cell><cell>9.9</cell><cell>74.3</cell><cell>11.2</cell></row><row><cell cols="2">(4) UpDown + ELMo + CBS + GT -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>84.2</cell><cell>12.6</cell><cell>82.1</cell><cell>11.9</cell><cell>86.7</cell><cell>10.6</cell><cell>83.3</cell><cell>11.8</cell></row><row><cell>(5) NBT</cell><cell>72.7</cell><cell>29.4</cell><cell>23.8</cell><cell>88.3</cell><cell>16.5</cell><cell>62.7</cell><cell>10.1</cell><cell>51.9</cell><cell>9.2</cell><cell>54.0</cell><cell>8.6</cell><cell>53.9</cell><cell>9.2</cell></row><row><cell>(6) NBT + CBS</cell><cell>70.2</cell><cell>28.2</cell><cell>25.1</cell><cell>80.2</cell><cell>15.8</cell><cell>62.3</cell><cell>10.3</cell><cell>61.2</cell><cell>9.9</cell><cell>63.7</cell><cell>9.1</cell><cell>61.9</cell><cell>9.8</cell></row><row><cell>(7) NBT + CBS + GT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.9</cell><cell>10.7</cell><cell>68.6</cell><cell>10.3</cell><cell>76.9</cell><cell>9.8</cell><cell>70.3</cell><cell>10.3</cell></row><row><cell>(8) Human</cell><cell>66.3</cell><cell>21.7</cell><cell>25.2</cell><cell>85.4</cell><cell>19.8</cell><cell>84.4</cell><cell>14.3</cell><cell>85.0</cell><cell>14.3</cell><cell>95.7</cell><cell>14.0</cell><cell>87.1</cell><cell>14.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>recipe book and sewing book on a craft table 4. Two men dressed in military outfits play the french horn. 4. Orange jellyfish swimming through the water. Several red jellyfish swimming in bright blue water. 9. A couple of books are on a table. 9. A man in uniform plays a French horn. 9. An orange jellyfish swimming with a blue background 10. The person is there looking into the book. 10. Two men are playing the trumpet standing nearby. Two people in masks fencing with each other. 2. The panda is large and standing over the plant. 3. several people racing horses around a turn outside 3. Two people in white garbs are fencing while people watch. 3. Two panda are eating sticks from plants. 4. Uniformed jockeys on horses racing through a grass field.4. Two people in full gear fencing on white mat. 4. Two panda bears sitting with greenery surrounding them. 5. Several horse jockies are riding horses around a turn.5.A couple of people in white outfits are fencing. 5. two panda bears in the bushes eating bamboo sticks 6. Six men and six horses are racing outside 6. Two fencers in white outfits are dueling indoors.</figDesc><table><row><cell></cell><cell></cell><cell>1. Some red invertebrate jellyfishes in dark blue</cell></row><row><cell></cell><cell></cell><cell>water.</cell></row><row><cell>2. Two magazines are sitting on a coffee table.</cell><cell>2. Military officers play brass horns next to each</cell><cell>2. orange and clear jellyfish in dark blue water</cell></row><row><cell></cell><cell>other.</cell><cell></cell></row><row><cell>3. Two books and many crafting supplies are on</cell><cell>3. Two men in camouflage clothing playing the</cell><cell>3. A red jellyfish is swimming around with other</cell></row><row><cell>this table.</cell><cell>trumpet.</cell><cell>red jellyfish.</cell></row><row><cell cols="2">4. a 5. Two people dressed in camoflauge uniforms</cell><cell>5. Bright orange and clear jellyfish swim in open</cell></row><row><cell></cell><cell>playing musical instruments.</cell><cell>water.</cell></row><row><cell>6. A table with two different books on it.</cell><cell>6. A couple people in uniforms holding tubas by</cell><cell></cell></row><row><cell></cell><cell>their mouths.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>10. A very vibrantly red jellyfish is seen swimming</cell></row><row><cell></cell><cell></cell><cell>in the water.</cell></row><row><cell>1. Jockeys on horses racing around a track.</cell><cell>1. Two people in a fencing match with a woman</cell><cell>1. A panda bear sitting beside a smaller panda</cell></row><row><cell></cell><cell>walking by in the background.</cell><cell>bear.</cell></row><row><cell>2. Several horses are running in a race thru the grass.</cell><cell cols="2">2. 6. two pandas sitting in the grass eating some</cell></row><row><cell></cell><cell></cell><cell>plants</cell></row><row><cell>7. A group of men wearing sunglasses and racing</cell><cell>7. A couple of people doing a fencing competi-</cell><cell>7. two pandas are eating a green leaf from a plant</cell></row><row><cell>on a horse</cell><cell>tion inside.</cell><cell></cell></row><row><cell>8. Six horses with riders are racing, leaning over</cell><cell cols="2">8. Two people in white clothes fencing each other. 8. Two pandas are eating bamboo in a wooded</cell></row><row><cell>at an incredible angle.</cell><cell></cell><cell>area.</cell></row><row><cell>9. Seveal people wearing goggles and helmets</cell><cell></cell><cell></cell></row><row><cell>racing horses.</cell><cell></cell><cell></cell></row></table><note>5. Two hardcover books are laying on a table.6. The fish is going through the very blue water.7. Two different books on sewing and cook- ing/baking on a table.7. Two people in uniform are playing the tuba. 7. A bright orange jellyfish floating in the water.8. Two magazine books are sitting on a table with arts and craft materials.8. A couple of military men playing the french horn.8.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>CIDEr scores on nocaps test deciles split by visual similarity to COCO (using CNN features). Our models exceed human performance on the decile of nocaps images that are most visually similar to COCO. This suggests that after controlling for visual variations the linguistic structure of COCO and nocaps captions is highly similar.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The images used in nocaps come from the Open Images V4 dataset and are provided under their original license (CC BY 2.0)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Jiasen Lu for helpful discussions about Neural Baby Talk. This work was supported in part by NSF, AFRL, DARPA, Siemens, Samsung, Google, Amazon, ONR YIPs and ONR Grants N00014-16-1-{2713,2793}. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.</p><p>We drop the third criterion, as the OI detector can predict several fine-grained classes in context of COCO, such as 'man' and 'woman' (while the ground truth object class would be 'person'). Keeping the third criterion intact in nocaps setting would suppress such region proposals, and result in lesser visual grounding, which is not desirable for NBT. Relaxation of this criterion might introduce false positives from detection in the caption but prevents reduction in visual grounding.</p><p>We encourage the reader to refer the authors' implementation for further details. We will release code for our modifications.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In Section 1, we provide the details about our data collection interface for nocaps. Further, in Section 2, we provide some qualitative examples from nocaps validation split. In Section 3, we provide additional details in relation to the nocaps benchmark. In Section 4, we provide implementation details for our baseline models and finally in Section 5, we provide examples of predicted captions on the three (in-domain, near-domain and out-of-domain) subsets of the nocaps validation set. <ref type="figure">Figure 7</ref>: Amazon Mechanical Turk (AMT) user interface with priming for gathering captions. The interface shows a subset of object categories present in the image as keywords. Note that the instruction explicitly states that it is not mandatory to mention any of the displayed keywords. Other instructions are similar to the interface described in <ref type="bibr" target="#b6">[7]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Data Collection Interface</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Example Reference Captions from nocaps in-domain near-domain out-of-domain</head><p>Integrating UpDown Model with ELMo When using ELMo <ref type="bibr" target="#b33">[34]</ref>, we use a dynamic representation of wc,h 1 t andh 2 t as the input word embedding w t ELM o for our caption model. wc is the character embedding of input words andh i t (i " 1, 2) is the hidden output of i th LSTM layer of ELMo. We combine them via:</p><p>where γi (i=0, 1, 2) are three trainable scalars. When using w t ELM o as the external word representation of other models, we fixed all the parameters of ELMo but γi (i=0, 1, 2).</p><p>In addition, to handle unseen objects in training data, following <ref type="bibr" target="#b1">[2]</ref>, we initialize the softmax layer matrix (Wp, bp) using word embedding and keep this layer fixed during training. This allow our caption model to produce similar logits score for the words that share similar vectors and values in Wp and bp. We have:</p><p>where WELMo and bELMo is the softmax layer in original ELMo language model. To align the different dimension in softmax layer and LSTM hidden state, we add an additional fully connected layer with a non-linearity function tanh. We have:</p><p>vt " tanhpWth 2 t`bt q (4) P pyt|y1:t´1, Iq " sof tmaxpWpvt`bpq <ref type="bibr" target="#b4">(5)</ref> where Wt P R HˆE , bt P R E , H is LSTM hidden dimension, E is the word embedding dimension, Wp P R EˆD , bp P R D and D is the vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other details of using ELMo</head><p>In our experiment, we use the full tensorflow checkpoint trained on 1 Billion Word Language Model Benchmark from official ELMo tensorflow implementation project <ref type="bibr" target="#b5">6</ref> .</p><p>When selecting vocabularies for our model, we first extract all words from COCO captions and open image object labels. We then extend the open image object labels to both singular and plural word forms. Finally, we remove all the words that are not in ELMo output vocabularies. This allow us to use ELMo LM prediction for each decoding step.</p><p>Our UpDown + ELMo model is optimized by SGD <ref type="bibr" target="#b4">[5]</ref>. We conduct hyper-parameter tuning the model and choose the model based on its performance on nocaps val.</p><p>http://www.statmt.org/lm-benchmark/ 6 https://github.com/allenai/bilm-tf/ A couple of men standing on top of a tank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Example Model Predictions</head><p>A couple of kettle jugs sitting next to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NBT</head><p>A group of men standing in a field.</p><p>A man standing on the back of a tank.</p><p>A couple of kettles are sitting on a table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NBT + CBS</head><p>A couple of men standing on a tennis court.</p><p>A man standing on top of a tank with a truck.</p><p>A close up of a kettle on a table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NBT + CBS + GT</head><p>A group of men are standing in a field.</p><p>A man standing on top of a tank plant.</p><p>Two kettles and teapot jugs are sitting on a table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head><p>Two people in karate uniforms spar in front of a crowd.</p><p>Two men sitting on a tank parked in the bush.</p><p>Ceramic jugs are on display in a glass case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>in-domain near-domain out-of-domain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method UpDown</head><p>A woman riding a bike with a statue on her head.</p><p>A couple of chairs sitting in front of a building.</p><p>A bird sitting on the ground in the grass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UpDown + ELMo</head><p>There is a woman that is riding a bike.</p><p>A room that has a lot of furniture in it.</p><p>A dog laying on the ground next to a stuffed animal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UpDown + ELMo + CBS</head><p>There is a woman that is riding a bike.</p><p>Two pillows and a table in the house.</p><p>A dog laying on the ground next to a tortoise. UpDown + ELMo + CBS + GT There is a woman that is riding a bike.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two couches and a table in a house.</head><p>A dog laying on the ground next to a tortoise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NBT</head><p>A man is riding a clothing on a bike.</p><p>A table with a couch and a table.</p><p>A tortoise is laying on top of the ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NBT + CBS</head><p>A woman is riding a clothing in the street.</p><p>A couple of pillows on a wooden table in a couch.</p><p>A tortoise that is sitting on the ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NBT + CBS + GT</head><p>A man is riding a clothing on a person.</p><p>A house and a studio couch of couches in a room.</p><p>A tortoise is laying on the ground in the grass. On the deck of a pool is a couch and a display of a safety ring.</p><p>Three tortoises crawl on soil and wood chips in an enclosure. A large white sign on a city street.</p><p>A bear laying in the grass near a tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UpDown + ELMo</head><p>A woman in a pink dress is holding a child.</p><p>A large white bus parked on the side of a road.</p><p>A bear that is laying down in the grass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UpDown + ELMo + CBS</head><p>A woman in a pink dress is holding a child.</p><p>A billboard that has a street light on it.</p><p>A red panda is walking through the grass. UpDown + ELMo + CBS + GT A woman in a pink dress is holding a child.</p><p>A large white bus parked next to a billboard.</p><p>A red panda is walking through the grass.</p><p>NBT A group of man are standing in a field.</p><p>A billboard sign on the side of a building.</p><p>A brown red panda is laying on the grass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NBT + CBS</head><p>A group of man are playing a baseball game.</p><p>A picture of billboard sign on the street light.</p><p>A tree and a brown red panda in a field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NBT + CBS + GT</head><p>A group of man are standing on a field.</p><p>A billboard sign on the side of a building.</p><p>A brown red panda lying on top of a field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head><p>Two sumo wrestlers are wrestling while a crowd of men and women watch.</p><p>A man is standing on the ladder and working at the billboard.</p><p>The red panda trots across the forest floor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>in-domain near-domain out-of-domain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method UpDown</head><p>A woman wearing a white shirt and a white shirt.</p><p>A person riding a yellow bike in the field.</p><p>A close up of a cat looking at a bird.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UpDown + ELMo</head><p>A woman wearing a white shirt and white shirt.</p><p>A woman sitting on a yellow bike.</p><p>A close up of a bird with its mouth open.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UpDown + ELMo + CBS</head><p>A woman wearing a white suit and a white shirt.</p><p>A person sitting on a bicycle with a wheelchair.</p><p>A sea lion standing with its mouth open. UpDown + ELMo + CBS + GT A woman wearing a white suit and a white shirt.</p><p>A person sitting on a bicycle with a wheelchair.</p><p>A sea lion standing next to a harbor seal.</p><p>NBT A woman wearing a white shirt is wearing a hat.</p><p>A man sitting on a wheelchair with a bike.</p><p>A close up of a sea lion and harbor seal with its head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NBT + CBS</head><p>A suit of woman wearing a white suit.</p><p>A man sitting on a wheelchair and a bike.</p><p>A close up of a harbor seal of a sea lion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NBT + CBS + GT</head><p>A suit of woman wearing a white shirt.</p><p>A bicycle sitting on a wheelchair with a bike.</p><p>A close up of a harbor seal of a sea lion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head><p>The man has a wrap on his head and a white beard.</p><p>A person sitting in a yellow chair with wheels.</p><p>A brown and gray sea lion looking at the photographer. <ref type="figure">Figure 17</ref>: Some challenging images from nocaps and corresponding captions generated by existing approaches. The constraints given to the CBS are shown in blue. The visual words associated with NBT are shown in red.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SPICE: Semantic Propositional Image Caption Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Guided open vocabulary image captioning with constrained beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Partially-supervised image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><forename type="middle">L</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft COCO Captions: Data Collection and Evaluation Server</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Advances in Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Journal of the royal statistical society. Series B (methodological)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
	<note>Maximum likelihood from incomplete data via the EM algorithm</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jointly predicting predicates and arguments in neural semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="364" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep compositional captioning: Describing novel object categories without paired training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Openimages: A public dataset for large-scale multi-label and multi-class image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="https://github.com/openimages,2017.2" />
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Babytalk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2891" to="2903" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for MT evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL): Second Workshop on Statistical Machine Translation</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL): Second Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dependency-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rouge: a package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL) Workshop: Text Summarization Branches Out</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL) Workshop: Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved image captioning via policy gradient optimization of SPIDEr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural baby talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Training and evaluating multimodal word embeddings with large-scale web annotated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">We don&apos;t need no bounding-boxes: Training object class detectors using only human verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Extreme clicking for efficient object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00451</idno>
		<title level="m">Do CIFAR-10 classifiers generalize to CIFAR-10</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rich Image Captioning in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Carapcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thrasher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sienkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Captioning Images with Diverse Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Decoupled novel object captioner. CoRR, abs/1804.03803</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03570</idno>
		<title level="m">Evalai: Towards better evaluation systems for ai agents</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cohen. Review networks for caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in image captioning for learning novel objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Constrained Beam Search (CBS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Determining Constraints When using constrained beam search (CBS) [2], we decoded the model in question while forcing the generated caption to include words corresponding to object classes detected in the image. For object detection, we use the same Faster-RCNN [36] model pre-trained using Open Images V4 [20] (OI detector) that is used in conjunction with NBT. However, not all detected object classes are used as constraints. We perform constraint filtering by removing the 39 object classes listed in Table from the constraint set, as these classes are either object parts</title>
		<imprint/>
	</monogr>
	<note>or classes that we consider to be either too rare or too broad. We also suppress highly overlapping objects as described in Section 4</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
