<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Word Embeddings for Evolving Semantic Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Yao</surname></persName>
							<email>zijun.yao@rutgers.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
							<email>yifan.sun@technicolor.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
							<email>nikhilrao86@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>hxiong@rutgers.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>New York</settlement>
									<region>NY, USA, 9 pages</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dynamic Word Embeddings for Evolving Semantic Discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3159652.3159703</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Word evolution refers to the changing meanings and associations of words throughout time, as a byproduct of human language evolution. By studying word evolution, we can infer social trends and language constructs over different periods of human history. However, traditional techniques such as word representation learning do not adequately capture the evolving language structure and vocabulary. In this paper, we develop a dynamic statistical model to learn time-aware word vector representation. We propose a model that simultaneously learns time-aware embeddings and solves the resulting &quot;alignment problem&quot;. This model is trained on a crawled NYTimes dataset. Additionally, we develop multiple intuitive evaluation strategies of temporal word embeddings. Our qualitative and quantitative tests indicate that our method not only reliably captures this evolution over time, but also consistently outperforms state-of-the-art temporal embedding approaches on both semantic accuracy and alignment quality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human language is an evolving construct, with word semantic associations changing over time. For example, apple which was traditionally only associated with fruits, is now also associated with a technology company. Similarly, the association of names of famous personalities (e.g., trump) changes with a change in their roles. For this reason, understanding and tracking word evolution is useful for time-aware knowledge extraction tasks (e.g., public sentiment analysis), and other applications in text mining. To this end, we aim to learn word embeddings with a temporal bent, for capturing time-aware meanings of words.</p><p>Word embeddings aim to represent words with low-dimensional vectors, where words with similar semantics are geometrically closer (e.g. red and blue are closer than red and squirrel). Classic word embedding techniques started in the 90s and relied on statistical approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>. Later, neural network approaches <ref type="bibr" target="#b4">[5]</ref>, as well as recent advances such as word2vec <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and GloVE <ref type="bibr" target="#b26">[27]</ref> have greatly improved the performance of word representation learning. However, these techniques usually do not consider temporal factors, and assume that the word is static across time.</p><p>In this paper, we are interested in computing time-aware embedding of words. Specifically, each word in a different time frame (e.g., years) is represented by a different vector. From these embeddings, we have a better notion of "distance" (the cosine distance between word embedding vectors), and by looking at word "neighborhoods" (defined through this distance), we can better understand word associations as well as word meanings, as they evolve over time.</p><p>For instance, by locating the embeddings of personality names such as trump's closest words, we can see that he can be associated with the trajectory : real estate → television → republican. Similarly, the trajectory of apple travels from the neighborhood of strawberry, mango to that of iphone, ipad.</p><p>A key practical issue of learning different word embeddings for different time periods is alignment. Specifically, most cost functions for training are invariant to rotations, as a byproduct, the learned embeddings across time may not be placed in the same latent space. We call this the alignment problem, which is an issue in general if embeddings are learned independently for each time slice.</p><p>Unlike traditional methods, literature on learning temporal word embedding is relatively short: <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40]</ref>. In general, the approaches in these works follow a similar two-step pattern: first compute static word embeddings in each time slice separately, then find a way to align the word embeddings across time slices. To achieve alignment, <ref type="bibr" target="#b14">[15]</ref> finds a linear transformation of words between any two time slices by solving a d-dimensional least squares problem of k nearest neighbor words (where d is the embedding dimension). Additionally, <ref type="bibr" target="#b39">[40]</ref> also use the linear transformation approach between a base and target time slices, and computes the linear transformation using anchor words, which does not change meaning between the two time slices. This method requires the prior knowledge of words that are in fact static, which involves additional expert supervision. Finally, <ref type="bibr" target="#b11">[12]</ref> imposes the transformation to be orthogonal, and solves a d-dimensional Procrustes problem between every two adjacent time slices.</p><p>Our main novelty in this paper is to learn the word embeddings across time jointly, thus obviating the need to solve a separate alignment problem. Specifically, we propose to learn temporal embeddings in all time slices concurrently, and apply regularization arXiv:1703.00607v2 [cs.CL] 13 Feb 2018 terms to smooth embedding changes across time. There are three main advantages to this proposed joint modeling approach:</p><p>• First, this can be seen as an improvement over traditional, "single-time" methods such as word2vec. • Second, our experimental results suggest that enforcing alignment through regularization yields better results than two-step methods. • Third, we can share information across time slices for the majority of vocabulary. As a result, our method is robust against data sparsity -we can afford to have time slices where some words are rarely present, or even missing. This is a crucial practical advantage offered by our method.</p><p>Since our model requires embeddings across all time slices to be learned at the same time, it can be computationally challenging. To mitigate this, we employ a block coordinate descent method which can handle large vocabulary sizes through decomposition.</p><p>In experimental study, we learn temporal embeddings of words from The New York Times articles between 1990 and 2016. In contrast, previous temporal word embedding works have focused on time-stamped novels and magazine collections (such as Google N-Gram and COHA). However, news corpora are naturally advantageous to studying language evolution through the lens of current events. In addition, it allows us to work with a corpus that maintains consistency in narrative style and grammar, as opposed to Facebook and Twitter posts. For evaluating our embeddings, we develop both qualitative and quantitative metrics.</p><p>Qualitatively, we illustrate the advantages of temporal embeddings for evolving semantics discovery by 1) plotting word vector trajectories to find evolving meanings and associations, 2) using alignment through time to identify associated words across time, and 3) looking at norms as a representative of concept popularity.</p><p>Quantitatively, we first use semantic topics extracted from Sections (e.g., Technology, World News) of news articles as ground truth to evaluate the semantic accuracy of temporal embeddings. Additionally, we provide two testsets to evaluate cross-time alignment quality: one consists of known changing roles (e.g., U.S. presidents), determined objectively, and one of concept replacements (e.g., compact disk to mp3), determined more subjectively. The experimental results show the effectiveness of our proposed model and demonstrate substantial improvements against baseline methods.</p><p>The rest of this paper is organized as follows. In Section 2, we present the proposed model for temporal embedding learning, and in Section 3, we describe a scalable algorithm to train it. In Section 4, we describe the news corpus dataset and setup details of experiments. We perform qualitative evaluations in Section 5. Finally, we quantitatively compare our embeddings against other state-of-the-art temporal embeddings in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY</head><p>We now set up our temporal word embedding model. We consider a text corpus collected across time. These kinds of corpora such as news article collections with published dates or social media discussion with time-stamps are ubiquitous. Formally, we denote by D = (D 1 , . . . , D T ) our text corpus where each D t , t = 1, . . . T , is the corpus of all documents in the t-th time slice. Without loss of generality, we assume the time slices are ordered chronologically.</p><p>The length of these time slices can be on the order of months, years, or decades. Moreover, the length of all the time slices could be different. We consider an overall vocabulary V = {w 1 , . . . , w V } of size V . We note that the vocabulary V consists of words present in the corpus at any point in time, and thus it is possible for some w ∈ V to not appear at all in some D t . This includes emerging words and dying words that are typical in real-world news corpora.</p><p>Given such a time-tagged corpus, our goal is to find a dense, low-dimensional vector representation u w (t) ∈ R d , d ≪ V for each word w ∈ V and each time period t = 1, . . . ,T . We denote by u w the static embedding for word w (for example, learned via word2vec), and d is the embedding dimension (typically 50 ≤ d ≤ 200). Compactly, we denote by U (t) (of size V × d) the embedding matrix of all words whose i-th row corresponds to the embedding vector of i-th word u w i (t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Time-agnostic word embeddings</head><p>A fundamental observation in static word embedding literature is that semantically similar words often have similar neighboring words in a corpus <ref type="bibr" target="#b9">[10]</ref>. This is the idea behind learning dense low-dimensional word representations both traditionally <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref> and recently <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>. In several of these methods, the neighboring structure is captured by the frequencies by which pairs of words co-occur within a small local window.</p><p>We compute the V × V pointwise mutual information (PMI) matrix specific to a corpus D, whose w, c-th entry is:</p><formula xml:id="formula_0">PMI(D, L) w,c = log #(w, c) • |D| #(w) • #(c) ,<label>(1)</label></formula><p>where #(w, c) counts the number of times that words w and c cooccur within a window of size L in corpus D , and #(w), #(c) counts the number of occurrences of words w and c in D. |D | is total number of word tokens in the corpus. L is typically around 5 to 10; we set L = 5 throughout this paper. The key idea behind both word2vec <ref type="bibr" target="#b23">[24]</ref> and GloVE <ref type="bibr" target="#b26">[27]</ref> is to find embedding vectors u w and u c such that for any w, c combination,</p><formula xml:id="formula_1">u T w u c ≈ PMI(D, L) w,c ,<label>(2)</label></formula><p>where each u w has length d ≪ V . While both <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b26">[27]</ref> offer highly scalable algorithms such as negative sampling to do this implicitly, <ref type="bibr" target="#b16">[17]</ref> shows that these are equivalent to low-rank factorization of PMI(D, L) 1 . Our approach is primarily motivated by this observation. We note that though the PMI matrices are of size V × V , in real-world datasets it is typically sparse as observed in <ref type="bibr" target="#b26">[27]</ref>, for which efficient factorization methods exist <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Temporal word embeddings</head><p>A natural extension of the static word embedding intuition is to use this matrix factorization technique on each time slice D t separately. Specifically, for each time slice t, we define the w, c-th entry of</p><formula xml:id="formula_2">positive PMI matrix (PPMI(t, L)) as 2 PPMI(t, L) w,c = max{PMI(D t , L) w,c , 0}. := Y (t).<label>(3)</label></formula><p>with a constant shift that can be zero.</p><p>The temporal word embeddings U (t) must satisfy</p><formula xml:id="formula_3">U (t)U (t) T ≈ PPMI(t, L).<label>(4)</label></formula><p>One way to find such U (t) is for each t, factorizing PPMI(t, L) by either using an eigenvalue method or solving a matrix factorization problem iteratively. The Alignment Problem: Imposing (4) is not sufficient for a unique embedding, since the solutions are invariant under rotation; that is, for any d × d orthogonal matrix R, we have the embedding U (t) = U (t)R , the approximation error in (4) is the same since</p><formula xml:id="formula_4">U (t) U (t) T = U (t)RR T U (t) T = U (t)U (t) T .</formula><p>For this reason, it is important to enforce alignment; if word w did not semantically shift from t to t + 1, then we additionally require u w (t) ≈ u w (t + 1).</p><p>To do this, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> propose two-step procedures; first, they factorize each Y (t) separately, and afterwards enforce alignment using local linear mapping <ref type="bibr" target="#b14">[15]</ref> or solving an orthogonal procrustes problem <ref type="bibr" target="#b11">[12]</ref>. Note that in these methods, aligning U (t) to U (t ′ ) assumes that we desire U (t) ≈ U (t ′ ). If we only pick t ′ = t + 1 (as done in <ref type="bibr" target="#b11">[12]</ref>), this assumption is reasonable because between any two years, only a few words experience semantic shift, emergence, or death. However, this becomes problematic if U (t) was a result of a time period with extremely sparse data (and hence poorly learned); all subsequent year embeddings and previous year embeddings will be poorly aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Our model</head><p>We propose finding temporal word embeddings as the solution of the following joint optimization problem:</p><formula xml:id="formula_5">min U (1), . . .,U (T ) 1 T t =1 ∥Y (t ) − U (t )U (t ) T ∥ 2 F (5) + λ T t =1 ∥U (t ) ∥ 2 F + τ T t =2 ∥U (t − 1) − U (t ) ∥ 2 F ,</formula><p>where Y (t) = PPMI(t, L) and λ, τ &gt; 0. Here the penalty term ∥U (t)∥ 2 F enforces the low-rank data-fidelity as widely adopted in previous literature. The key smoothing term ∥U (t − 1) − U (t)∥ 2 F encourages the word embeddings to be aligned. The parameter τ controls how fast we allow the embeddings to change; τ = 0 enforces no alignment, and picking τ → ∞ converges to a static embedding with U (1) = U (2) = . . . = U (T ). Note that the methods of <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> can be viewed as suboptimal solutions of (5), in that they optimize for each term separately. For one, while the strategies in <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b11">[12]</ref> enforce alignment pairwise, we enforce alignment across all time slices; that is, the final aligned solution U (t) is influenced by not only U (t − 1) and U (t + 1), but every other embedding as well. This avoids the propagation of alignment errors caused by a specific time frame's subsampling. Additionally, consider an extreme case in which word w is absent from D t but has similar meaning in both t − 1 and t + 1. Directly applying any matrix factorization technique to each time point would enforce u w (t) ≈ 0. However, for the right choice of τ , the solution u w (t) to (5) will be close to u w (t − 1) and u w (t + 1). Overall, our method achieves high fidelity embeddings with a much smaller corpus, and in particular, in Section 6, we demonstrate that our embeddings are robust against sudden undersampling of specific time slices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OPTIMIZATION</head><p>A key challenge in solving (5) is that for large V andT , one might not be able to fit all the PPMI matrices Y (1), . . . , Y (T ) in memory, even though Y (t) is sparse. Therefore, for scalability, an obvious solution is to first decompose the objective across time, using alternating minimization to solve for U (t) at each step:</p><formula xml:id="formula_6">min U (t ) f (U (t )) ∥Y (t ) − U (t )U (t ) T ∥ 2 F + λ 2 ∥U (t ) ∥ 2 F (6) + τ 2 ∥U (t − 1) − U (t ) ∥ 2 F + ∥U (t ) − U (t + 1) ∥ 2 F</formula><p>for a specific t. Note that f (U (t)) is quartic in U (t), and thus even if we only solve for a fixed t, (6) cannot be minimized analytically. Thus, our only option is to solve (6) iteratively using a fast firstorder method such as gradient descent. The gradient of the first term alone is given by</p><formula xml:id="formula_7">∇f (U (t)) = −2Y (t)U (t) + 2U (t)U (t) T U (t).<label>(7)</label></formula><p>Each gradient computation is of the order O(nnz(Y (t)</p><formula xml:id="formula_8">)d + d 2 V )</formula><p>(which is then nested in iteratively optimizing U (t) for each t). <ref type="bibr" target="#b2">3</ref> In practical applications, V is in the order of ten-thousands to hundredthousands, and T is in the order of tens to hundreds. Let us instead look at a slightly relaxed problem of minimizing min</p><formula xml:id="formula_9">U (t ),W (t ) 1 T t =1 ∥Y (t ) − U (t )W (t ) T ∥ 2 F + γ T t =1 ∥U (t ) − W (t ) ∥ 2 F (8) + λ T t =1 ∥U (t ) ∥ 2 F + τ T t =2 ∥U (t − 1) − U (t ) ∥ 2 F + λ T t =1 ∥W (t ) ∥ 2 F + τ T t =2 ∥W (t − 1) − W (t ) ∥ 2 F ,</formula><p>where variables W (t), t = 1, . . . ,T are introduced to break the symmetry of factorizing Y (t). Now, minimizing for each U (t) (and equivalently W (t)) is just the solution of a ridge regression problem, and can be solved in one step by setting the gradient of the objective of (8) to 0, i.e. U (t)A = B where Block coordinate descent vs. stochastic gradient descent: The method described here is commonly referred to as block coordinate descent (BCD) because it minimizes with respect to a single block (U (t) or W (t)) at a time, and the block size can be made even smaller (a few rows of U (t) or W (t)) to maintain scalability. The main appeal of BCD is scalability <ref type="bibr" target="#b38">[39]</ref>; however, a main drawback is lack of convergence guarantees, even in the case of convex optimization <ref type="bibr" target="#b28">[29]</ref>.  In practice, however, BCD is highly successful and has been used in many applications <ref type="bibr" target="#b37">[38]</ref>. Another choice of optimization is stochastic gradient descent (SGD), which decomposes the objective as a sum of smaller terms. For example, the first term of (8) can be written as a sum of terms, each using only one row of Y (t):</p><formula xml:id="formula_10">A = W (t) T W (t) + (γ + λ + 2τ )I, B = Y (t)W (t) + γW (t)+τ (U (t − 1) + U (t + 1</formula><formula xml:id="formula_11">f (U (t)) = V i=1 ∥Y (t)[i, :] − u i (t) T W (t)∥ 2 F .<label>(9)</label></formula><p>The complexity at first glance is smaller than that of BCD; however, SGD comes with the well-documented issues of slow progress and hard-to-tune step sizes, and in practice, can be much slower for matrix factorization applications <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref>. However, we point out that the choice of the optimization method is agnostic to our model; anything that successfully solves (5) should lead to an equally successful embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL DATASET AND SETUP</head><p>In this section we describe the specific procedure used to generate embeddings for the next two sections. News article dataset: First, we crawl a total of 99,872 articles from the New York Times, published between January 1990 and July 2016. <ref type="bibr" target="#b3">4</ref> In addition to the text, we also collected metadata including title, author, release date, and section label (e.g., Business, Sports, Technology); in total, there are 59 such sections. We use yearly time slices, dividing the corpus into T = 27 partitions. After removing rare words (fewer than 200 occurrences in all articles across time) and stop words, our vocabulary consists of V = 20, 936 unique words. We then compute a co-occurrence matrix for each time slice t with a window size L = 5, which is then used to compute the PPMI matrix as outlined in <ref type="bibr" target="#b2">(3)</ref>. All the embedding methods that we compared against are trained on this same dataset. Training details for our algorithm: We perform a grid search to find the best regularization and optimization parameters. As a result of our search, we obtain λ = 10, τ = γ = 50, and run for 5 epochs (5 complete pass over all time slices, and all rows and columns of Y (t)). Interestingly, setting λ = 0 also yielded good results, but required more iterations to converge. The block variable is one matrix (U (t) or V (t) for a specific t).</p><p>Distance metric: All distances between two words are calculated by the cosine similarity between embedding vectors:</p><p>The data is available at: https://sites.google.com/site/zijunyaorutgers/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>similarity(a, b)</head><formula xml:id="formula_12">= cosine(u a , u b ) = u T a u b ∥u a ∥ 2 • ∥u b ∥ 2 ,<label>(10)</label></formula><p>where u a and u b are the embeddings of words a and b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">QUALITATIVE EVALUATION</head><p>The embeddings we learn reveal interesting patterns in the shift of word semantics, cross-time semantic analogy, and popularity trends of concepts from the news corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Trajectory visualization</head><p>The trajectory of a word in the (properly aligned) embedded space provides tools to understand the shift in meanings of words over time. This can help broader applications, such as capturing and quantifying linguistic evolution, characterizing brands and people, and analyzing emerging association between certain words. <ref type="figure" target="#fig_2">Figure 1</ref> shows the trajectories of a set of example words. We plot the 2-D t-SNE projection of each word's temporal embedding across time. We also plot the closest words to the target word from each time slice. We pick four words of interest: apple and amazon as emerging corporate names while originally referring to a fruit and a rainforest, and obama and trump as people with changing professional roles.</p><p>In all cases, the embeddings illustrate significant semantic shifts of the words of interest during this 27-year time frame. We see apple shift from a fruit and dessert ingredient to space of technology. Interestingly, there is a spike in 1994 in the trajectory, when Apple led a short tide of discussion because of the replacement of the CEO and a collaboration with IBM; then the association shifted back to neighborhood of fruit and dessert until the recovery by Steve Jobs in early 2000s. Similarly, amazon shifts from a forest to an e-commerce company, finally landing in 2016 as a content creation and online-streaming provider due to the popularity of its Prime Video service. The US president names, obama and trump, are most telling, shifting from their pre-presidential lives (Obama as a civil rights attorney and university professor; Trump as a real estate developer and TV celebrity) to the political sphere. Overall, <ref type="figure" target="#fig_2">Figure  1</ref> demonstrates that first, our temporal word embeddings can well capture the semantic shifts of words across time, and second, our model provides high alignment quality in that same-meaning words across different years have geometrically close embeddings, without having to solve a separate optimization problem for alignment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Equivalence searching</head><p>Another key advantage of word alignment is the ability to find conceptually "equivalent" items or people over time. We provide examples in the field of technology, official roles, and sports professionals. In this type of test, we create a query consisting of a word-year pair that is particularly the representative of that word in that year, and look for other word-year pairs in its vicinity, for different years. <ref type="table" target="#tab_0">Table 1</ref> lists the closest words (top-1) of each year to the query vector. For visualization purpose we lump semantically similar words together. For example, the first column shows that iphone in 2012 is closely associated with smartphones in recent years, but is close to words such as desktop and macintosh in the 90's; interestingly, telephone never appears, suggesting the iPhone serves people more as a portable computer than a calling device. As another example, by looking at the trajectory of twitter, we see the evolution of news sources, from TV &amp; radio news broadcasts in the 90s to chatrooms, websites, and emails in the early 2000s, blogs in the late 2000s, and finally tweets today. The last example is fairly obvious; mp3 represents the main form of which music is consumed in 2000, replacing disk and stereo in 1990s ( cassette also appears in top-3) and is later replaced by online streaming. We can see a one-year spike of Napster which was shut down because of copyright infringement <ref type="bibr" target="#b4">5</ref> , and later a new streaming service -iTunes.</p><p>Next, we use embeddings to identify people in political roles. <ref type="table" target="#tab_1">Table 2</ref> attempts to discover who is the U.S. president and New York City mayor 7 of the time, using as query obama in 2016 and blasio in 2015. For president, only the closest word from each year is listed, and is always correct (accounting for the election years). For mayor, the top-1 closet word is shown unless it is mayor, in which case the second word is shown. We can see that both the roles of US president and NYC mayor have been well searched for different  persons in their terms of service. We see that the embedding for the President is consistent, and for the most part, so is that of the mayor of NYC. In 2011, cuomo is also partially relevant since he was the governor of NY state. We did not find any relevant words in query NYC mayor in year 2006. Finally, we search for equivalences in sports, repeating the experiment for the ATP rank 1 male tennis player as shown in <ref type="table" target="#tab_2">Table 3</ref>. In the case of president and mayor, we are heavily assisted by the fact that they are commonly referred to by a title: "President Obama" and "Mayor de Blasio". Tennis champions, on the other hand, are not referred by titles. Still, a surprising number of correct champions appear as the closest words, and all the names are those of famous tennis players for the given time period. A more exhaustive empirical study of alignment quality is provided in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Popularity determination</head><p>It has often been observed that word embeddings computed by factorizing PMI matrices have norms that grow with word frequency <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref>. These word vector norms across time can be viewed as a time series for detecting the trending concepts (e.g., sudden semantic shifts or emergences) behind words, with more robustness than word frequency. <ref type="figure" target="#fig_3">Figures 2 and 3</ref> illustrate the comparison between embedding norm and frequency for determining concept popularity per year, determined by key words in the New York Times corpus. Generally, comparing to frequencies which are much more sporadic and noisy, we note that the norm of our embeddings encourages smoothness and normalization while being indicative of the periods when the corresponding words were making news rounds. In <ref type="figure" target="#fig_3">Figure 2</ref>, the embedding norms display nearly even 4-year humps corresponding to each president's term. In every term, the name of each current president becomes a trending concept which plays an important role in the information structure at the time. Two interesting observations can be gleaned. First, since Hillary Clinton continuously served as Secretary of State during 2009-2013, the popularity of clinton was preserved; however it was still not as popular as president obama. Second, because of the presidential campaign, trump in 2016 has a rising popularity that greatly surpasses that of his former role as a business man, and eventually surpasses his opponent clinton in terms of news coverage.</p><p>In <ref type="figure" target="#fig_4">Figure 3</ref>, we can see smooth rises and falls of temporary phenomena (the enron scandal and qaeda rises). For qaeda, we see that there is a jump in 2001, and then it remains steady with a small decline. In contrast, enron shows a sharper decline, as despite its temporal hype, it did not linger in the news long. Note also the stability of using norms to track popularity over frequency, which spikes for enron above qaeda, although 9/11 was far more news-prevalent than the corporation's scandalous decline. For the basketball star pippen, although his publicity (e.g., frequency) was relatively fewer than business terms, his popularity is still recognized by the enhancement in vector norm. For another term isis, we can see that it begins to replace qaeda as the "trending terrorist organization" in news media. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">QUANTITATIVE EVALUATION</head><p>In this section, we empirically evaluate our proposed Dynamic Word2Vec model (DW2V) against other temporal word embedding methods. <ref type="bibr" target="#b7">8</ref> In all cases, we set the embedding dimension to d = 50. We have the following baselines:</p><p>• Static-Word2Vec (SW2V): the standard word2vec embeddings <ref type="bibr" target="#b24">[25]</ref>, trained on the entire corpus and ignoring time information. • Transformed-Word2Vec (TW2V) <ref type="bibr" target="#b14">[15]</ref>: the embeddings U (t)</p><p>are first trained separately by factorizing PPMI matrix for each year t, and then transformed by optimizing a linear transformation matrix which minimizes the distance between u w (t) and u w (t ′ ) for the k = 30 nearest words' embeddings to the querying word w. • Aligned-Word2Vec (AW2V) <ref type="bibr" target="#b11">[12]</ref>: the embeddings U (t) are first trained by factorizing the PPMI matrix for each year t, and then aligned by searching for the best othornormal transformation between U (t) and U (t + 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Semantic similarity</head><p>One of the most important properties of a word embedding is how accurately it carries the meaning of words. Therefore, we develop a test to see if words can be categorized by meaning based on embeddings. The news articles we collected are tagged with their "sections" such as Business, Sports. This information can be used to determine temporal word meanings. It is important to note that this information is not used in the word embedding learning. For example, we see that amazon occurs 41% of the time in World in 1995, associating strongly with the rainforest (not part of the USA), and 50% of the time in Technology in 2012, associating strongly with e-commerce. We thus use this to establish a ground truth of word category, by identifying words in years that are exceptionally numerous in one particular news section. That is, if a word is extremely frequent in a particular section, we associate that word with that section and use that as ground truth. We select the 11</p><p>The testsets are available at: https://sites.google.com/site/zijunyaorutgers/.  most popular and discriminative sections of the New York Times, and for each section s and each word w in year t, we compute its percentage p of occurrences in each section. To avoid duplicated word-time-section &lt; w, t, s &gt; triplets, for a particular w and s we only keep the year of the largest strength, and additionally filter away any triplet with strength less than p = 35%. Note that a random uniform distribution of words would result in it appearing about 9% of the time in each section, and our threshold is about 4 times that quantity. We do this to say with sufficient confidence that such associations can be treated as ground truth.</p><p>To limit the size differences among categories, for every section s with more than 200 qualified triplets, we keep the top-200 words by strength. In total, this results in 1888 triplets across 11 sections, where every word-year pair is strongly associated with a section as its true category.</p><p>We then apply spherical k-means, which uses cosine similarity between embeddings as the distance function for clustering, with K = 10, 15, and 20 clusters. We use two metrics to evaluate the clustering results:</p><formula xml:id="formula_13">• Normalized Mutual Information (NMI), defined as N MI (L, C) = I (L; C) [H (L) + H (C)]/2 ,<label>(11)</label></formula><p>where L represents the set of labels and C the set of clusters. I (L; C) denotes the sum of mutual information between any cluster c i and any label l j , and H (L) and H (C) the entropy for labels and clusters, respectively. This metric evaluates the purity of clustering results from an information-theoretic perspective.</p><formula xml:id="formula_14">• F β -measure (F β ), defined as F β = (β 2 + 1)PR β 2 P + R ,<label>(12)</label></formula><p>where P = T P T P +F P denotes the precision and R = T P T P +F N denotes the recall. (TP/FP = true/false positive, TN/FN = true/false negative.) As an alternative method to evaluate clustering, we can view every pair of words as a series of decisions. Pick any two (w, t) pairs. If they are clustered together and additionally have the same section label, this is a correct decision; otherwise, the clustering performed a wrong decision. The metric F β measures accuracy as the (β-weighted) harmonic mean of the precision and  recall. We set β = 5 to give more weight to recall by penalizing false negative more strongly. <ref type="table" target="#tab_3">Tables 4 and 5</ref> show the clustering evaluation. We can see that our proposed DW2V consistently outperforms other baselines for all values of K. These results show two advantages. First, the word semantic shift has been captured by the temporal embeddings (for example, by correlating correctly with the section label of amazon, which changes from World to Technology). Second, since embeddings of words of all years are used for clustering, a good clustering result indicates good alignment across years. We can also see that AW2V also performs well, as it also applies alignment between adjacent time slices for all words. However, TW2V does not perform well as others, suggesting that aligning locally (only a few words) is not sufficient for high alignment quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Alignment quality</head><p>We now more directly evaluate alignment quality, i.e. the property that the semantic distribution in temporal embedding space should be consistent over time. For example, if a word such as estate or republican does not change much throughout time, its embedding should remain relatively constant for different t. By the same logic, if a word such as trump does change association throughout time, its embedding should reflect this shift by moving from one position to another (e.g., estate → republican). We saw this in the previous section for static words like president or mayor; they do not change meanings, though they are accompanied by names that shift to them every few years.</p><p>To examine the quality of embedding alignment, we create a task to query equivalences across years. For example, given obama-2012, we want to query its equivalent word in 2002. As we know obama is the U.S. president in 2012; its equivalent in 2002 is bush, who was the U.S. president at that time. In this way, we create two testsets.</p><p>The first one is based on publicly recorded knowledge that for each year lists different names for a particular role, such as U.S. president, U.K. prime minister, NFL superbowl champion team, and so on. For each year (e.g., 2012), we put its word (e.g., obama) into the embedding set of every other year for query its equivalence in top closest words.</p><p>The second test is human-generated, for exploring more interesting concepts like emerging technologies, brands and major events (e.g., disease outbreaks and financial crisis). For constructing the test word pairs, we first select emerging terms which have not been popularized before 1994, then query their well known precedents during 1990 to 1994 (e.g., app-2012 can correspond to software-1990). For emerging word (e.g., app) we extract its most popular year (e.g., 2012) with maximum frequency, and put its embedding into each year from 1990 to 1994 for querying its precedent (e.g., software). Each word-year pair now forms a query and an answer; in total we have N = 11028 such pairs in the first testset, and N = 445 in the second one.</p><p>We use two metrics to evaluate the performance.</p><p>• For each test i, the correct answer word is identified at position rank[i] for closest words. The Mean Reciprocal Rank (MRR) is defined as</p><formula xml:id="formula_15">MRR = N N i=1 1 rank[i] ,<label>(13)</label></formula><p>where 1 rank[i] = 0 if the correct answer is not found in the top-10. Higher MRR means that correct answers appear more closely and unambiguously with the query embedding.</p><p>• Additionally, for test i consisting of a query and target word-year pair, consider the closest K words to the query embedding in the target year. If the target word is among these K words, then the Precision@K for test i (denoted P@K[i]) is 1; else, it is 0. Then the Mean Precision@K is defined as</p><formula xml:id="formula_16">MP@K = 1 N N i=1 (P@K[i]).<label>(14)</label></formula><p>Higher precision indicates a better ability to acquire correct answers using close embeddings. <ref type="table" target="#tab_5">Tables 6 and 7</ref> show the evaluation of the alignment test. We can see that our proposed method outperforms others and shows good alignment quality, sometimes by an order of magnitude. Comparing to testset 1 which has a large amount of queries with considerable short range alignments (e.g., from 2012 to 2013), the testset 2 mostly consists of fewer long range alignments (e.g., 2012 to 1990). Therefore, we can see that the performance of SW2V is relatively good in testset 1 since the semantic distribution does not change much in short ranges which makes this test favorable to static embeddings. However, SW2V degrades sharply in testset 2, where the long range alignment is needed more. For TW2V, since it does an individual year-to-year (e.g., 2012-to-1990) transformation by assuming that the local structure of target words does not shift, its overall alignment quality of whole embedding sets is not satisfied in testset 1 with many alignments. However, it does similarly to AW2V in testset 2 because its individual year-to-year transformation makes it more capable for long range alignment. AW2V, which enforces alignment for whole embedding sets between adjacent time slices, provides quite reliable performance. However, its alignment quality is still below ours, suggesting that their two-step approach is not as successful in enforcing global alignment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Robustness</head><p>Finally, we explore the robustness of our embedding model against subsampling of words for select years. <ref type="table" target="#tab_7">Table 8</ref> shows the result of the alignment task (testset 1) for vectors computed from subsampled co-occurrence matrices for every three years from 1991 to 2015. To subsample, each element C i j is replaced with a randomly drawn integerĈ i j from a Binomial distribution for rate r and n = C i j trials; this simulates the number of co-occurrences measured if they had been missed with probability r . The new frequencyf is then renormalized so thatf i /f i = jĈi j / j C i j . Listed are the alignment test results for r = 1, 0.1, 0.01, and 0.001 compared against <ref type="bibr" target="#b11">[12]</ref>, which otherwise performs comparably with our embedding. Unsurprisingly, for extreme attacks (leaving only 1% or 0.1% co-occurrences), the performance of <ref type="bibr" target="#b11">[12]</ref> degrades sharply; however, because of our joint optimization approach, the performance of our embeddings seems to hold steady.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Temporal effects in natural language processing: There are several studies that investigate the temporal features of natural language. Some are using topic modeling on news corpus <ref type="bibr" target="#b0">[1]</ref> or timestamped scientific journals <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> to find spikes and emergences of themes and viewpoints. Simpler word count features are used in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22</ref>] to find hotly discussed concepts and cultural phenomena, in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref> to analyze teen behavior in chatrooms, and in <ref type="bibr" target="#b12">[13]</ref> to discover incidents of influenza. Word embedding learning: The idea of word embeddings has existed at least since the 90s, with vectors computed as rows of the co-occurrence <ref type="bibr" target="#b19">[20]</ref>, through matrix factorization <ref type="bibr" target="#b8">[9]</ref>, and most famously through deep neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. They have recently been repopularized with the success of low-dimensional embeddings like GloVE <ref type="bibr" target="#b26">[27]</ref> and word2vec <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, which have been shown to greatly improve the performance in key NLP tasks, like document clustering <ref type="bibr" target="#b15">[16]</ref>, LDA <ref type="bibr" target="#b27">[28]</ref>, and word similarity <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. There is a close connection between these recent methods and our proposed method, in that both word2vec and GloVE have been shown to be equivalent to matrix factorization of a shifted PMI matrix <ref type="bibr" target="#b16">[17]</ref>.</p><p>Temporal word embeddings and evaluations: While NLP tools have been used frequently to discover emerging word meanings and societal trends, many of them rely on changes in the co-occurrence or PMI matrix <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref>, changes in parts of speech, <ref type="bibr" target="#b22">[23]</ref> or other statistical methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>. A few works use lowdimensional word embeddings, but either do no smoothing <ref type="bibr" target="#b30">[31]</ref>, or use two-step methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Semantic shift and emergence are also evaluated in many different ways. In <ref type="bibr" target="#b30">[31]</ref>, word shifts are identified by tracking the mean angle between a word and its neighbors. One of the several tests in <ref type="bibr" target="#b14">[15]</ref> create synthetic data with injected semantic shifts, and quantifies the accuracy of capturing them using various time series metrics. In <ref type="bibr" target="#b22">[23]</ref>, the authors show the semantic meaningfulness of key lexical features by using them to predict the time-stamp of a particular phrase. And, <ref type="bibr" target="#b25">[26]</ref> makes the connection that emergent meanings usually coexist with previous meanings, and use dynamic embeddings to discover and identify multisenses, evaluated against WordNet. Primarily, temporal word embeddings are evaluated against human-created databases of known semantically shifted words <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref> which is our approach as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We studied the evolution of word semantics as a dynamic word embedding learning problem. We proposed a model to learn timeaware word embeddings and used it to dynamically mine text corpora. Our proposed method simultaneously learns the embeddings and aligns them across time, and has several benefits: higher interpretability for embeddings, better quality with less data, and more reliable alignment for across-time querying. We solved the resulting optimization problem using a scalable block coordinate descent method. We designed qualitative and quantitative methods to evaluate temporal embeddings for evolving word semantics, and showed that our dynamic embedding method performs favorably against other temporal embedding approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>)) for t = 2, . . . ,T − 1, and constants adjusted for t = 0,T . Forming A and B requires O(V d 2 + nnz(Y (t))d), and solving U (t)A = B can be done in O(d 3 ) computations, in one step. This can be further decomposed to row-by-row blocks of size b, by minimizing over a row block of U (t) at a time, which reduces the complexity of forming A and B to O(bd 2 + nnz(Y (t)[: b, :])d), i.e.independent of V . This allows scaling for very large V , as only one row block of Y (t) must be loaded at a time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>nnz(•)is the number of nonzeros in the matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Trajectories of brand names and people through time: apple, amazon, obama, and trump.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Norm (top)  and relative frequency (bottom) throughout years 1990-2016. We select the names of U.S presidents within this time frame -clinton, bush, obama, and trump. We note that bush could refer to George H. W. Bush(1989)(1990)(1991)(1992)(1993) or George W. Bush(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008)(2009). Clinton could refer to Bill Clinton(1993)(1994)(1995)(1996)(1997)(1998)(1999)(2000)(2001) or Hillary Clinton (U.S. secretary of state in 2009-2013 and U.S. presidential candidate in 2014-2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Norm (top)  and relative frequency (bottom) per year in corpus of major event keywords: pippen for basketball stardom, enron for corporation scandal, qaeda and isis for emerging terrorism groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Equivalent technologies through time: iphone, twitter, and mp3.</figDesc><table><row><cell cols="2">Query iphone, 2012</cell><cell>twitter, 2012</cell><cell>mp3, 2000</cell></row><row><cell>90-94</cell><cell>desktop, pc,</cell><cell>broadcast, cnn,</cell><cell>stereo, disk,</cell></row><row><cell></cell><cell>dos, macintosh,</cell><cell>bulletin, tv,</cell><cell>disks, audio</cell></row><row><cell>95-96</cell><cell>software</cell><cell>radio, messages, correspondents</cell><cell>mp3</cell></row><row><cell>98-02</cell><cell></cell><cell>chat, messages,</cell><cell></cell></row><row><cell></cell><cell>pc</cell><cell>emails, web</cell><cell>napster</cell></row><row><cell></cell><cell></cell><cell></cell><cell>mp3</cell></row><row><cell cols="2">05-06 ipod 07-08 iphone 09-12 13-16 smartphone,</cell><cell>blog, posted twitter</cell><cell>itunes, downloaded</cell></row><row><cell></cell><cell>iphone</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>"Who governed?" The closest word to obama at year 2016 (role as president of United State) and blasio at year 2015 (role as mayor of New York City (NYC)). The stars indicate incorrect answers.</figDesc><table><row><cell cols="2">Question US president</cell><cell>NYC mayor</cell></row><row><cell>Query</cell><cell>obama, 2016</cell><cell>blasio, 2015</cell></row><row><cell>90-92</cell><cell>bush</cell><cell>dinkins</cell></row><row><cell>94-00</cell><cell>clinton</cell><cell>giuliani</cell></row><row><cell>02-05</cell><cell></cell><cell>bloomberg</cell></row><row><cell></cell><cell>bush</cell><cell>n/a*</cell></row><row><cell></cell><cell></cell><cell>bloomberg</cell></row><row><cell>09-10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>obama</cell><cell>cuomo* bloomberg</cell></row><row><cell>13-16</cell><cell></cell><cell>blasio</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>year</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>word</cell><cell>edberg</cell><cell>lendl</cell><cell>sampras</cell><cell>sampras</cell></row><row><cell cols="5">sampras sampras ivanisevic sampras sampras</cell></row><row><cell>sampras</cell><cell>sampras</cell><cell>agassi</cell><cell>capriati</cell><cell>roddick</cell></row><row><cell>federer</cell><cell>federer</cell><cell>roddick</cell><cell>federer</cell><cell>nadal</cell></row><row><cell>federer</cell><cell>nadal</cell><cell>djokovic</cell><cell>federer</cell><cell>federer</cell></row><row><cell>federer</cell><cell>djokovic</cell><cell></cell><cell></cell><cell></cell></row></table><note>"Who was the ATP No.1 ranked male player?" The closest word to nadal at year 2010 for each year is listed. The correct answer is based on ATP year-end ranking and are bolded in the table.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Normailized Mutual Information (NMI).</figDesc><table><row><cell cols="4">Method Clusters Clusters Clusters</cell></row><row><cell>SW2V</cell><cell>0.6736</cell><cell>0.6867</cell><cell>0.6713</cell></row><row><cell>TW2V</cell><cell>0.5175</cell><cell>0.5221</cell><cell>0.5130</cell></row><row><cell>AW2V</cell><cell>0.6580</cell><cell>0.6618</cell><cell>0.6386</cell></row><row><cell>DW2V</cell><cell>0.7175</cell><cell>0.7162</cell><cell>0.6906</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>F-measure (F β ).</figDesc><table><row><cell cols="4">Method Clusters Clusters Clusters</cell></row><row><cell>SW2V</cell><cell>0.6163</cell><cell>0.7147</cell><cell>0.7214</cell></row><row><cell>TW2V</cell><cell>0.4584</cell><cell>0.5072</cell><cell>0.5373</cell></row><row><cell>AW2V</cell><cell>0.6530</cell><cell>0.7115</cell><cell>0.7187</cell></row><row><cell>DW2V</cell><cell>0.6949</cell><cell>0.7515</cell><cell>0.7585</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Mean Reciprocal Rank (MRR) and Mean Precision (MP) for Testset 1.</figDesc><table><row><cell>Method</cell><cell>MRR</cell><cell cols="4">MP@1 MP@3 MP@5 MP@10</cell></row><row><cell>SW2V</cell><cell>0.3560</cell><cell>0.2664</cell><cell>0.4210</cell><cell>0.4774</cell><cell>0.5612</cell></row><row><cell>TW2V</cell><cell>0.0920</cell><cell>0.0500</cell><cell>0.1168</cell><cell>0.1482</cell><cell>0.1910</cell></row><row><cell>AW2V</cell><cell>0.1582</cell><cell>0.1066</cell><cell>0.1814</cell><cell>0.2241</cell><cell>0.2953</cell></row><row><cell cols="6">DW2V 0.4222 0.3306 0.4854 0.5488 0.6191</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Mean Reciprocal Rank (MRR) and Mean Precision (MP) for Testset 2.</figDesc><table><row><cell>Method</cell><cell>MRR</cell><cell cols="4">MP@1 MP@3 MP@5 MP@10</cell></row><row><cell>SW2V</cell><cell>0.0472</cell><cell>0.0000</cell><cell>0.0787</cell><cell>0.0787</cell><cell>0.2022</cell></row><row><cell>TW2V</cell><cell>0.0664</cell><cell>0.0404</cell><cell>0.0764</cell><cell>0.0989</cell><cell>0.1438</cell></row><row><cell>AW2V</cell><cell>0.0500</cell><cell>0.0225</cell><cell>0.0517</cell><cell>0.0787</cell><cell>0.1416</cell></row><row><cell cols="6">DW2V 0.1444 0.0764 0.1596 0.2202 0.3820</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>MRR and MP for alignment with every 3 years subsampling.</figDesc><table><row><cell>Method</cell><cell>r</cell><cell>MRR</cell><cell cols="2">MP@1 MP@3 MP@5 MP@10</cell></row><row><cell>AW2V</cell><cell cols="3">100% 0.1582 0.1066 0.1814 0.2241</cell><cell>0.2953</cell></row><row><cell>AW2V</cell><cell>10%</cell><cell cols="2">0.0884 0.0567 0.1020 0.1287</cell><cell>0.1727</cell></row><row><cell>AW2V</cell><cell>1%</cell><cell cols="2">0.0409 0.0255 0.0475 0.0605</cell><cell>0.0818</cell></row><row><cell>AW2V</cell><cell cols="3">0.1% 0.0362 0.0239 0.0416 0.0532</cell><cell>0.0690</cell></row><row><cell cols="4">DW2V 100% 0.4222 0.3306 0.4854 0.5488</cell><cell>0.6191</cell></row><row><cell>DW2V</cell><cell>10%</cell><cell cols="2">0.4394 0.3489 0.5036 0.5628</cell><cell>0.6292</cell></row><row><cell>DW2V</cell><cell>1%</cell><cell cols="2">0.4418 0.3522 0.5024 0.5636</cell><cell>0.6310</cell></row><row><cell>DW2V</cell><cell cols="3">0.1% 0.4427 0.3550 0.5006 0.5612</cell><cell>0.6299</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We consider the PPMI rather than the PMI because when#(w,c )•|D | #(w )•#(c )is very small, taking the log results in large negative values and is thus extremely unstable. Since for most significantly related pairs w and c the log argument is &gt; 1, thresholding it in this way will not affect the solution significantly, but will offer much better numerical stability. This approach is not unique to us;<ref type="bibr" target="#b16">[17]</ref> also factorizes the PPMI.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Napster ended its streaming service in 2001, so our equivalence is captured 2 years late; this delay could be because though the event happened in 2001, the legal ramifications were analyzed heavily in subsequent years.<ref type="bibr" target="#b5">6</ref> All data was scraped about half a year before Donald Trump was elected as U.S. president in 2016.<ref type="bibr" target="#b6">7</ref> We intentionally choose New York City because it is the most heavily discussed city in the New York Times.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Arts, Business, Fashion &amp; Style, Health, Home &amp; Garden, Real Estate, Science, Sports, Technology, U.S., World.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Temporal summaries of new topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Khandelwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 24th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03520</idno>
		<title level="m">Rand-walk: A latent variable model approach to word embeddings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysing word meaning over time by exploiting temporal random indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierpaolo</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annalina</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Semeraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Italian Conference on Computational Linguistics CLiC-it</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting the present with Google Trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Varian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic Record</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="2" to="9" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">391</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">{A synopsis of linguistic theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John R Firth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Gulordava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="67" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09096</idno>
		<title level="m">Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Change of Topics over Time-Tracking Topics by their Change of Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Heyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Teresniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="223" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-I</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Hanaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darshan</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3515</idno>
		<title level="m">Temporal analysis of language through neural language models</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistically significant detection of linguistic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="625" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From Word Embeddings To Document Distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicholas I Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analysing the Semantic Change Based on Word Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Processing of Oriental Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="213" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Producing high-dimensional semantic spaces from lexical co-occurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curt</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="203" to="208" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Teenagers in cyberspace: An investigation of language use and language change in internet chatrooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research in Reading</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="293" to="306" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quantitative analysis of culture using millions of digitized books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan Kui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviva</forename><surname>Presser Aiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Veres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hoiberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page" from="176" to="182" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Jon Orwant, and others</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word epoch disambiguation: Finding how words change over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="259" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">That&apos;s sick dude!: Automatic identification of word sense change across different timescales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunny</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritwik</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4392</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Word features for latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Petterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shravan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tibério</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1921" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On search directions for minimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="193" to="201" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collaborative filtering with graph information: Consistency and scalable methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2107" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tracing semantic change with latent semantic analysis. Current methods in historical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brady</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="161" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unnur Gretarsdottir, and Megan Huddleston</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Coreena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ginsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;02 extended abstracts on Human factors in computing systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="594" to="595" />
		</imprint>
	</monogr>
	<note>Teen use of messaging media</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Temporal corpus summarization using submodular word coverage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Sipos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pannaga</forename><surname>Shivaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Linguistic ruin? LOL! Instant messaging and teen language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Tagliamonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American speech</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="3" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic change computation: A successive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuri</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiguang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohe</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="375" to="415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Topics over time: a non-Markov continuous-time model of topical trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuerui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Understanding semantic change of words over centuries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reyyan</forename><surname>Derry Tanti Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeniterzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 international workshop on DETecting and Exploiting Cultural diversiTy on the social web</title>
		<meeting>the 2011 international workshop on DETecting and Exploiting Cultural diversiTy on the social web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Coordinate descent algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="3" to="34" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable coordinate descent approaches to parallel matrix factorization for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Past is Not a Foreign Country: Detecting Semantically Similar Terms across Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sourav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsumi</forename><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2793" to="2807" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
