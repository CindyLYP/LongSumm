<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-03-08">8 Mar 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Balog</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Also affiliated with Max-Planck Institute for Intelligent Systems</orgName>
								<address>
									<settlement>TÃ¼bingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-03-08">8 Mar 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1611.01989v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network&apos;s predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>A dream of artificial intelligence is to build systems that can write computer programs. Recently, there has been much interest in program-like neural network models <ref type="bibr">(Graves et al., 2014;</ref><ref type="bibr">Kurach et al., 2015;</ref><ref type="bibr">Joulin &amp; Mikolov, 2015;</ref><ref type="bibr">Grefenstette et al., 2015;</ref><ref type="bibr" target="#b15">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b5">Neelakantan et al., 2016;</ref><ref type="bibr">Kaiser &amp; Sutskever, 2016;</ref><ref type="bibr" target="#b8">Reed &amp; de Freitas, 2016;</ref><ref type="bibr" target="#b17">Zaremba et al., 2016;</ref><ref type="bibr">Graves et al., 2016)</ref>, but none of these can write programs; that is, they do not generate human-readable source code. Only very recently, <ref type="bibr" target="#b9">Riedel et al. (2016)</ref>; <ref type="bibr" target="#b1">Bunel et al. (2016)</ref>; <ref type="bibr">Gaunt et al. (2016)</ref> explored the use of gradient descent to induce source code from input-output examples via differentiable interpreters, and <ref type="bibr">Ling et al. (2016)</ref> explored the generation of source code from unstructured text descriptions. However, <ref type="bibr">Gaunt et al. (2016)</ref> showed that differentiable interpreterbased program induction is inferior to discrete search-based techniques used by the programming languages community. We are then left with the question of how to make progress on program induction using machine learning techniques.</p><p>In this work, we propose two main ideas: (1) learn to induce programs; that is, use a corpus of program induction problems to learn strategies that generalize across problems, and (2) integrate neural network architectures with search-based techniques rather than replace them.</p><p>In more detail, we can contrast our approach to existing work on differentiable interpreters. In differentiable interpreters, the idea is to define a differentiable mapping from source code and inputs to outputs. After observing inputs and outputs, gradient descent can be used to search for a program that matches the input-output examples. This approach leverages gradient-based optimization, which has proven powerful for training neural networks, but each synthesis problem is still solved independently-solving many synthesis problems does not help to solve the next problem.</p><p>We argue that machine learning can provide significant value towards solving Inductive Program Synthesis (IPS) by re-casting the problem as a big data problem. We show that training a neural network on a large number of generated IPS problems to predict cues from the problem description can help a search-based technique. In this work, we focus on predicting an order on the program space and show how to use it to guide search-based techniques that are common in the programming languages community. This approach has three desirable properties: first, we transform a difficult search problem into a supervised learning problem; second, we soften the effect of failures of the neural network by searching over program space rather than relying on a single prediction; and third, the neural network's predictions are used to guide existing program synthesis systems, allowing us to use and improve on the best solvers from the programming languages community. Empirically, we Published as a conference paper at ICLR 2017 show orders-of-magnitude improvements over optimized standard search techniques and a Recurrent Neural Network-based approach to the problem.</p><p>In summary, we define and instantiate a framework for using deep learning for program synthesis problems like ones appearing on programming competition websites. Our concrete contributions are:</p><p>1. defining a programming language that is expressive enough to include real-world programming problems while being high-level enough to be predictable from input-output examples;</p><p>2. models for mapping sets of input-output examples to program properties; and 3. experiments that show an order of magnitude speedup over standard program synthesis techniques, which makes this approach feasible for solving problems of similar difficulty as the simplest problems that appear on programming competition websites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BACKGROUND ON INDUCTIVE PROGRAM SYNTHESIS</head><p>We begin by providing background on Inductive Program Synthesis, including a brief overview of how it is typically formulated and solved in the programming languages community.</p><p>The Inductive Program Synthesis (IPS) problem is the following: given input-output examples, produce a program that has behavior consistent with the examples.</p><p>Building an IPS system requires solving two problems. First, the search problem: to find consistent programs we need to search over a suitable set of possible programs. We need to define the set (i.e., the program space) and search procedure. Second, the ranking problem: if there are multiple programs consistent with the input-output examples, which one do we return? Both of these problems are dependent on the specifics of the problem formulation. Thus, the first important decision in formulating an approach to program synthesis is the choice of a Domain Specific Language.</p><p>Domain Specific Languages (DSLs). DSLs are programming languages that are suitable for a specialized domain but are more restrictive than full-featured programming languages. For example, one might disallow loops or other control flow, and only allow string data types and a small number of primitive operations like concatenation. Most of program synthesis research focuses on synthesizing programs in DSLs, because full-featured languages like C++ enlarge the search space and complicate synthesis. Restricted DSLs can also enable more efficient special-purpose search algorithms. For example, if a DSL only allows concatenations of substrings of an input string, a dynamic programming algorithm can efficiently search over all possible programs <ref type="bibr" target="#b7">(Polozov &amp; Gulwani, 2015)</ref>. The choice of DSL also affects the difficulty of the ranking problem. For example, in a DSL without if statements, the same algorithm is applied to all inputs, reducing the number of programs consistent with any set of input-output examples, and thus the ranking problem becomes easier. Of course, the restrictiveness of the chosen DSL also determines which problems the system can solve at all.</p><p>Search Techniques. There are many techniques for searching for programs consistent with inputoutput examples. Perhaps the simplest approach is to define a grammar and then enumerate all derivations of the grammar, checking each one for consistency with the examples. This approach can be combined with pruning based on types and other logical reasoning <ref type="bibr">(Feser et al., 2015)</ref>. While simple, these approaches can be implemented efficiently, and they can be surprisingly effective.</p><p>In restricted domains such as the concatenation example discussed above, special-purpose algorithms can be used. FlashMeta <ref type="bibr" target="#b7">(Polozov &amp; Gulwani, 2015)</ref> describes a framework for DSLs which allow decomposition of the search problem, e.g., where the production of an output string from an input string can be reduced to finding a program for producing the first part of the output and concatenating it with a program for producing the latter part of the output string.</p><p>Another class of systems is based on Satisfiability Modulo Theories (SMT) solving. SMT combines SAT-style search with theories like arithmetic and inequalities, with the benefit that theorydependent subproblems can be handled by special-purpose solvers. For example, a special-purpose solver can easily find integers x, y such that x &lt; y and y &lt; â100 hold, whereas an enumeration strategy may need to consider many values before satisfying the constraints. Many program synthesis engines based on SMT solvers exist, e.g., Sketch <ref type="bibr" target="#b13">(Solar-Lezama, 2008)</ref> and Brahma <ref type="bibr">(Gulwani et al., 2011)</ref>. They convert the semantics of a DSL into a set of constraints between variables representing the program and the input-output values, and then call an SMT solver to find a satisfying setting of the program variables. This approach shines when special-purpose reasoning can be leveraged, but complex DSLs can lead to very large constraint problems where constructing and manipulating the constraints can be a lot slower than an enumerative approach.</p><p>Finally, stochastic local search can be employed to search over program space, and there is a long history of applying genetic algorithms to this problem. One of the most successful recent examples is the STOKE super-optimization system <ref type="bibr" target="#b10">(Schkufza et al., 2016)</ref>, which uses stochastic local search to find assembly programs that have the same semantics as an input program but execute faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ranking.</head><p>While we focus on the search problem in this work, we briefly mention the ranking problem here. A popular choice for ranking is to choose the shortest program consistent with inputoutput examples <ref type="bibr">(Gulwani, 2016)</ref>. A more sophisticated approach is employed by FlashFill <ref type="bibr" target="#b12">(Singh &amp; Gulwani, 2015)</ref>. It works in a manner similar to max-margin structured prediction, where known ground truth programs are given, and the learning task is to assign scores to programs such that the ground truth programs score higher than other programs that satisfy the input-output specification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING INDUCTIVE PROGRAM SYNTHESIS (LIPS)</head><p>In this section we outline the general approach that we follow in this work, which we call Learning Inductive Program Synthesis (LIPS). The details of our instantiation of LIPS appear in Sect. 4. The components of LIPS are (1) a DSL specification, (2) a data-generation procedure, (3) a machine learning model that maps from input-output examples to program attributes, and (4) a search procedure that searches program space in an order guided by the model from (3). The framework is related to the formulation of <ref type="bibr" target="#b4">Menon et al. (2013)</ref>; the relationship and key differences are discussed in Sect. 6.</p><p>(1) DSL and Attributes. The choice of DSL is important in LIPS, just as it is in any program synthesis system. It should be expressive enough to capture the problems that we wish to solve, but restricted as much as possible to limit the difficulty of the search. In LIPS we additionally specify an attribute function A that maps programs P of the DSL to finite attribute vectors a = A(P ). (Attribute vectors of different programs need not have equal length.) Attributes serve as the link between the machine learning and the search component of LIPS: the machine learning model predicts a distribution q(a | E), where E is the set of input-output examples, and the search procedure aims to search over programs P as ordered by q(A(P ) | E). Thus an attribute is useful if it is both predictable from input-output examples, and if conditioning on its value significantly reduces the effective size of the search space.</p><p>Possible attributes are the (perhaps position-dependent) presence or absence of high-level functions (e.g., does the program contain or end in a call to SORT). Other possible attributes include control flow templates (e.g., the number of loops and conditionals). In the extreme case, one may set A to the identity function, in which case the attribute is equivalent to the program; however, in our experiments we find that performance is improved by choosing a more abstract attribute function.</p><p>(2) Data Generation.</p><p>Step 2 is to generate a dataset ((P (n) , a <ref type="bibr">(n)</ref> , E (n) )) N n=1 of programs P (n) in the chosen DSL, their attributes a <ref type="bibr">(n)</ref> , and accompanying input-output examples E <ref type="bibr">(n)</ref> . Different approaches are possible, ranging from enumerating valid programs in the DSL and pruning, to training a more sophisticated generative model of programs in the DSL. The key in the LIPS formulation is to ensure that it is feasible to generate a large dataset (ideally millions of programs).</p><p>(3) Machine Learning Model.</p><p>The machine learning problem is to learn a distribution of attributes given input-output examples, q(a | E). There is freedom to explore a large space of models, so long as the input component can encode E, and the output is a proper distribution over attributes (e.g., if attributes are a fixed-size binary vector, then a neural network with independent sigmoid outputs is appropriate; if attributes are variable size, then a recurrent neural network output could be used). Attributes are observed at training time, so training can use a maximum likelihood objective.</p><p>(4) Search.</p><p>The aim of the search component is to interface with an existing solver, using the predicted q(a | E) to guide the search. We describe specific approaches in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEEPCODER</head><p>Here we describe DeepCoder, our instantiation of LIPS including a choice of DSL, a data generation strategy, models for encoding input-output sets, and algorithms for searching over program space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DOMAIN SPECIFIC LANGUAGE AND ATTRIBUTES</head><p>We consider binary attributes indicating the presence or absence of high-level functions in the target program. To make this effective, the chosen DSL needs to contain constructs that are not so lowlevel that they all appear in the vast majority of programs, but at the same time should be common enough so that predicting their occurrence from input-output examples can be learned successfully.</p><p>Following this observation, our DSL is loosely inspired by query languages such as SQL or LINQ, where high-level functions are used in sequence to manipulate data. A program in our DSL is a sequence of function calls, where the result of each call initializes a fresh variable that is either a singleton integer or an integer array. Functions can be applied to any of the inputs or previously computed (intermediate) variables. The output of the program is the return value of the last function call, i.e., the last variable. See <ref type="figure">Fig. 1</ref> for an example program of length T = 4 in our DSL.</p><formula xml:id="formula_0">a â [int] b â FILTER (&lt;0) a c â MAP ( * 4) b d â SORT c e â REVERSE d</formula><p>An input-output example: Input: <ref type="bibr">[-17, -3, 4, 11, 0, -5, -9, 13, 6, 6, -8, 11]</ref> Output: <ref type="bibr">[-12, -20, -32, -36, -68]</ref> Figure 1: An example program in our DSL that takes a single integer array as its input.</p><p>Overall, our DSL contains the first-order functions HEAD, LAST, TAKE, DROP, ACCESS, MINI-MUM, MAXIMUM, REVERSE, SORT, SUM, and the higher-order functions MAP, FILTER, COUNT, ZIPWITH, SCANL1. Higher-order functions require suitable lambda functions for their behavior to be fully specified: for MAP our DSL provides lambdas (+1), (-1), ( * 2), (/2), ( * (-1)), ( ** 2), ( * 3), (/3), ( * 4), (/4); for FILTER and COUNT there are predicates (&gt;0), (&lt;0), (%2==0), (%2==1) and for ZIPWITH and SCANL1 the DSL provides lambdas (+), (-), ( * ), MIN, MAX. A description of the semantics of all functions is provided in Appendix F.</p><p>Note that while the language only allows linear control flow, many of its functions do perform branching and looping internally (e.g., SORT, COUNT, ...). Examples of more sophisticated programs expressible in our DSL, which were inspired by the simplest problems appearing on programming competition websites, are shown in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DATA GENERATION</head><p>To generate a dataset, we enumerate programs in the DSL, heuristically pruning away those with easily detectable issues such as a redundant variable whose value does not affect the program output, or, more generally, existence of a shorter equivalent program (equivalence can be overapproximated by identical behavior on randomly or carefully chosen inputs). To generate valid inputs for a program, we enforce a constraint on the output value bounding integers to some predetermined range, and then propagate these constraints backward through the program to obtain a range of valid values for each input. If one of these ranges is empty, we discard the program. Otherwise, input-output pairs can be generated by picking inputs from the pre-computed valid ranges and executing the program to obtain the output values. The binary attribute vectors are easily computed from the program source codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MACHINE LEARNING MODEL</head><p>Observe how the input-output data in <ref type="figure">Fig. 1</ref> is informative of the functions appearing in the program: the values in the output are all negative, divisible by 4, they are sorted in decreasing order, and they happen to be multiples of numbers appearing in the input. Our aim is to learn to recognize such patterns in the input-output examples, and to leverage them to predict the presence or absence of individual functions. We employ neural networks to model and learn the mapping from input-output examples to attributes. We can think of these networks as consisting of two parts:</p><p>1. an encoder: a differentiable mapping from a set of M input-output examples generated by a single program to a latent real-valued vector, and 2. a decoder: a differentiable mapping from the latent vector representing a set of M inputoutput examples to predictions of the ground truth program's attributes.</p><p>For the encoder we use a simple feed-forward architecture. First, we represent the input and output types (singleton or array) by a one-hot-encoding, and we pad the inputs and outputs to a maximum length L with a special NULL value. Second, each integer in the inputs and in the output is mapped to a learned embedding vector of size E = 20. (The range of integers is restricted to a finite range and each embedding is parametrized individually.) Third, for each input-output example separately, we concatenate the embeddings of the input types, the inputs, the output type, and the output into a single (fixed-length) vector, and pass this vector through H = 3 hidden layers containing K = 256 sigmoid units each. The third hidden layer thus provides an encoding of each individual input-output example. Finally, for input-output examples in a set generated from the same program, we pool these representations together by simple arithmetic averaging. See Appendix C for more details.</p><p>The advantage of this encoder lies in its simplicity, and we found it reasonably easy to train. A disadvantage is that it requires an upper bound L on the length of arrays appearing in the input and output. We confirmed that the chosen encoder architecture is sensible in that it performs empirically at least as well as an RNN encoder, a natural baseline, which may however be more difficult to train.</p><p>DeepCoder learns to predict presence or absence of individual functions of the DSL. We shall see this can already be exploited by various search techniques to large computational gains. We use a decoder that pre-multiplies the encoding of input-output examples by a learned C ÃK matrix, where C = 34 is the number of functions in our DSL (higher-order functions and lambdas are predicted independently), and treats the resulting C numbers as log-unnormalized probabilities (logits) of each function appearing in the source code. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the predictions a trained neural network made from 5 input-output examples for the program shown in <ref type="figure">Fig. 1</ref>.  </p><formula xml:id="formula_1">(+1) (-1) (*2) (/2) (*-1) (**2) (*3) (/3) (*4) (/4) (&gt;0) (&gt;0) (%2==1)<label>(%2==0)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">SEARCH</head><p>One of the central ideas of this work is to use a neural network to guide the search for a program consistent with a set of input-output examples instead of directly predicting the entire source code. This section briefly describes the search techniques and how they integrate the predicted attributes.</p><p>Depth-first search (DFS). We use an optimized version of DFS to search over programs with a given maximum length T (see Appendix D for details). When the search procedure extends a partial program by a new function, it has to try the functions in the DSL in some order. At this point DFS can opt to consider the functions as ordered by their predicted probabilities from the neural network.</p><p>"Sort and add" enumeration. A stronger way of utilizing the predicted probabilities of functions in an enumerative search procedure is to use a Sort and add scheme, which maintains a set of active functions and performs DFS with the active function set only. Whenever the search fails, the next most probable function (or several) are added to the active set and the search restarts with this larger active set. Note that this scheme has the deficiency of potentially re-exploring some parts of the search space several times, which could be avoided by a more sophisticated search procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sketch.</head><p>Sketch <ref type="bibr" target="#b13">(Solar-Lezama, 2008</ref>) is a successful SMT-based program synthesis tool from the programming languages research community. While its main use case is to synthesize programs by filling in "holes" in incomplete source code so as to match specified requirements, it is flexible enough for our use case as well. The function in each step and its arguments can be treated as the "holes", and the requirement to be satisfied is consistency with the provided set of input-output examples. Sketch can utilize the neural network predictions in a Sort and add scheme as described above, as the possibilities for each function hole can be restricted to the current active set. <ref type="bibr">Feser et al., 2015</ref>) is a program synthesis tool from the programming languages community that combines enumerative search with deduction to prune the search space. It is designed to infer small functional programs for data structure manipulation from input-output examples, by combining functions from a provided library. Î» can be used in our framework using a Sort and add scheme as described above by choosing the library of functions according to the neural network predictions.</p><formula xml:id="formula_2">Î» 2 . Î» 2 (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">TRAINING LOSS FUNCTION</head><p>We use the negative cross entropy loss to train the neural network described in Sect. 4.3, so that its predictions about each function can be interpreted as marginal probabilities. The LIPS framework dictates learning q(a | E), the joint distribution of all attributes a given the input-output examples, and it is not clear a priori how much DeepCoder loses by ignoring correlations between functions. However, under the simplifying assumption that the runtime of searching for a program of length T with C functions made available to a search routine is proportional to C T , the following result for Sort and add procedures shows that their runtime can be optimized using marginal probabilities. Lemma 1. For any fixed program length T , the expected total runtime of a Sort and add search scheme can be upper bounded by a quantity that is minimized by adding the functions in the order of decreasing true marginal probabilities.</p><p>Proof. Predicting source code functions from input-output examples can be seen as a multi-label classification problem, where each set of input-output examples is associated with a set of relevant labels (functions appearing in the ground truth source code). Dembczynski et al. <ref type="formula">2010</ref>showed that in multi-label classification under a so-called Rank loss, it is Bayes optimal to rank the labels according to their marginal probabilities. If the runtime of search with C functions is proportional to C T , the total runtime of a Sort and add procedure can be monotonically transformed so that it is upper bounded by this Rank loss. See Appendix E for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIMENTS</head><p>In this section we report results from two categories of experiments. Our main experiments (Sect. 5.1) show that the LIPS framework can lead to significant performance gains in solving IPS by demonstrating such gains with DeepCoder. In Sect. 5.2 we illustrate the robustness of the method by demonstrating a strong kind of generalization ability across programs of different lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DEEPCODER COMPARED TO BASELINES</head><p>We trained a neural network as described in Sect. 4.3 to predict used functions from input-output examples and constructed a test set of P = 500 programs, guaranteed to be semantically disjoint from all programs on which the neural network was trained (similarly to the equivalence check described in Sect. 4.2, we have ensured that all test programs behave differently from all programs used during training on at least one input). For each test program we generated M = 5 inputoutput examples involving integers of magnitudes up to 256, passed the examples to the trained neural network, and fed the obtained predictions to the search procedures from Sect. 4.4. We also considered a RNN-based decoder generating programs using beam search (see Sect. 5.3 for details).</p><p>To evaluate DeepCoder, we then recorded the time the search procedures needed to find a program consistent with the M input-output examples. As a baseline, we also ran all search procedures using a simple prior as function probabilities, computed from their global incidence in the program corpus. In the first, smaller-scale experiment (program search space size â¼ 2 Ã 10 6 ) we trained the neural network on programs of length T = 3, and the test programs were of the same length. <ref type="table" target="#tab_0">Table 1</ref> shows the per-task timeout required such that a solution could be found for given proportions of the test tasks (in time less than or equal to the timeout). For example, in a hypothetical test set with 4 tasks and runtimes of 3s, 2s, 1s, 4s, the timeout required to solve 50% of tasks would be 2s. More detailed experimental results are discussed in Appendix B.</p><p>In the main experiment, we tackled a large-scale problem of searching for programs consistent with input-output examples generated from programs of length T = 5 (search space size on the order of 10 10 ), supported by a neural network trained with programs of shorter length T = 4. Here, we only consider P = 100 programs for reasons of computational efficiency, after having verified that this does not significantly affect the results in <ref type="table" target="#tab_0">Table 1</ref>. The table in <ref type="figure" target="#fig_2">Fig. 3a</ref> shows significant speedups for DFS, Sort and add enumeration, and Î» 2 with Sort and add enumeration, the search techniques capable of solving the search problem in reasonable time frames. Note that Sort and add enumeration without the neural network (using prior probabilities of functions) exceeded the 10 4 second timeout in two cases, so the relative speedups shown are crude lower bounds.  We hypothesize that the substantially larger performance gains on Sort and add schemes as compared to gains on DFS can be explained by the fact that the choice of attribute function (predicting presence of functions anywhere in the program) and learning objective of the neural network are better matched to the Sort and add schemes. Indeed, a more appropriate attribute function for DFS would be one that is more informative of the functions appearing early in the program, since exploring an incorrect first function is costly with DFS. On the other hand, the discussion in Sect. 4.5 provides theoretical indication that ignoring the correlations between functions is not cataclysmic for Sort and add enumeration, since a Rank loss that upper bounds the Sort and add runtime can still be minimized.</p><p>In Appendix G we analyse the performance of the neural networks used in these experiments, by investigating which attributes (program instructions) tend to be difficult to distinguish from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">GENERALIZATION ACROSS PROGRAM LENGTHS</head><p>To investigate the encoder's generalization ability across programs of different lengths, we trained a network to predict used functions from input-output examples that were generated from programs of length T train â {1, . . . , 4}. We then used each of these networks to predict functions on 5 test sets containing input-output examples generated from programs of lengths T test â {1, . . . , 5}, respectively. The test programs of a given length T were semantically disjoint from all training programs of the same length T and also from all training and test programs of shorter lengths T &lt; T .</p><p>For each of the combinations of T train and T test , Sort and add enumerative search was run both with and without using the neural network's predictions (in the latter case using prior probabilities) until it solved 20% of the test set tasks. <ref type="figure" target="#fig_2">Fig. 3b</ref> shows the relative speedup of the solver having access to predictions from the trained neural networks. These results indicate that the neural networks are able to generalize beyond programs of the same length that they were trained on. This is partly due to the search procedure on top of their predictions, which has the opportunity to correct for the presence of functions that the neural network failed to predict. Note that a sequence-to-sequence model trained on programs of a fixed length could not be expected to exhibit this kind of generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ALTERNATIVE MODELS</head><p>Encoder We evaluated replacing the feed-forward architecture encoder (Sect. 4.3) with an RNN, a natural baseline. Using a GRU-based RNN we were able to achieve results almost as good as using the feed-forward architecture, but found the RNN encoder more difficult to train.</p><p>Decoder We also considered a purely neural network-based approach, where an RNN decoder is trained to predict the entire program token-by-token. We combined this with our feed-forward encoder by initializing the RNN using the pooled final layer of the encoder. We found it substantially more difficult to train an RNN decoder as compared to the independent binary classifiers employed above. Beam search was used to explore likely programs predicted by the RNN, but it only lead to a solution comparable with the other techniques when searching for programs of lengths T â¤ 2, where the search space size is very small (on the order of 10 3 ). Note that using an RNN for both the encoder and decoder corresponds to a standard sequence-to-sequence model. However, we do do not rule out that a more sophisticated RNN decoder or training procedure could be possibly more successful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Machine Learning for Inductive Program Synthesis.</p><p>There is relatively little work on using machine learning for programming by example. The most closely related work is that of <ref type="bibr" target="#b4">Menon et al. (2013)</ref>, in which a hand-coded set of features of input-output examples are used as "clues." When a clue appears in the input-output examples (e.g., the output is a permutation of the input), it reweights the probabilities of productions in a probabilistic context free grammar by a learned amount. This work shares the idea of learning to guide the search over program space conditional on input-output examples. One difference is in the domains. <ref type="bibr" target="#b4">Menon et al. (2013)</ref> operate on short string manipulation programs, where it is arguably easier to hand-code features to recognize patterns in the input-output examples (e.g., if the outputs are always permutations or substrings of the input). Our work shows that there are strong cues in patterns in input-output examples in the domain of numbers and lists. However, the main difference is the scale. <ref type="bibr" target="#b4">Menon et al. (2013)</ref> learns from a small (280 examples), manually-constructed dataset, which limits the capacity of the machine learning model that can be trained. Thus, it forces the machine learning component to be relatively simple. Indeed, <ref type="bibr" target="#b4">Menon et al. (2013)</ref> use a log-linear model and rely on hand-constructed features. LIPS automatically generates training data, which yields datasets with millions of programs and enables high-capacity deep learning models to be brought to bear on the problem.</p><p>Learning Representations of Program State. <ref type="bibr" target="#b6">Piech et al. (2015)</ref> propose to learn joint embeddings of program states and programs to automatically extend teacher feedback to many similar programs in the MOOC setting. This work is similar in that it considers embedding program states, but the domain is different, and it otherwise specifically focuses on syntactic differences between semantically equivalent programs to provide stylistic feedback. Li et al. (2016) use graph neural networks (GNNs) to predict logical descriptions from program states, focusing on data structure shapes instead of numerical and list data. Such GNNs may be a suitable architecture to encode states appearing when extending our DSL to handle more complex data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning to Infer.</head><p>Very recently, <ref type="bibr" target="#b0">Alemi et al. (2016)</ref> used neural sequence models in tandem with an automated theorem prover. Similar to our Sort and Add strategy, a neural network component is trained to select premises that the theorem prover can use to prove a theorem. A recent extension <ref type="bibr" target="#b3">(Loos et al., 2017)</ref> is similar to our DFS enumeration strategy and uses a neural network to guide the proof search at intermediate steps. The main differences are in the domains, and that they train on an existing corpus of theorems. More broadly, if we view a DSL as defining a model and search as a form of inference algorithm, then there is a large body of work on using discriminativelytrained models to aid inference in generative models. Examples include <ref type="bibr" target="#b2">Dayan et al. (1995)</ref>; Kingma &amp; Welling <ref type="formula">2014</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND FUTURE WORK</head><p>We have presented a framework for improving IPS systems by using neural networks to translate cues in input-output examples to guidance over where to search in program space. Our empirical results show that for many programs, this technique improves the runtime of a wide range of IPS baselines by 1-3 orders. We have found several problems in real online programming challenges that can be solved with a program in our language, which validates the relevance of the class of problems that we have studied in this work. In sum, this suggests that we have made significant progress towards being able to solve programming competition problems, and the machine learning component plays an important role in making it tractable.</p><p>There remain some limitations, however. First, the programs we can synthesize are only the simplest problems on programming competition websites and are simpler than most competition problems. Many problems require more complex algorithmic solutions like dynamic programming and search, which are currently beyond our reach. Our chosen DSL currently cannot express solutions to many problems. To do so, it would need to be extended by adding more primitives and allow for more flexibility in program constructs (such as allowing loops). Second, we currently use five input-output examples with relatively large integer values (up to in magnitude), which are probably more informative than typical (smaller) examples. While we remain optimistic about LIPS's applicability as the DSL becomes more complex and the input-output examples become less informative, it remains to be seen what the magnitude of these effects are as we move towards solving large subsets of programming competition problems.</p><p>We foresee many extensions of DeepCoder. We are most interested in better data generation procedures by using generative models of source code, and to incorporate natural language problem descriptions to lessen the information burden required from input-output examples. In sum, Deep-Coder represents a promising direction forward, and we are optimistic about the future prospects of using machine learning to synthesize programs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXAMPLE PROGRAMS</head><p>This section shows example programs in our Domain Specific Language (DSL), together with inputoutput examples and short descriptions. These programs have been inspired by simple tasks appearing on real programming competition websites, and are meant to illustrate the expressive power of our DSL.</p><p>Program 0:</p><formula xml:id="formula_3">k â int b â [int] c â SORT b d â TAKE k c e â SUM d</formula><p>Input-output example: Input: 2, [3 5 4 7 5] Output:</p><formula xml:id="formula_4">[7]</formula><p>Description:</p><p>A new shop near you is selling n paintings. You have k &lt; n friends and you would like to buy each of your friends a painting from the shop. Return the minimal amount of money you will need to spend.</p><p>Program 1:</p><formula xml:id="formula_5">w â [int] t â [int] c â MAP ( * 3) w d â ZIPWITH (+) c t e â MAXIMUM d</formula><p>Input-output example: Input:</p><p>[6 2 4 7 9], [5 3 6 1 0] Output: 27</p><p>Description:</p><p>In soccer leagues, match winners are awarded 3 points, losers 0 points, and both teams get 1 point in the case of a tie. Compute the number of points awarded to the winner of a league given two arrays w, t of the same length, where</p><formula xml:id="formula_6">w[i] (resp. t[i])</formula><p>is the number of times team i won (resp. tied).</p><p>Program 2:</p><formula xml:id="formula_7">a â [int] b â [int] c â ZIPWITH (-) b a d â COUNT (&gt;0) c</formula><p>Input-output example: Input:</p><p>[6 2 4 7 9], [5 3 2 1 0] Output: 4</p><p>Description: Alice and Bob are comparing their results in a recent exam. Given their marks per question as two arrays a and b, count on how many questions Alice got more points than Bob.</p><p>Program 3:</p><formula xml:id="formula_8">h â [int] b â SCANL1 MIN h c â ZIPWITH (-) h b d â FILTER (&gt;0) c e â SUM d</formula><p>Input-output example: Input: [8 5 7 2 5] Output: 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description:</head><p>Perditia is very peculiar about her garden and wants that the trees standing in a row are all of non-increasing heights. Given the tree heights in centimeters in order of the row as an array h, compute how many centimeters she needs to trim the trees in total.</p><p>Program 4:</p><formula xml:id="formula_9">x â [int] y â [int] c â SORT x d â SORT y e â REVERSE d f â ZIPWITH ( * ) d e g â SUM f</formula><p>Input-output example: Input: [7 3 8 2 5], [2 8 9 1 3] Output: 79</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description:</head><p>Xavier and Yasmine are laying sticks to form non-overlapping rectangles on the ground. They both have fixed sets of pairs of sticks of certain lengths (represented as arrays x and y of numbers). Xavier only lays sticks parallel to the x axis, and Yasmine lays sticks only parallel to y axis. Compute the area their rectangles will cover at least. Umberto has a large collection of ties and matching pocket squares-too large, his wife says-and he needs to sell one pair. Given their values as arrays t and p, assuming that he sells the cheapest pair, and selling costs 2, how much will he lose from the sale? Description: Vivian loves rearranging things. Most of all, when she sees a row of heaps, she wants to make sure that each heap has more items than the one to its left. She is also obsessed with efficiency, so always moves the least possible number of items. Her dad really dislikes if she changes the size of heaps, so she only moves single items between them, making sure that the set of sizes of the heaps is the same as at the start; they are only in a different order.  <ref type="figure">Fig. 4</ref> shows the predictions made by a neural network trained on programs of length T = 4 that were ensured to be semantically disjoint from all example programs shown in this section. For each task, the neural network was provided with input-output examples. Figure 4: Predictions of a neural network on the example programs described in this section. Numbers in squares would ideally be close to (function is present in the ground truth source code), whereas all other numbers should ideally be close to 0 (function is not needed).</p><formula xml:id="formula_10">(+1) (-1) (*2) (/2) (*-1) (**2) (*3) (/3) (*4) (/4) (&gt;0) (&lt;0) (%2==1)<label>(%2==0)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTAL RESULTS</head><p>Results presented in Sect. 5.1 showcased the computational speedups obtained from the LIPS framework (using DeepCoder), as opposed to solving each program synthesis problem with only the in-formation about global incidence of functions in source code available. For completeness, here we show plots of raw computation times of each search procedure to solve a given number of problems. <ref type="figure" target="#fig_6">Fig. 5</ref> shows the computation times of DFS, of Enumerative search with a Sort and add scheme, of the Î» 2 and Sketch solvers with a Sort and add scheme, and of Beam search, when searching for a program consistent with input-output examples generated from P = 500 different test programs of length T = 3. As discussed in Sect. 5.1, these test programs were ensured to be semantically disjoint from all programs used to train the neural networks, as well as from all programs of shorter length (as discussed in Sect. 4.2).</p><p>10 -4 -3 -2 -1 0 1 2 3 Solver computation time DFS: using neural network DFS: using prior order L2: Sort and add using neural network L2: Sort and add in prior order Enumeration: Sort and add using neural network Enumeration: Sort and add in prior order Beam search Sketch: Sort and add using neural network Sketch: Sort and add in prior order The "steps" in the results for Beam search are due to our search strategy, which doubles the size of the considered beam until reaching the timeout (of 1000 seconds) and thus steps occur whenever the search for a beam of size 2 k is finished. For Î» 2 , we observed that no solution for a given set of allowed functions was ever found after about 5 seconds (on the benchmark machines), but that Î» 2 continued to search. Hence, we introduced a hard timeout after 6 seconds for all but the last iterations of our Sort and add scheme. <ref type="figure" target="#fig_8">Fig. 6</ref> shows the computation times of DFS, Enumerative search with a Sort and add scheme, and Î» 2 with a Sort and add scheme when searching for programs consistent with input-output examples generated from P = 100 different test programs of length T = 5. The neural network was trained on programs of length T = 4.</p><p>10 -4 -3 -2 -1 0 1 10 2 10 3 4 Solver computation time DFS: using neural network DFS: using prior order L2: Sort and add using neural network L2: Sort and add in prior order Enumeration: Sort and add using neural network Enumeration: Sort and add in prior order </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C THE NEURAL NETWORK</head><p>As briefly described in Sect. 4.3, we used the following simple feed-forward architecture encoder:</p><p>â¢ For each input-output example in the set generated from a single ground truth program:</p><p>-Pad arrays appearing in the inputs and in the output to a maximum length L = 20 with a special NULL value. -Represent the type (singleton integer or integer array) of each input and of the output using a one-hot-encoding vector. Embed each integer in the valid integer range (â256 to 255) using a learned embedding into E = 20 dimensional space. Also learn an embedding for the padding NULL value.</p><p>-Concatenate the representations of the input types, the embeddings of integers in the inputs, the representation of the output type, and the embeddings of integers in the output into a single (fixed-length) vector. -Pass this vector through H = 3 hidden layers containing K = 256 sigmoid units each.</p><p>â¢ Pool the last hidden layer encodings of each input-output example together by simple arithmetic averaging. <ref type="figure">Fig. 7</ref> shows a schematic drawing of this encoder architecture, together with the decoder that performs independent binary classification for each function in the DSL, indicating whether or not it appears in the ground truth source code.  <ref type="figure">Figure 7</ref>: Schematic representation of our feed-forward encoder, and the decoder.</p><p>While DeepCoder learns to embed integers into a E = 20 dimensional space, we built the system up gradually, starting with a E = 2 dimensional space and only training on programs of length T = 1. Such a small scale setting allowed easier investigation of the workings of the neural network, and indeed <ref type="figure" target="#fig_9">Fig. 8</ref> below shows a learned embedding of integers in R 2 . The figure demonstrates that the network has learnt the concepts of number magnitude, sign (positive or negative) and evenness, presumably due to FILTER (&gt;0), FILTER (&lt;0), FILTER (%2==0) and FILTER (%2==1) all being among the programs on which the network was trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DEPTH-FIRST SEARCH</head><p>We use an optimized C++ implementation of depth-first search (DFS) to search over programs with a given maximum length T . In depth-first search, we start by choosing the first function (and its arguments) of a potential solution program, and then recursively consider all ways of filling in the rest of the program (up to length T ), before moving on to a next choice of first instruction (if a solution has not yet been found).</p><p>A program is considered a solution if it is consistent with all M = 5 provided input-output examples.</p><p>Note that this requires evaluating all candidate programs on the M inputs and checking the results for equality with the provided M respective outputs. Our implementation of DFS exploits the sequential structure of programs in our DSL by caching the results of evaluating all prefixes of the currently considered program on the example inputs, thus allowing efficient reuse of computation between candidate programs with common prefixes.</p><p>This allows us to explore the search space at roughly the speed of â¼ 3 Ã 10 6 programs per second. When the search procedure extends a partial program by a new function, it has to try the functions in the DSL in some order. At this point DFS can opt to consider the functions as ordered by their predicted probabilities from the neural network. The probability of a function consisting of a higherorder function and a lambda is taken to be the minimum of the probabilities of the two constituent functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E TRAINING LOSS FUNCTION</head><p>In Sect. 4.5 we outlined a justification for using marginal probabilities of individual functions as a sensible intermediate representation to provide a solver employing a Sort and add scheme (we considered Enumerative search and the Sketch solver with this scheme). Here we provide a more detailed discussion.</p><p>Predicting program components from input-output examples can be cast as a multilabel classification problem, where each instance (set of input-output examples) is associated with a set of relevant labels (functions appearing in the code that generated the examples). We denote the number of labels (functions) by C, and note that throughout this work C = 34.</p><p>When the task is to predict a subset of labels y â {0, 1} C , different loss functions can be employed to measure the prediction error of a classifier h(x) or ranking function f (x). Dembczynski et al. (2010) discuss the following three loss functions:</p><p>â¢ Hamming loss counts the number of labels that are predicted incorrectly by a classifier h:</p><formula xml:id="formula_11">L H (y, h(x)) = C c=1 1 {yc =hc(x)}</formula><p>â¢ Rank loss counts the number of label pairs violating the condition that relevant labels are ranked higher than irrelevant ones by a scoring function f :</p><formula xml:id="formula_12">L r (y, f (x)) = C (i,j):yi=1,yj =0 1 {fi&lt;fj }</formula><p>â¢ Subset Zero-One loss indicates whether all labels have been correctly predicted by h: <ref type="bibr">et al. (2010)</ref> proved that Bayes optimal decisions under the Hamming and Rank loss functions, i.e., decisions minimizing the expected loss under these loss functions, can be computed from marginal probabilities p c (y c |x). This suggests that:</p><formula xml:id="formula_13">L s (y, h(x)) = 1 {y =h(x)} Dembczynski</formula><p>â¢ Multilabel classification under these two loss functions may not benefit from considering dependencies between the labels. â¢ "Instead of minimizing the Rank loss directly, one can simply use any approach for single label prediction that properly estimates the marginal probabilities." <ref type="bibr">(DembczyÅski et al., 2012)</ref> Training the neural network with the negative cross entropy loss function as the training objective is precisely a method for properly estimating the marginal probabilities of labels (functions appearing in source code). It is thus a sensible step in preparation for making predictions under a Rank loss.</p><p>It remains to discuss the relationship between the Rank loss and the actual quantity we care about, which is the total runtime of a Sort and add search procedure. Recall the simplifying assumption that the runtime of searching for a program of length T with C functions made available to the search is proportional to C T , and consider a Sort and add search for a program of length T , where the size of the active set is increased by whenever the search fails. Starting with an active set of size 1, the total time until a solution is found can be upper bounded by</p><formula xml:id="formula_14">1 T + 2 T + â¢ â¢ â¢ + C T A â¤ C T +1 A â¤ CC T A</formula><p>where C A is the size of the active set when the search finally succeeds (i.e., when the active set finally contains all necessary functions for a solution to exist). Hence the total runtime of a Sort and add search can be upper bounded by a quantity that is proportional to C T A . Now fix a valid program solution P that requires C P functions, and let y P â {0, 1} C be the indicator vector of functions used by P . Let D := C A â C P be the number of redundant operations added into the active set until all operations from P have been added. Example 1. Suppose the labels, as sorted by decreasing predicted marginal probabilities f (x), are as follows:</p><p>1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Then the solution P contains C P = 6 functions, but the active set needs to grow to size C A = 11 to include all of them, adding D = 5 redundant functions along the way. Note that the rank loss of the predictions f (x) is L r (y P , f (x)) = 2 + 5 = 7, as it double counts the two redundant functions which are scored higher than two relevant labels.</p><p>Noting that in general L r (y P , f (x)) â¥ D, the previous upper bound on the runtime of Sort and add can be further upper bounded as follows:</p><formula xml:id="formula_15">C T A = (C P + D) T â¤ const + const Ã D T â¤ const + const Ã L r (y P , f (x))</formula><p>T Hence we see that for a constant value of T , this upper bound can be minimized by optimizing the Rank loss of the predictions f (x). Note also that L r (y P , f (x)) = 0 would imply D = 0, in which case C A = C P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F DOMAIN SPECIFIC LANGUAGE OF DEEPCODER</head><p>Here we provide a description of the semantics of our DSL from Sect. 4.1, both in English and as a Python implementation. Throughout, NULL is a special value that can be set e.g. to an integer outside the working integer range. The INTâINT lambdas (+1), (-1), ( * 2), (/2), ( * (-1)), ( ** 2), ( * 3), (/3), ( * 4), (/4) provided by our DSL map integers to integers in a self-explanatory manner. The INTâBOOL lambdas (&gt;0), (&lt;0), (%2==0), (%2==1) respectively test positivity, negativity, evenness and oddness of the input integer value. Finally, the INTâINTâINT lambdas (+), (-), ( * ), MIN, MAX apply a function to a pair of integers and produce a single integer.</p><p>As an example, consider the function SCANL1 MAX, consisting of the higher-order function SCANL1 and the INTâINTâINT lambda MAX. Given an integer array a of length L, this function computes the running maximum of the array a. Specifically, it returns an array b of the same length L whose i-th element is the maximum of the first i elements in a.</p><formula xml:id="formula_16">HEAD LAST ACCESS MINIMUM MAXIMUM TAKE DROP FILTER (&gt;0) (&lt;0) (%2==1) (%2==0) COUNT MAP MIN MAX + - * ZIPWITH SCANL1 SORT REVERSE (*-1) (**2) (+1) (-1) (*2) (*3) (*4) (/2) (/3) (/4) SUM HEAD LAST ACCESS MINIMUM MAXIMUM TAKE DROP FILTER (&gt;0) (&lt;0) (%2==1) (%2==0) COUNT MAP MIN MAX + - * ZIPWITH SCANL1 SORT REVERSE (*-1) (**2) (+1) (-1) (*2) (*3) (*4) (/2) (/3) (/4) SUM .</formula><p>17 <ref type="bibr">(15)</ref> .05 <ref type="bibr">(12)</ref> .29 <ref type="bibr">(14)</ref> .09 <ref type="bibr">(15)</ref> .04 <ref type="bibr">(9)</ref> .06 <ref type="bibr">(14)</ref> .12 <ref type="bibr">(12)</ref> .06 <ref type="bibr">(14)</ref> .08 <ref type="bibr">(15)</ref> .07 <ref type="bibr">(13)</ref> .06 <ref type="bibr">(15)</ref> .06 <ref type="bibr">(15)</ref> .16 <ref type="bibr">(9)</ref> .11 <ref type="bibr">(15)</ref> .03 <ref type="bibr">(13)</ref> .06 <ref type="bibr">(14)</ref> .00 <ref type="bibr">(13)</ref> .09 <ref type="bibr">(12)</ref> .05 <ref type="bibr">(7)</ref> .05 <ref type="bibr">(15)</ref> .03 <ref type="bibr">(15)</ref> .03 <ref type="bibr">(14)</ref> .01 <ref type="bibr">(14)</ref> .01 <ref type="bibr">(13)</ref> .02 <ref type="bibr">(15)</ref> .03 <ref type="bibr">(14)</ref> .02 <ref type="bibr">(15)</ref> .00 <ref type="bibr">(14)</ref> .06 <ref type="bibr">(15)</ref> .01 <ref type="bibr">(14)</ref> .02 <ref type="bibr">(14)</ref> .08 <ref type="bibr">(15)</ref> .00 <ref type="bibr">(15)</ref> .34</p><p>.15 (6) .01 (6) .05 <ref type="bibr">(6)</ref> .07 (4) .04 (4) .09 (6) .04 (6) .01 (6) .02 (6) .11 <ref type="bibr">(6)</ref> .04 (6) .01</p><p>(2) .04 (5) .11 <ref type="bibr">(6)</ref> .05 (5) .11 <ref type="bibr">(6)</ref> .01 (5) .03</p><p>(3) .06 (6) .01 (6) .03 (6) .03 (5) .01 (6) .03 (6) .01 (5) .02 (5) .00 (5) .02 (6) .13 (6) .00 (6) .01 (6) .02</p><p>.10 (15) .19 <ref type="bibr">(18)</ref> .15 <ref type="bibr">(16)</ref> .05 <ref type="bibr">(18)</ref> .10 (16) .18 <ref type="bibr">(16)</ref> .16 <ref type="bibr">(14)</ref> .03 <ref type="bibr">(14)</ref> .09 <ref type="bibr">(17)</ref> .13 <ref type="bibr">(16)</ref> .12 <ref type="bibr">(16)</ref> .03 (13) .14 (13) .18 <ref type="bibr">(17)</ref> .06 <ref type="bibr">(16)</ref> .05 <ref type="bibr">(17)</ref> .00 <ref type="bibr">(16)</ref> .02 <ref type="bibr">(17)</ref> .10 (14) .07 <ref type="bibr">(15)</ref> .04 <ref type="bibr">(15)</ref> .07 <ref type="bibr">(17)</ref> .02 <ref type="bibr">(16)</ref> .01 <ref type="bibr">(17)</ref> .04 <ref type="bibr">(17)</ref> .03 <ref type="bibr">(17)</ref> .02 <ref type="bibr">(18)</ref> .00 <ref type="bibr">(18)</ref> .03 <ref type="bibr">(18)</ref> .02 <ref type="bibr">(18)</ref> .02 <ref type="bibr">(17)</ref> .03 <ref type="bibr">(18)</ref> .02 <ref type="bibr">(18)</ref> .16 <ref type="bibr">(8)</ref> .22 (9) .04</p><p>.12 (9) .12 (7) .08 <ref type="bibr">(8)</ref> .24 <ref type="bibr">(8)</ref> .03 (8) .16 (9) .13 (9) .13 (9) .13 (9) .06 (7) .34 (9) .02 <ref type="bibr">(8)</ref> .25 (8) .10 (5) .00 <ref type="bibr">(8)</ref> .01</p><p>(3) .05 <ref type="bibr">(8)</ref> .03 (7) .06 (9) .02 (8) .00 (9) .02 (9) .03 (9) .04 (9) .00 (9) .00 (9) .01 (9) .04 (8) .02 (9) .02</p><p>.25 <ref type="bibr">(10)</ref> .23 <ref type="bibr">(10)</ref> .22 <ref type="bibr">(10)</ref> .05 <ref type="bibr">(10)</ref> .08 (7) .02 (6) .10 (9) .09 (9) .04 (10) .08 (10) .06 (10) .10 (10) .05 (7) .08 <ref type="bibr">(8)</ref> .04 <ref type="bibr">(6)</ref> .09 <ref type="bibr">(9)</ref> .08 <ref type="bibr">(8)</ref> .00 <ref type="bibr">(10)</ref> .03 <ref type="bibr">(5)</ref> .02 <ref type="bibr">(7)</ref> .04 <ref type="bibr">(10)</ref> .09 <ref type="bibr">(10)</ref> .01 <ref type="bibr">(10)</ref> .00 <ref type="bibr">(10)</ref> .02 <ref type="bibr">(10)</ref> .02 <ref type="bibr">(10)</ref> .02 <ref type="bibr">(10)</ref> .00 <ref type="bibr">(10)</ref> .00 <ref type="bibr">(9)</ref> .07 <ref type="bibr">(10)</ref> .01 (9) .00 <ref type="bibr">(9)</ref> .00 <ref type="bibr">(10)</ref> .05 <ref type="bibr">(36)</ref> .06 <ref type="bibr">(40)</ref> .04 <ref type="bibr">(40)</ref> .06 <ref type="bibr">(40)</ref> .02 <ref type="bibr">(39)</ref> .05 <ref type="bibr">(39)</ref> .12 <ref type="bibr">(36)</ref> .06 <ref type="bibr">(40)</ref> .05 <ref type="bibr">(42)</ref> .09 <ref type="bibr">(40)</ref> .02 <ref type="bibr">(36)</ref> .01 <ref type="bibr">(38)</ref> .05 <ref type="bibr">(21)</ref> .06 <ref type="bibr">(40)</ref> .05 <ref type="bibr">(37)</ref> .03 <ref type="bibr">(34)</ref> .00 <ref type="bibr">(37)</ref> .02 <ref type="bibr">(32)</ref> .11 <ref type="bibr">(20)</ref> .02 <ref type="bibr">(35)</ref> .03 <ref type="bibr">(41)</ref> .04 <ref type="bibr">(40)</ref> .00 <ref type="bibr">(41)</ref> .03 <ref type="bibr">(40)</ref> .01 <ref type="bibr">(37)</ref> .04 <ref type="bibr">(39)</ref> .02 <ref type="bibr">(39)</ref> .00 <ref type="bibr">(42)</ref> .02 <ref type="bibr">(41)</ref> .03 <ref type="bibr">(39)</ref> .01 <ref type="bibr">(39)</ref> .00 <ref type="bibr">(41)</ref> .00 <ref type="bibr">(42)</ref> .04 <ref type="bibr">(44)</ref> .03 <ref type="bibr">(43)</ref> .03 <ref type="bibr">(43)</ref> .03 <ref type="bibr">(44)</ref> .03 <ref type="bibr">(41)</ref> .11 <ref type="bibr">(42)</ref> .09 <ref type="bibr">(36)</ref> .06 <ref type="bibr">(42)</ref> .08 <ref type="bibr">(44)</ref> .09 <ref type="bibr">(38)</ref> .05 <ref type="bibr">(38)</ref> .01 <ref type="bibr">(35)</ref> .14 <ref type="bibr">(29)</ref> .06 <ref type="bibr">(39)</ref> .04 <ref type="bibr">(40)</ref> .09 <ref type="bibr">(39)</ref> .09 <ref type="bibr">(39)</ref> .00 <ref type="bibr">(38)</ref> .10 (21) .03 <ref type="bibr">(40)</ref> .05 <ref type="bibr">(44)</ref> .08 <ref type="bibr">(43)</ref> .01 <ref type="bibr">(44)</ref> .01 <ref type="bibr">(45)</ref> .05 <ref type="bibr">(44)</ref> .06 <ref type="bibr">(40)</ref> .03 <ref type="bibr">(43)</ref> .02 <ref type="bibr">(41)</ref> .01 <ref type="bibr">(43)</ref> .02 <ref type="bibr">(44)</ref> .03 <ref type="bibr">(42)</ref> .01 <ref type="bibr">(44)</ref> .00 <ref type="bibr">(45)</ref> .01 <ref type="bibr">(115)</ref> .01 <ref type="bibr">(118)</ref> .01 <ref type="bibr">(114)</ref> .02 <ref type="bibr">(117)</ref> .00 <ref type="bibr">(117)</ref> .03 <ref type="bibr">(112)</ref> .06 <ref type="bibr">(109)</ref> .06 <ref type="bibr">(90)</ref> .05 <ref type="bibr">(87)</ref> .07 <ref type="bibr">(88)</ref> .08 <ref type="bibr">(81)</ref> .05 <ref type="bibr">(110)</ref> .11 <ref type="bibr">(86)</ref> .08 <ref type="bibr">(93)</ref> .04 <ref type="bibr">(91)</ref> .11 <ref type="bibr">(89)</ref> .07 <ref type="bibr">(92)</ref> .01 <ref type="bibr">(89)</ref> .03 <ref type="bibr">(22)</ref> .03 <ref type="bibr">(102)</ref> .02 <ref type="bibr">(113)</ref> .04 <ref type="bibr">(110)</ref> .05 <ref type="bibr">(115)</ref> .01 <ref type="bibr">(117)</ref> .02 <ref type="bibr">(114)</ref> .03 <ref type="bibr">(116)</ref> .01 <ref type="bibr">(114)</ref> .00 <ref type="bibr">(111)</ref> .01 <ref type="bibr">(112)</ref> .05 <ref type="bibr">(116)</ref> .03 <ref type="bibr">(115)</ref> .02 <ref type="bibr">(114)</ref> .00 <ref type="bibr">(45)</ref> .01 <ref type="bibr">(45)</ref> .00 <ref type="bibr">(43)</ref> .00 <ref type="bibr">(45)</ref> .00 <ref type="bibr">(45)</ref> .02 <ref type="bibr">(39)</ref> .06 <ref type="bibr">(38)</ref> .03 <ref type="bibr">(8)</ref> .09 <ref type="bibr">(42)</ref> .09 <ref type="bibr">(45)</ref> .17 <ref type="bibr">(44)</ref> .04 <ref type="bibr">(33)</ref> .12 <ref type="bibr">(33)</ref> .05 <ref type="bibr">(40)</ref> .04 <ref type="bibr">(32)</ref> .09 <ref type="bibr">(40)</ref> .02 <ref type="bibr">(33)</ref> .00 <ref type="bibr">(33)</ref> .04 <ref type="bibr">(11)</ref> .02 <ref type="bibr">(42)</ref> .00 <ref type="bibr">(44)</ref> .05 <ref type="bibr">(43)</ref> .05 <ref type="bibr">(45)</ref> .00 <ref type="bibr">(45)</ref> .02 <ref type="bibr">(42)</ref> .04 <ref type="bibr">(44)</ref> .02 <ref type="bibr">(43)</ref> .00 <ref type="bibr">(43)</ref> .02 <ref type="bibr">(43)</ref> .06 <ref type="bibr">(45)</ref> .01 <ref type="bibr">(44)</ref> .01 <ref type="bibr">(43)</ref> .00 <ref type="bibr">(44)</ref> .04 <ref type="bibr">(32)</ref> .02 <ref type="bibr">(32)</ref> .01 <ref type="bibr">(27)</ref> .01 <ref type="bibr">(32)</ref> .01 <ref type="bibr">(32)</ref> .03 <ref type="bibr">(28)</ref> .04 <ref type="bibr">(22)</ref> .18 <ref type="bibr">(24)</ref> .14 <ref type="bibr">(21)</ref> .24 <ref type="bibr">(30)</ref> .20 <ref type="bibr">(20)</ref> .16 <ref type="bibr">(20)</ref> .19 <ref type="bibr">(28)</ref> .15 <ref type="bibr">(29)</ref> .11 <ref type="bibr">(27)</ref> .10 (24) .14 <ref type="bibr">(25)</ref> .00 <ref type="bibr">(28)</ref> .09 <ref type="bibr">(14)</ref> .07 <ref type="bibr">(26)</ref> .03 <ref type="bibr">(31)</ref> .05 <ref type="bibr">(30)</ref> .02 <ref type="bibr">(32)</ref> .00 <ref type="bibr">(32)</ref> .05 <ref type="bibr">(32)</ref> .03 <ref type="bibr">(31)</ref> .01 <ref type="bibr">(32)</ref> .00 <ref type="bibr">(32)</ref> .00 <ref type="bibr">(31)</ref> .03 <ref type="bibr">(31)</ref> .04 <ref type="bibr">(31)</ref> .06 <ref type="bibr">(32)</ref> .02 <ref type="bibr">(32)</ref> .01 <ref type="bibr">(246)</ref> .01 <ref type="bibr">(248)</ref> .01 <ref type="bibr">(247)</ref> .01 <ref type="bibr">(250)</ref> .00 <ref type="bibr">(249)</ref> .01 <ref type="bibr">(231)</ref> .02 <ref type="bibr">(236)</ref> .02 <ref type="bibr">(220)</ref> .02 <ref type="bibr">(243)</ref> .01 <ref type="bibr">(246)</ref> .02 <ref type="bibr">(243)</ref> .02 <ref type="bibr">(240)</ref> .01 <ref type="bibr">(248)</ref> .08 <ref type="bibr">(186)</ref> .04 <ref type="bibr">(203)</ref> .11 <ref type="bibr">(178)</ref> .06 <ref type="bibr">(188)</ref> .05 <ref type="bibr">(193)</ref> .04 <ref type="bibr">(40)</ref> .04 <ref type="bibr">(206)</ref> .02 <ref type="bibr">(246)</ref> .04 <ref type="bibr">(235)</ref> .03 <ref type="bibr">(225)</ref> .03 <ref type="bibr">(231)</ref> .05 <ref type="bibr">(213)</ref> .11 <ref type="bibr">(224)</ref> .03 <ref type="bibr">(228)</ref> .01 <ref type="bibr">(214)</ref> .02 <ref type="bibr">(217)</ref> .06 <ref type="bibr">(220)</ref> .03 <ref type="bibr">(218)</ref> .05 <ref type="bibr">(225)</ref> .00 <ref type="bibr">(250)</ref> .02 <ref type="bibr">(123)</ref> .00 <ref type="bibr">(122)</ref> .01 <ref type="bibr">(122)</ref> .00 <ref type="bibr">(123)</ref> .00 <ref type="bibr">(121)</ref> .01 <ref type="bibr">(121)</ref> .02 <ref type="bibr">(117)</ref> .01 <ref type="bibr">(98)</ref> .01 <ref type="bibr">(115)</ref> .01 <ref type="bibr">(114)</ref> .03 <ref type="bibr">(117)</ref> .02 <ref type="bibr">(118)</ref> .02 <ref type="bibr">(120)</ref> .10 <ref type="formula" target="#formula_102">57</ref>.07 <ref type="bibr">(107)</ref> .08 <ref type="bibr">(90)</ref> .04 <ref type="bibr">(92)</ref> .02 <ref type="bibr">(102)</ref> .00 <ref type="bibr">(6)</ref> .04 <ref type="bibr">(76)</ref> .03 <ref type="bibr">(117)</ref> .06 <ref type="bibr">(113)</ref> .03 <ref type="bibr">(111)</ref> .02 <ref type="bibr">(118)</ref> .04 <ref type="bibr">(114)</ref> .08 <ref type="bibr">(119)</ref> .03 <ref type="bibr">(118)</ref> .01 <ref type="bibr">(115)</ref> .01 <ref type="bibr">(112)</ref> .06 <ref type="bibr">(117)</ref> .03 <ref type="bibr">(116)</ref> .03 <ref type="bibr">(116)</ref> .00 <ref type="bibr">(122)</ref> .01 <ref type="bibr">(128)</ref> .01 <ref type="bibr">(130)</ref> .01 <ref type="bibr">(128)</ref> .01 <ref type="bibr">(129)</ref> .00 <ref type="bibr">(126)</ref> .02 <ref type="bibr">(125)</ref> .01 <ref type="bibr">(125)</ref> .02 <ref type="bibr">(103)</ref> .03 <ref type="bibr">(125)</ref> .02 <ref type="bibr">(124)</ref> .02 <ref type="bibr">(122)</ref> .02 <ref type="bibr">(117)</ref> .02 <ref type="bibr">(125)</ref> .15 <ref type="bibr">(81)</ref> .22 <ref type="bibr">(114)</ref> .07 <ref type="bibr">(83)</ref> .05 <ref type="bibr">(99)</ref> .02 <ref type="bibr">(110)</ref> .00 <ref type="bibr">(5)</ref> .03 <ref type="bibr">(78)</ref> .03 <ref type="bibr">(120)</ref> .05 <ref type="bibr">(124)</ref> .03 <ref type="bibr">(125)</ref> .03 <ref type="bibr">(124)</ref> .03 <ref type="bibr">(126)</ref> .04 <ref type="bibr">(130)</ref> .03 <ref type="bibr">(125)</ref> .01 <ref type="bibr">(124)</ref> .02 <ref type="bibr">(122)</ref> .04 <ref type="bibr">(120)</ref> .04 <ref type="bibr">(124)</ref> .03 <ref type="bibr">(122)</ref> .00 <ref type="bibr">(127)</ref> .01 <ref type="bibr">(175)</ref> .01 <ref type="bibr">(175)</ref> .01 <ref type="bibr">(175)</ref> .00 <ref type="bibr">(175)</ref> .01 <ref type="bibr">(175)</ref> .02 <ref type="bibr">(168)</ref> .01 <ref type="bibr">(170)</ref> .01 <ref type="bibr">(147)</ref> .02 <ref type="bibr">(172)</ref> .02 <ref type="bibr">(159)</ref> .02 <ref type="bibr">(165)</ref> .02 <ref type="bibr">(171)</ref> .00 <ref type="bibr">(168)</ref> .15 <ref type="bibr">(102)</ref> .06 <ref type="bibr">(143)</ref> .03 <ref type="bibr">(129)</ref> .19 <ref type="bibr">(138)</ref> .02 <ref type="bibr">(136)</ref> .00 (4) .04 <ref type="bibr">(120)</ref> .02 <ref type="bibr">(169)</ref> .04 <ref type="bibr">(169)</ref> .00 <ref type="bibr">(169)</ref> .02 <ref type="bibr">(171)</ref> .04 <ref type="bibr">(157)</ref> .08 <ref type="bibr">(171)</ref> .09 <ref type="bibr">(171)</ref> .02 <ref type="bibr">(169)</ref> .04 <ref type="bibr">(166)</ref> .03 <ref type="bibr">(166)</ref> .03 <ref type="bibr">(169)</ref> .03 <ref type="bibr">(170)</ref> .00 <ref type="bibr">(174)</ref> .01 <ref type="bibr">(152)</ref> .01 <ref type="bibr">(154)</ref> .02 <ref type="bibr">(152)</ref> .01 <ref type="bibr">(150)</ref> .00 <ref type="bibr">(152)</ref> .02 <ref type="bibr">(149)</ref> .00 <ref type="bibr">(148)</ref> .02 <ref type="bibr">(128)</ref> .03 <ref type="bibr">(145)</ref> .03 <ref type="bibr">(145)</ref> .03 <ref type="bibr">(150)</ref> .02 <ref type="bibr">(142)</ref> .01 <ref type="bibr">(147)</ref> .15 <ref type="bibr">(90)</ref> .10 (123) .07 <ref type="bibr">(123)</ref> .24 <ref type="bibr">(116)</ref> .02 <ref type="bibr">(119)</ref> .01 <ref type="formula" target="#formula_19">7</ref>.03 <ref type="bibr">(93)</ref> .02 <ref type="bibr">(147)</ref> .03 <ref type="bibr">(146)</ref> .03 <ref type="bibr">(149)</ref> .04 <ref type="bibr">(149)</ref> .03 <ref type="bibr">(143)</ref> .07 <ref type="bibr">(147)</ref> .03 <ref type="bibr">(149)</ref> .02 <ref type="bibr">(144)</ref> .04 <ref type="bibr">(146)</ref> .03 <ref type="bibr">(147)</ref> .04 <ref type="bibr">(144)</ref> .06 <ref type="bibr">(149)</ref> .00 <ref type="bibr">(154)</ref> .00 <ref type="bibr">(139)</ref> .01 <ref type="bibr">(141)</ref> .03 <ref type="bibr">(141)</ref> .02 <ref type="bibr">(141)</ref> .01 <ref type="bibr">(142)</ref> .03 <ref type="bibr">(132)</ref> .01 <ref type="bibr">(135)</ref> .02 <ref type="bibr">(113)</ref> .03 <ref type="bibr">(137)</ref> .02 <ref type="bibr">(135)</ref> .03 <ref type="bibr">(133)</ref> .01 <ref type="bibr">(130)</ref> .00 <ref type="bibr">(138)</ref> .24 <ref type="bibr">(83)</ref> .08 <ref type="bibr">(121)</ref> .03 <ref type="bibr">(122)</ref> .08 <ref type="bibr">(102)</ref> .08 <ref type="bibr">(107)</ref> .04 <ref type="bibr">(116)</ref> .02 <ref type="bibr">(135)</ref> .01 <ref type="bibr">(131)</ref> .06 <ref type="bibr">(138)</ref> .09 <ref type="bibr">(139)</ref> .03 <ref type="bibr">(134)</ref> .06 <ref type="bibr">(133)</ref> .05 <ref type="bibr">(138)</ref> .05 <ref type="bibr">(131)</ref> .06 <ref type="bibr">(136)</ref> .04 <ref type="bibr">(132)</ref> .01 <ref type="bibr">(138)</ref> .01 <ref type="bibr">(137)</ref> .00 <ref type="bibr">(138)</ref> .01 <ref type="bibr">(423)</ref> .01 <ref type="bibr">(428)</ref> .01 <ref type="bibr">(427)</ref> .01 <ref type="bibr">(425)</ref> .01 <ref type="bibr">(426)</ref> .02 <ref type="bibr">(409)</ref> .01 <ref type="bibr">(407)</ref> .02 <ref type="bibr">(335)</ref> .03 <ref type="bibr">(408)</ref> .02 <ref type="bibr">(401)</ref> .03 <ref type="bibr">(403)</ref> .02 <ref type="bibr">(397)</ref> .02 <ref type="bibr">(413)</ref> .14 (219) .09 <ref type="bibr">(314)</ref> .04 <ref type="bibr">(306)</ref> .11 <ref type="bibr">(259)</ref> .08 <ref type="bibr">(284)</ref> .02 <ref type="bibr">(289)</ref> .04 <ref type="bibr">(327)</ref> .02 <ref type="bibr">(404)</ref> .04 (400) .03 <ref type="bibr">(410)</ref> .03 <ref type="bibr">(414)</ref> .04 <ref type="bibr">(397)</ref> .07 <ref type="bibr">(411)</ref> .04 <ref type="bibr">(413)</ref> .02 <ref type="bibr">(399)</ref> .03 <ref type="bibr">(404)</ref> .04 <ref type="bibr">(402)</ref> .03 <ref type="bibr">(406)</ref> .04 <ref type="bibr">(406)</ref> .00 <ref type="bibr">(424)</ref> .01 <ref type="bibr">(125)</ref> .01 <ref type="bibr">(125)</ref> .03 <ref type="bibr">(122)</ref> .01 <ref type="bibr">(124)</ref> .00 <ref type="bibr">(122)</ref> .03 <ref type="bibr">(118)</ref> .01 <ref type="bibr">(120)</ref> .01 <ref type="bibr">(109)</ref> .02 <ref type="bibr">(121)</ref> .01 <ref type="bibr">(116)</ref> .02 <ref type="bibr">(119)</ref> .02 <ref type="bibr">(122)</ref> .00 <ref type="bibr">(119)</ref> .14 (79) .09 <ref type="bibr">(78)</ref> .05 <ref type="bibr">(73)</ref> .11 <ref type="bibr">(69)</ref> .09 <ref type="bibr">(64)</ref> .02 <ref type="bibr">(99)</ref> .03 <ref type="bibr">(21)</ref> .04 <ref type="bibr">(121)</ref> .03 <ref type="bibr">(118)</ref> .03 <ref type="bibr">(120)</ref> .03 <ref type="bibr">(119)</ref> .02 <ref type="bibr">(119)</ref> .05 <ref type="bibr">(123)</ref> .02 <ref type="bibr">(122)</ref> .03 <ref type="bibr">(123)</ref> .06 <ref type="bibr">(118)</ref> .01 <ref type="bibr">(118)</ref> .03 <ref type="bibr">(119)</ref> .04 <ref type="bibr">(121)</ref> .00</p><p>that do possess attribute i and do not possess attribute j. Intuitively, the i-th row of this matrix shows how the presence of attribute i confuses the network into incorrectly predicting each other attribute j. <ref type="figure">Figure 9</ref> shows this conditional confusion matrix for the neural network and P = 500 program test set configuration used to obtain <ref type="table" target="#tab_0">Table 1</ref>. We re-ordered the confusion matrix to try to expose block structure in the false positive probabilities, revealing groups of instructions that tend to be difficult to distinguish. <ref type="figure">Figure 10</ref> show the conditional confusion matrix for the neural network used to obtain the table in <ref type="figure" target="#fig_2">Fig. 3a</ref>. While the results are somewhat noisy, we observe a few general tendencies:</p><p>â¢ There is increased confusion amongst instructions that select out a single element from an array: HEAD, LAST, ACCESS, MINIMUM, MAXIMUM.  .15 <ref type="bibr">(33)</ref> .12 <ref type="bibr">(41)</ref> .16 <ref type="bibr">(39)</ref> .12 <ref type="bibr">(24)</ref> .09 <ref type="bibr">(19)</ref> .12 <ref type="bibr">(26)</ref> .06 <ref type="bibr">(33)</ref> .04 <ref type="bibr">(34)</ref> .08 <ref type="bibr">(38)</ref> .09 <ref type="bibr">(37)</ref> .07 <ref type="bibr">(36)</ref> .06 <ref type="bibr">(12)</ref> .10</p><p>.09 <ref type="bibr">(34)</ref> .06 <ref type="bibr">(33)</ref> .07 <ref type="bibr">(33)</ref> .06 <ref type="bibr">(37)</ref> .18 <ref type="bibr">(21)</ref> .07 <ref type="bibr">(25)</ref> .04 <ref type="bibr">(37)</ref> .06 <ref type="bibr">(39)</ref> .04 <ref type="bibr">(37)</ref> .01 <ref type="bibr">(38)</ref> .08 <ref type="bibr">(38)</ref> .02 <ref type="bibr">(36)</ref> .04 <ref type="bibr">(41)</ref> .05 <ref type="bibr">(41)</ref> .00 <ref type="bibr">(39)</ref> .02 <ref type="bibr">(32)</ref> .06 <ref type="bibr">(34)</ref> .07 <ref type="bibr">(35)</ref> .01 <ref type="bibr">(41)</ref> .14 <ref type="bibr">(44)</ref> .29 <ref type="bibr">(38)</ref> .12 <ref type="bibr">(44)</ref> .17 <ref type="bibr">(40)</ref> .09 <ref type="bibr">(24)</ref> .10</p><p>.12 <ref type="bibr">(27)</ref> .07 <ref type="bibr">(38)</ref> .11 <ref type="bibr">(38)</ref> .12 <ref type="bibr">(44)</ref> .13 <ref type="bibr">(37)</ref> .16 <ref type="bibr">(39)</ref> .09 <ref type="bibr">(16)</ref> .12 <ref type="bibr">(36)</ref> .14 <ref type="bibr">(39)</ref> .05 <ref type="bibr">(38)</ref> .09 <ref type="bibr">(40)</ref> .00 <ref type="bibr">(38)</ref> .19 <ref type="bibr">(25)</ref> .07 <ref type="bibr">(31)</ref> .04 <ref type="bibr">(42)</ref> .05 <ref type="bibr">(41)</ref> .02 <ref type="bibr">(42)</ref> .02 <ref type="bibr">(46)</ref> .06 <ref type="bibr">(39)</ref> .02 <ref type="bibr">(44)</ref> .03 <ref type="bibr">(42)</ref> .05 <ref type="bibr">(44)</ref> .02 <ref type="bibr">(42)</ref> .02 <ref type="bibr">(44)</ref> .04 <ref type="bibr">(40)</ref> .04 <ref type="bibr">(41)</ref> .02 <ref type="bibr">(45)</ref> .14 <ref type="bibr">(106)</ref> .26 <ref type="bibr">(106)</ref> .08 <ref type="bibr">(91)</ref> .16 <ref type="bibr">(96)</ref> .14 <ref type="bibr">(83)</ref> .17 <ref type="bibr">(94)</ref> .17 <ref type="bibr">(77)</ref> .13 <ref type="bibr">(84)</ref> .11 <ref type="bibr">(88)</ref> .14 <ref type="bibr">(96)</ref> .14 <ref type="bibr">(89)</ref> .10</p><p>.10</p><p>.13 <ref type="bibr">(88)</ref> .12 <ref type="bibr">(90)</ref> .06 <ref type="bibr">(92)</ref> .08 <ref type="bibr">(97)</ref> .04 <ref type="bibr">(110)</ref> .18 <ref type="bibr">(66)</ref> .08 <ref type="bibr">(74)</ref> .05 <ref type="bibr">(102)</ref> .06 <ref type="bibr">(97)</ref> .04 <ref type="bibr">(108)</ref> .01 <ref type="bibr">(104)</ref> .05 <ref type="bibr">(99)</ref> .05 <ref type="bibr">(107)</ref> .02 <ref type="bibr">(106)</ref> .04 <ref type="bibr">(108)</ref> .01 <ref type="bibr">(111)</ref> .03 <ref type="bibr">(102)</ref> .06 <ref type="bibr">(101)</ref> .05 <ref type="bibr">(106)</ref> .03 <ref type="bibr">(113)</ref> .19 <ref type="bibr">(60)</ref> .24 <ref type="bibr">(58)</ref> .12 <ref type="bibr">(37)</ref> .15 <ref type="bibr">(59)</ref> .09 <ref type="bibr">(34)</ref> .13 <ref type="bibr">(40)</ref> .16 <ref type="bibr">(39)</ref> .06 <ref type="bibr">(50)</ref> .11 <ref type="bibr">(49)</ref> .09 <ref type="bibr">(55)</ref> .13 <ref type="bibr">(52)</ref> .10</p><p>.09 <ref type="bibr">(19)</ref> .13 <ref type="bibr">(50)</ref> .10</p><p>.06 <ref type="bibr">(50)</ref> .06 <ref type="bibr">(51)</ref> .07 <ref type="bibr">(52)</ref> .20 <ref type="bibr">(36)</ref> .10</p><p>.04 <ref type="bibr">(54)</ref> .05 <ref type="bibr">(55)</ref> .05 <ref type="bibr">(56)</ref> .04 <ref type="bibr">(53)</ref> .03 <ref type="bibr">(54)</ref> .05 <ref type="bibr">(53)</ref> .02 <ref type="bibr">(58)</ref> .10</p><p>.01 <ref type="bibr">(57)</ref> .03 <ref type="bibr">(49)</ref> .04 <ref type="bibr">(49)</ref> .06 <ref type="bibr">(57)</ref> .03 <ref type="bibr">(59)</ref> .16 <ref type="bibr">(48)</ref> .26 <ref type="bibr">(44)</ref> .18 <ref type="bibr">(32)</ref> .14 <ref type="bibr">(49)</ref> .18 <ref type="bibr">(32)</ref> .10</p><p>.10</p><p>.09 <ref type="bibr">(43)</ref> .09 <ref type="bibr">(46)</ref> .09 <ref type="bibr">(46)</ref> .08 <ref type="bibr">(44)</ref> .14 <ref type="bibr">(45)</ref> .10</p><p>.09 <ref type="bibr">(34)</ref> .12 <ref type="bibr">(34)</ref> .05 <ref type="bibr">(43)</ref> .08 <ref type="bibr">(44)</ref> .03 <ref type="bibr">(41)</ref> .10</p><p>.10</p><p>.05 <ref type="bibr">(44)</ref> .08 <ref type="bibr">(48)</ref> .03 <ref type="bibr">(47)</ref> .02 <ref type="bibr">(45)</ref> .03 <ref type="bibr">(41)</ref> .05 <ref type="bibr">(50)</ref> .02 <ref type="bibr">(50)</ref> .04 <ref type="bibr">(44)</ref> .02 <ref type="bibr">(50)</ref> .02 <ref type="bibr">(43)</ref> .04 <ref type="bibr">(47)</ref> .05 <ref type="bibr">(45)</ref> .01 <ref type="bibr">(50)</ref> .09 <ref type="bibr">(128)</ref> .11 <ref type="bibr">(123)</ref> .10</p><p>.06 <ref type="bibr">(119)</ref> .07 <ref type="bibr">(127)</ref> .17 <ref type="bibr">(132)</ref> .22 <ref type="bibr">(94)</ref> .09 <ref type="bibr">(111)</ref> .11 <ref type="bibr">(108)</ref> .08 <ref type="bibr">(116)</ref> .12 <ref type="bibr">(116)</ref> .04 <ref type="bibr">(70)</ref> .06 <ref type="bibr">(43)</ref> .14 <ref type="bibr">(123)</ref> .09 <ref type="bibr">(107)</ref> .09 <ref type="bibr">(118)</ref> .07 <ref type="bibr">(120)</ref> .05 <ref type="bibr">(124)</ref> .19 <ref type="bibr">(73)</ref> .06 <ref type="bibr">(95)</ref> .04 <ref type="bibr">(126)</ref> .07 <ref type="bibr">(132)</ref> .04 <ref type="bibr">(131)</ref> .02 <ref type="bibr">(134)</ref> .03 <ref type="bibr">(126)</ref> .05 <ref type="bibr">(139)</ref> .03 <ref type="bibr">(133)</ref> .04 <ref type="bibr">(135)</ref> .01 <ref type="bibr">(138)</ref> .03 <ref type="bibr">(129)</ref> .04 <ref type="bibr">(125)</ref> .04 <ref type="bibr">(127)</ref> .00 <ref type="bibr">(143)</ref> .05 <ref type="bibr">(131)</ref> .11 <ref type="bibr">(135)</ref> .09 <ref type="bibr">(133)</ref> .06 <ref type="bibr">(133)</ref> .05 <ref type="bibr">(130)</ref> .16 <ref type="bibr">(140)</ref> .22 <ref type="bibr">(98)</ref> .11 <ref type="bibr">(109)</ref> .14 <ref type="bibr">(117)</ref> .14 <ref type="bibr">(125)</ref> .13 <ref type="bibr">(124)</ref> .03 <ref type="bibr">(69)</ref> .07 <ref type="bibr">(46)</ref> .12 <ref type="bibr">(118)</ref> .13 <ref type="bibr">(120)</ref> .08 <ref type="bibr">(112)</ref> .11 <ref type="bibr">(129)</ref> .05 <ref type="bibr">(129)</ref> .15 <ref type="bibr">(68)</ref> .09 <ref type="bibr">(99)</ref> .05 <ref type="bibr">(137)</ref> .12 <ref type="bibr">(143)</ref> .05 <ref type="bibr">(143)</ref> .02 <ref type="bibr">(139)</ref> .04 <ref type="bibr">(136)</ref> .03 <ref type="bibr">(140)</ref> .02 <ref type="bibr">(145)</ref> .04 <ref type="bibr">(138)</ref> .01 <ref type="bibr">(141)</ref> .02 <ref type="bibr">(134)</ref> .06 <ref type="bibr">(138)</ref> .05 <ref type="bibr">(136)</ref> .01 <ref type="bibr">(150)</ref> .05 <ref type="bibr">(200)</ref> .09 <ref type="bibr">(196)</ref> .07 <ref type="bibr">(178)</ref> .05 <ref type="bibr">(194)</ref> .05 <ref type="bibr">(199)</ref> .09 <ref type="bibr">(164)</ref> .09 <ref type="bibr">(160)</ref> .08 <ref type="bibr">(130)</ref> .11 <ref type="bibr">(144)</ref> .10</p><p>.11 <ref type="bibr">(147)</ref> .07 <ref type="bibr">(150)</ref> .08 <ref type="bibr">(73)</ref> .12 <ref type="bibr">(156)</ref> .11 <ref type="bibr">(155)</ref> .08 <ref type="bibr">(150)</ref> .12 <ref type="bibr">(166)</ref> .04 <ref type="bibr">(170)</ref> .11 <ref type="bibr">(69)</ref> .07 <ref type="bibr">(144)</ref> .04 <ref type="bibr">(203)</ref> .08 <ref type="bibr">(194)</ref> .04 <ref type="bibr">(197)</ref> .03 <ref type="bibr">(203)</ref> .06 <ref type="bibr">(195)</ref> .04 <ref type="bibr">(192)</ref> .03 <ref type="bibr">(204)</ref> .03 <ref type="bibr">(191)</ref> .01 <ref type="bibr">(205)</ref> .04 <ref type="bibr">(194)</ref> .06 <ref type="bibr">(192)</ref> .04 <ref type="bibr">(185)</ref> .01 <ref type="bibr">(213)</ref> .04 <ref type="bibr">(124)</ref> .11 <ref type="bibr">(124)</ref> .06 <ref type="bibr">(102)</ref> .04 <ref type="bibr">(122)</ref> .05 <ref type="bibr">(125)</ref> .08 <ref type="bibr">(98)</ref> .08 <ref type="bibr">(88)</ref> .15 <ref type="bibr">(47)</ref> .21 <ref type="bibr">(125)</ref> .16 <ref type="bibr">(111)</ref> .15 <ref type="bibr">(112)</ref> .05</p><p>.06 <ref type="bibr">(38)</ref> .16 <ref type="bibr">(102)</ref> .14 <ref type="bibr">(90)</ref> .09 <ref type="bibr">(93)</ref> .11 <ref type="bibr">(105)</ref> .04 <ref type="bibr">(113)</ref> .15 <ref type="bibr">(50)</ref> .09 <ref type="bibr">(82)</ref> .05 <ref type="bibr">(126)</ref> .10</p><p>.05 <ref type="bibr">(117)</ref> .02 <ref type="bibr">(125)</ref> .05 <ref type="bibr">(117)</ref> .04 <ref type="bibr">(116)</ref> .02 <ref type="bibr">(122)</ref> .03 <ref type="bibr">(117)</ref> .01 <ref type="bibr">(123)</ref> .04 <ref type="bibr">(120)</ref> .06 <ref type="bibr">(118)</ref> .05 <ref type="bibr">(118)</ref> .01 <ref type="bibr">(131)</ref> .05 <ref type="bibr">(101)</ref> .07</p><p>.08 <ref type="bibr">(82)</ref> .07 <ref type="bibr">(97)</ref> .06 <ref type="bibr">(104)</ref> .09</p><p>.10</p><p>.14 <ref type="bibr">(37)</ref> .19 <ref type="bibr">(101)</ref> .10</p><p>.16 <ref type="bibr">(93)</ref> .03 <ref type="bibr">(40)</ref> .08 <ref type="bibr">(40)</ref> .16 <ref type="bibr">(80)</ref> .12 <ref type="bibr">(81)</ref> .09 <ref type="bibr">(85)</ref> .10</p><p>.03</p><p>.15 <ref type="bibr">(46)</ref> .06 <ref type="bibr">(70)</ref> .05 <ref type="bibr">(95)</ref> .08 <ref type="bibr">(93)</ref> .06 <ref type="bibr">(103)</ref> .02 <ref type="bibr">(105)</ref> .05 <ref type="bibr">(93)</ref> .06 <ref type="bibr">(101)</ref> .03 <ref type="bibr">(99)</ref> .03 <ref type="bibr">(97)</ref> .01 <ref type="bibr">(104)</ref> .04 <ref type="bibr">(96)</ref> .04 <ref type="bibr">(98)</ref> .05 <ref type="bibr">(98)</ref> .01 <ref type="bibr">(106)</ref> .03</p><p>.06 <ref type="bibr">(92)</ref> .06 <ref type="bibr">(76)</ref> .04 <ref type="bibr">(89)</ref> .04 <ref type="bibr">(90)</ref> .08 <ref type="bibr">(65)</ref> .08 <ref type="bibr">(66)</ref> .11 <ref type="bibr">(21)</ref> .14 <ref type="bibr">(73)</ref> .12 <ref type="bibr">(70)</ref> .20 <ref type="bibr">(90)</ref> .04 <ref type="bibr">(37)</ref> .06 <ref type="bibr">(34)</ref> .15 <ref type="bibr">(73)</ref> .13 <ref type="bibr">(69)</ref> .10</p><p>.14 <ref type="bibr">(69)</ref> .05</p><p>.10</p><p>.09 <ref type="bibr">(63)</ref> .05 <ref type="bibr">(87)</ref> .09 <ref type="bibr">(85)</ref> .05 <ref type="bibr">(88)</ref> .03 <ref type="bibr">(82)</ref> .04 <ref type="bibr">(92)</ref> .03 <ref type="bibr">(88)</ref> .03 <ref type="bibr">(92)</ref> .02 <ref type="bibr">(88)</ref> .00 <ref type="bibr">(89)</ref> .04 <ref type="bibr">(81)</ref> .04 <ref type="bibr">(87)</ref> .04 <ref type="bibr">(81)</ref> .02 <ref type="bibr">(92)</ref> .06 <ref type="bibr">(93)</ref> .10</p><p>.05 <ref type="bibr">(72)</ref> .03 <ref type="bibr">(89)</ref> .07</p><p>.06 <ref type="bibr">(68)</ref> .10</p><p>.13 <ref type="bibr">(29)</ref> .12 <ref type="bibr">(77)</ref> .16 <ref type="bibr">(82)</ref> .23 <ref type="bibr">(93)</ref> .07 <ref type="bibr">(45)</ref> .09 <ref type="bibr">(33)</ref> .10</p><p>.13</p><p>.09 <ref type="bibr">(74)</ref> .09 <ref type="bibr">(75)</ref> .03 <ref type="bibr">(79)</ref> .12 <ref type="bibr">(35)</ref> .05 <ref type="bibr">(59)</ref> .05 <ref type="bibr">(87)</ref> .09 <ref type="bibr">(85)</ref> .04</p><p>.02 <ref type="bibr">(93)</ref> .06</p><p>.04 <ref type="bibr">(83)</ref> .03 <ref type="bibr">(92)</ref> .03 <ref type="bibr">(87)</ref> .01 <ref type="bibr">(92)</ref> .04 <ref type="bibr">(90)</ref> .07 <ref type="bibr">(83)</ref> .04 <ref type="bibr">(85)</ref> .01 <ref type="bibr">(97)</ref> .04 <ref type="bibr">(190)</ref> .09 <ref type="bibr">(188)</ref> .07 <ref type="bibr">(136)</ref> .04 <ref type="bibr">(183)</ref> .06 <ref type="bibr">(190)</ref> .05 <ref type="bibr">(120)</ref> .08 <ref type="bibr">(111)</ref> .29 <ref type="bibr">(130)</ref> .14 <ref type="bibr">(114)</ref> .16 <ref type="bibr">(127)</ref> .17 <ref type="bibr">(138)</ref> .16 <ref type="bibr">(143)</ref> .07 <ref type="bibr">(64)</ref> .16 <ref type="bibr">(154)</ref> .15 <ref type="bibr">(143)</ref> .10</p><p>.11 <ref type="bibr">(160)</ref> .04 <ref type="bibr">(167)</ref> .16 <ref type="bibr">(83)</ref> .08 <ref type="bibr">(118)</ref> .05 <ref type="bibr">(171)</ref> .10</p><p>.06 <ref type="bibr">(185)</ref> .02 <ref type="bibr">(182)</ref> .04 <ref type="bibr">(176)</ref> .05 <ref type="bibr">(181)</ref> .02 <ref type="bibr">(180)</ref> .02 <ref type="bibr">(179)</ref> .01 <ref type="bibr">(181)</ref> .04 <ref type="bibr">(175)</ref> .05 <ref type="bibr">(174)</ref> .05 <ref type="bibr">(177)</ref> .02 <ref type="bibr">(192)</ref> .06 <ref type="bibr">(345)</ref> .08 <ref type="bibr">(344)</ref> .05 <ref type="bibr">(302)</ref> .04 <ref type="bibr">(333)</ref> .06 <ref type="bibr">(342)</ref> .06 <ref type="bibr">(272)</ref> .08 <ref type="bibr">(267)</ref> .15 <ref type="bibr">(232)</ref> .07 <ref type="bibr">(280)</ref> .10</p><p>.10</p><p>.11 <ref type="bibr">(310)</ref> .05 <ref type="bibr">(243)</ref> .13 <ref type="bibr">(275)</ref> .11 <ref type="bibr">(266)</ref> .10</p><p>.11 <ref type="bibr">(295)</ref> .07 <ref type="bibr">(302)</ref> .12 <ref type="bibr">(127)</ref> .07 <ref type="bibr">(251)</ref> .04 <ref type="bibr">(340)</ref> .08 <ref type="bibr">(328)</ref> .05 <ref type="bibr">(327)</ref> .03 <ref type="bibr">(327)</ref> .05 <ref type="bibr">(310)</ref> .05 <ref type="bibr">(329)</ref> .03 <ref type="bibr">(336)</ref> .04 <ref type="bibr">(325)</ref> .02 <ref type="bibr">(337)</ref> .05 <ref type="bibr">(305)</ref> .07 <ref type="bibr">(303)</ref> .05 <ref type="bibr">(299)</ref> .01 <ref type="bibr">(368)</ref> .05 <ref type="bibr">(131)</ref> .08 <ref type="bibr">(132)</ref> .06 <ref type="bibr">(116)</ref> .06 <ref type="bibr">(132)</ref> .04 <ref type="bibr">(126)</ref> .11 <ref type="bibr">(120)</ref> .06 <ref type="bibr">(107)</ref> .10</p><p>.07 <ref type="bibr">(112)</ref> .08 <ref type="bibr">(114)</ref> .10</p><p>.11 <ref type="bibr">(117)</ref> .08 <ref type="bibr">(101)</ref> .06 <ref type="bibr">(43)</ref> .15 <ref type="bibr">(112)</ref> .10</p><p>.11 <ref type="bibr">(112)</ref> .05 <ref type="bibr">(114)</ref> .05 <ref type="bibr">(19)</ref> .04 <ref type="bibr">(58)</ref> .05 <ref type="bibr">(131)</ref> .08 <ref type="bibr">(122)</ref> .05 <ref type="bibr">(133)</ref> .02 <ref type="bibr">(131)</ref> .05 <ref type="bibr">(126)</ref> .06 <ref type="bibr">(132)</ref> .03 <ref type="bibr">(130)</ref> .02 <ref type="bibr">(131)</ref> .01 <ref type="bibr">(132)</ref> .04 <ref type="bibr">(123)</ref> .07 <ref type="bibr">(129)</ref> .05 <ref type="bibr">(117)</ref> .01 <ref type="bibr">(141)</ref> .04 <ref type="bibr">(147)</ref> .07 <ref type="bibr">(147)</ref> .04 <ref type="bibr">(130)</ref> .02 <ref type="bibr">(144)</ref> .04 <ref type="bibr">(138)</ref> .06 <ref type="bibr">(116)</ref> .07 <ref type="bibr">(121)</ref> .15 <ref type="bibr">(94)</ref> .08 <ref type="bibr">(112)</ref> .10</p><p>.09 <ref type="bibr">(129)</ref> .10</p><p>.06 <ref type="bibr">(102)</ref> .07 <ref type="bibr">(46)</ref> .18 <ref type="bibr">(124)</ref> .09 <ref type="bibr">(112)</ref> .10</p><p>.05 <ref type="bibr">(125)</ref> .06 <ref type="bibr">(28)</ref> .04 <ref type="bibr">(63)</ref> .05 <ref type="bibr">(141)</ref> .08 <ref type="bibr">(138)</ref> .04 <ref type="bibr">(139)</ref> .03 <ref type="bibr">(145)</ref> .07 <ref type="bibr">(138)</ref> .05 <ref type="bibr">(143)</ref> .03 <ref type="bibr">(142)</ref> .02 <ref type="bibr">(138)</ref> .01 <ref type="bibr">(138)</ref> .04 <ref type="bibr">(133)</ref> .05 <ref type="bibr">(132)</ref> .05 <ref type="bibr">(135)</ref> .01 <ref type="bibr">(154)</ref> .04 <ref type="bibr">(136)</ref> .08 <ref type="bibr">(136)</ref> .03 <ref type="bibr">(122)</ref> .04 <ref type="bibr">(134)</ref> .03 <ref type="bibr">(137)</ref> .06 <ref type="bibr">(117)</ref> .05 <ref type="bibr">(103)</ref> .14 <ref type="bibr">(79)</ref> .09 <ref type="bibr">(105)</ref> .12 <ref type="bibr">(121)</ref> .09 <ref type="bibr">(106)</ref> .10</p><p>.04 <ref type="bibr">(92)</ref> .08 <ref type="bibr">(48)</ref> .11 <ref type="bibr">(105)</ref> .11 <ref type="bibr">(102)</ref> .30 <ref type="bibr">(121)</ref> .07 <ref type="bibr">(124)</ref> .04 <ref type="bibr">(21)</ref> .08 <ref type="bibr">(69)</ref> .04 <ref type="bibr">(130)</ref> .08 <ref type="bibr">(132)</ref> .02 <ref type="bibr">(135)</ref> .03 <ref type="bibr">(135)</ref> .03 <ref type="bibr">(131)</ref> .05 <ref type="bibr">(134)</ref> .05 <ref type="bibr">(135)</ref> .04 <ref type="bibr">(131)</ref> .02 <ref type="bibr">(127)</ref> .05 <ref type="bibr">(129)</ref> .06 <ref type="bibr">(126)</ref> .05 <ref type="bibr">(126)</ref> .02 <ref type="bibr">(142)</ref> .04 <ref type="bibr">(104)</ref> .10</p><p>.01 <ref type="bibr">(95)</ref> .03 <ref type="bibr">(103)</ref> .04 <ref type="bibr">(106)</ref> .03 <ref type="bibr">(87)</ref> .07 <ref type="bibr">(88)</ref> .11 <ref type="bibr">(63)</ref> .07 <ref type="bibr">(85)</ref> .08 <ref type="bibr">(92)</ref> .08 <ref type="bibr">(87)</ref> .08 <ref type="bibr">(90)</ref> .05 <ref type="bibr">(77)</ref> .06 <ref type="bibr">(33)</ref> .13 <ref type="bibr">(82)</ref> .13 <ref type="bibr">(85)</ref> .28 <ref type="bibr">(89)</ref> .07 <ref type="bibr">(95)</ref> .04 <ref type="bibr">(17)</ref> .06 <ref type="bibr">(54)</ref> .04 <ref type="bibr">(98)</ref> .07 <ref type="bibr">(95)</ref> .06 <ref type="bibr">(103)</ref> .03 <ref type="bibr">(96)</ref> .05 <ref type="bibr">(95)</ref> .04 <ref type="bibr">(103)</ref> .04 <ref type="bibr">(109)</ref> .04 <ref type="bibr">(110)</ref> .01 <ref type="bibr">(109)</ref> .04 <ref type="bibr">(95)</ref> .07 <ref type="bibr">(96)</ref> .05 <ref type="bibr">(94)</ref> .02 <ref type="bibr">(111)</ref> .03 <ref type="bibr">(92)</ref> .05 <ref type="bibr">(88)</ref> .04 <ref type="bibr">(92)</ref> .03 <ref type="bibr">(88)</ref> .04 <ref type="bibr">(87)</ref> .05 <ref type="bibr">(75)</ref> .03 <ref type="bibr">(72)</ref> .12 <ref type="bibr">(51)</ref> .08 <ref type="bibr">(77)</ref> .05 <ref type="bibr">(79)</ref> .11 <ref type="bibr">(73)</ref> .07 <ref type="bibr">(78)</ref> .03 <ref type="bibr">(68)</ref> .07 <ref type="bibr">(24)</ref> .09 <ref type="bibr">(68)</ref> .10</p><p>.10</p><p>.11 <ref type="bibr">(79)</ref> .07 <ref type="bibr">(65)</ref> .03 <ref type="bibr">(90)</ref> .08 <ref type="bibr">(87)</ref> .05 <ref type="bibr">(86)</ref> .11 <ref type="bibr">(88)</ref> .03 <ref type="bibr">(87)</ref> .04 <ref type="bibr">(88)</ref> .03 <ref type="bibr">(90)</ref> .03 <ref type="bibr">(87)</ref> .03 <ref type="bibr">(90)</ref> .04 <ref type="bibr">(80)</ref> .07 <ref type="bibr">(81)</ref> .04 <ref type="bibr">(81)</ref> .02 <ref type="bibr">(96)</ref> .05 <ref type="bibr">(318)</ref> .08 <ref type="bibr">(317)</ref> .04 <ref type="bibr">(290)</ref> .04 <ref type="bibr">(314)</ref> .04 <ref type="bibr">(305)</ref> .06 <ref type="bibr">(266)</ref> .06 <ref type="bibr">(253)</ref> .13 <ref type="bibr">(192)</ref> .08 <ref type="bibr">(256)</ref> .09 <ref type="bibr">(276)</ref> .09 <ref type="bibr">(272)</ref> .09 <ref type="bibr">(276)</ref> .05 <ref type="bibr">(226)</ref> .06 <ref type="bibr">(91)</ref> .11 <ref type="bibr">(215)</ref> .10</p><p>.11 <ref type="bibr">(215)</ref> .13 <ref type="bibr">(243)</ref> .06 <ref type="bibr">(242)</ref> .07 <ref type="bibr">(219)</ref> .03 <ref type="bibr">(303)</ref> .09 <ref type="bibr">(297)</ref> .04 <ref type="bibr">(310)</ref> .04 <ref type="bibr">(305)</ref> .05 <ref type="bibr">(301)</ref> .05 <ref type="bibr">(310)</ref> .04 <ref type="bibr">(308)</ref> .03 <ref type="bibr">(304)</ref> .01 <ref type="bibr">(310)</ref> .04 <ref type="bibr">(287)</ref> .06 <ref type="bibr">(294)</ref> .04 <ref type="bibr">(283)</ref> .01 <ref type="bibr">(334)</ref> .04 <ref type="bibr">(175)</ref> .09 <ref type="bibr">(176)</ref> .05 <ref type="bibr">(151)</ref> .05 <ref type="bibr">(174)</ref> .05 <ref type="bibr">(176)</ref> .09 <ref type="bibr">(141)</ref> .07 <ref type="bibr">(137)</ref> .15 <ref type="bibr">(120)</ref> .09 <ref type="bibr">(141)</ref> .12 <ref type="bibr">(153)</ref> .10</p><p>.11 <ref type="bibr">(153)</ref> .07 <ref type="bibr">(114)</ref> .08 <ref type="bibr">(68)</ref> .10</p><p>.10</p><p>.10</p><p>.13 <ref type="bibr">(133)</ref> .05 <ref type="bibr">(160)</ref> .13 <ref type="bibr">(72)</ref> .06 <ref type="bibr">(175)</ref> .08 <ref type="bibr">(174)</ref> .05 <ref type="bibr">(173)</ref> .02 <ref type="bibr">(180)</ref> .04 <ref type="bibr">(168)</ref> .04 <ref type="bibr">(180)</ref> .02 <ref type="bibr">(180)</ref> .03 <ref type="bibr">(184)</ref> .01 <ref type="bibr">(175)</ref> .03 <ref type="bibr">(169)</ref> .05 <ref type="bibr">(168)</ref> .05 <ref type="bibr">(168)</ref> .02 <ref type="bibr">(191)</ref> .05 <ref type="bibr">(52)</ref> .09 <ref type="bibr">(52)</ref> .06 <ref type="bibr">(44)</ref> .02 <ref type="bibr">(50)</ref> .03 <ref type="bibr">(50)</ref> .03 <ref type="bibr">(37)</ref> .09 <ref type="bibr">(40)</ref> .22 <ref type="bibr">(44)</ref> .12 <ref type="bibr">(50)</ref> .08 <ref type="bibr">(43)</ref> .16 <ref type="bibr">(49)</ref> .10</p><p>.04 <ref type="bibr">(32)</ref> .11 <ref type="bibr">(22)</ref> .14 <ref type="bibr">(45)</ref> .12 <ref type="bibr">(43)</ref> .10</p><p>.09 <ref type="bibr">(42)</ref> .07 <ref type="bibr">(50)</ref> .10</p><p>.10</p><p>.13 <ref type="bibr">(51)</ref> .07 <ref type="bibr">(53)</ref> .02 <ref type="bibr">(51)</ref> .04 <ref type="bibr">(51)</ref> .05 <ref type="bibr">(54)</ref> .03 <ref type="bibr">(52)</ref> .02 <ref type="bibr">(52)</ref> .00 <ref type="bibr">(51)</ref> .04 <ref type="bibr">(51)</ref> .05 <ref type="bibr">(51)</ref> .04 <ref type="bibr">(50)</ref> .00 <ref type="bibr">(56)</ref> .05 <ref type="bibr">(62)</ref> .07 <ref type="bibr">(59)</ref> .02 <ref type="bibr">(47)</ref> .02 <ref type="bibr">(59)</ref> .06 <ref type="bibr">(62)</ref> .05 <ref type="bibr">(51)</ref> .11 <ref type="bibr">(54)</ref> .15 <ref type="bibr">(43)</ref> .09 <ref type="bibr">(52)</ref> .06 <ref type="bibr">(49)</ref> .10</p><p>.11 <ref type="bibr">(52)</ref> .05 <ref type="bibr">(39)</ref> .08 <ref type="bibr">(18)</ref> .15 <ref type="bibr">(44)</ref> .14 <ref type="bibr">(48)</ref> .12 <ref type="bibr">(52)</ref> .08 <ref type="bibr">(47)</ref> .07 <ref type="bibr">(55)</ref> .15 <ref type="bibr">(23)</ref> .10</p><p>.10</p><p>.06 <ref type="bibr">(58)</ref> .02 <ref type="bibr">(57)</ref> .05 <ref type="bibr">(57)</ref> .10</p><p>.04 <ref type="bibr">(61)</ref> .05 <ref type="bibr">(59)</ref> .02 <ref type="bibr">(60)</ref> .07 <ref type="bibr">(57)</ref> .07 <ref type="bibr">(51)</ref> .05 <ref type="bibr">(55)</ref> .00 <ref type="bibr">(64)</ref> .05 <ref type="bibr">(43)</ref> .06 <ref type="bibr">(43)</ref> .03 <ref type="bibr">(41)</ref> .03 <ref type="bibr">(43)</ref> .05 <ref type="bibr">(44)</ref> .03 <ref type="bibr">(33)</ref> .07 <ref type="bibr">(37)</ref> .12 <ref type="bibr">(29)</ref> .06 <ref type="bibr">(32)</ref> .15 <ref type="bibr">(42)</ref> .06 <ref type="bibr">(41)</ref> .06 <ref type="bibr">(41)</ref> .04 <ref type="bibr">(37)</ref> .18 <ref type="bibr">(38)</ref> .09 <ref type="bibr">(32)</ref> .11 <ref type="bibr">(38)</ref> .11 <ref type="bibr">(38)</ref> .04 <ref type="bibr">(37)</ref> .13 <ref type="bibr">(19)</ref> .05 <ref type="bibr">(29)</ref> .06 <ref type="bibr">(44)</ref> .12 <ref type="bibr">(41)</ref> .03 <ref type="bibr">(45)</ref> .06 <ref type="bibr">(45)</ref> .03 <ref type="bibr">(39)</ref> .01 <ref type="bibr">(39)</ref> .03 <ref type="bibr">(42)</ref> .01 <ref type="bibr">(43)</ref> .05 <ref type="bibr">(42)</ref> .08 <ref type="bibr">(45)</ref> .03 <ref type="bibr">(42)</ref> .03 <ref type="bibr">(47)</ref> .07 <ref type="bibr">(44)</ref> .09 <ref type="bibr">(47)</ref> .03 <ref type="bibr">(37)</ref> .04 <ref type="bibr">(40)</ref> .04 <ref type="bibr">(42)</ref> .04 <ref type="bibr">(36)</ref> .03 <ref type="bibr">(33)</ref> .18 <ref type="bibr">(35)</ref> .09 <ref type="bibr">(40)</ref> .10</p><p>.06 <ref type="bibr">(35)</ref> .09 <ref type="bibr">(43)</ref> .03 <ref type="bibr">(34)</ref> .13 <ref type="bibr">(36)</ref> .12 <ref type="bibr">(38)</ref> .14 <ref type="bibr">(38)</ref> .07 <ref type="bibr">(31)</ref> .43 <ref type="bibr">(39)</ref> .18 <ref type="bibr">(14)</ref> .11 <ref type="bibr">(36)</ref> .02 <ref type="bibr">(42)</ref> .06 <ref type="bibr">(40)</ref> .07 <ref type="bibr">(45)</ref> .02 <ref type="bibr">(40)</ref> .05 <ref type="bibr">(43)</ref> .02 <ref type="bibr">(47)</ref> .13 <ref type="bibr">(45)</ref> .03 <ref type="bibr">(46)</ref> .03 <ref type="bibr">(38)</ref> .07 <ref type="bibr">(43)</ref> .05 <ref type="bibr">(40)</ref> .01 <ref type="bibr">(46)</ref> .05 <ref type="bibr">(61)</ref> .09 <ref type="bibr">(57)</ref> .06 <ref type="bibr">(49)</ref> .04 <ref type="bibr">(58)</ref> .06 <ref type="bibr">(55)</ref> .05 <ref type="bibr">(45)</ref> .09 <ref type="bibr">(47)</ref> .13 <ref type="bibr">(44)</ref> .05 <ref type="bibr">(49)</ref> .07 <ref type="bibr">(49)</ref> .16 <ref type="bibr">(62)</ref> .10</p><p>.07 <ref type="bibr">(45)</ref> .12 <ref type="bibr">(48)</ref> .15 <ref type="bibr">(48)</ref> .08 <ref type="bibr">(51)</ref> .06 <ref type="bibr">(47)</ref> .08 <ref type="bibr">(55)</ref> .14 <ref type="bibr">(27)</ref> .10</p><p>.06 <ref type="bibr">(59)</ref> .08 <ref type="bibr">(57)</ref> .07 <ref type="bibr">(62)</ref> .04 <ref type="bibr">(57)</ref> .10</p><p>.02 <ref type="bibr">(62)</ref> .03 <ref type="bibr">(61)</ref> .02 <ref type="bibr">(58)</ref> .04 <ref type="bibr">(59)</ref> .06 <ref type="bibr">(58)</ref> .08 <ref type="bibr">(53)</ref> .01 <ref type="bibr">(62)</ref> .07 <ref type="bibr">(40)</ref> .08 <ref type="bibr">(43)</ref> .04 <ref type="bibr">(38)</ref> .03 <ref type="bibr">(38)</ref> .08 <ref type="bibr">(45)</ref> .10</p><p>.08 <ref type="bibr">(32)</ref> .10</p><p>.04 <ref type="bibr">(29)</ref> .15 <ref type="bibr">(38)</ref> .12 <ref type="bibr">(39)</ref> .03 <ref type="bibr">(31)</ref> .06 <ref type="bibr">(31)</ref> .13 <ref type="bibr">(35)</ref> .11 <ref type="bibr">(34)</ref> .08 <ref type="bibr">(35)</ref> .11 <ref type="bibr">(36)</ref> .06 <ref type="bibr">(37)</ref> .10</p><p>.06 <ref type="bibr">(34)</ref> .06 <ref type="bibr">(43)</ref> .07 <ref type="bibr">(42)</ref> .04 <ref type="bibr">(37)</ref> .03 <ref type="bibr">(41)</ref> .16 <ref type="bibr">(45)</ref> .01 <ref type="bibr">(40)</ref> .03 <ref type="bibr">(40)</ref> .02 <ref type="bibr">(44)</ref> .09 <ref type="bibr">(43)</ref> .07 <ref type="bibr">(42)</ref> .05 <ref type="bibr">(38)</ref> .00 <ref type="bibr">(45)</ref> .03 <ref type="bibr">(38)</ref> .06 <ref type="bibr">(34)</ref> .04 <ref type="bibr">(30)</ref> .02 <ref type="bibr">(36)</ref> .05 <ref type="bibr">(38)</ref> .05 <ref type="bibr">(26)</ref> .10</p><p>.16 <ref type="bibr">(27)</ref> .04 <ref type="bibr">(28)</ref> .11 <ref type="bibr">(29)</ref> .05 <ref type="bibr">(36)</ref> .06 <ref type="bibr">(33)</ref> .01 <ref type="bibr">(23)</ref> .11 <ref type="bibr">(26)</ref> .07 <ref type="bibr">(26)</ref> .25 <ref type="bibr">(29)</ref> .19 <ref type="bibr">(35)</ref> .03 <ref type="bibr">(32)</ref> .08 <ref type="bibr">(8)</ref> .04 <ref type="bibr">(27)</ref> .03 <ref type="bibr">(34)</ref> .13 <ref type="bibr">(35)</ref> .02 <ref type="bibr">(30)</ref> .03 <ref type="bibr">(38)</ref> .07 <ref type="bibr">(36)</ref> .03 <ref type="bibr">(33)</ref> .02 <ref type="bibr">(33)</ref> .03 <ref type="bibr">(36)</ref> .07 <ref type="bibr">(35)</ref> .07 <ref type="bibr">(35)</ref> .01 <ref type="bibr">(33)</ref> .01 <ref type="bibr">(37)</ref> .04 <ref type="bibr">(49)</ref> .08 <ref type="bibr">(47)</ref> .04 <ref type="bibr">(43)</ref> .01 <ref type="bibr">(42)</ref> .04 <ref type="bibr">(43)</ref> .05 <ref type="bibr">(39)</ref> .05 <ref type="bibr">(34)</ref> .10</p><p>.04 <ref type="bibr">(34)</ref> .09 <ref type="bibr">(38)</ref> .12 <ref type="bibr">(43)</ref> .07 <ref type="bibr">(39)</ref> .03 <ref type="bibr">(33)</ref> .09 <ref type="bibr">(38)</ref> .10</p><p>.04 <ref type="bibr">(36)</ref> .11 <ref type="bibr">(47)</ref> .04 <ref type="bibr">(40)</ref> .12 <ref type="bibr">(15)</ref> .06 <ref type="bibr">(42)</ref> .04 <ref type="bibr">(45)</ref> .05 <ref type="bibr">(44)</ref> .04 <ref type="bibr">(44)</ref> .02 <ref type="bibr">(47)</ref> .04 <ref type="bibr">(46)</ref> .04 <ref type="bibr">(44)</ref> .05 <ref type="bibr">(44)</ref> .03 <ref type="bibr">(48)</ref> .05 <ref type="bibr">(44)</ref> .05 <ref type="bibr">(45)</ref> .02 <ref type="bibr">(38)</ref> .01 <ref type="bibr">(49)</ref> .04 <ref type="bibr">(35)</ref> .05 <ref type="bibr">(33)</ref> .05 <ref type="bibr">(34)</ref> .01 <ref type="bibr">(34)</ref> .03 <ref type="bibr">(37)</ref> .01 <ref type="bibr">(30)</ref> .05 <ref type="bibr">(25)</ref> .20 <ref type="bibr">(27)</ref> .05 <ref type="bibr">(28)</ref> .14 <ref type="bibr">(33)</ref> .10</p><p>.17 <ref type="bibr">(32)</ref> .03</p><p>.10</p><p>.05 <ref type="bibr">(21)</ref> .07 <ref type="bibr">(20)</ref> .15 <ref type="bibr">(34)</ref> .13 <ref type="bibr">(31)</ref> .06</p><p>.06 <ref type="bibr">(21)</ref> .03 <ref type="bibr">(32)</ref> .10</p><p>.05 <ref type="bibr">(33)</ref> .05 <ref type="bibr">(36)</ref> .04 <ref type="bibr">(31)</ref> .04 <ref type="bibr">(36)</ref> .07 <ref type="bibr">(35)</ref> .07 <ref type="bibr">(36)</ref> .04 <ref type="bibr">(35)</ref> .05 <ref type="bibr">(33)</ref> .03 <ref type="bibr">(34)</ref> .02 <ref type="bibr">(37)</ref> .04 <ref type="bibr">(60)</ref> .10</p><p>.05 <ref type="bibr">(57)</ref> .03 <ref type="bibr">(58)</ref> .06 <ref type="bibr">(62)</ref> .06 <ref type="bibr">(53)</ref> .07 <ref type="bibr">(50)</ref> .14 <ref type="bibr">(48)</ref> .09 <ref type="bibr">(57)</ref> .09 <ref type="bibr">(57)</ref> .09 <ref type="bibr">(56)</ref> .11 <ref type="bibr">(62)</ref> .08 <ref type="bibr">(49)</ref> .10</p><p>.13 <ref type="bibr">(48)</ref> .13 <ref type="bibr">(54)</ref> .11 <ref type="bibr">(52)</ref> .05 <ref type="bibr">(53)</ref> .08 <ref type="bibr">(18)</ref> .06 <ref type="bibr">(47)</ref> .04 <ref type="bibr">(64)</ref> .08 <ref type="bibr">(62)</ref> .05 <ref type="bibr">(64)</ref> .03 <ref type="bibr">(60)</ref> .04 <ref type="bibr">(64)</ref> .07 <ref type="bibr">(67)</ref> .03 <ref type="bibr">(66)</ref> .03 <ref type="bibr">(64)</ref> .02 <ref type="bibr">(67)</ref> .11 <ref type="bibr">(64)</ref> .08 <ref type="bibr">(63)</ref> .01 <ref type="bibr">(69)</ref> .05 <ref type="bibr">(64)</ref> .08 <ref type="bibr">(65)</ref> .03 <ref type="bibr">(58)</ref> .03 <ref type="bibr">(60)</ref> .05 <ref type="bibr">(68)</ref> .05</p><p>.08 <ref type="bibr">(56)</ref> .17 <ref type="bibr">(48)</ref> .09 <ref type="bibr">(57)</ref> .09</p><p>.07 <ref type="bibr">(64)</ref> .10</p><p>.08 <ref type="bibr">(50)</ref> .16 <ref type="bibr">(58)</ref> .13 <ref type="bibr">(49)</ref> .07 <ref type="bibr">(53)</ref> .13 <ref type="bibr">(55)</ref> .04 <ref type="bibr">(56)</ref> .12 <ref type="bibr">(27)</ref> .06 <ref type="bibr">(48)</ref> .04 <ref type="bibr">(66)</ref> .06 <ref type="bibr">(58)</ref> .07 <ref type="bibr">(69)</ref> .03 <ref type="bibr">(67)</ref> .05 <ref type="bibr">(65)</ref> .05 <ref type="bibr">(68)</ref> .03 <ref type="bibr">(68)</ref> .04 <ref type="bibr">(67)</ref> .02 <ref type="bibr">(67)</ref> .06 <ref type="bibr">(66)</ref> .09 <ref type="bibr">(63)</ref> .01 <ref type="bibr">(70)</ref> .05 <ref type="bibr">(69)</ref> .07 <ref type="bibr">(70)</ref> .04 <ref type="bibr">(67)</ref> .04 <ref type="bibr">(72)</ref> .03 <ref type="bibr">(70)</ref> .04 <ref type="bibr">(57)</ref> .08 <ref type="bibr">(58)</ref> .12 <ref type="bibr">(45)</ref> .06</p><p>.11 <ref type="bibr">(65)</ref> .07 <ref type="bibr">(62)</ref> .09 <ref type="bibr">(63)</ref> .03 <ref type="bibr">(57)</ref> .12 <ref type="bibr">(50)</ref> .09 <ref type="bibr">(56)</ref> .10</p><p>.14 <ref type="bibr">(57)</ref> .05 <ref type="bibr">(60)</ref> .11 <ref type="bibr">(20)</ref> .06 <ref type="bibr">(52)</ref> .03 <ref type="bibr">(69)</ref> .07 <ref type="bibr">(66)</ref> .07 <ref type="bibr">(70)</ref> .03 <ref type="bibr">(68)</ref> .07 <ref type="bibr">(64)</ref> .07 <ref type="bibr">(68)</ref> .02 <ref type="bibr">(70)</ref> .01 <ref type="bibr">(64)</ref> .00 <ref type="bibr">(72)</ref> .09 <ref type="bibr">(69)</ref> .16 <ref type="bibr">(67)</ref> .00 <ref type="bibr">(74)</ref> .07</p><p>.09</p><p>.27</p><p>.20</p><p>.14</p><p>.07</p><p>.05</p><p>.09</p><p>.06</p><p>.15</p><p>.16</p><p>.14 <ref type="bibr">(6)</ref> .14</p><p>.18</p><p>.22</p><p>.30</p><p>.28</p><p>.18</p><p>.11 <ref type="bibr">(2)</ref> .59</p><p>.03</p><p>.02</p><p>.04</p><p>.02</p><p>.01</p><p>.06</p><p>.01</p><p>.13</p><p>.00</p><p>.02</p><p>.05</p><p>.02</p><p>(5) <ref type="figure">Figure 10</ref>: Conditional confusion matrix for the neural network and test set of P = 500 programs of length T = 5. The presentation is the same as in <ref type="figure">Figure 9</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Neural network predicts the probability of each function appearing in the source code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Search speedups on programs of length T = 5 and influence of length of training programs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>; Shotton et al. (2013); StuhlmÃ¼ller et al. (2013); Heess et al. (2013); Jampani et al. (2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>MIN a | ZIPWITH -a b | FILTER (&gt;0) c | SUM d 4: SORT a | SORT b | REVERSE d | ZIPWITH * d e | SUM f 5: REVERSE a | ZIPWITH MIN a b 6: MAP (-1) a | MAP (-1) b | ZIPWITH + c d | MINIMUM e 7: SCANL1 + b | ZIPWITH * a c | SUM d 8: REVERSE a | ZIPWITH -b a | FILTER (&gt;0) c | SUM d .0 .2 .0 .1 .4 .0 .0 .2 .0 .1 .0 .2 .1 .0 .1 .0 .3 .4 .2 .1 .5 .2 .2 .6 .5 .2 .4 .0 .9 .1 .0 .1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Number of test problems solved versus computation time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Number of test problems solved versus computation time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>FirstFigure 8 :</head><label>8</label><figDesc>A learned embedding of integers {â256, â255, . . . , â1, 0, 1, . . . , 255} in R 2 . The color intensity corresponds to the magnitude of the embedded integer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Firstâ¢</head><label></label><figDesc>HEAD :: [int] -&gt; int lambda xs: xs[0] if len(xs)&gt;0 else Null Given an array, returns its first element (or NULL if the array is empty). â¢ LAST :: [int] -&gt; int lambda xs: xs[-1] if len(xs)&gt;0 else Null Given an array, returns its last element (or NULL if the array is empty). â¢ TAKE :: int -&gt; [int] -&gt; int lambda n, xs: xs[:n] Given an integer n and array xs, returns the array truncated after the n-th element. (If the length of xs was no larger than n in the first place, it is returned without modification.) â¢ DROP :: int -&gt; [int] -&gt; int lambda n, xs: xs[n:] Given an integer n and array xs, returns the array with the first n elements dropped. (If the length of xs was no larger than n in the first place, an empty array is returned.) â¢ ACCESS :: int -&gt; [int] -&gt; int lambda n, xs: xs[n] if n&gt;=0 and len(xs)&gt;n else Null Given an integer n and array xs, returns the (n+1)-st element of xs. (If the length of xs was less than or equal to n, the value NULL is returned instead.) â¢ MINIMUM :: [int] -&gt; int lambda xs: min(xs) if len(xs)&gt;0 else Null Given an array, returns its minimum (or NULL if the array is empty). â¢ MAXIMUM :: [int] -&gt; int lambda xs: max(xs) if len(xs)&gt;0 else Null Given an array, returns its maximum (or NULL if the array is empty). â¢ REVERSE :: [int] -&gt; [int] lambda xs: list(reversed(xs)) Given an array, returns its elements in reversed order. â¢ SORT :: [int] -&gt; [int] lambda xs: sorted(xs) Given an array, return its elements in non-decreasing order. â¢ SUM :: [int] -&gt; int lambda xs: sum(xs) Given an array, returns the sum of its elements. (The sum of an empty array is 0.) Higher-order functions: â¢ MAP :: (int -&gt; int) -&gt; [int] -&gt; [int] lambda f, xs: [f(x) for x in xs] Given a lambda function f mapping from integers to integers, and an array xs, returns the array resulting from applying f to each element of xs. â¢ FILTER :: (int -&gt; bool) -&gt; [int] -&gt; [int] lambda f, xs: [x for x in xs if f(x)] Given a predicate f mapping from integers to truth values, and an array xs, returns the elements of xs satisfying the predicate in their original order. â¢ COUNT :: (int -&gt; bool) -&gt; [int] -&gt; int lambda f, xs: len([x for x in xs if f(x)]) Given a predicate f mapping from integers to truth values, and an array xs, returns the number of elements in xs satisfying the predicate. â¢ ZIPWITH :: (int -&gt; int -&gt; int) -&gt; [int] -&gt; [int] -&gt; [int] lambda f, xs, ys: [f(x, y) for (x, y) in zip(xs, ys)] Given a lambda function f mapping integer pairs to integers, and two arrays xs and ys, returns the array resulting from applying f to corresponding elements of xs and ys. The length of the returned array is the minimum of the lengths of xs and ys. â¢ SCANL1 :: (int -&gt; int -&gt; int) -&gt; [int] -&gt; [int] Given a lambda function f mapping integer pairs to integers, and an array xs, returns an array ys of the same length as xs and with its content defined by the recurrence ys[0] = xs[0], ys[n] = f(ys[n-1], xs[n]) for n â¥ 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>â¢</head><label></label><figDesc>Some common attributes get predicted more often regardless of the ground truth program:FILTER, (&gt;0), (&lt;0), (%2==1), (%2==0), MIN, MAX, (+), (-), ZIPWITH.â¢ There are some groups of lambdas that are more difficult for the network to distinguish within: (+) vs (-); (+1) vs (-1); (/2) vs (/3) vs (/4). â¢ When a program uses ( ** 2), the network often thinks it's using ( * ), presumably because both can lead to large values in the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Search speedups on programs of length T = due to using neural network predictions.</figDesc><table><row><cell>Timeout needed</cell><cell></cell><cell>DFS</cell><cell></cell><cell cols="3">Enumeration</cell><cell>Î» 2</cell><cell cols="2">Sketch</cell><cell>Beam</cell></row><row><cell>to solve</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>20% 40% 60%</cell><cell>20%</cell><cell>40%</cell><cell>20%</cell></row><row><cell>Baseline</cell><cell cols="3">41ms 126ms 314ms</cell><cell cols="3">80ms 335ms 861ms</cell><cell>18.9s 49.6s 84.2s</cell><cell cols="2">&gt;10 3 s &gt;10 3 s</cell><cell>&gt;10 3 s</cell></row><row><cell>DeepCoder</cell><cell cols="3">2.7ms 33ms 110ms</cell><cell cols="3">1.3ms 6.1ms 27ms</cell><cell>0.23s 0.52s 13.5s</cell><cell>2.13s</cell><cell>455s</cell><cell>292s</cell></row><row><cell>Speedup</cell><cell cols="3">15.2Ã 3.9Ã 2.9Ã</cell><cell cols="3">62.2Ã 54.6Ã 31.5Ã</cell><cell>80.4Ã 94.6Ã 6.2Ã</cell><cell cols="2">&gt;467Ã &gt;2.2Ã</cell><cell>&gt;3.4Ã</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Krzysztof DembczyÅski, Willem Waegeman, Weiwei Cheng, and Eyke HÃ¼llermeier. On label dependence and loss minimization in multi-label classification. Machine Learning, 88(1):5-45, 2012. Krzysztof J. Dembczynski, Weiwei Cheng, and Eyke Hllermeier. Bayes optimal multilabel classification via probabilistic classifier chains. In Proceedings of the 27th International Conference on Machine Learning (ICML), 2010. John K. Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data structure transformations from input-output examples. In Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 2015. Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction. CoRR, abs/1608.04428, 2016. URL http://arxiv.org/abs/1608.04428. Kingma and Max Welling. Stochastic gradient VB and the variational auto-encoder. In Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2014. Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. In Proceedings of the 4th International Conference on Learning Representations 2016, 2015.</figDesc><table /><note>Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing machines. CoRR, abs/1410.5401, 2014. URL http://arxiv.org/abs/1410.5401. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska- BarwiÅska, Sergio GÃ³mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 2016. Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with unbounded memory. In Proceedings of the 28th Conference on Advances in Neural Information Processing Systems (NIPS), 2015. Sumit Gulwani. Programming by examples: Applications, algorithms, and ambiguity resolution. In Proceedings of the 8th International Joint Conference on Automated Reasoning (IJCAR), 2016. Sumit Gulwani, Susmit Jha, Ashish Tiwari, and Ramarathnam Venkatesan. Synthesis of loop-free programs. In Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 2011. Nicolas Heess, Daniel Tarlow, and John Winn. Learning to pass expectation propagation messages. In Proceedings of the 26th Conference on Advances in Neural Information Processing Systems (NIPS), 2013. Varun Jampani, Sebastian Nowozin, Matthew Loper, and Peter V Gehler. The informed sampler: A discriminative approach to Bayesian inference in generative computer vision models. Computer Vision and Image Understanding, 136:32-44, 2015. Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Proceedings of the 28th Conference on Advances in Neural Information Processing Systems (NIPS), 2015. Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In Proceedings of the 4th Inter- national Conference on Learning Representations, 2016. Diederik PYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neu- ral networks. In Proceedings of the 4th International Conference on Learning Representations (ICLR), 2016. Wang Ling, Edward Grefenstette, Karl Moritz Hermann, TomÃ¡Å¡ KoÄiskÃ½, Andrew Senior, Fumin Wang, and Phil Blunsom. Latent predictor networks for code generation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to express their gratitude to Rishabh Singh and Jack Feser for their valuable guidance and help on using the Sketch and Î» 2 program synthesis systems.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">(123)</ref> <p>.02 <ref type="bibr">(33)</ref> .05 <ref type="bibr">(33)</ref> .00 <ref type="bibr">(30)</ref> .01 <ref type="bibr">(31)</ref> .01 <ref type="bibr">(33)</ref> .04 <ref type="bibr">(32)</ref> .07 <ref type="bibr">(32)</ref> .05 <ref type="bibr">(28)</ref> .01 <ref type="bibr">(31)</ref> .04 <ref type="bibr">(33)</ref> .03 <ref type="bibr">(30)</ref> .01 <ref type="bibr">(32)</ref> .06 <ref type="bibr">(32)</ref> .17 <ref type="bibr">(27)</ref> .18 <ref type="bibr">(27)</ref> .02 <ref type="bibr">(23)</ref> .09 <ref type="bibr">(26)</ref> .06 <ref type="bibr">(26)</ref> .00 <ref type="bibr">(26)</ref> .02 <ref type="bibr">(6)</ref> .09 <ref type="bibr">(29)</ref> .09 <ref type="bibr">(31)</ref> .06 <ref type="bibr">(32)</ref> .04 <ref type="bibr">(33)</ref> .03 <ref type="bibr">(31)</ref> .07 <ref type="bibr">(33)</ref> .03 <ref type="bibr">(33)</ref> .02 <ref type="bibr">(33)</ref> .01 <ref type="bibr">(33)</ref> .02 <ref type="bibr">(32)</ref> .03 <ref type="bibr">(33)</ref> .01 <ref type="bibr">(31)</ref> .00 <ref type="bibr">(31)</ref> .00 <ref type="bibr">(39)</ref> .01 <ref type="bibr">(40)</ref> .00 <ref type="bibr">(39)</ref> .00 <ref type="bibr">(40)</ref> .00 <ref type="bibr">(40)</ref> .01 <ref type="bibr">(38)</ref> .09 <ref type="bibr">(38)</ref> .02 <ref type="bibr">(32)</ref> .02 <ref type="bibr">(38)</ref> .01 <ref type="bibr">(38)</ref> .02 <ref type="bibr">(36)</ref> .04 <ref type="bibr">(38)</ref> .06 <ref type="bibr">(38)</ref> .21 <ref type="bibr">(23)</ref> .16 <ref type="bibr">(30)</ref> .05 <ref type="bibr">(34)</ref> .01 <ref type="bibr">(33)</ref> .02 <ref type="bibr">(32)</ref> .03 <ref type="bibr">(29)</ref> .06 (9) .08 <ref type="bibr">(33)</ref> .08 <ref type="bibr">(38)</ref> .09 <ref type="bibr">(39)</ref> .01 <ref type="bibr">(38)</ref> .06 <ref type="bibr">(37)</ref> .11 <ref type="bibr">(39)</ref> .01 <ref type="bibr">(39)</ref> .00 <ref type="bibr">(37)</ref> .00 <ref type="bibr">(39)</ref> .04 <ref type="bibr">(40)</ref> .03 <ref type="bibr">(37)</ref> .03 <ref type="bibr">(37)</ref> .00 <ref type="bibr">(40)</ref> .03 <ref type="bibr">(26)</ref> .04 <ref type="bibr">(26)</ref> .01 <ref type="bibr">(25)</ref> .01 <ref type="bibr">(26)</ref> .00 <ref type="bibr">(27)</ref> .02 <ref type="bibr">(26)</ref> .03 <ref type="bibr">(26)</ref> .05 <ref type="bibr">(24)</ref> .03 <ref type="bibr">(26)</ref> .03 <ref type="bibr">(26)</ref> .04 <ref type="bibr">(26)</ref> .03 <ref type="bibr">(27)</ref> .02 <ref type="bibr">(27)</ref> .16 <ref type="bibr">(15)</ref> .10 (22) .06 <ref type="bibr">(20)</ref> .09 <ref type="bibr">(22)</ref> .02 <ref type="bibr">(23)</ref> .05 <ref type="bibr">(6)</ref> .06 <ref type="bibr">(22)</ref> .02 <ref type="bibr">(26)</ref> .13 <ref type="bibr">(26)</ref> .02 <ref type="bibr">(27)</ref> .05 <ref type="bibr">(24)</ref> .10 (27) .02 <ref type="bibr">(26)</ref> .00 <ref type="bibr">(25)</ref> .03 <ref type="bibr">(26)</ref> .05 <ref type="bibr">(26)</ref> .02 <ref type="bibr">(27)</ref> .03 <ref type="bibr">(27)</ref> .00 <ref type="bibr">(27)</ref> .00 <ref type="bibr">(19)</ref> .01 <ref type="bibr">(21)</ref> .00 <ref type="bibr">(20)</ref> .03 <ref type="bibr">(21)</ref> .01 <ref type="bibr">(21)</ref> .01 <ref type="bibr">(19)</ref> .07 <ref type="bibr">(21)</ref> .01 <ref type="bibr">(20)</ref> .04 <ref type="bibr">(21)</ref> .00 <ref type="bibr">(20)</ref> .00 <ref type="bibr">(21)</ref> .01 <ref type="bibr">(21)</ref> .00 <ref type="bibr">(21)</ref> .12 <ref type="bibr">(16)</ref> .02 <ref type="bibr">(15)</ref> .06 <ref type="bibr">(16)</ref> .04 <ref type="bibr">(16)</ref> .39 <ref type="bibr">(18)</ref> .09 (4) .05 <ref type="bibr">(15)</ref> .00 <ref type="bibr">(21)</ref> .02 <ref type="bibr">(19)</ref> .01 <ref type="bibr">(21)</ref> .04 <ref type="bibr">(20)</ref> .05 <ref type="bibr">(18)</ref> .01 <ref type="bibr">(20)</ref> .05 <ref type="bibr">(19)</ref> .07 <ref type="bibr">(21)</ref> .04 <ref type="bibr">(21)</ref> .01 <ref type="bibr">(19)</ref> .02 <ref type="bibr">(21)</ref> .00 <ref type="bibr">(21)</ref> .00 <ref type="bibr">(39)</ref> .01 <ref type="bibr">(39)</ref> .00 <ref type="bibr">(38)</ref> .00 <ref type="bibr">(39)</ref> .00 <ref type="bibr">(39)</ref> .01 <ref type="bibr">(34)</ref> .04 <ref type="bibr">(38)</ref> .01 <ref type="bibr">(35)</ref> .01 <ref type="bibr">(39)</ref> .00 <ref type="bibr">(38)</ref> .04 <ref type="bibr">(39)</ref> .01 <ref type="bibr">(36)</ref> .01 <ref type="bibr">(39)</ref> .08 <ref type="bibr">(30)</ref> .03 <ref type="bibr">(35)</ref> .09 <ref type="bibr">(20)</ref> .02 <ref type="bibr">(28)</ref> .03 <ref type="bibr">(31)</ref> .06 (5) .03 <ref type="bibr">(33)</ref> .01 <ref type="bibr">(37)</ref> .07 <ref type="bibr">(36)</ref> .03 <ref type="bibr">(36)</ref> .02 <ref type="bibr">(38)</ref> .43 <ref type="bibr">(39)</ref> .04 <ref type="bibr">(39)</ref> .01 <ref type="bibr">(39)</ref> .00 <ref type="bibr">(39)</ref> .06 <ref type="bibr">(38)</ref> .02 <ref type="bibr">(39)</ref> .03 <ref type="bibr">(38)</ref> .00 <ref type="bibr">(39)</ref> .01 <ref type="bibr">(27)</ref> .01 <ref type="bibr">(27)</ref> .01 <ref type="bibr">(27)</ref> .01 <ref type="bibr">(28)</ref> .02 <ref type="bibr">(28)</ref> .02 <ref type="bibr">(25)</ref> .00 <ref type="bibr">(23)</ref> .02 <ref type="bibr">(26)</ref> .04 <ref type="bibr">(26)</ref> .03 <ref type="bibr">(28)</ref> .02 <ref type="bibr">(28)</ref> .01 <ref type="bibr">(27)</ref> .01 <ref type="bibr">(27)</ref> .04 <ref type="bibr">(24)</ref> .07 <ref type="bibr">(28)</ref> .12 (23) .01 <ref type="bibr">(21)</ref> .05 <ref type="bibr">(19)</ref> .08 <ref type="bibr">(8)</ref> .03 <ref type="bibr">(26)</ref> .02 <ref type="bibr">(28)</ref> .05 <ref type="bibr">(27)</ref> .01 <ref type="bibr">(28)</ref> .07 <ref type="bibr">(25)</ref> .23 <ref type="bibr">(28)</ref> .04 <ref type="bibr">(28)</ref> .00 <ref type="bibr">(23)</ref> .02 <ref type="bibr">(27)</ref> .08 <ref type="bibr">(28)</ref> .03 <ref type="bibr">(24)</ref> .04 <ref type="bibr">(27)</ref> .00 <ref type="bibr">(28)</ref> .01 <ref type="bibr">(24)</ref> .00 <ref type="bibr">(23)</ref> .00 <ref type="bibr">(24)</ref> .00 <ref type="bibr">(24)</ref> .01 <ref type="bibr">(24)</ref> .00 <ref type="bibr">(21)</ref> .00 <ref type="bibr">(22)</ref> .01 <ref type="bibr">(20)</ref> .01 (23) .00 <ref type="bibr">(23)</ref> .00 <ref type="bibr">(24)</ref> .03 <ref type="bibr">(22)</ref> .00 <ref type="bibr">(24)</ref> .02 <ref type="bibr">(19)</ref> .01 <ref type="bibr">(19)</ref> .39 <ref type="bibr">(19)</ref> .03 <ref type="bibr">(19)</ref> .00 <ref type="bibr">(20)</ref> .10 (6) .05 <ref type="bibr">(21)</ref> .07 <ref type="bibr">(24)</ref> .02 <ref type="bibr">(23)</ref> .01 <ref type="bibr">(23)</ref> .00 <ref type="bibr">(23)</ref> .01 <ref type="bibr">(24)</ref> .03 <ref type="bibr">(24)</ref> .01 <ref type="bibr">(23)</ref> .06 <ref type="bibr">(21)</ref> .03 <ref type="bibr">(21)</ref> .06 <ref type="bibr">(24)</ref> .02 <ref type="bibr">(21)</ref> .00 <ref type="bibr">(24)</ref> .01 <ref type="bibr">(37)</ref> .00 <ref type="bibr">(37)</ref> .00 <ref type="bibr">(38)</ref> .00 <ref type="bibr">(38)</ref> .00 <ref type="bibr">(38)</ref> .00 <ref type="bibr">(38)</ref> .00 <ref type="bibr">(34)</ref> .00 <ref type="bibr">(31)</ref> .00 <ref type="bibr">(36)</ref> .00 <ref type="bibr">(37)</ref> .00 <ref type="bibr">(36)</ref> .00 <ref type="bibr">(36)</ref> .00 <ref type="bibr">(38)</ref> .02 <ref type="bibr">(30)</ref> .02 (32) .14 (31) .02 <ref type="bibr">(28)</ref> .08 <ref type="bibr">(27)</ref> .02 (6) .06 <ref type="bibr">(36)</ref> .01 <ref type="bibr">(38)</ref> .06 <ref type="bibr">(35)</ref> .02 <ref type="bibr">(36)</ref> .04 <ref type="bibr">(36)</ref> .02 <ref type="bibr">(38)</ref> .02 <ref type="bibr">(33)</ref> .04 <ref type="bibr">(37)</ref> .05 <ref type="bibr">(36)</ref> .06 <ref type="bibr">(37)</ref> .03 <ref type="bibr">(36)</ref> .03 <ref type="bibr">(36)</ref> .00 <ref type="bibr">(38)</ref> .01 <ref type="bibr">(35)</ref> .00 <ref type="bibr">(35)</ref> .03 <ref type="bibr">(35)</ref> .00 <ref type="bibr">(35)</ref> .00 <ref type="bibr">(34)</ref> .02 <ref type="bibr">(34)</ref> .00 <ref type="bibr">(33)</ref> .00 <ref type="bibr">(29)</ref> .01 (32) .00 <ref type="bibr">(34)</ref> .00 <ref type="bibr">(34)</ref> .04 <ref type="bibr">(33)</ref> .00 <ref type="bibr">(34)</ref> .02 <ref type="bibr">(24)</ref> .00 <ref type="bibr">(27)</ref> .11 <ref type="bibr">(25)</ref> .16 <ref type="bibr">(27)</ref> .05 <ref type="bibr">(29)</ref> .06 <ref type="bibr">(8)</ref> .06 <ref type="bibr">(28)</ref> .04 <ref type="bibr">(35)</ref> .05 <ref type="bibr">(34)</ref> .02 <ref type="bibr">(34)</ref> .02 <ref type="bibr">(35)</ref> .03 <ref type="bibr">(35)</ref> .04 <ref type="bibr">(34)</ref> .06 (32) .06 <ref type="bibr">(33)</ref> .05 <ref type="bibr">(35)</ref> .00 (32) .00 <ref type="bibr">(35)</ref> .00 <ref type="bibr">(34)</ref> .00 <ref type="bibr">(31)</ref> .00 <ref type="bibr">(32)</ref> .00 <ref type="bibr">(32)</ref> .00 <ref type="bibr">(32)</ref> .00 <ref type="bibr">(32)</ref> .00 <ref type="bibr">(29)</ref> .00 <ref type="bibr">(31)</ref> .04 <ref type="bibr">(30)</ref> .01 <ref type="bibr">(30)</ref> .01 <ref type="bibr">(32)</ref> .00 <ref type="bibr">(31)</ref> .01 <ref type="bibr">(32)</ref> .01 <ref type="bibr">(31)</ref> .16 <ref type="bibr">(26)</ref> .03 <ref type="bibr">(22)</ref> .08 <ref type="bibr">(22)</ref> .07 <ref type="bibr">(25)</ref> .00 <ref type="bibr">(22)</ref> .03</p><p>(3) .07 <ref type="bibr">(25)</ref> .03 <ref type="bibr">(31)</ref> .02 <ref type="bibr">(32)</ref> .04 <ref type="bibr">(31)</ref> .03 <ref type="bibr">(32)</ref> .08 <ref type="bibr">(31)</ref> .13 <ref type="bibr">(32)</ref> .05 <ref type="bibr">(29)</ref> .00 <ref type="bibr">(31)</ref> .01 <ref type="bibr">(32)</ref> .10 (30) .13 <ref type="bibr">(30)</ref> .00 <ref type="bibr">(32)</ref> .02 <ref type="bibr">(33)</ref> .02 <ref type="bibr">(34)</ref> .03 <ref type="bibr">(33)</ref> .01 <ref type="bibr">(33)</ref> .00 <ref type="bibr">(33)</ref> .00 <ref type="bibr">(31)</ref> .01 <ref type="bibr">(31)</ref> .07 <ref type="bibr">(31)</ref> .03 <ref type="bibr">(34)</ref> .02 <ref type="bibr">(34)</ref> .01 (31) .04 <ref type="bibr">(33)</ref> .01 <ref type="bibr">(33)</ref> .07 <ref type="bibr">(27)</ref> .04 <ref type="bibr">(28)</ref> .11 <ref type="bibr">(27)</ref> .04 <ref type="bibr">(24)</ref> .06 <ref type="bibr">(30)</ref> .06 (9) .02 <ref type="bibr">(28)</ref> .01 <ref type="bibr">(34)</ref> .01 (31) .06 <ref type="bibr">(34)</ref> .02 (32) .10 (34) .08 <ref type="bibr">(30)</ref> .02 <ref type="bibr">(34)</ref> .00 (32) .01 (31) .12 <ref type="bibr">(32)</ref> .15 <ref type="bibr">(34)</ref> .00 <ref type="bibr">(33)</ref> .02 <ref type="bibr">(27)</ref> .02 <ref type="bibr">(27)</ref> .00 <ref type="bibr">(27)</ref> .00 <ref type="bibr">(27)</ref> .00 <ref type="bibr">(26)</ref> .01 <ref type="bibr">(26)</ref> .00 <ref type="bibr">(26)</ref> .01 (23) .02 <ref type="bibr">(27)</ref> .00 <ref type="bibr">(26)</ref> .00 <ref type="bibr">(26)</ref> .03 <ref type="bibr">(25)</ref> .00 <ref type="bibr">(27)</ref> .07 <ref type="bibr">(20)</ref> .03 <ref type="bibr">(19)</ref> .09 (21) .10 (22) .00 <ref type="bibr">(22)</ref> .03</p><p>(2) .02 <ref type="bibr">(23)</ref> .03 <ref type="bibr">(25)</ref> .02 <ref type="bibr">(24)</ref> .01 <ref type="bibr">(27)</ref> .01 <ref type="bibr">(27)</ref> .02 <ref type="bibr">(26)</ref> .02 <ref type="bibr">(26)</ref> .01 <ref type="bibr">(24)</ref> .00 <ref type="bibr">(25)</ref> .01 <ref type="bibr">(27)</ref> .13 <ref type="bibr">(25)</ref> .17 <ref type="bibr">(27)</ref> .00 <ref type="bibr">(27)</ref> .01 <ref type="bibr">(8)</ref> .07 <ref type="bibr">(8)</ref> .13 <ref type="bibr">(8)</ref> .06 <ref type="bibr">(8)</ref> .01 (8) .10 (8) .00 <ref type="bibr">(8)</ref> .15 <ref type="bibr">(6)</ref> .07 <ref type="bibr">(8)</ref> .04 <ref type="bibr">(7)</ref> .06 <ref type="bibr">(8)</ref> .03 <ref type="bibr">(7)</ref> .00 <ref type="bibr">(8)</ref> .31 <ref type="bibr">(6)</ref> .09 (7) .02 (5) .11 <ref type="bibr">(6)</ref> .18 <ref type="bibr">(8)</ref> .02 (4) .06</p><p>(1) .18 (6) .01 (6) .00 <ref type="bibr">(8)</ref> .07 <ref type="bibr">(8)</ref> .09 <ref type="bibr">(8)</ref> .12 (8) .09 <ref type="bibr">(8)</ref> .06 <ref type="bibr">(8)</ref> .14 (8) .00 <ref type="bibr">(7)</ref> .02 <ref type="bibr">(8)</ref> .05 <ref type="bibr">(7)</ref> .11 (8) <ref type="figure">Figure 9</ref>: Conditional confusion matrix for the neural network and test set of P = 500 programs of length T = 3 that were used to obtain the results presented in <ref type="table">Table 1</ref>. Each cell contains the average false positive probability (in larger font) and the number of test programs from which this average was computed (smaller font, in brackets). The color intensity of each cell's shading coresponds to the magnitude of the average false positive probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G ANALYSIS OF TRAINED NEURAL NETWORKS</head><p>We analyzed the performance of trained neural networks by investigating which program instructions tend to get confused by the networks. To this end, we looked at a generalization of confusion matrices to the multilabel classification setting: for each attribute in a ground truth program (rows) measure how likely each other attribute (columns) is predicted as a false positive. More formally, in this matrix the (i, j)-entry is the average predicted probability of attribute j among test programs</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DeepMathdeep sequence models for premise selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">FranÃ§ois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proocedings of the 29th Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive neural compilation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Rudy R Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Mudigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the 29th Conference on Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Helmholtz machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep network guided proof search. CoRR, abs/1701.06972</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1701.06972" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A machine learning framework for programming by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Tamuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Butler W Lampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 4th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning program embeddings to propagate feedback on student code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Piech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Phulsuksombati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FlashMeta: a framework for inductive program synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA)</title>
		<meeting>the International Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural programmer-interpreters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nando De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 4th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Programming with a differentiable forth interpreter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matko</forename><surname>Bosnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>RocktÃ¤schel</surname></persName>
		</author>
		<idno>abs/1605.06640</idno>
		<ptr target="http://arxiv.org/abs/1605.06640" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic program optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Schkufza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Commununications of the ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="114" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mat</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting a correct program in programming by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Conference on Computer Aided Verification (CAV)</title>
		<meeting>the 27th Conference on Computer Aided Verification (CAV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Program Synthesis By Sketching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armando</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>EECS Dept., UC Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning stochastic inverses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>StuhlmÃ¼ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the 26th Conference on Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Conference on Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the 28th Conference on Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning simple algorithms from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
