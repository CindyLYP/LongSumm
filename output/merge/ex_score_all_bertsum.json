{"1000": "supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems. unfortunately, the best completely unsupervised english pos tagger , making its practical usability questionable at best. syntactic universals are a well studied concept in linguistics for multilingual grammar induction. graph construction does not require any labeled data, but makes use of two similarity functions. ensure: f , a set of parameters learned using a constrained unsupervised model (5). to establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (3.3).3 since we have no labeled foreign data, our goal is to project syntactic information from the english side to the foreign side. by aggregating the pos labels of the english tokens to types, we can generate label distributions for the english vertices. the following three sections elaborate these different stages is more detail. in graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples, and whose weighted edges encode the degree to which the examples they link have the same label graph construction for structured prediction problems such as pos tagging is non-trivial: on the one hand, using individual words as the vertices throws away the context 3the word alignment methods do not use pos information. because all english vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams. our monolingual similarity function we briefly review it here for completeness. to define a similarity function between the english and the foreign vertices, we rely on high-confidence word alignments. since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the english sentences de 5note that many combinations are impossible giving a pmi value of 0; e.g., when the trigram type and the feature instantiation don’t have words in common. so far the graph has been completely unlabeled. in this particular case, all english vertices are labeled as nouns by the supervised tagger. in general, the neighborhoods can be more diverse and we allow a soft label distribution over the vertices. we use label propagation in two stages to generate soft labels on all the vertices in the graph. after running label propagation (lp), we compute tag probabilities for foreign word types x by marginalizing the pos tag distributions of foreign trigrams ui x x x over the left and right context words: p(yx) x,x qi(y) x,x,y qi(y ) (6) we then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : tx(y) if p(yx) otherwise (7) we describe how we choose in this vector tx is constructed for every word in the foreign vocabulary and will be used to provide features for the unsupervised foreign language pos tagger. 2011) provide a mapping from the fine-grained language specific pos tags in the foreign treebank to the universal pos tags. therefore, the number of fine tags varied across languages for our experiments; however, one could as well have fixed the set of hmm states to be a constant across languages, and created one mapping to the universal pos tagset. no additional constraint feature) served as a second baseline. because many foreign word types are not aligned to an english word (see table 3), and we do not run label propagation on the foreign side, we expect the projected information to have less coverage. we used c as the l2 regularization constant in (eq. as expected, the vanilla hmm trained with em performs the worst. the feature-hmm model works better for all languages, generalizing the results achieved for english by berg-kirkpatrick et al. our projection baseline is able to benefit from the bilingual information and greatly improves upon the monolingual baselines, but falls short of the no lp model by on an average. the no lp model does not outperform direct projection for german and greek, but performs better for six out of eight languages. it falls short of the projection baseline for german, but is statistically indistinguishable in terms of accuracy. although the tag distributions of the foreign words (eq. while the first three models get three to four tags wrong, our best model gets only one word wrong and is the most accurate among the four models for this example. correct tag is available as a constraint feature in the with lp case.", "1001": "in spite of being richer than the mean vector pooling method, fisher vectors based on a probabilistic mixture model are invariant to order. in image annotation and image search tasks, the rnn-fv method is used for the representation of sentences and achieves state-of-the-art results on the flickr8k dataset and competitive results on other benchmarks. designed an effective approach for spatiotemporal feature learning using 3-dimensional convnets. the image is usually represented by applying a pre-trained cnn on the image and taking the activations from the last hidden layer. used a dependency tree recursive neural network. the notation of a multiset is used to clarify that the order of the words in a sentence does not affect the representation, and that a vector can appear more than once. consider, for example, the text encoding task, where each word is represented by its word2vec embedding. the pooling methods described above share a common disadvantage: insensitivity to the order of the elements in the sequence. one type is based on training a regression problem, and the other on training a classification problem. therefore, given the input sequence, the target sequence would be: y (x1, x2, ...xn ). there are several regression loss functions that can be used. the target at time step i is xi1 (the next element in the sequence), and the loss is: loss(xi1, vi) xi1 vi (2) the rnn can be seen as a generative model, and the likelihood of any vector x being the next element of the sequence, given x0, ..., xi, can be defined as: p (xx0, ..., xi) (2) d/2 exp ( x vi ) (3) we are generally interested in the likelihood of the correct prediction, i.e., in the likelihood of the vector xi1 given x0, ..., xi: p (xi1x0, ..., xi). therefore, the desired gradient can be computed by backpropagation, i.e. feeding x to the network and performing forward and backward passes. therefore, we approximated it directly from the gradients of the training sequences, by computing the mean of l(xw ) for each w the normalized partial derivatives of the fv are then: f 1/2 w l(xw ) the action recognition pipeline contains the underlying appearance features used to encode the video, the sequence encoding using the rnn-fv, and an svm classifier on top. have presented a similar pipeline for gmm-fv. we replace this representation with rnn-fv. the two network alternatives are explored: classification and regression. rnn training data we employed either the training data of each split in the respective benchmark, or the 2010-englishwikipedia-1m dataset made available by the leipzig corpora collection this dataset contains million sentences randomly sampled from english wikipedia. as mentioned, applying the fim normalization (sec. another form of normalization we have tried, is to normalize each dimension of the gradient by subtracting its mean and dividing by its standard deviation. this also did not lead to an improved performance. the ucf101 dataset consists of 13,320 realistic action videos, collected from youtube, and divided into action categories. we use the three splits provided with this dataset in order to evaluate our results and report the mean average accuracy over these splits. the mean average accuracy over these splits is reported. these results are considerably worse than all pooling methods. the median and mean rank of the first ground truth result are also reported. in addition, for each of the three datasets, we trained three rnns with the dataset’s training sentences as training data: one with word2vec as word embedding; one with the cca word embedding derived from the semantic vector space of 18, as explained in sec. we observed no change in performance (flickr8k) or slightly worse results (flickr30k and coco). this paper introduces a novel fv representation for sequences that is derived from rnns. the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs. a transfer learning result from the image annotation task to the video action recognition task was shown.", "1002": "the rate of publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research. au- tomatic text summarization could help mitigate this problem. 2017; nikola nikolov and hahnloser, 2018). 2017), while summarization of scientific papers is less studied, mainly due to the lack of large-scale training data. the papers’ length and complexity require substantial summarization effort from ex- perts. in such talks, the presenter (usually a co-author) must describe their paper coherently and concisely (since there is a time limit), provid- ing a good basis for generating summaries. table gives an example of an alignment between 1vimeo.com/aclweb icml.cc/conferences/2017/videos a paper and its talk transcript (see table in the appendix for a complete example). moreover, our approach can generate sum- maries of various lengths. to our knowl- edge, this is the first approach to automatically cre- ate extractive summaries for scientific papers by utilizing the videos of conference talks. then, they create a summary of words on average. using this approach, they generated summaries, costing person-hours. both kan (2007) and bahrani and kan (2013) studied the problem of document to presentation alignment for schol- arly documents. 2015) utilized books to enrich videos with descriptive explanations. ( a similar trend occurs in other domains such as physics4, biology5, etc. thus, the talk must be coherent and concentrate on the most important aspects of a paper. we downloaded the 4www.cleoconference.org igem.org/videos/lecturevideos videos and extracted the speech data. this enables to control the summary length by considering only the most im- portant sentences up to some threshold. we use an hmm to model the assumed genera- tive process. the sequence of spoken words is the output sequence. each hidden state of the hmm corresponds to a single paper sentence. 2015), where transition is allowed between con- secutive sentences only. although these assumptions do not perfectly re- flect reality, they are a reasonable approximation in practice. finally, we define the start-probabilities assum- ing that the first spoken word must be conditioned on a sentence from the introduction section, hence p(s(1)) is defined as a uniform distribution over the introduction section’s sentences. note that sentences which appear in the ab- stract, related work, and acknowledgments sec- tions of each paper are excluded from the hmm’s hidden states, as we observed that presenters sel- dom refer to them. given a desired summary length, one can draw a subset of top- ranked sentences up to this length. the evaluation is on the same test data used by yasunaga et al. ( in the second approach, a 150-words summary is created by augmenting the abstract with non-redundant sentences extracted by our model, similarly to the hybrid ap- proach of yasunaga et al. automatic evaluation table summarizes the results: both gcn cited text spans and talksumm-only models, are not able to obtain better performance than abstract8 however, for the hybrid approach, where the abstract is aug- mented with sentences from the summaries emit- ted by the models, our talksumm-hybrid out- performs both gcn hybrid and abstract. importantly, our model, trained on automatically- generated summaries, performs on par with mod- els trained over scisummnet, in which training data was created manually. as our goal is to test more comprehensive summaries, we generated summaries composed of sentences (approximately of a long paper). we hope our method and dataset will unlock new opportu- nities for scientific paper summarization.", "1003": "emotions are an important element of human nature and detecting them in the textual messages written by users has many applications in information retrieval and human-computer interaction a common approach to emotion analysis and modeling is categorization, e.g., according to ekman’s basic emotions; namely, anger, disgust, fear, happiness, sadness, and surprise approaches to categorical emotion classi cation based on text often employ supervised machine learning classi ers, which require labeled training data. manual annotation requires high cognitive capabilities of multiple human annotators per sample. however, the task is tedious, time-consuming, and expensive 4, and thus, these datasets are usually small (in the order of thousands of annotated samples). copyrights for components of this work owned by others than acm must be honored. request permissions from permissionsacm.org. for example, in such a representation the words awful and terrible are expected to have similar vector representations. when generation is not an option, one can utilize pre-trained representations; the most popular pre-trained representations are based on word2vec and glove algorithms, and were trained on large corpora. in the domain of sentiment analysis, which is closely related to emotion detection, tang et al. these works all require the availability of large-scale data. in our setting, we only have small datasets, thus, we used pre-trained vectors. in this section we describe the two di erent classi ers we created and an ensemble method that combines their output. the two classi ers are based on di erent document representations. depending on the dataset being used, the classi cation task can be represented as a multi-class or a multi-label problem. in our rst approach we used an svm classi er with a linear kernel, and represented every document as a bow. we extracted various ngrams (after lemmatization), punctuation and social media features. we further removed of the remaining features that got the lowest scores in a statistical test. we have experimented with several document representations, combining the word vectors, following the notation: dwe (t1, ..., tk ) 1k i1 ai k i1 ai v (ti ) , (1) where dwe is the word embedding based vector representation for document d with k terms, v is some pre-trained word-to-vector mapping (described in section 5), and ai is some weight indicating the relative importance of term ti the document representations we experimented with include: cbow (continuous bag of words) 18: in this case, i,ai 1, which means uniform weights for all terms. tfidf weights: ai is the tfidf weight for term ti in case ti was not present in the training data, we smoothed its idf weight as if it appeared in one document (this yielded better performance than discarding the term). in words, a low constant weight is assigned to terms which did not appear in the training data, or were found to be less discriminative. similarly to our bow classi er, we used an svm classi er in this approach as well, but since we represented a document as a low-dimensional vector, we allowed non-linearity by using an rbf kernel. this yielded better results than using a linear kernel. our baseline is the bow classi er described above. emotion detection datasets are labeled with multiple emotions and are imbalanced. thus, we evaluated the classi cation performance for all emotion classes by using macro average f1-score. we used scikit-learn for an svm implementation and spacy for n-grams extraction. table depicts the macro f1-scores for each dataset and each document representation method that is based on word vectors, as detailed in section results show that for all datasets and representation methods, word vectors trained by glove achieved higher performance than using word2vec based vectors. thus, we used glove based pre-trained word vectors below. table depicts the macro f1-scores for each dataset, and for the di erent models: bow is our baseline, presented in section en-cbow, en-tfidf and en-class are the ensemble models of bow and the corresponding embedded representations presented in section our ensemble methods outperformed the baseline for each document representation method. the best result, which is also signi - cantly better for each dataset, is of en-class model that achieved an average relative improvement of in f1-score over all datasets. these results indicate the advantage in combining both bow and embedded document representations for emotion detection from text. we also plan to investigate transfer learning for multidomain emotion detection.", "1004": "an interesting use case for social media is customer support that can now take place over public social media channels. businesses also benefit from the publicity of giving good services almost in real-time, online, building an online community of customers and encouraging more brand mentions in social media. in this paper, we analyze customer support dialogues using the twitter platform and show the utility of such analyses. the particular aspect of such dialogues that we concentrate on is emotions. this is why we talk in this paper about agent emotional techniques rather than agent emotions. consider, for example, the real (anonymized) twitter dialogue depicted in figure in this dialogue, customer disappointment is expressed in the first turn (’bummer. /’), 2) this is the first research using unique dialogue features (e.g., emotions expressed in previous dialogue turns by the agent and customer, time between dialogue turns) to improve emotion detection. ( the works in presented dialogue systems that sense the user emotions, such that the system further optimizes its affect response. in this section we describe the data collection process and provide some statistics about the twitter dialogue dataset we have collected. table summarizes some statistics about the collected data, and figure depicts the frequencies of dialogue lengths which follow a power-law relationship. in addition, we removed dialogues that contained only turns as these are too short to be meaningful as the customer never replied or provided more details about the issue. we treated these two objectives as two classification tasks. relevant agent emotional techniques to be predicted are: empathy, gratitude, apology, and cheerfulness. using these features for emotion classification in written dialogues is novel, and as our experimental results show, it improves performance compared to a model based only on features extracted from the turn’s text. a feature can be global, namely its value is constant across an entire dialogue or it can be a local, meaning that its value may change at each turn. in addition, a feature can be historical (as will be discussed below). the integral family of features includes three sets of features: dialogue topic: a set of global binary features representing the intent of the customer who initiated the support inquiry. examples of topics include ac- count issues, payments, technical problem and more this feature set captures the notion that customer emotions are influenced by the event that led the customer to contact the customer service agent essence: a set of local binary features that represent the action used by the agent to address the last customer turn, independently of any emotional technique expressed. the emotional family of features includes agent emotion and customer emotion: these two sets of local binary features represent emotions predicted for previous turns. our model generates predictions of emotions for each customer and agent turn, and uses these predictions as features to classify a later customer or agent turn with emotion expression. 2currently this feature is not supported in social media. we considered these tasks as multi-label classification tasks. the history size is also a parameter of this model. 2) the svm-hmm classifier generates models that are isomorphic to a kth-order hidden markov model. then, ti was tagged with its sequence classification result. since history size is a parameter of our models, we first tested the classification results for all possible history sizes (given that that maximum dialogue size in our dataset is 8). in this work we studied emotions being expressed in customer service dialogues in the social media. we studied the impact of dialogue features and dialogue history on the quality of the classification and showed improvement in performance for both models and both classification tasks. we also showed the robustness of our models across different data sources. in the future, we plan to run experiments in which the predicted emotional technique is actually applied in the context of new dialogues to measure the effect of such predictions on real support dialogues. (", "1005": "recently, there has been a lot of exciting progress in the field of few-shot learning in general, and in few-shot classification (fsc) in particular. d-nas optimization techniques are especially designed to mitigate over-fitting, making them attractive to extreme situations with the greatest risk of overfitting, such as in the case of fsc. d-nas in particular, to the best of our knowledge, has not been applied to few-shot problems yet. to this end, we employ a set of small neural networks, metadapt controllers, responsible for controlling the connections in the dag given the current task. the embedding space based metric-learning approaches are either posed as a general discriminative distance metric learning 49,8, or optimized on the fewshot tasks 55,64,16,41, via the meta-learning paradigm that will be described next. these approaches include: (i) semi-supervised approaches using additional unlabeled data 9,14; (ii) fine tuning from pre-trained models 31,62,63; (iii) applying domain transfer by borrowing examples from relevant categories or using semantic vocabularies 3,15; (iv) rendering synthetic examples 42,10,56; (v) augmenting the training examples using geometric and photometric transformations or learning adaptive augmentation strategies 21; (vi) example synthesis using generative adversarial networks (gans) 69,25,20,48,45,35,11,23,2. in 7,67 label and attribute semantics are used as additional information for training an example synthesis network. in matching networks 60, a non-parametric k-nn classifier is meta-learned such that for each few-shot task the learned model generates an adaptive embedding space for which the task can be better solved. in metaoptnet a cnn backbone is trained end-to-end with an unrolled convex optimization solution of an optimal classifier, such as svm. two notable works on nas are amoebanet and nasnet the first one used a genetic algorithm and the second used a reinforcement learning based method. recently, differentiable methods with lower demand for computing have been introduced. darts relaxes the search space into a continuous one, allowing a differentiable search. these coefficients indicate the importance of the attached operations and connections. asap addresses the issue that harsh pruning at the end of the search makes the found architecture sub-optimal. it does so by performing gradual pruning. this distribution is pushed closer to binary by using a temperature parameter and gradually decreasing it. we introduce the task-adaptable block, it has a graph structure with adaptable connections that can modulate the architecture, adapting it to the few-shot task at hand. finally, we describe the training procedure. potentially, more than one block can be used. note that as we use nodes in our block, there exists a single path in our search space that is the regular residual block (resnet3 block). figure 2a schematically illustrates the block architecture. the ’s are optimized using sgd with a secondorder approximation of the model after convergence of w, by applying: losstrain(w losstrainw(w,), ) (5) where is the learning rate for the metadapt controllers’ parameters are trained as a final step, with all other parameters freezed, using sgd on the entire training set for a single epoch. these are in turn partitioned into classes from super-classes for meta-training, classes from super-classes for meta-validation, and classes from super-classes for meta-testing. this minimizes the semantic overlap between classes of different splits. for the initial training we use the sgd optimizer with intial learning rate 0.1, momentum and weight decay decreasing the learning rate to at epoch 20, at epoch and at epoch for weights optimization during the search and meta adaptation phases we use the sgd optimizer with learning rate 0.001, momentum and weight decay for the architecture optimization we use adam optimizer with learning rate 104, 0.5, 0.99, weight decay and the cosine annealing learning rate scheduler with min following previous works, e.g. 13,5, we perform test time augmentations and fine-tuning. we perform horizontal flip augmentation, effectively doubling the number of support-set. this is also evident by observing the architectures usually used in the few-shot literature. to this end i,j are initialized so each operation is given the same weight and are kept fixed. 5: training curves with and without optimization of for fc100. finally, we add test time flip augmentations and fine-tuning as described in this helps in the case of 1-shot with improvement, but has no noticeable effect for 5-shot (see table 5g-h). some interesting future work directions include extending the proposed approach to progressively searching the full network architecture (instead of just the last block), applying the approach to other few-shot tasks such as detection and segmentation, and researching into different variants of task-adaptivity including global connections modifiers and inter block adaptive wiring.", "1006": "automated conversational agents (chatbots) are becoming widely used for various tasks such as personal assistants or as customer service agents. recent studies project that of businesses plan to use chatbots by 20201, and that chatbots will power of customer service interactions by the year this increasing usage is mainly due to advances in artificial intelligence and natural language processing (hirschberg and manning, 2015) 1url 2url along with increasingly capable chat development environments, leading to improvements in conversational richness and robustness. in this article, we focus on customer care systems. consequently, the chatbot will continue as if the conversation is proceeding well, usually 3defined by the dictionary as outstandingly bad. 4url leading to conversational breakdown. being able to automatically detect such conversations, either in real time or through log analysis, could help to improve chatbot quality. if, though, we can automatically detect the worst conversations (in our experience, typically under of the total), the focus can be on fixing the worst problems. our goal in this paper is to study conversational features that lead to egregious conversations. the rest of this paper is organized as follows. detecting egregious conversations is a new task, however, there is related work that aim at measuring the general quality of the interactions in conversational systems. the works of studied reasons why users reformulated utterances in such systems. specifically, in they focused on how to automatically predict the reason for user’s dissatisfaction using different features. created a measure of dialogue appropriateness to determine its role in maintaining a conversation. simi- larly, in they developed a taxonomy of available measures for an enduser’s quality of experience for multimodel dialogue systems, some of which touch on conversational quality. to perform egregious conversation detection, features from both customer inputs and agent responses are extracted, together with features related to the combination of specific inputs and responses. the customer’s emotional state during the conversation is known to correlate with the conversation’s quality (oliver, 2014). 7url this service was trained using customer care interactions, and infers emotions such as frustration, sadness, happiness. into a single negative sentiment score (denoted as neg sent). usually, high positive emotions capture different styles of thanking the agent, or indicate that the customer is somewhat satisfied (rychalski and hudson, 2017), thus, the conversation is less likely to become egregious. moreover, even if there are human agents, they might not be available at all times, and thus, a rejection of such a request is sometimes reasonable, but might still lead to customer frustration (amsel, 1992). for example, a pair may contain a turn in which the customer expressed negative emotions and received a response of not trained by the agent. such interactions may divert the conversation towards becoming egregious. these features are summarized in the last part of table we also calculated the similarity between the customer’s turn and the virtual agent’s response in cases of customer rephrasing. we further removed conversations that contained fewer than turns, as these are too short to be meaningful since the customer never replied or provided more details about the issue at hand. figure depicts the frequencies of conversation lengths which follow a power-law relationship. each conversation was tagged by four different expert judges8. this process generated the egregious class sizes of (8.6) and (8) for company a and company b, respectively. the egr model significantly outperformed both baselines10. figure depicts the results for the classification task. this may occur since textual features are closely tied to the training domain. in this paper, we have shown how it is possible to detect egregious conversations using a combination of customer utterances, agent responses, and customer-agent interactional features.", "1007": "convolutional neural networks as well as other traditional natural language processing , even when considering relatively simple one-layer models (kim, 2014). given a trained model and a single example, prediction interpretability aims to explain how the model arrived at its prediction. these can be further divided into white-box and black-box techniques. we identify and refine current intuitions as to how cnns work. 2) we improve prediction interpretability by focusing on informative ngrams and taking into account also the negative cues. the vectors are combined using max-pooling followed by a relu activation. for word embeddings, we use the pre-trained glove wikipedia 2014gigaword embeddings , which we fine-tune with the model. models are implemented in pytorch and trained with the adam optimizer. current common wisdom posits that filters serve as ngram detectors: each filter searches for a specific class of ngrams, which it marks by assigning them high scores. previous attempts in prediction-based interpretation of cnns for text highlight the ngrams in sp and their scores as means of explaining the prediction. deliberate ngrams end up in sp because they were scored high by their filter, likely because they are informative regarding the final decision. 1although this work focuses on text classification, the findings in this section apply to any neural architecture which utilizes global max pooling, for both discrete and continuous domains. we thus search for the threshold that separate the two classes. possible methods include saliency map methods or gradient-based methods. ( purity (we experimentally find that a purity value of works well). where the threshold is set for each filter separately, based on a shared purity value. the percentage of informative (non-accidental) values in p is roughly a linear function of the purity (figure 1c). in essence, the threshold classifier 4we note that empirically and intuitively, the more filters we utilize in the network, the less correlation there is between each filter’s class and the final classification, as the decision is being made by a greater consensus. this means that demanding a higher purity will be accompanied by lower coverage, relative to other experiments, and more ngrams will be discarded. identified and effectively discarded a filter which is not useful to the model. however, focusing on the top-k does not tell a complete story. common intuition suggests that each filter is homogeneous and specializes in detecting a specific classes of ngrams. for example, a filter may specializing in detecting ngrams such as had no issues, had zero issues, and had no problems. we also show that filters may not only identify good ngrams, but may also actively supress bad ones. as discussed in section 2, for each ngram u w1, ...,w and for each filter f we calculate the score u, f. the ngram score can be decomposed as a sum of individual word scores by considering the inner products between every word embedding wi in u and every parallel slice in f : u, f i0 wi, fid:i(d1) we refer to slice fid:i(d1) as slot i of the filter weights, denoted as f(i). regardless, as this bias is identical for all ngrams for the filter in question, it has no role in identifying which ngrams the filter is most similar to, and we can ignore it in this context. each cluster captures a different slot activation pattern. the clustering algorithm identified two clusters: one primarily containing ngrams of the pattern det intensity-adverb positive-word, while the second contains ngrams that begin with phrases like go wrong.7 the centroids for these clusters capture the activation patterns well: low-medium-high and highhigh-low for clusters and respectively. finally, we can also mark cases of negative-ngrams (section 5.4), where an ngram has high slot activations for some words, but these are negated by a highly-negative slot and as a consequence are not selected by max-pooling, or are selected but do not pass the filter’s threshold. we used this information to identify which ngrams are important to the classification. we also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words.", "1008": "automatic text summarizers condense a given piece of text into a shorter version (the summary). existing summarization methods can be classified into two main types, either extractive or abstractive. extractive methods select and order text fragments (e.g., sentences) from the original text source. yet, extractive summaries tend to be less fluent, coherent and readable and may include superfluous text. a common approach is based on the encoder-decoder , with the original text sequence being encoded while the summary is the decoded sequence. soft attention methods first locate salient text regions within the input text and then bias the abstraction process to prefer such regions during decoding on the other hand, hard attention methods perform abstraction only on text regions that were initially selected by some extraction process compared to previous works, whose final summary is either entirely extracted or generated using an abstractive process, in this work, we suggest a new idea of editorial network (editnet) a mixed extractive-abstractive summarization approach. a summary generated by editnet may include sentences that were either extracted, abstracted or of both types. moreover, per considered sentence, editnet may decide not to take either of these decisions and completely reject the sentence. using the cnn/dailymail dataset we demonstrate that, editnet’s summarization quality is highly competitive to that obtained by both state-of-the-art abstractive-only and extractive-only baselines. editnet is applied as a post-processing step over a given input summary whose sentences were initially selected by some extractor. for each sentence in s, the editor may make three possible decisions. the first decision is to keep the extracted sentence untouched (represented by label e in figure 1). for example, the editor may wish to ignore a superfluous or duplicate information expressed in the current sentence. for a given sentence s d, we now denote by se and sa its original (extracted) and paraphrased (abstracted) versions. such representations allow to compare both sentence versions on the same grounds. the first auxiliary representation is that of the whole document d itself, hereinafter denoted d rn. such a representation provides a global context for decision making. assuming document d has n sentences, let e 1n n sd es. the second auxiliary representation is that of the summary that was generated by the editor so far, denoted at step i as gi1 rn, with g0 such a representation provides a local context for decision making. for example, the network may learn that given the global context, one of the sentence versions may allow to produce a summary with a better coverage. recall that, in order to compare sei with s a i , we need to represent both sentence versions on as similar grounds as possible. we then obtain sai ’s representation by encoding it using the extractor’s encoder in a similar way in which sentence sei was originally supposed to be encoded. this results in a representation asi that provides a comparable alternative to esi , whose encoding is expected to be effected by similar contextual grounds. overall, for a given text input s with l sentences, there are 3l possible summaries s to consider. , i1, i) denote the average r() value obtained by decision sequences that start with the prefix (1, based on , the soft label y(i) is then calculated3 as follows: y(i) r(1, we trained, validated and tested our approach using the non-annonymized version of the cnn/dailymail dataset following , we used the story highlights associated with each article as its ground truth summary. we chose the best model over the validation set for testing. considering the complexity of these models, and the slow down that can incur during training and inference, we think that editnet still provides a useful, high quality and relatively simple extension on top of standard encoder aligned decoder architectures. on average, and of editnet’s decisions were to abstract (a) or reject (r), respectively. moreover, editnet implements a novel sentence rejection decision, allowing to correct initial sentence selection decisions which are predicted to negatively effect summarization quality. we further plan to explore reinforcement learning (rl) as an alternative decision making approach.", "1009": "performing the mental gymnastics of transforming i’m hear’ to i’m here,’ or, i can’t so buttons’ to i can’t sew buttons,’ is familiar to anyone who has encountered autocorrected text messages, punny social media posts, or just friends with bad grammar. this paper presents dimsim, a learned ndimensional phonetic encoding for chinese along with a phonetic similarity algorithm, which uses the encoding to generate and rank phonetically similar words. an encoding for chinese pinyin leveraging chinese pronunciation characteristics. similarity is measured by a phonetic distance metric based on n-dimensional encodings, as introduced below. for example, the phonetic similarity of the finals ie and ue is identical in the pinyin pairs xie2,xue2 and lie2,lue2, in spite of the varying initials. english, by contrast, does not have this characteristic. for example, the pinyins of and are tong2xie2 and tong2xue2. we adopt a supervised learning approach to obtain these encodings, using linguistic characteristics combined with a labeled dataset. the initial, final, and tone components of the pinyin pci are denoted as pici , p f ci , and p t ci , respectively. a change that affects more than one component is the result of multiple independent and therefore additive changes. following the same logic, the phonetic similarity between two words w and w0 is computed as the sum of the distances between the pinyins. chinese has pairs of initials and pairs of finals. manually annotating each pair similarity requires a very large number of examples: assuming ten or twenty word pairs are provided as context for each pair, the task quickly blows up to nine or eighteen thousand annotations. specifically, this is done by grouping the pinyin components into initial clusters according to the pinyin pronunciation tables (iso, 2015) and only annotating the pairs within each cluster along with a single pairwise distance between clusters. annotating examples of all these pairs is labor intensive and error-prone. we then eliminate the comparison of pairs that are highly similar or highly dissimilar. to compare between clusters, we randomly choose one initial from each cluster and generate just those comparison pairs. we invite three native chinese speakers to perform the annotations. the distance sp of a pair p of points (x1, x2, ..., xn), (y1, y2, ..., yn) is calculated using euclidean distance as shown in equation sp 1in (xi yi)2 (3) the model aims to minimize the sum of the absolute differences between the euclidean distances of component pairs and the average distances obtained from the annotated training data across all pairs for initials (or finals) c. we also incorporate a penalty function, p, for pairs deviating from the manually annotated distance so that more phonetically similar pairs are penalized more highly (we discuss further in section 3.2). figures and visualize the computed encodings of initials when setting n1 and n2 we see that when n 2, the locations of initial coordinates align well with table 2,. there are five tones in chinese, represented by a tone number scale ranging from to it is simple to use tone numbers for tone encodings and the difference between the tones of two pinyins as the raw measure of distance, ranging in value from to (e.g., st (xue2, xue4) 2). dm considers word spelling, pronunciation and other miscellaneous characteristics to encode the word into a primary and a secondary code. baseline, demonstrating the inherent problem with dm’s coarse encodings. while aline has a similar recall to dimsim, it performs worse on mrr than dimsim2 because it does not have a direct representation of compound vowels for pinyin. figure demonstrates how sensitive our model is to the different combinations of scoring and penalty functions. we see that although recall is entirely insensitive to the variations, the performance of mrr is impacted. as demonstrated above, encoding initials and finals into a two-dimensional space is more effective than a one-dimensional space. we also see that moving from n1 to n2 increases the average mrr by 1.14x however, further increasing the number of dimensions to n2 no longer improves average mrr, indicating that learning a two-dimensional encoding is enough to capture the phonetic relationships between pinyin components. thus, the probability of including the labeled gold standard words in the results increases. in contrast, dimsim achieves high accuracy by learning the encodings both from high quality training data sets and linguistic pinyin features. several works in named entity translation focus on learning the phonetic similarity between english and chinese automatically.", "1010": "kb query, which can be executed to retrieve the answers from a kb. although general relation detection1 methods are well studied in the nlp community, such studies usually do not take the end task of kbqa into consideration. as a result, there is a significant gap between general relation detection studies and kb-specific relation detection. owing to these reasons, kb relation detection is significantly more challenging compared to general relation detection tasks. this paper improves kb relation detection to cope with the problems mentioned above. finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching. given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the kbqa process: (1) re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model. steps is used to query the kb for answers. all are much fewer than thousands of relations in kbqa. as a result, few work in this field focuses on dealing with large number of relations or unseen relations. ( relation detection in kbqa systems relation detection for kbqa also starts with featurerich approaches and attention models many of the above relation detection research could naturally support large relation vocabulary and open relation sets ), in order to fit the goal of open-domain question answering. use character tri-grams as inputs on both question and relation sides. golub and he (2016) propose a generative framework for single-relation kbqa which predicts relation with a character-level sequenceto-sequence model. however, while the questions are natural word sequences, how to represent relations as sequences remains a challenging problem. here we give an overview of two types of relation sequence representations commonly used in previous work. ( in this case, each relation name is treated as a unique token. in this case, the relation is treated as a sequence of words from the tokenized relation name. the two types of relation representation contain different levels of abstraction. this is important for our task since each relation token can correspond to phrases of different lengths, mainly because of syntactic variations. intuitively, the proposed method should benefit from hierarchical training since the second layer is fitting the residues from the first layer of matching, so the two layers of representations are more likely to be complementary to each other. we call this step relation detection on entity set since it does not work on a single topic entity as the usual settings. given the input question in the example, a relation detector is very likely to assign high scores to relations such as episodes written, author of and profession. in this step, for each candidate entity e el0k(q), we use the question text as the input to a relation detector to score all the relations r re that are associated to the entity e in the kb.4 because we have a single topic entity input in this step, we do the following question reformatting: we replace the the candidate e’s entity mention in 4note that the number of entities and the number of relation candidates will be much smaller than those in the previous step. hence we can directly evaluate relation detection performance independently as well as evaluate on the kbqa end task. 9for cnns we double the size for fair comparison. our proposed hr-bilstm outperformed the best baselines on both tasks by margins of 2-3 (p and compared to the best baseline bilstm w/ words on sq and wq respectively). 82.53), while it does not help as much on simplequestions. webqsp benefits more from residual and deeper architecture, possibly because in this dataset it is more important to handle larger scope of context matching. finally, on webqsp, replacing bilstm with cnn in our hierarchical matching framework results in a large performance drop. our experiments suggest that deeper bilstm does not always result in lower training accuracy. kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks.", "1011": "dictionary expansion is one area where close integration of humans into the discovery loop has been shown to enhance task performance substantially over more traditional post-adjudication. thus even with a system which finds similar terms (e.g., word2vec) guidance is important to keep the system focused on the subject matter expert’s notion of lexicon. multi-term phrases are a challenge for word2vec style systems as they need to be known prior to model creation. however, these phrases are likely to occur in future texts from the same source, and thus are important to include in any entity extraction lexicon. in the exploit phase, the approach generates new phrases by analyzing the single terms of the instances in the input dictionary. combining the explore and exploit approaches in an unsupervised fashion (or an infrequently supervised fashion) is not particularly effective. the rest of this paper is structured as follows. in section 2, we give an overview of related work. riloff and jones 12, is one of the first works to propose an automatic iterattive approach for dictionary extraction from unstructured text. in the following years, many similar approaches have been developed 13,3,7,2. hence, such approaches underperform on not-so-well structured texts, like user-generated text. our work is closely related to glimpse and glimpseld glimpse is a statistical algorithm for dictionary extraction based on spot with a faster underlying matching engine. contexts are scored retrospectively in terms of how many good results they generate. while both approaches have been proven to achieve high effectiveness for dictionary extension, both of the approaches can only identify new dictionary entries that are only present in the input text corpus. nership architecture and extend it with the explore/exploit algorithm for more effective dictionary expansion. the input of the algorithm is a text corpus tc and a set of dictionary seed example terms s. in the preprocessing step, we build a word2vec model 9, with the skip-gram implementation using tc as an input. word2vec is a particularly computationally-efficient two-layer neural net model for learning term embeddings from raw text. the accepted candidates are added in the input dictionary, which are then used in the exploit phase and the next iteration. this is critical to help future proof a lexicon against new text. in the second approach, we generate new phrases by extending the instances with terms from the text corpus that are related to the terms in the instance. related terms are terms that often share the same context, which means they often are surrounded by similar words. for example, given the instance clotting problems in the input dictionary the approach first tries to identify related terms in the text corpus for clotting. as an input text corpus we use user blogs extracted from url. askapatient.com (a forum where patients report their experience with medication drugs). as an input set of seed examples we use a set of instances referring to adverse drug events, which were labeled by a medical doctor in this experiment we compare the performance of the explore, exploit and the explore/exploit approaches for discovering new dictionary instances. we run the evaluation in iterations, where after each iteration we count how many new instances are discovered in the top proposed candidates by the algorithm. to do so, we run the explore/exploit approach with different feedback intervals. for example, when using feedback interval of 10, the user gives their feedback after candidates are identified by the system. that yields improvement in effectivness of the system. it incorporates human feedback to improve performance and control semantic drift at every iteration cycle. the experiments showed high importance of tight huml integration on discovery efficiency.", "1012": "grounding of textual phrases, i.e., finding bounding boxes in images which relate to textual phrases, is an important problem for human-computer interaction, robotics and mining of knowledge bases, three applications that are of increasing importance when considering autonomous systems, augmented and virtual reality environments. more specifically, deep net models are designed to extract features from given bounding boxes and textual data, which are then compared to measure their fitness. in this work we describe an interpretable mechanism which additionally alleviates any issues arising due to a limited number of region proposals. we find the bounding box with highest accumulated score contained in its interior. a woman in a green shirt is getting ready to throw her bowling ball down the lane... two women wearing hats covered in flowers are posing. young man wearing a hooded jacket sitting on snow in front of mountain area. second bike from right in front painting next to the two on theleft person all the way to the right figure 1: results on the test set for grounding of textual phrases using our branch and bound based algorithm. bottom row: referitgame dataset (groundtruth box in green and predicted box in red). 42, which learns a structure-preserving embedding for image-sentence retrieval. in 11, text is generated for a set of candidate object regions which is subsequently compared to a query. it is based on an extremely effective branch and bound scheme that can be applied to a large class of energy functions. importantly, by leveraging efficient branch-and-bound techniques, we are able to find the global minimizer for a given energy function very effectively. it is trivial to add additional information to our approach by adding additional score maps. general problem formulation: for simplicity we use x to refer to both given input data modalities, i.e., x (q, i), with query text, q, and image, i we will differentiate them in the narrative. for notational simplicity only, we assume all images to be scaled to identical dimensions, i.e., yi,max is not dependent on the input data x. we obtain a bounding box prediction y given our data x, by solving the energy minimization y arg min yy e(x, y, w), (1) to global optimality. note that w refers to the parameters of our model. therefore, we pursue a branch-and-bound technique in the following. note that the features (x, y, wr) may still depend non-linearly on all but the top-layer parameters. the feature accumulates the scores within the hypothesized bounding box y. inference: the algorithm to find the bounding box y with lowest energy as specified in eq. ( 1) is based on an iterative decomposition of the output space y 25, summarized in fig. to lower bound the intersection of the groundtruth box with the hypothesis space we use the smallest hypothesized bounding box. we map all the remaining words into an additional token. we don’t differentiate between uppercase and lower case characters and we also ignore punctuation. for detection, we use the yolo object detection system 37, to extract categories, trained on pascal voc-2012, and trained on mscoco for pose estimation, we use the system from to extract the body part location, then post-process to get the head, upper body, lower body, and hand regions. for the flickr 30k entities, we also fine-tuned the last layer of the deeplab system using the eight coarse-grained types and eleven colors from preprocessing and post-processing: for word prior feature maps and the semantic segmentation maps, we take an element-wise logarithm to convert the normalized feature counts into logprobabilities. we also provide an ablation study of the word and image information as shown in tab. we note that our results have been surpassed by 3, 7, 34, where they fine-tuned the entire network including the feature extractions or trained more feature detectors; cca, grounder and our approach uses a fixed pre-trained network for extracting image features. b), we visualize the cosine similarity between pairs of word vectors. expected groups of words form, for example (bicycle, bike), (camera, cellphone), (coffee, cup, drink), (man woman), (snowboarder, skier). for extracting language features, our method requires index lookups, which takes negligible amount of time (less than 1e-6 ms). acknowledgments: this material is based upon work supported in part by the national science foundation under grant no.", "1013": "the use of features from deep convolutional neural networks (dcnns) pretrained on imagenet has led to important advances in computer vision. from feature matching loss to moving averages. gmmn, mmd-gan convergence: mmd matching with universal kernels. given a kernel k, let p, q be two distributions, their mmd is: mmd2(k, p, q) p q2hk , where p expkx is the mean embedding. if k is universal then mmd2(k, p, q) if and only if p q. given a universal kernel such as a gaussian kernel as outlined in gmmn 22, 8, one can learn implicit generative models g that defines a family of distribution q by minimizing the mmd distance: inf mmd(k, pdata, q) (5) assuming pdata is in the family q (, q pdata), the infimum of mmd minimization for a universal kernel is achieved for q pdata (immediate consequence of theorem 1). while universality is usually thought on the kernel level, it is not straightforward to define universality for kernels defined by feature maps. hence, assuming universality of pfs defined by imagenet pretrained vgg or resnet, gfmn is guaranteed to converge to the data distribution by prop. our work relates also to plug and play generative models of where a pretrained classifier is used to sample new images, using mcmc sampling methods. we use batch normalization and relu non-linearity after each convolution. classifier features: we perform our experiments on classifier features with vgg19 and resnet18 networks which we pretrained using the whole imagenet dataset with classes. pretrained imagenet classifiers details can be found in the supplementary material. shows is and fid for increasing number of layers (i.e. number of features) in our extractor vgg19. we se- 1average result of five runs with different random seeds. this encoder is pretrained on imagenet and produces a total of 296k features. this feature extractor is smaller than vgg19/resnet18 allowing for minibatches of size up to for image size shows generated images from gfmn trained with either ma or our proposed ama. 4c shows results for minibatch size in ma training, the minibatch size has a tremendous impact on the quality of generated images: with minibatches smaller than 512, almost all images generated are quite distorted. in the supplementary material, we show a comparison between ma and ama with vgg19 imagenet classifier as feature extractor for a minibatch size of ama also displays a very positive effect on the quality of generated images when a stronger feature extractor is used. moreover, many practitioners do not have access to a gpu cluster, and the development of methods that can also work on a single gpu with small memory footprint is essential. shows the evolution of the generator loss per epoch and generated examples when using ama. there is a clear correlation between the quality of generated images and the loss. moreover, mode collapsing was not observed in our experiments with ama. and 4) which make it an attractive alternative. however, there are two important points to note: (1) we use a cross domain feature extractor and do not use labels from the target datasets (cifar10, stl10, lsun, celeba); (2) classifier accuracy does not seem to be the most important factor for generating good features: vgg19 classifier produces features as good as the ones from resnet18, although the former is less accurate (more details in supplementary material). let s j , j i, where i is a countable set and j : x r continuous function. let d be the dimension of the encoding e. for ma, using classic regret bounds for gradient descents we obtain: rmat t t1 vmat t22 f o( dt ). in tables and 7, and figure we detail the neural net architectures used in our experiments. for these two last datasets, the resnet generator has resblocks only, and the output size of the dense layer is both vgg19 and resnet18 networks are trained with sgd with fixed learning rate, momentum term, and weight decay set to we pick models with best top-1 accuracy on the validation set over epochs of training; for vgg19 (image size 3232), and for resnet18 (image size 3232). we compute fid using two sample sizes of generated images: 5k and 50k. this (real data) statistics are used in the fid computation of both 5k and 50k samples of generated images. indeed, the discriminator, being pretrained on imagenet, can quickly learn to distinguish between real and fake images. figure shows a visual comparison between images generated by gfmn (figs.", "1014": "e-commerce recommender systems aim to present items with high utility to the consumers utility may be decomposed into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time 28; recommender systems should take both types of utility into account. thus, durable and nondurable goods have differing time utility characteristics which lead to differing demand characteristics. however, users only purchase durable goods when the time is right. for instance, most users will not buy televisions the day after they have already bought one. therefore, recommending an item for which a user has no immediate demand can hurt user experience and waste an opportunity to drive sales. to this end, we quantify purchase intention as a combined effect of form utility and time utility. in contrast, d t may indicate that the item needs to be replaced, and she may be open to related recommendations. our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective 6, time-aware recommender systems 4, 29, 8, 19, and pu learning 20, 9, 13, 14, 23, the extensive consumer modeling literature is concerned with descriptive and analytical models of choice rather than prediction or recommendation, but nonetheless 2a detailed illustration can be found in the supplementary material forms the basis for our modeling approach. as discussed above, the demand is mediated by the time elapsed since the last purchase of an item in the same category. on the other hand, dcj ticjk indicates that the item is nearing the end of its lifetime and the user may be open to recommendations in category cj we use a hinge loss max(0, dcj ticjk) to model such time utility. given this observation, we follow and include a label-dependent loss trading the relative cost of positive and unlabeled samples: l(x ,p) ijk: pijk1 max1 (xijk max(0, dcj ticjk)), (1 ) ijk: pijk0 l(xijk, 0), where l(x, c) (x c)2 denotes the squared loss. for notational simplicity, we use a matrix x to denote the frontal slice x::1, and use xij to denote the entry (i, j) of the matrix x. since x is a low-rank tensor, its frontal slice x should be of low-rank as well. the detailed description can be found in the supplementary material. we then sort sijks such that s(i1j1k1) s(iqjqkq). for each interval s(iqjqkq), s(iq1jq1kq1), the function is quadratic, thus can be solved in a closed form. we note that the algorithm is guaranteed to converge to the true solution. we observe that the proposed algorithm is extremely efficient, e.g., even with million users, million items, and more than million purchase records, the running time of the proposed algorithm is less than hours. our testbeds are two real-world datasets tmall6 and amazon review7. this leads to a total of 5, items belonging to categories. the full amazon review dataset is significantly larger than its subset. the collected reviews span a long range of time: from may to july 2014, which leads to 6, time slots in total. ideally, good recommendations should have both small category rankings and small time errors. the algorithms m3f, pmf, and wr-mf are excluded from the purchase time prediction task since they are static models that do not consider time information. table reports the inter-review durations of amazon review subset estimated by our algorithm. we do not report the estimated durations of tmall herein since the item categories are anonymized in the dataset. therefore, we only sample a subset of items for each user and estimate the rankings of her purchased items. since wr-mf and pmf are both static models, our algorithm is the only approach evaluated here that considers time utility while being scalable enough to handle the full amazon review dataset. as a final note, we want to point out that tmall and amazon review may not take full advantage of the proposed algorithm, since (i) their categories are relatively coarse and may contain multiple sub-categories with different durations, and (ii) the time stamps of amazon review reflect the review time instead of purchase time, and inter-review durations could be different from inter-purchase durations. in this paper, we examine the problem of demand-aware recommendation in settings when interpurchase duration within item categories affects users’ purchase intention in combination with intrinsic properties of the items themselves. our empirical studies show that the proposed approach can yield perfect recovery of duration vectors in noiseless settings; it is robust to noise and scalable as analyzed theoretically.", "1015": "automated conversational agents are becoming popular for various tasks, such as personal assistants, shopping assistants, or as customer service agents. personality is defined as a set of traits which represent durable characteristics of a person. in this paper we study how to encode personality traits as part of neural response generation for conversational agents. the response is then generated conditioned on these features. results indicate that conscientiousness figure shows examples of customer utterances, followed by two automatically generated responses. the first response (in each example), is generated by a standard seq2seq response generation system that ignores personality modeling and in effect generates the consensus response of the humans represented in the training data. the second response is generated by our system, and is aimed to generate data for an agent that expresses a high level of a specific trait. in example 1, the agreeableness-agent is more compassionate (expresses empathy) and is more cooperative (asks questions). in example 2, the conscientiousness-agent is more thoughtful (will check the issue). generating responses that express a target personality was previously discussed in different settings. early work on the personage system (mairesse and walker, 2007; mairesse and walker, 2008; mairesse and walker, 2010; mairesse and walker, 2011) presented a framework projecting different traits throughout the different modules of an nlg system. our approach utilizes a neural network that automatically learns to represent high level personality based features. neural response generation models are based on a seq2seq architecture and employ an encoder to represent the user utterance and an attention-based decoder that generates the agent response one token at a time. models that aim to generate a coherent persona also exist. we review the seq2seq attention based model on which our model is based. this kind of response does not characterize a specific personality and thus can result in inconsistent or unwanted personality cues. in this section we present our personality-based model (figure 2) which generates responses conditioned on a target set of personality traits values which the responses should express. as in (mairesse and walker, 2011), we argue that personality traits are exhibited as different types of stylistic linguistic variation. each personality feature hi is a weighted sum of the targeted traits values (following a sigmoid activation). formally, equation is changed to: sj1 lstm((out)(yj), cj , hp, sj), (4) conditioning on hp captures the relation of text generation to the underlining personality traits. for our experiments we utilized the dataset presented in , which exhibits a large variety of customer service properties. this resulted in 87.5k conversation pairs in total including different agents (138160 pairs per agent on average). the persona-based model achieved similar perplexity but higher bleu score than our model. this is reasonable since persona-based is not restricted to personality based features. data for the other of the agents formed the training set. these agents represent new personality distributions we would like to generate responses for. note that, we extracted target personality traits for agents in the training set using their training data, or, for agents in the test set, using validation data. in this setting, it is not possible to test the persona-based model since no representation is learned during training for agents in the test set. each triplet methodology, the two responses were presented in a random order, and judged on a 5-point zero-sum scale. if we ignore the somewhat more expressive judgments, the high-trait responses win in of cases.", "1016": "sparsity and parallel asynchronous computation are two key principles of information processing in the brain. event-based cameras can generate events at microsecond resolution. second, consecutive frames in videos are usually highly redundant, which waste downstream data transfer, computing resources and power. most global methods 40, 17, 49, are derived from the marr and poggio cooperative stereo algorithm the algorithm assumes depth continuity and often event-based implementations are not tested with objects tilted in depth. we propose a fully neuromorphic event-based stereo disparity algorithm. the algorithm converges well when object surfaces are fronto-parallel and candidate matches injected to the network are close together 40, later mahowald modified the vlsi embodied algorithm to solve tilted depth maps using a network of analog valued disparity units, which linearly interpolates the cooperative network output. employ message passing on a markov random field with depth continuity for a global solution. the proposed method and its fpga implementations 20, are equivalent to the cooperative stereo algorithm with noisy time difference inputs. furthermore, upon producing an event, a neuron is reset to a user-specified value. unless specified otherwise, we assume initial membrane potentials and reset values of zero. depicts the sequence of operations performed by the corelets using inputs from stereo event sensors. potentials are initialized to zero and set to also reset to zero upon spiking. the event rate of an event-based sensor depends on factors, such as scene contrast, sensor bias parameters, and object velocity. buffers can encode the input at various temporal scales. such control neurons are used to probe (prb) or reset (rst) neuron membrane potentials. notice that new inputs (a1(v spl p ())) are always routed to the cell r that was reset in the previous tick. in practice pixel neighborhoods are used. the ith of these coordinates is represented by neuron activations a1(v l, x (i) l,p (t)) and a1(v r, x (i) r,p (t)) in 1disjunction is implemented by sending input events to the same neuron input axon, effectively merging any input events to a single input event. the winner-take-all (wta) system is a feed-forward neural network that takes as input d thermometer code representations of the hadamard products for d distinct candidate disparity levels, and finds the disparity with the largest value, at every tick. note that a thermometer code of length 2n bits can be represented by a qtc of length n/2 bits. notice that (a1(v cnv 2,,d (t)), a1(v cnv 1,,d (t)), a1(v cnv 0,,d (t))) is a length-3 thermometer code representation of a value in 0, 1, 2, 3, representing the th digit in the base-4 representation of vd(t1). 4a-f), and two real world sets of sequences, consisting of a fast rotating fan (fig. 4n-u) captured using the davis stereo cameras. self-occluded pixels are assigned random values. this also entails a calibration process for transforming the undistorted kinect coordinate frame to the undistorted davis sensor coordinate frame. nine fan sequences (3 distances orientations) and three butterfly sequences (3 distances) are used. measurements are reported at supply voltages of 0.8v, 1.0v. we also experiment with a postprocessing phase with erosion and dilation applied to output disparity maps in order to better regularize the output. see supplementary materials for more details. the implemented neuromorphic stereo disparity system achieves these advantages, while consuming less power per pixel per disparity map compared to the stateof-the-art the homogeneous computational substrate provides the first example of a fully end-to-end low-power, high throughput fully event-based neuromorphic stereo system capable of running on live input event streams, using a fully graph-based computation model, where no frames, arrays or other such data-structures are used.", "1017": "explaining a complex system through their cause and effect relations is one of the fundamental challenges in science. how a complex system works and lead to better understanding about the phenomenon under investigation. the task of learning the causal structure entails a search over the space of causal graphs that are compatible with the observed data; the collection of these graphs forms what is called an equivalence class. these relations are the most basic type of probabilistic invariances used in the field and have been studied at large in the context of graphical models since, at least, (see also 5). for our characterization, we first extend the causal calculus rules to operate between arbitrary sets of interventions. finally, we propose a sound algorithm for learning the augmented graph from interventional data. we show a graphical characterization of i-markov equivalence of causal graphs with latents. this procedure comes with a new set of orientation rules. 3there may be constraints that can be obtained by applying the rules multiple times we do not consider here. a mixed graph can contain directed and bi-directed edges. a is an ancestor of b if there is a directed path from a to b. a is a spouse of b if a b is present. a mixed graph is ancestral if it does not contain a directed or almost directed cycle. it is maximal if there is no inducing path (relative to the empty set) between any two non-adjacent nodes. dx denotes the graph obtained fromd where all the incoming edges to the set of nodes in x are removed. we assume that there is no selection bias. a star on an endpoint of an edge is used as a wildcard to denote circle, arrowhead, or tail. the first rule of the calculus is a d-separation type of statement relative to a specific interventional distribution px, which says that y z w ind implies the corresponding conditional independence px(yw, z) px(yw). then, we will characterize when two causal graphs are equivalent in accordance to the proposed definition. consider the tuples of absolutely continuous probability distributions (pi)ii over a set of variables v. a tuple (pi)ii satisfies the i-markov property with respect to a graphd (vl,e) if the following holds for disjoint y,z,w v: (1) for i i: pi(yw, z) pi(yw) if y z w ind. ( one challenge with definition is that testing for the d-separation statement in condition (2) requires a mutilated graph where we cut some of the edges in d. this makes it harder to represent all the constraints imposed by a causal graph compactly. in words, the construction of the augmented graph goes as follows. first, initialize the augmented graph to the input causal graph. the augmented graph of d with respect to i , denoted as augi (d), is the graph constructed as follows: augi (d) (v f ,e e) where f b fiik and e (fi, j)ik, jsi the significance of the augmented graph construction is illustrated by proposition 1, which provides criteria to test the d-separation statements in definition equivalently from the corresponding augmented graph of a causal graph. however, similar to the observational case, it is typically impossible to completely determine the causal graph from the available measured data, especially when latents are present. then, the objective is to learn a class of augmented mags consistent with data. a tuple of distributions (pi)ii p(d,v) is called c-faithful to graphd if the converse for each of the conditions given in definition holds. to search for a separating set using the corresponding do-constraints. therefore, phase iii algorithm find m-separation sets via calculus tests. similarly, we can infer the separation of fx and w by the test p(w x) px(w x). figure 3c shows the graph obtained after applying the seven rules of the fci together with rule finally, by applying rule 9, we infer that the edge between x and y has a tail at x and we obtain the graph in figure 3d.", "1018": "structured knowledge bases , or multi-hop , or complex queries such as how many countries have more rivers and lakes than brazil?’’ another transactions of the association for computational linguistics, vol. submission batch: 8/2018; revision batch: 11/2018; final submission: 1/2019; published 4/2019. c association for computational linguistics. however, csqa poses a much greater challenge, with its more diverse classes of complex queries and almost 20-times larger scale. subsequently, it was evaluated only on webquestionssp , that requires relatively simpler programs. availability of kb metadata helps standardize comparisons across techniques (explained subsequently). relevant statistics of the resulting data set are presented in table use of gold entity, type, and relation annotations to standardize comparisons: our focus being on the reasoning aspect of the kbqa problem, we use the gold annotations of canonical kb entities, types, and relations available in the data set along with the the queries, in order to remove a prominent source of confusion in comparing kbqa systems (i.e., all systems take as inputs the natural language query, with spans identified with kb ids of entities, types, relations, and integers). questions in the webquestionssp data set are answerable from the freebase kb and tyically require up to 2-hop inference chains, sometimes with additional requirements of satisfying specific constraints. the key embedding is used for looking up and retrieving an entry from the operator vocabulary and the corresponding value embedding encodes the operator information. the variable type has only the value embedding mvtype val rnum opdval as no lookup is needed on it. the generic constraints can help it adopt more pragmatic programming styles like not repeating lines of code or avoiding syntactical errors. along with the transformed distribution, the top-k entries xsampled is also returned. output variable generator: the new variable up of type u type p mop outp is generated by the procedure outvargen by invoking a sampled operator p with m variables vp,1 vp,m of type vtypep,1 v type p,m as arguments. handling complex queries by expanding the operator set and generating longer programs blows up the program space to a huge size of (numop (maxvar) m)t this, in absence of gold programs, poses serious training challenges for the programmer. we now describe how to guide cipitr more efficiently through such a challenging environment using both generic and task-specific constraints. in the second phase (algorithm phase) the model is allowed to operate on all the generated variables in order to reach the answer. note that this is a generic characteristic, as for every task, this kind of phase division is possible. such a curriculum learning mechanism, while being particularly useful for the more complex queries, is still quite generic as it does not require any additional task-specific prior knowledge. further, to gauge the proficiency of the proposed program induction model, we construct a rule-based model which is aware of the human annotated semantic parsed form of the querythat is, the inference chain of relations and the exact constraints that need to be additionally applied to reach the answer. kvmnet with decoder (as discussed in section 2), learns to attend on a kb subgraph in memory and decode the attention over memory-entries as their likelihood of being in the answer. ninety percent of training and of test count-queries have answers less than though this makes it unfair to compare npi models (that are oblivious to the answer vocabulary) with kvmnet on such queries, we still train a kvmnet version on a balanced resample of csqa, where, for only the count queries, the answer distribution over integers has been made uniform. nsm (2017) uses a key-variable memory and decodes the program as a sequence of operators and memory variables. as the nsm code was not available, we implemented it and further incorporated most of the six techniques presented in table however, constraints like action repetition, biasing last operator selection, and phase change cannot be incorporated in nsm while keeping the model generic, as it decodes the program token by token. on the other hand, training the kvmnet model on the balanced data helps showcase the real performance of the model, where cipitr outperforms kvmnet significantly on most of the harder query classes. we see in the table that each of the techniques helped the model significantly. some of them boosted f1 by times, while others proved to be instrumental to obtained large improvements in f1 score of over times. we experimentally established that for programs of length these various techniques reduced the average program space from to 2,998 programs. cipitr uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic pragmatic programming styles to constrain the combinatorial program space to only semantically correct programs. as future directions of work, cipitr can be further improved to handle the hardest question types by making the search more strategic, and can be further generalized to a diverse set of goals when training on all question categories together. additionally, further improvements are required to induce complex programs without availability of gold program input variables.", "1019": "the vast amounts of textual data end users need to consume motivates the need for automatic summarization an automatic summarizer gets as an input one or more documents and possibly also a limit on summary length (e.g., maximum number of words). therefore, the summarizer will need to produce a focused summary which includes the most relevant information to that need. while both saliency and focus goals should be considered within a query-focused summarization setting, these goals may be actually conflicting with each other higher saliency usually comes at the expense of lower focus and vice-versa. moreover, such a tradeoff may directly depend on summary length. contact author: haggaiil.ibm.com to illustrate the effect of summary length on this tradeoff, using the duc dataset, figure reports the summarization quality which was obtained by the cross entropy summarizer (ces) a state of the art unsupervised query-focused multidocument extractive summarizer saliency was measured according to cosine similarity between the summary’s bigram representation and that of the input documents. aiming at better handling the saliency versus focus tradeoff, in this work, we propose dual-ces an extended ces summarizer similar to ces, dual-ces is an unsupervised query-focused, multi-document, extractive summarizer. this allows to better account for context-sensitive features during summarization. formally, let q denote some user information need for documents summarization, which may be expressed by one or more queries. for example, ces is implemented by invoking cem(qces(q,d), lmax, ). differently from ces, dual-ces does not attempt to maximize both saliency and focus goals in a single optimization step. yet, at the second step, similar to ces, the primary goal is actually to produce a focused summary (with maximum length limit lmax). both qsal(q,d) and qfoc(q,d) are implemented as a product of several basic predictors. we next elaborate the implementation details of dual-ces’s dual optimization steps. next, we shortly describe each predictor. the symbol marks whether it was originally employed in ces this predictor estimates to what extent (candidate) summary s (generally) covers the document set d. here, we represent both s and d as term-frequency vectors, considering only bigrams, which commonly represent more important content units for a given text x, let cos(s, x) def sx sx the coverage predictor is then defined by qcov(sq,d) def cos(s,d). such summaries contain fewer and longer sentences, and therefore, tend to be more informative. let len(x) denote the length of text x (in number of words). apart from salient words in sl that are used as feedback, we note that, sentences in sl may also provide additional hints about other properties of informative sentences in d, which may potentially be selected to improve saliency. we evaluated both dual-ces and its adaptive-length variant (hereinafter denoted dualces-a). each sub-query was further expanded with top-100 (unigram) wikipedia related-words we then obtained the topic query-sensitive predictions by summing up its various sub-queries’ predictions. we report both recall and f-measure of rouge-1, rouge-2 and rouge-su4. the first one, is the original ces summarizer, whose results are reported in the second one, denoted hereinafter ces, utilizes predictors 16, which are combined within a single optimized objective (by taking their product). the main results of our evaluation are reported in table (rouge-x f-measure) and table (rouge-x recall). unfortunately, not all baselines fully reported their results for all benchmarks and measures. on recall, dualces has achieved between better rouge-1. on f-measure, dual-ces has achieved at least between and better rouge-2 and rouge-1, respectively. a case in point is the ces variant which is even inferior to ces. the pseudo-feedback distillation approach employed between the two steps of dualces has some resemblance to attention models that are used by state-of-the-art deep learning summarization methods 1, 12, first we note that, dual-ces significantly improves over these attentive baselines on rouge-1. dual-ces-a, therefore, serves as a robust alternative for flexibly estimating such hyperparameter value during runtime. dual-ces-a can provide similar quality and may outperform dual-ces. moreover, in many cases, dual-ces even outperforms state-of-the-art supervised summarizers.", "1020": "in recent years we are experiencing a dramatic improvement of the synthesized speech quality in tts systems, with the introduction of systems that are based on neural networks (nn). a major improvement in quality was achieved by using attention based models such as tacotron and by replacing vocoders with a nn based waveform generators such as wavenet a useful feature of systems with trainable models is the ability to adapt the tts to an unseen voice using a small amount of training data (from a few seconds to an hour of speech). furthermore, a multi-speaker model usually needs much more trainable parameters than a single speaker model. this may lead to a computationally heavy and slow synthesis process even on a strong gpu. this system used the world vocoder we demonstrated that this tts allows simple adaptation to new voices. this was carried out by retraining nn models that had already been trained using a large highquality voice, on a small amount of data from the new voice. recently, an efficient neural vocoder called lpcnet was introduced the lpcnet inference runs faster than realtime on a single cpu while producing a high quality speech output. lpcnet uses cepstrum representing spectral envelopes, pitch and pitch correlation as input features. this makes it a simple alternative to other vocoders, e.g. world, which work with similar features. an overview of our new tts system is presented in figure the system is a cascade of a rule-based front-end, a nn based prosody generator, a nn synthesizer and an lpcnet decoder. each sub-phoneme element represents either a heading, a middle or a trailing part of a phoneme. each block has its own model which is trained independently for each voice. hence, the system is modular and provides easy control, flexibility and adaptability at the component level. forced alignment of audio at the sub-phoneme level using proprietary acoustic modeling and speech recognition tools. pitch detection for prosody modeling using a proprietary tool. the sub-phoneme labels are represented by element vectors, using a trainable embedding table. a longer time dependent context is extracted by an lstm layer that merges the phonetic and pitch context. following this are fully connected layers with relu non-linearity. the energy is now the integration of this power spectrum. we use mse loss function on all output parameters. to adapt the model to a smaller unseen voice, we first initialize the training with the weights of the base model of the same gender. in this work we used the code published by the mozilla team on github1 with some adjustments: we replaced the pitch and pitch correlation values with values that were produced by our tools in order to maintain data consistency over all blocks. both were produced by native us english speakers and 1url recorded in a professional studio. the audio was recorded sentence by sentence. one should note relatively low mos scores for the original natural samples, which can be explained by the assumption that the listeners subjectively judged speaker pleasantness together with the speech quality and naturalness. in these tests a subject is presented with a pair of samples that convey different text messages. we can compare these results to those of the vcc hub task although the task setup and the listening tests conditions are a bit different we can see that our system mos and similarity score are comparable to those of the best vcc system (n10 with quality of and similarity of where the corresponding scores for the original speech are and 95). to clarify, the normalization ranges are different for each voice. we have presented in this article a new tts system that addresses the challenging goals of producing high quality speech while operating at faster than real-time rate without an expensive gpu support. we tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems. for future work, we plan to allow voice modifications by adding control over voice parameters such as pitch, breathiness and vocal tract.", "1021": "feature selection is an important problem in statistics and machine learning for interpretable predictive modeling and scientific discoveries. our goal in this paper is to design a dependency measure that is interpretable and can be reliably used to control the false discovery rate in feature selection. the mutual information between two random variables x and y is the most commonly used dependency measure. for instance, the hilbert-schmidt independence criterion (hsic) uses the maximum mean discrepancy (mmd) to assess the dependency between two variables, i.e. hsic(x,y ) mmd(pxy, pxpy), which can be easily estimated from samples via kernel mean embeddings in a reproducing kernel hilbert space (rkhs) in this paper we introduce the sobolev independence criterion (sic), a form of gradient regularized integral probability metric (ipm) between the joint distribution and the product of marginals. hence, promoting its sparsity is a natural constraint for feature selection. the work was done when they were at ibm research. in section we show how sic and conditional generative models can be used to control the false discovery rate using the recently introduced holdout randomization test and knockoffs we validate sic and its fdr control on synthetic and real datasets in section motivation: feature selection. let d be an integral probability metric associated with a function space f , i.e for two distributions p, q: d(p, q) sup ff expf(x) exqf(x). with p pxy and q pxpy this becomes a generalized definition of mutual information. in image processing the total variation norm is used for instance as a regularizer to induce smoothness. we therefore elect to instead use the nonlinear sparsity penalty introduced in : 0(f) je(x,y) f(x,y)xj 0, and its relaxation : s(f) dx j1 e(x,y) f(x, y)xj as discussed in 14, e(x,y) f(x,y)xj implies that f is constant with respect to variable xj , if the function f is continuously differentiable and the support of is connected. this is similar to practices with linear models such as elastic net. here we will consider pxpy (although we could also use (pxy pxpy)). as it was just presented, the sic objective is a difficult function to optimize in practice. moreover, the fact that the expectation is inside the nonlinearity introduces a gradient estimation bias when the optimization of the sic objective is performed using stochastic gradient descent (i.e. we alleviate these problems (non-smoothness and biased expectation estimation) by making the expectation linear in the objective thanks to the introduction of auxiliary variables j that will end up playing an important role in this work. hence, substituting (s)(f) with s,(f) in its equivalent form we obtain the perturbed sic: sic(l1)2,(pxy, pxpy) infl(f, ) : f f , j , j 0, dx j1 j where l(f, ) (f, pxy, pxpy) dx j1 epxpy f(x,y)xj j 2epxpyf 2(x, y), and (f, pxy, pxpy) epxyf(x, y) epxpyf(x, y). theorem (existence of a solution, uniqueness, convexity and continuity). corollary (interpretability of convex sic ). as an alternative, we propose to learn the feature map as a deep neural network. the architecture of the network can be problem dependent, but we focus here on a particular architecture: deep relu networks with biases removed. other neural measures of dependencies such as mine estimate the kl divergence using neural networks, or that of that estimates a proxy to the wasserstein distance using neural networks. interpretability, sparsity, saliency and sensitivity analysis. finally, sic relates to saliency based post-hoc interpretation of deep models such as while those method use the gradient information for a post-hoc analysis, sic incorporates this information to guide the learning towards the important features. tpr top fdr top tpr hrt fdr hrt tpr top fdr top tpr hrt fdr hrt po we r a nd f dr elastic net random forest tpr top fdr top tpr hrt fdr hrt tpr top fdr top tpr hrt fdr hrt mse sobolev penalty sic dataset sinexp, n125 samples dataset sinexp, n500 samples feature selection on drug response dataset. the goal here is to quantify the predictiveness of features selected by sic on its own, without the full randomized testing machinery. the sic critic and regressor nn were respectively the bigcritic and regressornn described with training details in appendix f.3, while the random forest is trained with default hyper parameters from scikit-learn we can see that, with just j , informative features are selected for the downstream regression task, with performance comparable to those selected by elasticnet, which was trained explicitly for this task. we use the pre-processing of each dataset (drug-class, drug-type) of the knockoff tutorial made available by the authors. concretely, we construct a dataset (x, x) of the concatenation of the real data and gaussian knockoffs 9, and fit sic(x, x, y ). for sic experiments, we use smallcritic architecture (see appendix f.3 for training details). we use boosted sic, by varying the batch sizes in n 10, 30, 50, and computing the geometric mean of produced by those three setups as the feature importance needed for knockoffs."}