<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Musketeer: all for one, one for all in data processing systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ionel</forename><surname>Gog</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems * now at Google</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems * now at Google</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natacha</forename><surname>Crooks</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems * now at Google</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">P</forename><surname>Grosvenor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems * now at Google</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allen</forename><surname>Clement</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems * now at Google</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hand</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Software Systems * now at Google</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Musketeer: all for one, one for all in data processing systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/10.1145/2741948.2741968</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Many systems for the parallel processing of big data are available today. Yet, few users can tell by intuition which system, or combination of systems, is &quot;best&quot; for a given workflow. Porting workflows between systems is tedious. Hence, users become &quot;locked in&quot;, despite faster or more efficient systems being available. This is a direct consequence of the tight coupling between user-facing front-ends that express workflows (e.g., Hive, SparkSQL, Lindi, GraphLINQ) and the back-end execution engines that run them (e.g., MapReduce, Spark, PowerGraph, Naiad). We argue that the ways that workflows are defined should be decoupled from the manner in which they are executed. To explore this idea, we have built Musketeer, a workflow manager which can dynamically map front-end workflow descriptions to a broad range of back-end execution engines. Our prototype maps workflows expressed in four highlevel query languages to seven different popular data processing systems. Musketeer speeds up realistic workflows by up to 9× by targeting different execution engines, without requiring any manual effort. Its automatically generated back-end code comes within 5%-30% of the performance of hand-optimized implementations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Choosing the "right" parallel data processing system is difficult. It requires significant expert knowledge about the programming paradigm, design goals and implementation of the many available systems. Even with this knowledge, any move between systems requires time-consuming reimplementation of workflows. Furthermore, head-to-head comparisons are difficult because systems often make different assumptions and target different use cases. Users therefore stick to their known, favorite system even if other, "better" systems offer superior performance or efficiency gains.</p><p>We evaluated a range of contemporary data processing systems -Hadoop, Spark, Naiad, PowerGraph, Metis and GraphChi -under controlled and comparable conditions. We found that (i) their performance varies widely depending on the high-level workflow; (ii) no single system always outperforms all others; and (iii) almost every system performs best under some circumstances ( §2).</p><p>It thus makes little sense to force the user to target a single system at workflow implementation time. Instead, we argue that users should, in principle, be able to execute their highlevel workflow on any data processing system ( §3). Being able to do this has three main benefits:</p><p>1. Users write their workflow once, in a way they choose, but can easily execute it on alternative systems; 2. Multiple sub-components of a workflow can be executed on different back-end systems; and 3. Existing workflows can easily be ported to new systems.</p><p>In this paper, we present our Musketeer proof-of-concept workflow manager to show that this is feasible, and that the resulting implementations are competitive with hand-written baseline implementations for specific systems.</p><p>To decouple workflows from their execution, we rely on the fact that users prefer to express their workflows using high-level frameworks, which abstract the low-level details of distributed execution engines <ref type="figure" target="#fig_0">(Figure 1</ref>). For example, the Hive <ref type="bibr" target="#b39">[40]</ref> and Pig <ref type="bibr" target="#b34">[35]</ref> frameworks present users with a SQL-like querying interface over the Hadoop MapReduce execution engine; SparkSQL and GraphX <ref type="bibr" target="#b14">[15]</ref> offer SQL primitives and vertex-centric interfaces over Spark <ref type="bibr" target="#b42">[43]</ref>; and Lindi and GraphLINQ <ref type="bibr" target="#b30">[31]</ref> offer the same over Naiad <ref type="bibr" target="#b33">[34]</ref>.</p><p>Musketeer breaks the tight coupling between frameworks and execution engines ( §3). It achieves this by (i) mapping workflow specifications for front-end frameworks to a common intermediate representation; (ii) determining a good decomposition of the workflow into jobs; and (iii) autogenerating efficient code for the chosen back-end systems.</p><p>Musketeer currently supports four front-end frameworks (Hive, Lindi, a custom SQL-like DSL with iteration, and a graph-oriented "Gather-Apply-Scatter" DSL). It can map workflows to seven back-end execution systems: Hadoop, Spark, Naiad, PowerGraph, GraphChi, Metis and simple, serial C code. Users can explicitly target back-end execution engines, or leave it to Musketeer to automatically choose a good mapping using a simple heuristic ( §5).</p><p>In a range of experiments with real-world workflows, Musketeer offers compelling advantages ( §6): 1. Better system mapping: Musketeer enables existing workflows implemented for Hive on Hadoop MapReduce to be executed on alternative systems, and achieves a 2× speedup on a TPC-H query workflow as a result.</p><p>2. Optimization of executed code: by choosing the most suitable Naiad execution primitive independent of the front-end used to implement the workflow, Musketeer speeds up a TPC-H query workflow by up to 9×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Intelligent system combination:</head><p>Musketeer can combine different execution engines within a workflow, and doing so outperforms fixed, single system mappings for a cross-community PageRank workflow. <ref type="bibr" target="#b3">4</ref>. Competitive generated code: Musketeer's generated code has no more than a 5-30% overhead over handoptimized baseline implementations.</p><p>5. Good automatic system choice: an automated heuristic for choosing back-ends in Musketeer derives reasonably good mappings without manual user action.</p><p>These results and our experience of using Musketeer in practice ( §7) indicate that decoupling front-end and back-end systems can bring real benefits. Nonetheless, we believe our work represents only the first step in a promising direction. In Section 8, we describe current limitations of our system, and suggest some concrete future work before discussing related research ( §9) and concluding ( §10).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation</head><p>There are many diverse "big data" processing systems and more keep appearing. <ref type="bibr" target="#b1">2</ref> This makes it difficult to determine which system is best for a given workflow, data set and cluster setup. We illustrate this using a set of simple benchmarks. In all cases, we run over a shared HDFS installation that stores input and output data, and we either implement jobs directly against a particular execution engine, or use a front-end framework with its corresponding native back-end. We measure the makespan -i.e., the entire time to execute a workflow, including the computation itself and any data loading, pre-processing and output materialization required.</p><p>We find that no single system systematically outperforms all others. Roughly speaking, system performance depends on: (i) the size of the input data, as single machine frameworks outperform distributed frameworks for small inputs; (ii) the structure of the data, since skew and selectivity impact I/O performance and work distribution; (iii) engineering decisions, with e.g., the cost of loading inputs varying significantly across systems; and (iv) the computation type, since specialized systems often operate more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query processing micro-benchmarks</head><p>Query-based data analytics workflows often consist of relational operators. In the following, we consider the behavior of two operators in an isolated micro-benchmark. Here we use a local cluster of seven nodes as an example of a smallscale data analytics deployment. Later experiments show that our results generalize to more complex workflows and to larger clusters.</p><p>Input size. We first look at a simple string processing workload in which we extract one column from a spaceseparated, two column ASCII input. This corresponds to a PROJECT query in SQL terms, but is also reminiscent of a common pattern in log analysis batch jobs: lines are read from storage, split into tokens, and a few are written back. We consider input sizes ranging from 128MB up to For a brief summary of systems' properties, see <ref type="table">Table 3</ref>.  32GB. <ref type="figure" target="#fig_2">Figure 2a</ref> compares the makespan of this workflow on five different systems. Two of these are programmerfriendly SQL-like front-ends (Hive, Lindi), while the others require the user to program against a lower-level API (Hadoop, Metis and Spark). For small inputs (≤ 0.5GB), the Metis single-machine MapReduce system performs best. This matters, as small inputs are common in practice: 40-80% of Cloudera customers' MapReduce jobs and 70% of jobs in a Facebook trace have ≤ 1GB of input <ref type="bibr" target="#b7">[8]</ref>.</p><p>I/O efficiency. Once the data size grows, Hive, Spark and Hadoop all surpass the single-machine Metis, not least since they can stream data from and to HDFS in parallel. However, since there is no data re-use in this workflow, Spark performs worse than Hadoop: it loads all data into a distributed inmemory RDD <ref type="bibr" target="#b42">[43]</ref> before performing the projection. The Lindi front-end implementation for Naiad performs surprisingly poorly; we tracked this down to an implementation decision in the Naiad back-end, which uses only a single input reader thread per machine, rather than having multi-threaded parallel reads. Since the PROJECT benchmark is primarily limited by I/O bandwidth, this decision proves detrimental.</p><p>Data structure. Second, we consider a JOIN workflow. This is highly dependent on the structure of the input data: it may generate less, more, or an equal amount of output compared to its input. We therefore measure two different cases: (i) an input-skewed, asymmetric join of the 4.8M vertices of a social network graph (LiveJournal) and its 69M edges, and (ii) a symmetric join of two uniformly randomly generated 39M row data sets. <ref type="figure" target="#fig_2">Figure 2b</ref> shows the makespan of different systems (plus a simple implementation in serial C code) for this workflow. The unchallenging asymmetric join (producing 1.28M rows/1.9GB) works best when executed in single-threaded C code on a single machine, as the computation is too small to amortize the overheads of distributed solutions. The far larger symmetric join (1.5B rows/29GB), however, works best on Hadoop. Other systems suffer from inefficient I/O (e.g., Lindi using a single-threaded writer),</p><p>The bottleneck here is not the computation, but reading the data from HDFS. With the data already local, Metis performs best up to 2 GB. or have overhead due to constructing in-memory state and scheduling tasks sub-optimally (Spark).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Iterative graph processing</head><p>Many common workflows involve iterative computations on graphs (e.g., social networks). In the following, we compare different systems running PageRank on such graphs. We also vary the size of the EC2 cluster (m1.xlarge instances) in order to determine systems' efficiency at different scales.</p><p>Performance. Several specialized graph processing systems based on a vertex-centric or gather-and-scatter (GAS) approach have been built. These computation paradigms are limited, but can deliver significantly better performance for graph workloads. In <ref type="figure" target="#fig_4">Figure 3</ref>, we show the makespan of a five-iteration PageRank workflow on the small Orkut and the large Twitter graph. It is evident that graph-oriented paradigms have significant advantages for this computation: a GraphLINQ implementation running on Naiad outperforms all other systems. <ref type="bibr" target="#b3">4</ref> PowerGraph also performs very well, since its vertex-centric sharding reduces the communication overhead that dominates PageRank. Resource efficiency. However, the fastest system is not always the most efficient. While PageRank in GraphLINQ using 100 Naiad nodes has the lowest runtime in <ref type="figure" target="#fig_4">Figure 3b</ref>, PowerGraph performs better than GraphLINQ when using only 16 nodes (due to its improved sharding). <ref type="bibr" target="#b4">5</ref> Moreover, when the graph is small (e.g., in <ref type="figure" target="#fig_4">Figure 3a</ref>), GraphChi performs only 50% worse than Spark on 100 nodes, and only slightly worse than PowerGraph on 16 nodes, despite using only one machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Summary</head><p>Our experiments show that the "best" system for a given workflow varies considerably. The right choice -i.e., the fastest or most efficient system -depends on the workflow, the input data size and the scale of parallelism available. This information may not be available at workflow implementation time, which motivates our approach of decoupling workflow expression from the execution engine used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">All for one, one for all data processing</head><p>We believe that a decoupled data processing architecture ( <ref type="figure">Figure 4)</ref> gives users additional flexibility. In this approach, we break the execution of a data processing workflow into three layers. First, a user specifies her workflow using a front-end framework. Next, this workflow specification is translated into an intermediate representation. Third, jobs are generated from this representation and executed on one or more back-end execution engines. We give an overview of the three layers below; §4 will describe their realization in our Musketeer prototype.</p><p>Front-ends. User-facing high-level abstractions for workflow expression ("frameworks") act as front-ends to the system. Many such frameworks exist: SQL-like querying languages and vertex-centric graph abstractions are especially popular. We assume that users write their workflows for such frameworks. The front-end workflow specifications must then be translated to a common form; we do this by either parsing the user input directly, or by building an APIcompatible shim for the front-end.</p><p>Intermediate representation. Ideally, all available frontend frameworks and back-end execution engines would agree on a single common intermediate representation (IR). The IR must simultaneously (i) be sufficiently expressive to support a broad range of workflows, and (ii) maintain enough information to optimize back-end job code to a level competitive with a competent hand-coded implementation.</p><p>Our intermediate representation is a dynamic directed acyclic graph (DAG) of data-flow operators, with edges corresponding to input-output dependencies. This abstraction is general: it supports specific operator types (cf. Dryad's vertices <ref type="bibr" target="#b18">[19]</ref>) and general user-defined functions (UDFs); it can handle iteration by successive expansion of the DAG (as in CIEL <ref type="bibr" target="#b31">[32]</ref> and Pydron <ref type="bibr" target="#b29">[30]</ref>); and it can be extended with new operators in order to enable end-to-end optimizations.</p><p>We can also perform query optimizations on the dataflow DAG (e.g., to reduce intermediate data volume where possible), as is already commonly done between front-end and back-end in other systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Back-ends. Finally, the system must generate code for specific distributed data processing systems ("execution engines") at the back-end. A naïve approach would simply generate a job for each operator, but this fails to exploit opportunities for optimization within the execution engines (e.g., sharing data scans). Instead, we typically want to run as few independent jobs as possible.</p><p>However, some execution engines have limited expressivity and therefore require the data-flow DAG to be partitioned into multiple jobs. Many valid partitioning options exist, depending on the workflow and the execution engines available. In §5, we show that exploring this space is an instance of an NP-hard problem (k-way graph partitioning), and introduce a heuristic to solve it efficiently for large DAGs.</p><p>Given a suitable partitioning, we generate jobs for the chosen execution engines and dispatch them for execution.</p><p>Extensibility. Our approach is extensible: new front-end frameworks can be added by providing translation logic from framework constructs to the intermediate representation. Similarly, further back-end execution engines can be supported as they emerge by adding appropriate code templates and code generation logic.</p><p>Limitations. Decoupling increases flexibility, but it may obfuscate some end-to-end optimization opportunities from expert users. Our scheme is best suited for non-specialist users writing analytics workflows for high-level front-end frameworks. This is common in industry: up to 80% of jobs running in production clusters come from front-end frameworks such as Pig <ref type="bibr" target="#b34">[35]</ref>, Hive <ref type="bibr" target="#b39">[40]</ref>, Shark <ref type="bibr" target="#b40">[41]</ref> or DryadLINQ <ref type="bibr" target="#b41">[42]</ref>, according to a recent study <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Musketeer implementation</head><p>Musketeer is our proof-of-concept implementation of the decoupled "all for one, one for all" approach that we advocate. It translates a workflow defined in a front-end framework into an intermediate representation, applies optimizations and generates code for suitable back-end execution engines. In this section, we describe Musketeer in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Workflow expression</head><p>Distributed execution engines simplify data processing by shielding users from the intricacies of writing parallel, faulttolerant code. However, they still require users to express their computation in terms of low-level primitives, such as map and reduce functions <ref type="bibr" target="#b9">[10]</ref> or message-passing ver-1 SELECT id, street, town FROM properties AS locs; 2 locs JOIN prices ON locs.id = prices.id Listing 1: Hive code for max-property-price workflow.</p><p>tices <ref type="bibr" target="#b33">[34]</ref>. Hence, higher-level "frameworks" that expose more convenient abstractions are commonly built on top.</p><p>Musketeer supports two types of front-end frameworks: (i) SQL-like query languages, and (ii) vertex-centric graph processing abstractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">SQL-like data analytics queries</head><p>Query languages based on SQL are used to express relational queries on data of tabular structure. Listing 1 shows an example analytics workflow that computes the most expensive property on each street for a real-estate data set.</p><p>Musketeer currently supports three SQL-like data analytics front-ends: Hive, Lindi and BEER, our own domainspecific workflow language with support for iteration. Translation from these front-ends to the intermediate representation proceeds by mapping the relational operations to operators in the IR DAG; most relational primitives have directly corresponding Musketeer IR operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Graph computations</head><p>Domain-specific front-end frameworks for expressing graph computations are popular: Pregel <ref type="bibr" target="#b25">[26]</ref> and Giraph abstract over MapReduce, the GreenMarl DSL <ref type="bibr" target="#b17">[18]</ref> can emit code for multi-threaded and distributed runtimes, and GraphLINQ offers graph-specific APIs over Naiad vertices and timestamps <ref type="bibr" target="#b27">[28]</ref>. These front-ends include user-defined code that is concurrently instantiated for every vertex, and adjacent vertices communicate using messages in repeated rounds. This vertex-centric programming pattern is generalized by the Gather, Apply and Scatter (GAS) model in Power-Graph <ref type="bibr" target="#b13">[14]</ref>. In this paradigm, data are first gathered from neighboring nodes, then vertex state is updated and, finally, the new state is disseminated (scattered) to the neighbors.</p><p>Musketeer currently supports graph computations via a domain-specific front-end framework built around combining the GAS model with our BEER DSL. Users run graph computations by defining the three GAS steps, with each step represented by relational operators or UDFs. In Listing 2, we show the implementation of PageRank in Musketeer's GAS front-end framework.</p><p>While HiveQL and our BEER DSL have directly corresponding operators in the IR, the GAS DSL requires both syntactic translation and transformation from the vertexcentric paradigm to the data-flow DAG. Musketeer uses idiom recognition to achieve this, which we describe in §4.3.1.</p><formula xml:id="formula_0">GATHER = { 2 SUM (vertex_value) 3 } 4 APPLY = { 5 MUL [vertex_value, 0.85] 6 SUM [vertex_value, 0.15] 7 } 8 SCATTER = { 9 DIV [vertex_value, vertex_degree] 10 } 11 ITERATION_STOP = (iteration &lt; 20) 12 ITERATION = { 13 SUM [iteration, 1]) 14 }</formula><p>Listing 2: Gather-Apply-Scatter DSL code for PageRank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Other workloads</head><p>In addition to SQL-like query languages and GAS-style graph computations, Musketeer can also support other types of front-ends. If their abstractions map to Musketeer IR operators, they can be translated directly. Abstractions for which no IR operator exists can be mapped to a user-defined function (UDF), or to a "native" back-end via a "black box" operator. We discuss extension to other front-ends in §8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Intermediate representation</head><p>Musketeer uses a directed acyclic graph (DAG) of dataflow operators as its intermediate representation. We chose this abstraction because it is expressive <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref> and amenable to analysis and optimization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Musketeer's set of operators is extensible: not all frontends use all operators, and not all back-ends must support all operators. Our initial set of operators is loosely based on relational algebra and covers the most common operations in industry workflows <ref type="bibr" target="#b7">[8]</ref>. It includes SELECT, PROJECT, UNION, INTERSECT, JOIN and DIFFERENCE, plus aggregators (AGG, GROUP BY), column-level algebraic operations (SUM, SUB, DIV, MUL), and extremes (MAX, MIN). This set of operators is, in our experience, already sufficient to model many widely-used processing paradigms. For example, MapReduce workflows can be directly modeled as a MAP, GROUP BY and AGG step, and many complex graph workflows can be mapped to a specific JOIN, MAP, GROUP BY pattern, as shown by GraphX <ref type="bibr" target="#b14">[15]</ref> and Pregelix <ref type="bibr" target="#b2">[3]</ref>.</p><p>However, workflows may also involve iterative computations. To allow data-dependent iteration, Musketeer must be able to dynamically extend the IR DAG based on operators' output. We use a WHILE operator to do this: it successively extends the DAG every time another iteration is required. As shown by Murray <ref type="bibr">[33, §3.3.3</ref>], a DAG with this facility is sufficient to achieve Turing completeness as it can express all while-programs (though not necessarily efficiently).</p><p>Optimizing the IR. Many front-end frameworks already optimize workflows before execution. For example, Pig <ref type="bibr" target="#b34">[35]</ref>  Hive <ref type="bibr" target="#b39">[40]</ref>, Shark <ref type="bibr" target="#b40">[41]</ref> and SparkSQL optimize relational queries via rewriting rules and FlumeJava <ref type="bibr" target="#b5">[6]</ref>, Optimus <ref type="bibr" target="#b20">[21]</ref> and RoPE <ref type="bibr" target="#b0">[1]</ref> apply optimizations to DAGs. Yet, each such optimization must be implemented independently for each front-end framework.</p><p>One of the advantages of decoupling front-ends from back-ends is the ability to apply optimizations at the intermediate level, as observed e.g., in the LLVM modular compiler framework <ref type="bibr" target="#b24">[25]</ref>. Musketeer can likewise provide benefits to all supported systems (and future ones) by applying optimizations to the intermediate representation.</p><p>We currently perform a small set of standard query rewriting optimizations on the IR. Most of these re-order operators -e.g., bringing selective ones closer to the start of the workflow and pushing generative operators to the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Code generation</head><p>After Musketeer has translated the workflow to the intermediate representation, it must generate code for execution from the IR. For the time being, we assume that the user explicitly specifies which back-end execution engines to use; in §5, we show how Musketeer can decide automatically.</p><p>Musketeer has code templates for specific combinations of operators and back-ends. Conceptually, it instantiates and concatenates these templates to produce executable jobs. In practice, however, optimizations are required to make the performance of the generated code competitive with handwritten baselines. Musketeer uses traditional database optimizations (e.g., sharing data scans and operator merging), combined with compiler techniques (e.g., idiom recognition and type inference) to improve upon the naïve approach. In the following, we explain these optimizations with respect to the max-property-price Hive workflow example (Listing 1) and the GAS PageRank example (Listing 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Idiom recognition</head><p>Some back-end execution engines are specialized for a specific type of computation. For example, GraphChi has a vertex-centric computation model and PowerGraph uses the GAS decomposition. Neither system can express computations that do not fit its model. Musketeer must therefore rec-ognize specific computational idioms in the IR to decide if a back-end is suitable for a workflow. Idiom recognition is a technique used in parallelizing compilers to detect computational idioms that allow transformations to be applied <ref type="bibr" target="#b35">[36]</ref>. We use a similar approach to detect high-level paradigms in Musketeer's IR DAG.</p><p>Our prototype detects vertex-oriented graph-processing algorithms in the IR, even if they were originally expressed in a relational front-end (e.g., in Hive instead of the GAS DSL). The idiom is a reverse variant of the way GraphX abstracts graph computation as data-flow operators <ref type="bibr">[15, §3]</ref>. Musketeer looks for a combination of the WHILE and JOIN operators with a GROUP BY operator in a particular structure: the body of the WHILE loop must contain a JOIN operator with two inputs that represent vertices and edges. This JOIN operator must be followed by a GROUP BY operator that groups data by the vertex column.</p><p>This structure maps to the graph computation paradigms as follows: the JOIN on the vertex column represents sending messages to neighbors (vertex-centric model), or the "scatter" phase (GAS decomposition); the GROUP BY is equivalent to receiving messages, or the "gather" step (GAS); and any other operators in the WHILE body are part of the superstep (vertex-centric) or the "apply" step (GAS), updating the state of each vertex.</p><p>Other idioms can also be detected: for example, depending on whether the AGG operator performs an associative or a non-associative (e.g., subtraction, division) aggregation, different operator implementations are appropriate in different back-ends. We plan to support this in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Merging operators</head><p>General-purpose systems such as Spark and Naiad can express complex workflows as a single job. However, more restricted systems only support particular idioms (e.g., Power-Graph, GraphChi) or require multiple jobs to express certain operations (e.g., MapReduce). Each back-end job generated for an execution engine comes with some per-job overhead. Musketeer therefore merges operators in order to reduce the number of jobs executed. Consider an example: MapReduce-based execution engines only support one group-by-key operation per job <ref type="bibr" target="#b34">[35]</ref>. Hence, even a simple workflow like max-property-price requires at least two jobs: (i) Lines 1-3 in Listing 1 ( §4.1.1) result in a job that selects columns from the properties relation and joins the result with the prices relation using id as the key; and (ii) lines 4-5 group by a different key than the prior join, requiring a second job. By contrast, Listing 3 shows simple generated code for the max-propertyprice workflow in Spark, where only one job is required.</p><p>To model these limitations and avoid extra jobs when possible, Musketeer has a set of per-back-end mergeability rules. These indicate whether operators can be merged into one job either (i) bidirectionally, (ii) unidirectionally, or (iii) not at all. If execution engines only support certain idioms, only operator merges corresponding to these idioms are feasible. The operator merge rules are used by the DAG partitioning algorithm ( §5) to decide upon job boundaries. Operator merging is necessary for good performance: in §6.5, we show that it reduces workflow makespan by 2-5×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Sharing data scans</head><p>Operator merging allows Musketeer to execute several operators as a single job, and thus eliminates job creation overheads where possible. However, this is not enough to obtain competitive results compared to hand-coded baselines. For example, the first SELECT and the JOIN operator from the max-property-price in Spark get translated into two map transformations and a join (Listing 3, lines 3-6). The first map selects only the columns required by the workflow, while the second map establishes a key → tuple mapping over which the join is going to be conducted.</p><p>Even though Spark holds the intermediate RDDs in memory, scanning over the data twice yields a significant performance penalty. Musketeer avoids redundant scans by combining them where supported by the back-end. For example, in the optimized generated Spark code (Listing 4) for the max-property-price workflow, the anonymous lambdas from the first two map transformations (Listing 3, lines 4 and 6) are combined into a single one (Listing 4, lines 5-6). As a result, the generated code only scans the data once, selecting the required columns and preparing the relation for the join transformation in one go.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Look-ahead and type inference</head><p>Many execution engines (e.g., Spark and Naiad) expose a rich API for manipulating different data types. For example, the SELECT . . . GROUP BY clause in the maxproperty-price workflow (Listing 1, lines 4-5) can be implemented directly in Spark using a reduceByKey transformation. However, such API calls often require a specific representation of the input data. In the example, Spark's reduceByKey requires the data to be represented as a set of key, value tuples. Unfortunately, the preceding join transformation outputs the data in a different format (viz. key, left relation, right relation ). Hence, the naïve generated code for Spark ends up generating two map transformations, one to flatten the output of the join (Listing 3, line 6), and another to key the relation by a town, street tuple (line 8).</p><p>To mitigate this, Musketeer looks ahead and uses type inference to determine the input format of the operators that ingest the current operator's output. With this optimization, the two map transformations can be expressed as a single transformation (Listing 4, lines 5-6). In combination with shared scans, look-ahead and type inference enable Musketeer to guarantee that no unnecessary data scans will take place in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DAG partitioning and automatic mapping</head><p>To generate back-end jobs, Musketeer partitions the IR DAG into sub-regions, each representing a job. As we explained in §4.3.2, some execution engines constrain the operators that can be combined in a single job. Our method of partitioning the IR DAG must therefore generalize over different back-end constraints and extend to future systems' properties. Hence, we consider all possible partitionings; when this is too expensive, we apply an efficient heuristic based on dynamic programming ( §5.1).</p><p>Provided that back-ends' relative performance can be predicted with reasonable accuracy, Musketeer can also automatically decide which back-ends to use. The goodness of different options is quantified using a simple cost function that considers information specific to both workflows and back-ends ( §5.2).  <ref type="figure">Figure 6</ref>: The dynamic partitioning heuristic takes an IR DAG, (1) transforms it to a linear order, and (2) computes job boundaries via dynamic programming. On the right, we show several possible partitions and system mappings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">DAG partitioning</head><p>There are many ways of breaking the IR DAG into partitions (≡ back-end jobs). Musketeer uses a simple cost function to compare different partitioning options. The cost of any partition containing non-mergeable operators is infinite; otherwise it is finite and depends on the back-ends (see §5.2). With a known optimal number of jobs, k, partitioning the DAG is an instance of the k-way graph partitioning problem <ref type="bibr" target="#b21">[22]</ref>. Unfortunately, k-way graph partitioning is NP-hard <ref type="bibr" target="#b12">[13]</ref>: the best solution is guaranteed to be found only by exploring all k-way partitions. Moreover, the optimal number of jobs into which to partition the DAG is unknown. Hence, Musketeer must solve k-way graph partitioning for all k ≤ N, where N is the number of operators in the DAG.</p><p>Where possible, Musketeer uses an exhaustive search to find the cheapest partitioning (in practice, up to about 18 operators). It switches to a dynamic programming heuristic for larger, more complex workflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Exhaustive search</head><p>The exhaustive search explores all possible graph partitionings. It first considers the cost of running each operator in isolation. Next, it looks at all merge opportunities, and finally, it recursively generates all valid (finite-cost) partitions. The algorithm is guaranteed to find the optimal solution with respect to the cost function. However, it requires exponential time in the number of operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Dynamic heuristic</head><p>Some industry workflows consist of large DAGs containing up to hundreds of operators <ref type="bibr">[6, §6.2]</ref>. To support these workflows, we use a dynamic heuristic. Its execution time scales linearly with the number of operators, and it obtains good solutions in practice. The dynamic heuristic explores only a subset of the possible partitions by focusing on a single linear ordering of operators. In <ref type="figure">Figure 6</ref>, we illustrate the algorithm using the IR DAG for PageRank (Listing 2).</p><p>First, Musketeer topologically sorts the DAG to produce a linear ordering. This ordering maintains operator precedence -i.e., an operator does not appear in the linear ordering before any of its ancestors. Second, Musketeer finds the optimal partitioning of the linear ordering using dynamic programming In other words, we determine the best combination that runs a k-element prefix of operators in m − 1 jobs and the remainder in a single job. This approach finds a good solution because it considers all partitions of the linear ordering. The cost function guides it to merge as many operators as possible within each individual job.</p><p>The dynamic heuristic can miss out on opportunities to merge operators due to the linear ordering breaking operator adjacencies. We discuss this further and show an example in §8; in practice, we found the dynamic heuristic to work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Automatic system mapping</head><p>A simple extension of the DAG partitioning algorithm allows Musketeer to automatically choose back-end execution engine mappings. To achieve this, we use Musketeer's cost function and run the DAG partitioning algorithm for all back-ends. We then pick the best k-way partitioning.</p><p>The cost function scores the performance of a particular combination of operators, input data and execution engine. The score is based on three high-level components:</p><p>1. Data volume. Each operator has bounds on its output size based on its behavior (e.g., whether it is generative or selective). These bounds are applied to the run-time input data size to predict intermediate and output sizes. 2. Operator performance. In a one-off calibration, Musketeer measures each operator in each back-end and records the rate at which it processes data. 3. Workflow history. Musketeer collects information about each job it runs (e.g., runtime and input/output sizes), and uses this information to refine the scores for subsequent runs of the same workflow.</p><p>The operator performance calibration only requires modest one-off profiling for a deployed cluster. It supplies Musketeer with the four rates listed in <ref type="table" target="#tab_3">Table 1</ref>. PULL and PUSH quantify read and write HDFS throughput at the start and end of a job. We measure them using a "no-op" operator. LOAD,  by contrast, corresponds to back-end-specific data loading or transformation steps (e.g., partitioning the input in Pow-erGraph). Finally, PROCESS approximates the rate at which the operator's computation proceeds. In some systems, we measure this directly, while in others, we subtract the estimated duration of the ingest (from PULL) and output (from PUSH) stages from the overall runtime to obtain PROCESS. This information lets us estimate the benefit of shared scans: we pay the cost of PULL, LOAD and PUSH just once (rather than once per-operator) and combine those with the costs of PROCESS for all the operators.</p><p>These rate parameters enable generic cost estimates, but we can achieve more accurate scoring by using workflowspecific historical information. When a workflow first executes, no such information is available. Musketeer thus applies conservative data size bounds and only merges selective operators and generative operators with small output bounds. As a result, more jobs may be generated on the first execution -e.g., due to JOIN operators, which have unknown data size bounds. On subsequent executions, Musketeer tightens the bounds using historical information, which may unlock additional merge opportunities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>Musketeer's goal is to improve flexibility and performance of data processing workflows by dynamically mapping from front-end frameworks to back-end execution engines. In this section, we show that Musketeer meets these goals:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup</head><p>Most of our experiments run on a 100-node cluster of m1.</p><p>xlarge instances on Amazon EC2. However, we also run some experiments on a local, dedicated seven-machine cluster with low variance in performance. We deployed all systems supported by Musketeer 6 on these clusters. We use a shared HDFS as the storage layer; this makes sense as HDFS is already supported by Hadoop, Spark and PowerGraph. In order to establish a level playing field for our experiments, we tuned and modified some systems (see <ref type="table" target="#tab_5">Table 2</ref>).</p><p>Metrics. As in §2, the makespan of a workflow refers to its total execution time, measured from its launch to the final result appearing in HDFS. This includes time to load inputs from HDFS, pre-process or transform them (e.g. in PowerGraph and GraphChi) and write the outputs back. As a result, the numbers we present are not directly comparable to those in some other papers, which measure the actual computation time only.</p><p>Resource efficiency, on the other hand, is a measure of the efficiency loss incurred due to scaling out over multiple machines. We compute it by normalizing a workflow's fastest single-node execution (assumed to be maximally resource-efficient) to its aggregate execution time over all nodes in a distributed system. For example, a workflow that runs for 30s on all 100 nodes of the EC2 cluster has an aggregate execution time of 3,000s. If the best single-node system completes the same workflow in 2,000s, the resource efficiency of the distributed execution is 66%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dynamic mapping to back-end execution engines</head><p>We consider both batch and iterative graph processing workflows to investigate the benefits of Musketeer's ability to dynamically map workflows to back-ends. This mirrors our motivational experiments in §2.</p><p>Batch workflows. To illustrate the flexibility offered by Musketeer, we run query 17 from the TPC-H business decision benchmark using the HiveQL and Lindi front-ends. <ref type="figure" target="#fig_12">Figure 7</ref> shows the resulting makespan as the data size increases from 7.5 GB (scale factor 10) to 75 GB (factor 100).   When running the Hive workflow directly using its native Hadoop back-end, the makespan ranges from 200-400s.</p><p>Musketeer, however, can map the Hive workflow specification to different back-ends. In this case, mapping it to Naiad reduces the makespan by 2×. This is not surprising: Hive must run the workflow as three Hadoop jobs due to the restrictive MapReduce paradigm, while Naiad can run the entire workflow in one job.</p><p>However, a user might also specify the workflow using the Lindi front-end and target Naiad directly. When using Lindi however, the query scales less well than using Hive and Hadoop, despite running in Naiad. This result comes because Lindi's high-level GROUP BY operator is non-associative, meaning that data must be collected on a single machine before the operator can be applied. Musketeer supplies an improved GROUP BY operator implemented against Naiad's low-level vertex API. Consequently, its generated Naiad code scales far better than the Lindi version (up to 9× at scale 100). The Naiad developers may of course improve Lindi's GROUP BY in the future, but this example illustrates that Musketeer's decoupling can improve performance even for a front-end's native execution engine by generating improved code.</p><p>Iterative workflows. While batch workflows can be expressed using SQL-like front-end frameworks such as Hive and Lindi, iterative graph processing workflows are typically expressed differently (see §4.1.2). To evaluate Musketeer's benefit for graph computations, we implemented PageRank using our GAS DSL front-end (Listing 2, §4.1.2). We run this workflow on the two social network graphs we evaluated PageRank on in §2 (Orkut and Twitter). <ref type="figure" target="#fig_11">Figure 8</ref> compares the makespan of a five iterations of PageRank using Musketeer-generated jobs to hand-written baselines for general-purpose systems (Hadoop, Spark), an implementation using Naiad's GraphLINQ front-end and special-purpose graph processing systems (PowerGraph, GraphChi). Different systems achieve their best performance at different scales, and we only show the best result for each system. The only exception to this is GraphLINQ on Naiad, which is competitive at both 16 and 100 nodes. At each scale, Musketeer's best mapping is almost as good as the bestin-class baseline. On one node, Musketeer does best when mapping to GraphChi, while a mapping to Naiad (Orkut) or PowerGraph (Twitter) is best at 16 nodes, and a mapping to Naiad is always best at 100 nodes. <ref type="figure" target="#fig_11">Figure 8c</ref> shows the resource efficiency for the same configurations for PageRank on the Twitter graph. Musketeer achieves resource efficiencies close to the best stand-alone implementations at all three scales.</p><p>This demonstrates that Musketeer's dynamic mapping approach adds flexibility for batch and iterative computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Combining back-end execution engines</head><p>In addition to mapping an entire workflow to different backends, Musketeer can also map parts of a workflow to different systems. We find that this ability to explore many different combinations of systems can yield useful (and sometimes surprising) results.</p><p>We use the hybrid cross-community PageRank workflow to demonstrate this. This workflow yields the relative popu-  <ref type="figure">Figure 9</ref>: A cross-community PageRank workflow is accelerated by combined back-ends. Local cluster, all jobs apart from the "Lindi &amp; GraphLINQ" combination were generated by Musketeer. Error bars show the min/max of 3 runs.</p><p>larity of the users present in both of two web communities. It involves a batch computation followed by an iterative computation: first, the edge sets of two communities (e.g., all LiveJournal and WordPress users) are intersected, and subsequently, the PageRank of all links present in both communities is computed. <ref type="figure">Figure 9</ref> shows the makespan of cross-community PageRank for different combinations of systems, explored using Musketeer. <ref type="bibr" target="#b6">7</ref> The inputs are the LiveJournal graph (4.8M nodes and 68M edges) and a synthetically generated web community graph (5.8M nodes and 82M edges). Out of the three single-system executions, the workflow runs fastest in Lindi at 153s. However, the makespan is comparable when Musketeer combines Hadoop with a special-purpose graph processing system (e.g., PowerGraph), even though these systems use fewer machines. This happens because generalpurpose systems (like Hadoop) work well for the batch phase of the workflow, but do not run the iterative PageRank as fast as specialized systems. However, a combination of Lindi and GraphLINQ, which both use Naiad as their back-end execution engine, does even better. This comes as this combination avoids the extra I/O to move intermediate data across system boundaries. Musketeer currently does not fully automatically generate the low-level Naiad code to combine Lindi and GraphLINQ; we will support this in future work.</p><p>Musketeer's ability to flexibly partition a workflow makes it easy to explore different combinations of systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Overhead over hand-tuned, non-portable jobs</head><p>For Musketeer to be attractive to some users, its generated code must add minimal overhead over an optimized handwritten implementation. In the following, we show that the overhead over an optimized baseline does not exceed 30% and is usually around 5-20%.</p><p>Batch processing. We measure the NetFlix movie recommendation workflow <ref type="bibr" target="#b1">[2]</ref>. This workflow highlights any overheads: it contains a large number of operators <ref type="bibr" target="#b12">(13)</ref> and is <ref type="bibr" target="#b6">7</ref> The 100-node EC2 cluster had similar results, albeit at increased variance.  very data-intensive, with up to 600 GB of intermediate data generated. The workflow takes two inputs: a 100 millionrow movie ratings table (2.5GB) and a 17,000-row movie list (0.5MB). The algorithm computes movie recommendations for all users, and finally outputs the top recommended movie for each user. We control the amount of data processed by the algorithm by varying the number of movies used for the prediction. <ref type="figure" target="#fig_0">Figure 10</ref> compares Musketeer-generated code for the NetFlix workflow to hand-optimized baselines for the three general-purpose systems that support it (Hadoop, Spark and Lindi on Naiad). We extensively tuned each of the baselines to deliver good performance for the given system, taking advantage of system-specific optimizations available. For all three systems, the overhead added by Musketeer's generated code is low: it is virtually non-existent for Naiad and remains under 30% for Spark and Hadoop even as the input grows. The remaining overhead for Spark is primarily due to the simplicity of our type-inference algorithm, which can cause the Musketeer-generated code to make an extra pass over the data.</p><p>Graph processing. We also measure Musketeer's overhead for the iterative PageRank workflow. <ref type="figure" target="#fig_0">Figure 11</ref> shows the overhead of Musketeer-generated jobs over hand-written baselines for the back-ends compatible with the PageRank workflow. The average overhead remains below 30% in all cases. Variability in overhead (and improvements over the (a) top-shopper workflow running on the EC2 cluster.  In conclusion, Musketeer generates code that performs nearly as well as hand-written baselines. Combined with the improved portability and the ability to dynamically explore multiple execution engines, we believe that this makes for a compelling case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Impact of operator merging and shared scans</head><p>One key technique that Musketeer uses to reduce overhead is operator merging ( §4.3.2). We measure its impact on workflow makespan using a simple micro-benchmark: the top-shopper workflow. This benchmark finds the largest spenders in a certain geographic region by first filtering a set of purchases by region, then aggregating their values by user ID and finally selecting all users above a threshold. The workflow consists of three operators that can be merged into a single job, and indeed a single scan of the data. <ref type="figure" target="#fig_0">Figure 12a</ref> shows top-shopper's makespan for varying data size with operator merging turned off and on. In <ref type="figure" target="#fig_0">Figure 12b</ref>, we show that cross-community PageRank sees the same benefit.</p><p>These results illustrate that the impact of operator merging can be significant: we observe a one-off reduction in makespan of ≈25-50s due to avoiding per-job overheads, along with an additional 5-10% linear benefit per 10M users attributable to the use of shared scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">DAG partitioning runtime</head><p>Next, we focus on the DAG partitioning algorithm ( §5). We measure the time it takes the exhaustive search and dynamic heuristic algorithms to partition the operator DAG. Ideally, they should not noticeably affect the total runtime of the workflow. <ref type="figure" target="#fig_0">Figure 13</ref> compares the runtimes of the two algorithms as the number of operators in a workflow increases. In the experiment, we run subsets of an extended version of the NetFlix workflow with a total of 18 operators. This workload affords many operator merging opportunities, thus making a good test case for the DAG partitioning algorithms. Up to 13 operators, the exhaustive search runs in under a second, but its runtime grows exponentially beyond 13 operators. While it guarantees that the optimal partitioning subject to the cost function is found, the delay soon becomes impractical. The dynamic programming heuristic, however, runs in under 10ms even at 18 operators and scales gracefully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Automated mapping performance</head><p>Musketeer can be used to manually map jobs to back-end execution engines, but we believe that the framework choice should be automated. We first investigate the quality of Musketeer's automated mapping decisions ( §5.2) using the workflows discussed so far, and then test its performance on two additional workflows. We tested Musketeer's automated choices using the six workflows described before in 33 different configurations by varying the input data size. For each decision, we compare (i) Musketeer's choice on the first run (with no workflowspecific history), (ii) its choice with incrementally acquired partial history, and (iii) the choice it makes when it has a full history of the per-operator intermediate data sizes. We also  compare the choices to those that emerge from a decision tree that we developed. The decision tree considers different back-ends' features and known characteristics. We consider a choice that achieves a makespan within 10% of the best option to be "good", and one within 30% as "reasonable". <ref type="figure" target="#fig_0">Figure 14</ref> shows the results: without any knowledge, Musketeer chooses good or optimal back-ends in about 50% of the cases. When partial workflow history is available, over 80% of its choices are good ones. If each workflow is initially executed operator-by-operator for profiling, Musketeer always makes good or optimal choices. By contrast, using the decision tree yields many poor choices. This is due to its inflexible decision thresholds and its inability to consider the benefits of operator merging and shared scans. We also test the automatic mapping on two new workflows: single-source shortest path (SSSP) and k-means clustering. SSSP can be expressed in vertex-centric systems, while k-means cannot. <ref type="figure" target="#fig_0">Figure 15</ref> shows the workflows' makespan for different back-ends and Musketeer's automated choice. The input for SSSP was the Twitter graph extended with costs, and we used 100M random points for k-means (100 clusters, two dimensions). <ref type="bibr" target="#b7">8</ref> Even with our simple proof-of-concept cost function and a small training set, Musketeer in both cases correctly identifies the appropriate back-end (Naiad).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Practical experience with Musketeer</head><p>System integration complexity. We found the effort it take to integrate a front-end framework or a back-end execution engine with Musketeer to be reasonable. A graduate student typically takes a few days to add full support for another back-end execution engine. Our experiences with front-end frameworks were similar, although the effort required varies depending on their expressivity. Additional time and careful profiling is required to fully optimize the performance of generated code, but such improvements only need to be made once in order to benefit all Musketeer users.</p><p>Our k-means uses the CROSS JOIN operator, which is inefficient. By replacing it, we could reduce the makespan and address Spark's OOM condition. However, we are only interested in the automated mapping here.  <ref type="figure" target="#fig_0">Figure 16</ref>: The dynamic heuristic does not return the minimum-cost partitioning for this workflow: it misses the opportunity to merge JOIN with PROJECT.</p><p>Benefit over hand-coded jobs. To anecdotally compare the performance of Musketeer's generated code to a baseline written by an average programmer, we asked eight CS undergraduate students to implement and optimize the simple JOIN workflow from §2.1 for a given input data set using Hadoop. The best student implementation took 608s, compared to the Musketeer-generated job at 223s. While not a rigorous evaluation, we take this as an indication that using Musketeer offers benefit for non-expert programmers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Limitations and future work</head><p>In the following, we highlight some of the current limitations in Musketeer and how they can be addressed in future work.</p><p>Front-ends ( §4.1). In the future, we see Musketeer offering better support for user-defined functions in front-ends. Many vertex-centric systems, for example, allow the user to specify arbitrary per-vertex code in Java (Giraph) or C++ (GraphChi). This increases flexibility, but restricts the level of optimization that Musketeer can offer. It either limits its choice to back-ends that can directly execute the userprovided code, or requires use of inefficient foreign-function interfaces. In the future, techniques that perform query synthesis on arbitrary user code <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref> might help here.</p><p>Idiom recognition ( §4.3.1). As with many idiom recognition techniques, our approach is sound, but not complete. Musketeer may occasionally fail to detect graph workloads, consequently generating less efficient code. For example, a triangle counting workflow may encounter this problem: the user may represent it as a workflow that joins the edges twice and then filters out the triangles. In the latter case, Musketeer fails to detect the opportunity of running the computation in a graph-oriented execution engine. A "reverse loop unrolling" heuristic that detects when multiple operators take the same input and produce the same output (or a closure thereof) can partly solve this.</p><p>Dynamic DAG partitioning heuristic ( §5.1.2). The dynamic programming heuristic returns the optimal k-way partitioning of a given linear order. However, it may miss fruitful merging opportunities, since it only explores a single linear ordering of operators. In <ref type="figure" target="#fig_0">Figure 16</ref>, we show an example of a workflow for which the dynamic heuristic u n i t s i z e F a u l t t o l e r a n c e L a n g u a g e MapReduce <ref type="bibr" target="#b9">[10]</ref>, Hadoop MapReduce cluster --user-def. large C++/Java Spark <ref type="bibr" target="#b42">[43]</ref> transformations cluster -uniform med. Scala Dryad <ref type="bibr" target="#b18">[19]</ref> static data-flow cluster --user-def. large C# Naiad <ref type="bibr" target="#b33">[34]</ref> timely data-flow cluster () () user-def. small () C# Pregel <ref type="bibr" target="#b25">[26]</ref>, Giraph PowerGraph <ref type="bibr" target="#b13">[14]</ref> vertex-centric cluster -, -uniform, power-law med. () C++ CIEL <ref type="bibr" target="#b31">[32]</ref> dynamic data-flow cluster () -user-def. med. various Serial C code none/serial machine ----small -C Phoenix <ref type="bibr" target="#b36">[37]</ref>, Metis <ref type="bibr" target="#b26">[27]</ref> MapReduce machine --user-def. small -C++ GraphChi <ref type="bibr" target="#b23">[24]</ref> vertex-centric machine --short -C++ X-Stream <ref type="bibr" target="#b37">[38]</ref> edge-centric machine ---med.</p><p>-C++ <ref type="table">Table 3</ref>: A selection of contemporary data processing systems with their features and properties. Systems supported by Musketeer are highlighted in bold. () indicates that the system can be extended to support this feature.</p><p>does not achieve optimality. In the MapReduce paradigm, it makes sense to run the top JOIN in the same job as the PROJECT, but the linear ordering based on depth-first exploration breaks this merge opportunity. This limitation does not affect general-purpose back-ends (e.g., Naiad and Spark), which are able to merge any sub-region of operators. However, merging opportunities are occasionally be missed for systems with restricted expressivity, such as Hadoop and Metis. A simple solution generates multiple linear orderings and runs the heuristic for each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Related Work</head><p>Musketeer is, to our knowledge, the first "big data" workflow manager that decouples front-ends from back-ends and supports multiple execution engines <ref type="table">(Table 3)</ref>. Nonetheless, there is considerable related work:</p><p>Workflow managers. Pig <ref type="bibr" target="#b34">[35]</ref> and Hive <ref type="bibr" target="#b39">[40]</ref> are widely used workflow managers on top of Hadoop that present a SQL-like interface to the user. Shark <ref type="bibr" target="#b40">[41]</ref> replaces Hive's physical plan generator to use Spark RDDs and supports fast interactive in-memory queries. SCOPE <ref type="bibr" target="#b3">[4]</ref> and Tenzing <ref type="bibr" target="#b6">[7]</ref> make the relationship to SQL more explicit, with Tenzing providing an almost complete SQL implementation on top of MapReduce. The semantics of these tools, however, are heavily influenced by the execution engine to which they compile (e.g., Pig relies on COGROUP clauses to delineate MapReduce jobs).</p><p>Dynamic paradigm choice. FlumeJava <ref type="bibr" target="#b4">[5]</ref> defers execution of operations on Java parallel collections until runtime. The implementation of operations is abstracted away from the user and can range from a local iterator to a MapReduce job, depending on data size. QoX <ref type="bibr" target="#b38">[39]</ref> combines databases and data processing engines by separating logical operations and physical implementation, but, unlike Musketeer, is limited to ETL workflows.</p><p>Automatic system tuning. A number of efforts have looked at automatically tuning the configuration of data processing systems. Starfish <ref type="bibr" target="#b16">[17]</ref> automatically infers Hadoop configuration variables, while Jockey <ref type="bibr" target="#b11">[12]</ref> and Quasar <ref type="bibr" target="#b10">[11]</ref> automatically determine the resources to allocate to a workflow in order to meet its deadline or QoS requirements. Musketeer could be extended to perform these tasks as well <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>Musketeer decouples front-end frameworks from back-end execution engines. As a result, users benefit from increased flexibility: workflows can be written once and mapped to many systems, different systems can be combined within a workflow and existing workflows seamlessly ported to new execution engines. Musketeer enables compelling performance gains and its generated code performs almost as well as unportable, hand-optimized baseline implementations. Musketeer is open-source, and available from:</p><p>http://www.cl.cam.ac.uk/netos/musketeer/</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Decoupling front-end frameworks and back-end execution engines (right) increases flexibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>JOIN two data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Different systems perform best for simple queries. Lower is better; error bars show min/max of three runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Twitter (43M vertices, 1.4B edges).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Varying makespan for PageRank on social network graphs; lower is better; error bars: ±σ of 10 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 5 illustrates the different stages a Musketeer workflow proceeds through from specification to execution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 AS id_price; 4 5 GROUP</head><label>345</label><figDesc>SELECT street, town, MAX(price) FROM id_price BY street AND town AS street_price;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Phases of a Musketeer workflow execution. Dotted, gray operators show previous state; changes are in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>locs = properties.map(c =&gt; (c.uid, c.street, c.town)) id_price = locs .map(l =&gt; (l.uid, (l.street, l.town))) .join(prices) .map((key, (l_rel, r_rel)) =&gt; (key, l_rel, r_rel)) street_price = id_price .map(ip =&gt; ((ip.street, ip.town), ip.price) .reduceByKey((left, right) =&gt; Max(left, right)) Listing 3: Naïve Spark code for max-property-price. Four maps are required as data structures must be transformed. locs = properties.map(c =&gt; (c.uid, (c.street, c.town))) id_price = locs .join(prices) .map((key, (l_rel, r_rel)) =&gt; ((l_rel.street, l_rel.town), r_rel.price)) street_price = id_price .reduceByKey((left, right) =&gt; Max(left, right)) Listing 4: Optimized Spark code for max-property-price. Scan sharing and type inference reduce the maps to two.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>. The dynamic programming algorithm uses the cost function, c s (o 1 , o 2 , . . . , o j ), which estimates the cost of running the operators o 1 , o 2 , . . . , o j in a single job. It then computes the matrix C[n][m], which stores the minimum cost of running the first n operators in exactly m jobs: C[n][m] = min k&lt;n C[k][m − 1] + min s (c s (o k+1 . . . o n ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Musketeer's performs close to the best-in-class system for five iterations of PageRank on 100, 16 and 1 nodes. In (a) and (b), the x-axis is makespan (less is better); in (c), it is resource efficiency (more is better). Error bars are ±σ over 5 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Musketeer reduces the makespan of TPC-H query 17 on EC2 compared to Hive and Lindi on native back-ends. Less is better; error bars show min/max of three runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Makespan of the NetFlix movie recommendation workflow on the EC2 cluster. Error bars: ±σ over five runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Generated code overhead for PageRank on the Twitter graph (PG ≡ PowerGraph). Error bars show σ over five runs; negative overheads are due to variance on EC2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Hybrid cross-community PageRank on the local cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 12 :</head><label>12</label><figDesc>Operator merging ( §4.3.2) helps bring generated code performance close to hand-written baselines. baseline) are due to performance variance on EC2. Further optimizations of the code generation are possible. Most such optimizations benefit all code Musketeer generates for a particular back-end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Runtime of Musketeer's DAG partitioning algorithms when considering the first x operators of an extended version of the NetFlix workflow (N.B.: log 10 -scale y-axis). Makespan overhead of Musketeer's automated choice compared to the best option. Workflow history helps, and our cost function outperforms a simple decision tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>k-means clustering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 15 :</head><label>15</label><figDesc>Makespan of SSSP and k-means on the EC2 cluster (5 iterations). A club (♣) indicates Musketeer's choice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>p r o c e s s i n g D e f a u l t s h a r d i n g W o r k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Parameter DescriptionPULL Rate of data ingest from HDFS. LOAD Rate of loading or transforming data. PROCESS Rate of processing operator on in-memory data.PUSH Rate of writing output to HDFS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Rate parameters used by Musketeer's cost function.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>ModificationHadoop Tuned configuration to best practices.Spark Tuned configuration to best practices. GraphChi Added HDFS connector for I/O. Naiad Added support for parallel I/O and HDFS.</figDesc><table><row><cell>System</cell></row><row><cell>1. Legacy workflow speedup: Musketeer reduces legacy</cell></row><row><cell>workflows' makespan by up to 2× by mapping them to a</cell></row><row><cell>different back-end execution system ( §6.2).</cell></row><row><cell>2. Flexible combinations of back-ends: by exploring com-</cell></row><row><cell>binations of multiple execution systems for a workflow,</cell></row><row><cell>Musketeer finds combinations that perform well ( §6.3).</cell></row><row><cell>3. Increased portability: compared to time-consuming,</cell></row><row><cell>hand-tuned implementations for specific back-ends, Mus-</cell></row><row><cell>keteer's automatically generated code has low overhead,</cell></row><row><cell>yet offers superior portability ( §6.4,  §6.5).</cell></row><row><cell>4. Promising automatic system mapping: our automated</cell></row><row><cell>mapping prototype makes good choices based on simple</cell></row><row><cell>parameters characterizing execution engines ( §6.6,  §6.7).</cell></row></table><note>We implemented seven real-world workflows to evaluate Musketeer: three batch workflows, three iterative workflows and a hybrid one. The batch workflows are (i) TPC-H query 17, (ii) top-shopper, which identifies an online shop's top spenders, and (iii) the Netflix movie recommendation algo-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Modifications made to systems deployed.</figDesc><table><row><cell>rithm. The iterative ones are (i) PageRank, (ii) single-source</cell></row><row><cell>shortest path (SSSP), and (iii) k-means clustering; the hybrid</cell></row><row><cell>workflow is PageRank with a batch pre-processing stage.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Note: In the electronic version of this paper, most figures link to descriptions of the experiments and our data sets (http://goo.gl/BMdT0o).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Only the GraphLINQ front-end for Naiad is shown here; Lindi is not optimized for graph computations and performs poorly.<ref type="bibr" target="#b4">5</ref> Running PowerGraph on 32 or 64 nodes showed no benefit over 16 nodes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Hadoop 2.0.0-mr1-chd4.5.0, Spark 0.9, PowerGraph 2.2, GraphChi 0.2, Naiad 0.2 and Metis commit e5b04e2.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Derek G. Murray, Frank McSherry, John Wilkes, Kim Keeton, Robert N. M. Watson, Jon Crowcroft, Tim Harris and our anonymous reviewers for their valuable feedback. Thanks also go to Anne-Marie Kermarrec, our shepherd. Natacha Crooks and Ionel Gog were partly supported by Google Europe Fellowships; parts of this work were supported by the EPSRC INTERNET Project EP/H040536/1, the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL), under contract FA8750-11-C-0249. The views, opinions, and/or findings contained in this paper are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of the DARPA or the Department of Defense.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Re-optimizing data-parallel computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NSDI</title>
		<meeting>NSDI</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The BellKor solution to the Netflix prize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Bell Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pregelix: Big(ger) Graph Analytics on A Dataflow Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tyson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="161" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SCOPE: easy and efficient parallel processing of massive data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shakib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1265" to="1276" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FlumeJava: easy, efficient data-parallel pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raniwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weizenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="363" to="375" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FlumeJava: Easy, Efficient Data-parallel Pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raniwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weizenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PLDI</title>
		<meeting>PLDI</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="363" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tenzing: a SQL implementation on the MapReduce framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aragonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lychagina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive analytical processing in big data systems: a cross-industry study of MapReduce workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alspaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing Database-backed Applications with Query Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of PLDI</title>
		<meeting>PLDI</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MapReduce: a flexible data processing tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quasar: Resourceefficient and QoS-aware Cluster Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASPLOS</title>
		<meeting>ASPLOS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="127" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jockey: guaranteed job latency in data parallel clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroSys</title>
		<meeting>EuroSys</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="99" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Some simplified NP-complete graph problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stockmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="237" to="267" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PowerGraph: Distributed graph-parallel computation on natural graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GraphX: Graph Processing in a Distributed Dataflow Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="599" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Profiling, what-if analysis, and cost-based optimization of MapReduce programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herodotou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1111" to="1122" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Starfish: A Self-tuning System for Big Data Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herodotou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIDR</title>
		<meeting>CIDR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Green-Marl: a DSL for easy and efficient graph analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sedlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASPLOS</title>
		<meeting>ASPLOS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="349" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dryad: Distributed data-parallel programs from sequential building blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fet-Terly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroSys</title>
		<meeting>EuroSys</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="59" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HadoopToSQL: A MapReduce Query Optimizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Iu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroSys</title>
		<meeting>EuroSys</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="251" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimus: a dynamic rewriting framework for data-parallel execution plans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroSys</title>
		<meeting>EuroSys</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An efficient heuristic procedure for partitioning graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Kernighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="291" to="307" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mizan: a system for dynamic load balancing in large-scale graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Khayyat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Awara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alonazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jamjoom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kalnis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EuroSys</title>
		<meeting>EuroSys</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale Graph Computation on Just a PC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">LLVM: A Compilation Framework for Lifelong Program Analysis &amp; Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CGO</title>
		<meeting>CGO</meeting>
		<imprint>
			<date type="published" when="2004-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pregel: a system for large-scale graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Optimizing MapReduce for multicore architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<idno>MIT- CSAIL-TR-2010-020</idno>
		<imprint>
			<date type="published" when="2010-05" />
		</imprint>
		<respStmt>
			<orgName>MIT Computer Science and Artificial Intelligence Laboratory</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GraphLINQ: A graph library for Naiad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<ptr target="http://goo.gl/Q0F9gw;accessed03/10/2014" />
	</analytic>
	<monogr>
		<title level="m">Big Data at SVC blog</title>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ParaTimer: a progress indicator for MapReduce DAGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="507" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pydron: Semi-Automatic Parallelization for Multi-Core and the Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Csil-Laghy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="645" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building new frameworks on Naiad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<ptr target="http://goo.gl/qqvAQv;accessed03/10/2014" />
	</analytic>
	<monogr>
		<title level="m">Big Data at SVC blog</title>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CIEL: a universal execution engine for distributed data-flow computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Smowton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madhavapeddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NSDI</title>
		<meeting>NSDI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="113" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A distributed execution engine supporting data-dependent control flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Naiad: a timely dataflow system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SOSP</title>
		<meeting>SOSP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="439" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pig Latin: A Not-So-Foreign Language for Data Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1099" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Idiom Recognition in the Polaris Parallelizing Compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pottenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eigenmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SC</title>
		<meeting>SC</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="444" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating MapReduce for multi-core and multiprocessor systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ranger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raghuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Penmetsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brad-Ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procedings of HPCA</title>
		<meeting>edings of HPCA</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">X-Stream: edge-centric graph processing using streaming partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mihailovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SOSP</title>
		<meeting>SOSP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optimizing analytic data flows for multiple execution engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simitsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Dayal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="829" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hive -A Warehousing Solution over a Map-Reduce Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thusoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wyckoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1626" to="1629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Shark: SQL and Rich Analytics at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DryadLINQ: A System for General-Purpose Distributed Data-Parallel Computing Using a High-Level Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ú</forename><surname>Erlings-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gunda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NSDI</title>
		<meeting>NSDI</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
