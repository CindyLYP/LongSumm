<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Crowd Counting Using CNN and MRF</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanggen</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Communication and Information Engineering</orgName>
								<orgName type="department" key="dep2">Institute of Smart City</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Crowd Counting Using CNN and MRF</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>crowd counting</term>
					<term>convolutional neural network</term>
					<term>Markov random field</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this paper, we propose a method called Convolutional Neural Network-Markov Random Field (CNN-MRF) to estimate the crowd count in a still image. We first divide the dense crowd visible image into overlapping patches and then use a deep convolutional neural network to extract features from each patch image, followed by a fully connected neural network to regress the local patch crowd count. Since the local patches have overlapping portions, the crowd count of the adjacent patches has a high correlation. We use this correlation and the Markov random field to smooth the counting results of the local patches. Experiments show that our approach significantly outperforms the state-of-the-art methods on UCF and Shanghaitech crowd counting datasets. Code available on GitHub https://github.com/hankong/crowdcounting.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In modern society, more and more people gather to live in the city. This lifestyle provides convenience for people's lives and improves the utilization rate of urban public resources. Meanwhile, a large number of people living in cities also led to urban congestion problems. This crowding phenomenon can be observed at traffic junctions, airports, stations, stadiums and other public places. Many accidents caused by overcrowding have led to many deaths, such as massive stampede happened in Shanghai Bund in 2015, where 36 persons died and 49 persons were injured. Therefore, automatic crowd density estimation and early warning for overcrowding are important to prevent such tragedies from happening again.</p><p>Existing methods for people density estimation and counting of the crowd can be divided into two main categories: direct and indirect approaches <ref type="bibr" target="#b0">[1]</ref>. The main idea of the direct approach (also called object detection based) is to detect and segment each individual in crowd scenes to get the total count, while the indirect approach (also called feature based) takes an image as a whole and extracts some features, and then get the final count through the regression analysis of the extracted features. The advantage of the direct method is that people or head detection have been widely studied and applied, these methods can be easily adapted to the crowd of few tens <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. However, a crowd of more than hundreds does not have a well-defined shape as a single object does, so the direct detection method is not applicable. In this scene, the indirect methods are generally more reliable and efficient, since the overall features of a crowd image are easier to obtain and have a stronger correlation with the number of people. Some surveys of these methods can be seen in <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this paper, we focus on the indirect method to estimate the crowd count of more than hundreds. Like other computer vision tasks, features extraction is the first step and the most important step in the crowd counting problem. Many hand-crafted computer vision features have been used to represent the density of the crowd, such as Local Binary Patterns (LBP) <ref type="bibr" target="#b5">[6]</ref>, Scale Invariant Feature Transform (SIFT) <ref type="bibr" target="#b6">[7]</ref> and Histogram of Oriented Gradient (HOG) <ref type="bibr" target="#b1">[2]</ref>. However, due to the variation of viewpoint, scene and crowd count, these hand-crafted features can not represent the crowd density discriminatively. In recent years, with the advancement of deep learning in computer vision, some researchers try to apply deep learning to crowd density estimation and achieved state-of-the-art results <ref type="bibr" target="#b7">[8]</ref>. Different with the hand-crafted features extraction which follows the certain steps, deep learning can automatically learn the features from the data. Following the certain steps means that the features will not be better with the increase of the data, while deep learning can learn more discriminative features from abundant data.</p><p>Some auxiliary methods are also used to improve the accuracy of the crowd density estimation. For example, in the crowd of more than hundreds, the density of crowd is continuously gradient, so MRF can be used to smooth the counting between adjacent patches <ref type="bibr" target="#b8">[9]</ref>.</p><p>Inspired by the superior feature representation of deep learning and the smoothness of MRF, we propose a CNN and MRF based framework for the problem of people counting in still images. Firstly, we divide the image into patches with overlaps and use a pre-trained CNN model to extract deep features from each overlapping patch, followed by a fully connected deep neural network to regress the patch people count. Finally, we use MRF to smooth the counts of adjacent patches in order to make the counts closer to the true value. The reason is that the overlap be- tween the adjacent patches leads to the people count of adjacent patches keep a certain consistency.</p><p>The rest of the paper is organized as follows. In Section 2, we briefly review the related work of crowd density estimation and counting. And then the system architecture and the implement detail of the proposed approach will be illustrated in Section 3, followed by the experiment in Section 4. Finally, Section 5 concludes our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Direct method. Li et al <ref type="bibr" target="#b9">[10]</ref> proposed a people count estimation method combining the foreground segmentation and the head-shoulder detection. A Mosaic Image Difference based foreground segmentation algorithm is performed first to detect active areas, and then a headshoulder detection algorithm is utilized to detect heads and count the number from the detected foreground areas. Cheriyadat et al <ref type="bibr" target="#b10">[11]</ref> presented an object detection system based on coherent motion region detection for counting and locating objects in the presence of high object density and inter-object occlusions. They used the locations of tracked low-level feature points to construct all possible coherent-motion-regions and chose a good disjoint set of coherent motion regions representing individual objects using a greedy algorithm. Brostow et al <ref type="bibr" target="#b11">[12]</ref> described an unsupervised data-driven Bayesian clustering algorithm to detect individual entities. The method tracked simple image features and probabilistically group them into clusters representing independently moving entities.</p><p>Indirect method. Lempitsky et al <ref type="bibr" target="#b12">[13]</ref> used dense SIFT features and Maximum Excess over SubArrays distance as a loss function to train a regression model on randomly selected patches. In order to adapt to the change of the crowd density and perspective, Zhang et al <ref type="bibr" target="#b7">[8]</ref> proposed a Multi-column CNN architecture to map the image to its crowd density map. The network structure included three parallel CNN with different sizes to extract features from different scales. The features learned by each column CNN were adaptive to variations in people/head size due to perspective effect or image resolution. Shang et al <ref type="bibr" target="#b13">[14]</ref> proposed an end-to-end CNN architecture that directly maps the whole image to the counting result. A pretrained GoogLeNet model was used to extract high-level deep features and the long-short time memory (LSTM) decoders for the local count and fully connected layers for the final count. A cross-scene crowd counting architecture was proposed by Zhang et al <ref type="bibr" target="#b14">[15]</ref>. Firstly, they trained a deep CNN with two related learning objectives, crowd density, and crowd count. And then a data-driven method was introduced to fine-tune the learned CNN to an unseen target scene, where the training samples similar to the target scene were retrieved from the training scenes for fine-tuning.</p><p>Some researchers tried to combine two methods to get a more accurate estimate count. Idrees et al <ref type="bibr" target="#b8">[9]</ref> proposed a multi-source multi-scale counting method to compute an estimation of the number of individuals presented in an extremely dense crowd visible in a single image. This method combined the detection approach (low confidence head detection) and the features regression approach (repetition of texture element and frequency-domain analysis) to solve the perspective, occlusion, clutter and few pixels per person problems in a crowd of more than hundreds. Then the spatial consistency constraints are employed by MRF to smooth the counting between adjacent patches. Rodriguez et al. <ref type="bibr" target="#b15">[16]</ref> addressed the problem of person detection and tracking in crowded video scenes. They explored constraints imposed by the crowd density and formulate person detection as the optimization of a joint energy function combining crowd density estimation and the localization of individual people. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>For dense crowd images, the distribution of crowd density is rarely uniform due to changes in perspective and scene. Some example images can be seen in <ref type="figure" target="#fig_0">Fig. 1</ref>. Therefore, it is unreasonable to count the crowd by taking the image as an entirety. So our framework adapted the divide-count-sum strategy. The images are firstly divided into patches, then a regression model is learned to map the image patch to the local count. Finally, the global image count is computed as the total sum over these patches. The image segmentation has two advantages: Firstly, in the small image patches, the crowd density is approximately uniform distribution. Secondly, the segmentation of image increases the number of training data for the regression model. The above advantages allow us to train a more robust regression model.</p><p>Even though the distribution of crowd density is not uniform, the overall crowd density distribution is continuous. This means that the density of adjacent image patches should be similar. Furthermore, we divide the image with overlaps, which enhances the association between image patches. The Markov random field is used to smooth the estimation count between overlapping image patches to compensate for the possible estimation errors of the image patches and to bring the overall result closer to the true density distribution. The overview of the proposed method can be seen as <ref type="figure">Fig. 2</ref>.  <ref type="figure">Fig. 2</ref>. The overview of our proposed CNN-MRF crowd counting method. The proposed method contains three parts, a pre-trained deep residual network to extract features, a fully connected neural network for count regress and a MRF to smooth the counting results of the local patches. The final count is obtained by the sum of the patches count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Patches counting</head><p>We use a pre-trained deep residual network to extract features from image patches and a fully connected neural network to learn a map from the above features to the local count. The features learned from deep convolutional networks have been used for many computer vision tasks such as image recognition, object detection, and image segmentation <ref type="bibr" target="#b16">[17]</ref>. This indicates that the features learned from the deep convolutional network are universal to many computer vision tasks. With the increase in the number of network layers, the representation ability of the learned features becomes stronger. However, a deeper model means that more data is needed for training. The existing crowd counting datasets are not large enough to train a very deep convolutional neural network from scratch. So we use a pre-trained deep residual network to extract features from image patches. The deep residual network was proposed by He et al. <ref type="bibr" target="#b16">[17]</ref>. Their method addressed the degradation problem by reformulating the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We employ the residual network, which is trained on Im-ageNet dataset for image classification task, to extract the deep features to represent the density of the crowd. This pre-trained CNN network created a residual item for every three convolution layer to bring the layer of the network to 152. We resize the image patches to the size of 224 × 224 as the input of the model and extract the output of the fc1000 layer to get the 1000 dimensional features.</p><p>The features are then used to train 5 layers fully connected neural network. The network's input is 1000dimensional, and the number of neurons in the network is given by 100-100-50-50-1. The network's output is the local crowd count. The learning task of the fully connected neural network is to minimize the mean squared error of the training patches, which can be written as:</p><formula xml:id="formula_0">Loss = 1 M M ∑ i=1 (c g − c r ) 2 . . . . . . . . (1)</formula><p>where M is the number of the training image patches and c g and c r are the ground truth count and the regression count of the image patches, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Images counting</head><p>Due to the overlapping of the adjacent image patches, there is a high correlation between adjacent local people count. This correlation can be used by the Markov random field to smooth the estimation count between adjacent image patches. As previously analyzed, the people count of adjacent images patches is generally similar and may change dramatically at some places due to buildings or other objects in the scene. This characteristic can be well modeled by the Markov random field. Formally, the Markov random field framework for the crowd counting can be defined as follows (we follow the notation in <ref type="bibr" target="#b17">[18]</ref>). Let P be the set of patches in an image and C be a possible set of counts. A counting c assigns a count c p ∈ C to each patch p ∈ P. The quality of a counting is given by an energy function:</p><formula xml:id="formula_1">E(c) = ∑ p∈P D p (c p ) + ∑ (p,q)∈N V (c p − c q ) . . . (2)</formula><p>where N are the (undirected) edges in the four-connected image patch graph. D p (c p ) is the cost of assigning count c p to patch p, and is referred to as the data cost. V (c p −c q ) measures the cost of assigning count c p and c q to two neighboring patch, and is normally referred to as the discontinuity cost.</p><p>For the problem of smoothing the adjacent patches count, D p (c p ) and V (c p − c q ) can take the form of the following functions:</p><formula xml:id="formula_2">D p (c p ) = λ min((I(p) − c p ) 2 , DATA K) . . . (3) V (c p − c q ) = min((c p − c q ) 2 , DISC K) . . . (4)</formula><p>where λ is a weight of the energy items, I(p) is the ground truth count of the patch p, DATA K and DISC K are the truncating item of D p (c p ) and V (c p − c q ), respectively. The truncating item makes the cost function stop growing after the difference becomes large, which allows for large discontinuities. The above energy function minimization problem can be efficiently solved by belief propagation algorithm <ref type="bibr" target="#b17">[18]</ref>. <ref type="figure">Fig. 3</ref> shows the smoothing effect of the MRF on the local counting results. We can see that the density map is closer to the ground truth after smoothed by the MRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Journal of Advanced Computational Intelligence 3 and Intelligent Informatics Images</head><p>Ground Truth Before MRF After MRF <ref type="figure">Fig. 3</ref>. Smoothing of adjacent local counts by MRF. The first column are crowd images, the second, third, and final columns are the ground truth density map, the estimated density map, and the density map smoothed by the MRF, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our method on UCF and Shanghaitech crowd counting datasets. Each image is divided with patch size 100 x 100 pixels and stride size 50 pixels. The final count of the whole image is obtained by calculating the sum of the count of all non-overlapping patches. If the image patch is on the edge and its previous image patch has been summed, then only half count of this image patch will be summed. The proposed method is implemented in Matlab, and we utilize MatConvNet <ref type="bibr" target="#b18">[19]</ref>, a Matlab toolbox implementing CNN for computer vision applications, which provides many pre-trained CNN model for image classification, segmentation, face recognition and text detection.</p><p>We utilize two evaluation criteria: the mean absolute error (MAE) and the mean squared error (MSE), which are defined as follows:</p><formula xml:id="formula_3">MAE = 1 N N ∑ i=1 |g i − e i | . . . . . . . . . . (5) MSE = 1 N N ∑ i=1 (g i − e i ) 2 . . . . . . . . (6)</formula><p>where N is the number of test images and g i and e i are the ground truth and the estimate count of the i-th image, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">UCF dataset</head><p>The UCF dataset <ref type="bibr" target="#b8">[9]</ref> is a very challenging dataset because the scene of the each image are different and the crowd count of the image changes dramatically. More specifically, there are 50 images with counts ranging between 94 and 4543 with an average of 1280 individuals per image. The ground truth positions of individuals are marked by the authors and there is a total of 63705 annotations in the 50 images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MAE MSE Rodriguez et al. <ref type="bibr" target="#b15">[16]</ref> 655.7 697.8 Lempitsky et al. <ref type="bibr" target="#b12">[13]</ref> 493.4 487.1 Idrees et al. <ref type="bibr" target="#b8">[9]</ref> 419.5 541.6 Zhang et al. <ref type="bibr" target="#b14">[15]</ref> 467.0 498.5 MCNN <ref type="bibr" target="#b7">[8]</ref> 295.1 490.2 Shang et al. <ref type="bibr" target="#b13">[14]</ref> 270.3 -Proposed 254.1 352.5 Followed by Idrees et al. <ref type="bibr" target="#b5">[6]</ref>, we use 5-fold crossvalidation to test the performance of our algorithm. Since the image of the UCF dataset is gray, we extend the image to three channels by copying the data. The counting result of our method and the comparison with other methods can be viewed in <ref type="table" target="#tab_2">Table 1</ref>. The experimental results of other methods come from their papers and the same for Shanghaitech <ref type="bibr" target="#b7">[8]</ref> dataset. We can see that our proposed CNN-MRF outperforms the other methods, including the state of the art methods MCNN <ref type="bibr" target="#b7">[8]</ref> and Shang et al. <ref type="bibr" target="#b13">[14]</ref>. In <ref type="figure" target="#fig_1">Fig  4,</ref> we compare the estimated count with the ground truth in more details. The images are divided into 10 groups according to crowd counts in an increasing order. The divide-count-sum strategy narrows the range of the crowd count so that the proposed method can give an accurate estimate of the total counts of the images at all ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Shanghaitech dataset</head><p>Shanghaitech dataset <ref type="bibr" target="#b7">[8]</ref> is a large-scale crowd counting dataset which contains 330,165 people heads annotation in 1,198 images. The dataset consists of two parts, Part A is collected from the Internet and Part B is taken from the busy streets of metropolitan areas in Shanghai. The average crowd count of the Part A is 501.4, and the average number of the Part B dataset is 123.6. The crowd density of Part A is significantly larger than that in Part B. This dataset has been randomly divided into training and testing: 300 images of Part A are used for training and the remaining 182 images for testing, and 400 images of Part B are for training and 316 images for testing. Table 2 reports the results of different methods in the two parts. LBP+RR <ref type="bibr" target="#b7">[8]</ref> is a regression based method which uses Local Binary Pattern (LBP) features extracted from the original image as input and uses ridge regression (RR) to predict the crowd number for each image. Our method significantly outperforms state-of-the-art methods.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a CNN-MRF based approach to counting the crowd in a still image from different scenes. The features extracted from the CNN model trained for other computer vision tasks show a strong ability to represent crowd density. With the overlapping patches divided strategies, the adjacent local counts are highly correlated. This correlation can be used by the MRF to smooth the adjacent local counts to obtain a more accurate overall count. Experimental results demonstrate that the proposed approach achieve superior performance compared with several recent related methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Some example images from UCF crowd dataset. The crowd density become sparse with the view from far to near in image (a). The crowd density of the buildings and mountains is 0 in the image (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>The comparison of the ground truth and the estimated count on UCF dataset. Absolute counts in the vertical axis is the average crowd number of images in each group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>The comparison of the ground truth and the estimated count on Shanghaitech dataset.Similar toFig 4,the comparison of the ground truth and the estimated count on Shanghaitech dataset can be seen inFig 5.We can see that the proposed method can estimate the crowd count accurately and is robust to the large variation in crowd density. Some counting examples of the images with the associated ground truth counts can be seen inFig 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Sample images with their respective ground truth and estimated count. GT is ground truth of the image, and C is the predicted count by the proposed method. Image (a) and (b) from the UCF dataset, (c) and (d) from the Shanghaitech dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparing results of different methods on the UCF dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparing results of different methods on the Shanghaitech dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Method</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Part A MAE MSE MAE MSE Part B</cell></row><row><cell cols="6">LBP+RR [8]</cell><cell></cell><cell></cell><cell cols="3">303.2 371.0 59.1</cell><cell>81.7</cell></row><row><cell cols="11">Zhang et al. [15] 181.8 277.7 32.0</cell><cell>49.8</cell></row><row><cell></cell><cell cols="4">MCNN [8]</cell><cell></cell><cell></cell><cell></cell><cell cols="3">110.2 173.2 26.4</cell><cell>41.3</cell></row><row><cell></cell><cell></cell><cell cols="3">Proposed</cell><cell></cell><cell></cell><cell></cell><cell cols="3">79.1 130.1 17.8</cell><cell>26.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Ground truth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Estimated count</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Absolute Counts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Group ID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Journal of Advanced Computational Intelligence and Intelligent Informatics</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was partially supported by the National Nature Science Foundation of China (No.61373084).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recent survey on crowd density estimation and counting for visual surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Suandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ibrahim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2325" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simulation of Human Detection System Using BRIEF and Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yasuoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shinomiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advanced Computational Intelligence and Intelligent Informatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1159" to="1164" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Survey of Video-Based Crowd Anomaly Detection in Dense Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Junjie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yaping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaoru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advanced Computational Intelligence and Intelligent Informatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="246" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiresolution grayscale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant key-Journal of Advanced Computational Intelligence 5 and Intelligent Informatics points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-source multiscale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detecting multiple moving objects in crowded environments with coherent motion regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Cheriyadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Bhaduri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised bayesian detection of independent motion in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="594" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end crowd counting via joint learning local and global count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1215" to="1219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densityaware person detection and tracking in crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2423" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient belief propagation for early vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="54" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
