{"1013": "moment matching approaches for generative modeling are based on the assumption that one can learn the data distribution by matching the moments of the model distribution to the empirical data distribution. two representative meth- equal contribution. ods of this family are based on maximum mean discrepancy by using a specially designed objective function. we propose a simple but effective moment matching method that : ( 1) breaks away from the problematic min/max game completely; ( 2) does not use online learning of kernel functions; and ( 3) is very efficient with respect to both number of used moments and required minibatch size. our proposed approach, named generative feature matching networks ( gfmn) , learns implicit generative models by performing mean and covariance matching of features extracted from all convolutional layers of pretrained deep convnets. some interesting properties of gfmns include: ( a) the loss function is directly correlated to the generated image quality; ( b) mode collapsing is not an issue; and ( c) the same pretrained feature extractor can be used across different datasets. in this paper, we propose a new feature matching method based on moving averages of the difference of the means of the features extracted by the hidden layer from real and generated data. instead of computing the ( memory ) expensive feature matching loss in eq. 1, we keep moving averages of the difference of feature means ( covariances) at layer j between real and generated data. using these moving averages we replace the first term of the loss given in eq. 1, we can now rely on vj to get better estimates of the population feature means of real and generated data while using a small minibatch of sizen for a similar result using the feature matching loss given in eq. 1, one would need a minibatch of sizen for a similar result using the feature matching loss given in eq. 1 moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages moving averages gfmn is a generative moment matching network (gfmn) that performs mean and covariance matching in a pf space induced by a nonlinear kernel function ( dcnn) that is orders of magnitude larger than the ae latent code, and that we argued is universal in the image domain. gfmn is related to the recent body of work on mmd and moment matching based generative models 22 , 8 , 21 , 3, the closest to our method is the generative moment matching network autoencoder ( gmmnae) proposed in in gmmnae, the objective is to train a generator g that maps from a prior uniform distribution to the latent code learned by a pretrained ae, and then uses the frozen pretrained decoder to map back to image space. gfmn training: gfmns are trained with an adam optimizer; most hyperparameters are kept. gfmn is a feature extractor that can be used to generate images from dcnns. this paper presents a new approach to training implicit generative models through moment matching (gfmn) that uses a cross domain feature extractor and a loss function that only performs feature matching (vgg19 resnet18 ) . compared to recent adversarial mmd methods ( mmd gan) 21 , gfmn also presents significantly better results while avoiding the problematic min/max game. compared to recent learned moments (molm) 33, gfmn achieves better results than the method of learned moments (molm) 33, while using a much smaller number of features to perform matching. the best performing model from 33, molm1536, uses around million moments to train the cifar10 generator, while our best gfmn model uses around 850k moments/features only, almost 50x less. we achieve successful nonadversarial training of implicit generative models by introducing different key ingredients: ( 1 ) moment matching on perceptual features from all layers of pretrained neural networks; ( 2 ) a more robust way to compute the moving average of the mean features by using adam optimizer, which allows us to use small minibatches; and ( 3 ) the use of perceptual features from multiple neural networks at the same time. a feature map is universal if ( y) is dense in c ( z) for all z compact subsets of x.i.e a feature map is universal if ( y) c ( z) . a feature map is universal if ( y) is dense in c ( z) for all z compact subsets of x.i.e a feature map is universal if ( y) c ( z) the objective of this paper is to show that wgan-gp can learn to distinguish between real and fake images. this is a well - known issue with gan training where the training of the generator and discriminator must strike a balance. if a discriminator can distinguish perfectly between real and fake early on, the generator can not learn properly and the min/max game becomes unbalanced, having no good discriminator gradients for the generator to learn from, producing degenerate models. in this paper, we show that wgan-gp can learn to distinguish between real and fake images. the use of perceptual features ( pfs) in the context of learning implicit generative models through moment matching ( mm) is not well studied. more specifically, we propose a new effective mm approach that learns implicit generative models by performing mean and covariance matching of features extracted from pretrained convnets. gfmn is a generative moment matching network autoencoder ( gmmnae) that uses a dcganlike architecture in the generator. the generator is trained with a dcganlike architecture in the generator and then uses the frozen pretrained decoder to map back to image space. gfmns are trained with an adam optimizer (adam optimizer) that uses adam optimizer to compute the moving average of the mean features. gfmns are trained with an adam optimizer (adam optimizer) to perform mean and covariance matching in a pf space induced by a nonlinear kernel function ( a dcnn) that is orders of magnitude larger than the ae latent code, and that we argued is universal in the image domain. the discriminator, being pretrained on imagenet, can quickly learn to distinguish between real and fake images. this limits the reliability of the gradient information from the discriminator, which in turn renders the training of a proper generator extremely challenging or even impossible.", "1000": "###", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1007": "###", "1008": "###", "1009": "###", "1010": "###", "1011": "###", "1012": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1018": "###", "1019": "###", "1020": "###", "1021": "###"}