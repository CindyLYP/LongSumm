<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating images with recurrent adversarial networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-05-29">29 May 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">Jiwoong</forename><surname>Im</surname></persName>
							<email>imdaniel@iro.umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Montreal Institute for Learning Algorithm University of Montreal</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">Dongjoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering and Computer Scienc</orgName>
								<orgName type="institution">York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Engineering and Computer Scienc</orgName>
								<orgName type="institution">York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
							<email>memisevr@iro.umontreal.ca</email>
							<affiliation key="aff3">
								<orgName type="department">Montreal Institute for Learning Algorithm</orgName>
								<orgName type="institution">University of Montreal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generating images with recurrent adversarial networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-05-29">29 May 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1602.05110v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network is a way to render images of high visual quality. Unrolling this gradient-based optimization can be thought of as a recurrent computation, that creates images by incrementally adding onto a visual &quot;canvas&quot;. Inspired by this view we propose a recurrent generative model that can be trained using adversarial training. In order to quantitatively compare adversarial networks we also propose a new performance measure, that is based on letting the generator and discriminator of two models compete against each other.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating realistic-looking images has been a long-standing goal in machine learning. The early motivation for generating images was mainly as a diagnostic tool, based on the belief that a good generative model can count as evidence for the degree of "understanding" that a model has of the visual world (see, example, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, or <ref type="bibr" target="#b15">[16]</ref> and references in these). More recently, due to immense quality improvements over the last two years (for example, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2]</ref>), and the successes of discriminative modeling overall, image generation has become a goal on its own, with industrial applications within close reach.</p><p>Currently, most common image generation models can be roughly categorized into two classes: The first is based on probabilistic generative models, such as the variational autoencoder <ref type="bibr" target="#b10">[11]</ref> and a variety of equivalent models introduced at the same time. The idea in these models is to train an autoencoder whose latent representation satisfies certain distributional properties, which makes it easy to sample from the hidden variables, as well as from the data distribution (by plugging samples into the decoder).</p><p>The second class of generative models is based on adversarial sampling <ref type="bibr" target="#b3">[4]</ref>. This approach forgoes the need to encourage a particular latent distribution (and, in fact, the use of an encoder altogether), by training a simple feed-forward neural network to generate "data-like" examples. "Data-likeness" is judged by a simultaneously trained, but otherwise separate, discriminator neural network.</p><p>For both types of approach, sequential variants were introduced recently, which were shown to work much better in terms of visual quality: The DRAW network <ref type="bibr" target="#b4">[5]</ref>, for example, is a sequential version of the variational autoencoder, where images are generated by accumulating updates into a canvas using a recurrent network. An example of a sequential adversarial network is the LAPGAN model <ref type="bibr" target="#b0">[1]</ref>, which generates images in a coarse-to-fine fashion, by generating and upsampling in multiple steps.</p><p>Motivated by the successes of sequential generation, in this paper, we propose a new image generation model based on a recurrent network. Similar to <ref type="bibr" target="#b0">[1]</ref>, our model generates an image in a sequence of structurally identical steps, but in contrast to that work we do not impose a coarse-to-fine (or any other) structure on the generation procedure. Instead we let the recurrent network learn the optimal procedure by itself. In contrast to <ref type="bibr" target="#b4">[5]</ref>, we obtain very good samples without resorting to an attention mechanism and without variational training criteria (such as a KL-penalty on the hiddens).</p><p>Our model is mainly inspired by a third type of image generation method proposed recently by <ref type="bibr" target="#b1">[2]</ref>. In this work, the goal is to change the texture (or "style") of a given reference image by generating a new image that matches image features and texture features within the layers of a pretrained convolutional network. As shown by <ref type="bibr" target="#b1">[2]</ref>, ignoring the style-cost in this approach and only matching image features, it is possible to render images which are similar to the reference image. As we shall show, unrolling the gradient descent based optimization that generates the target image yields a recurrent computation, in which an "encoder" convolutional network extracts images of the current "canvas". The resulting code and the code for the reference image get fed into a "decoder" which decides on an update to the "canvas". This view, along with the successes of trained sequential generation networks, suggests that an iterative convolutional network that is trained to accumulate updates onto a visual canvas should be good at generating images in general, not just those shown as reference images. We show in this paper that this indeed is the case.</p><p>To evaluate and compare the relative performance of adversarial generative models quantitatively, we also introduce a new evaluation scheme based on a "cross-over" battle between the discriminators and generators of the two models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Generative Adversarial Networks (GAN) are built upon the concept of a non-cooperative game <ref type="bibr" target="#b13">[14]</ref>, that two networks are trained to play against each other. The two networks are a generative and a discriminative model, G and D. The generative model generates samples that are hard for the discriminator D to distinguish from real data. At the same time, the discriminator tries to avoid getting fooled by the generative model G.</p><p>Formally, the discriminative model is a classifier D : R M → {0, 1} that tries to determine whether a given point x ∈ R M is real or generated data. The generative model G : R K → R M generates samples x ∈ R M that are similar to the data by mapping a sample z ∈ R K drawn randomly from some prior distribution p(z) to the data space. These models can be trained by playing a minmax game as follows:</p><formula xml:id="formula_0">min θ G max θ D V (D, G) = min G max D E x∼p D log D(x) + E z∼p G log 1 − D(G(z)) .<label>(1)</label></formula><p>where θ G and θ D are the parameters of discriminator and generator, respectively.</p><p>In practice, the second term in Equation 1 is troublesome due to the saturation of log 1−D(G(z)) . This makes insufficient gradient flow through the generative model G as the magnitude of gradients get smaller and prevent them from learning. To remedy the vanishing gradient problem, the objective function in Equation 1 is reformulated into two separate objectives:</p><formula xml:id="formula_1">max θ D E x∼p D log D(x) + E z∼p G log 1 − D(G(z)) + max θ G E z∼p G log D G(z) . (2)</formula><p>Although Equation 2 is not the same as Equation 1, the underlying intuition is the same. Moreover, the gradient of generators for the two different objectives are always pointing in the same direction and the two objectives have the same fixed points.</p><p>The generating and discriminating procedure are simple. We consider a Gaussion prior distribution with zero-mean and unit variance. Then, the process of generating an output is simply to pass a sample z ∼ N (µ = 0, σ = 1) to the generative model to obtain the sample x ∼ G(z; θ G ). Note that the generative model G can be a deterministic or a probabilistic model. However, only deterministic models have been deployed in the past, so that x = G(z; θ G ). Subsequently, the sample can be passed on to the discriminator to predict D(x; θ D ).</p><p>After computing the cost in Equation 2, the model parameters can be updated through backpropagation. Due to the two different min-max operators in Equation 2, the update rule is defined as follows:</p><formula xml:id="formula_2">{θ D , θ G } ←    Update θ D if D(x) predicts wrong Update θ D if D(G(z)) predicts wrong Update θ G if D(G(z)) predicts correct</formula><p>Ideally, we would like the generative model to learn a distribution such that p G = p D . This requires the generative model to be capable of transforming a simple prior distribution p(z) to more complex distributions. In general, deep neural networks are good candidates as they are capable of modeling complicated functions and they were shown to be effective in previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Recently, <ref type="bibr" target="#b14">[15]</ref> showed excellent samples of realistic images using a fully convolutional neural network as the discriminative model and fully deconvolutional neural network <ref type="bibr" target="#b18">[19]</ref> as the generative model. The l th convolutional layer in the discriminative network takes the form</p><formula xml:id="formula_3">h k (l) j = f   j∈M k h l−1 j * W k (l) + b k (l) j   ,<label>(3)</label></formula><p>and the l th convolutional transpose layer 1 in the generative network takes the form</p><formula xml:id="formula_4">g c (l) j = f   j∈Mc g l−1 j W c (l) + b c (l) j   .<label>(4)</label></formula><p>In these equations, * is the convolution operator, is the convolutional transpose operator, M j is the selection of inputs from the previous layer ("input maps"), f is an activation function, and</p><formula xml:id="formula_5">{W k (l) , b k (l) j } and {W c (l) , b c (l)</formula><p>j } are the parameters of the discriminator and generator at layer l. The detailed explanation of convolutional transpose is explained in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We propose sequential modeling using GANs on images. Before introducing our proposed methods, we discuss some of the motivations for our approach. One interesting aspect of models such as the Deep Recurrent Attentive Writer (DRAW) <ref type="bibr" target="#b4">[5]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and the Laplacian Generative Adversarial Networks</head><p>It is more proper to say "convolutional transpose operation" rather than "deconvolutional" operation. Hence, we will be using the term "convolutional transpose" from now on.  <ref type="bibr" target="#b0">[1]</ref> is that they generate image samples in a sequential process, rather than generating them in one shot. Both were shown to outperform their ancestor models, which are the variational auto-encoder and GAN, respectively. The obvious advantage of such sequential models is that repeatedly generating outputs conditioned on previous states simplifies the problem of modeling complicated data distributions by mapping them to a sequence of simpler problems.</p><p>There is a close relationship between sequential generation and Backpropgating to the Input (BI). BI is a well-known technique where the goal is to obtain a neural network input that minimizes a given objective function derived from the network. For example, <ref type="bibr" target="#b1">[2]</ref> recently introduced a model for stylistic rendering by optimizing the input image to simultaneously match higher-layer features of a reference content image and a non-linear, texture-sensitive function of the same features of a reference style image. They also showed that in the absence of the style-cost, this optimization yields a rendering of the content image (in a quality that depends on the chosen feature layer).</p><p>Interestingly, rendering by feature matching in this way is itself closely related to DRAW: optimizing a matching cost with respect to the input pixels with backprop amounts to first extracting the current image features f x at the chosen layer using a forward path through the network (up to that layer). Computing the gradient of the feature reconstruction error then amounts to back-propogating the difference f x −f I back to the pixels. This is equivalent to traversing a "decoder" network, defined as the linearized, inverse network that computes the backward pass. The negative of this derivative is then added into the current version, x, of the generated image. We can thus think of image x as a buffer or "canvas" onto which updates are accumulated sequentially (see the left of <ref type="figure" target="#fig_2">Figure 2</ref>). Like in the DRAW model, where the updates are computed using a (forward) pass through an encoder network, followed by a (backward) pass through a decoder network. This approach is almost identical to the DRAW network, except for two subtle differences (see, <ref type="bibr" target="#b4">[5]</ref>): (i) in DRAW, the difference between the current image and the image to be rendered is used in the forward pass, whereas here this difference is computed in the feature space (after encoding); (ii) DRAW uses a learned, attention-based decoder and encoder rather than (fixed) convolutional network. (see the right of <ref type="figure" target="#fig_2">Figure 2</ref>). We elaborate on the relationship between the two methods in the supplementary material.</p><p>In this work, we explore a generative recurrent adversarial network as an intermediate between DRAW and gradient-based optimization based on a generative adversarial objective function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative Recurrent Adversarial Networks</head><p>We propose Generative Recurrent Adversarial Networks (GRAN), whose underlying structure is similar to other GANs. The main difference between GRAN versus other generative adversarial models is that the generator G consists of a recurrent feedback loop that takes a sequence of noise samples drawn from the prior distribution z ∼ p(z) and draws an ouput at multiple time steps</p><formula xml:id="formula_6">∆C 1 , ∆C 2 , • • • , ∆C T .</formula><p>Accumulating the updates at each time step yields the final sample drawn to the canvas C.  Ultimately, the function f (•) acts as a decoder that receives the input from the previous hidden state h c,t and noise sample z, and function g(•) acts as an encoder that provides a hidden representation of the output ∆C t−1 for time step t. One interesting aspect of GRAN is that the procedure of GRAN starts with a decoder instead of an encoder. This is in contrast to most auto-encoder like models such as VAE or DRAW, which start by encoding an image (see <ref type="figure" target="#fig_2">Figure 2</ref>).</p><p>In the following, we describe the procedure in more detail. We have an initial hidden state h c,0 that is set as a zero vector in the beginning. We then compute the following for each time step t = 1 . . . T :</p><formula xml:id="formula_7">z t ∼ p(Z) (5) h c,t = g(∆C t−1 ) (6) h z,t = tanh(W z t + b). (7) ∆C t = f ([h z,t , h c,t ]),<label>(8)</label></formula><p>where [h z,t , h c,t ] denotes the concatenation of h z,t and h c,t 2 . Finally, we sum the generated images and apply the logistic function in order to scale the final output to be in (0, 1):</p><formula xml:id="formula_8">C = σ( T t=1 ∆C t ).<label>(9)</label></formula><p>The reason for using tanh(•) in Equation 7 is to rescale z to (−1, 1). Hence, rescaling it to the same (bounded) domain as h c,t .</p><p>In general, one can declare the functions f (•) and g(•) to be any type of model. We used a variant of DCGAN <ref type="bibr" target="#b14">[15]</ref> in our experiments. Supp. <ref type="figure" target="#fig_0">Figure 11</ref> demonstrates the architecture of GRAN at time step t. The function f (•) starts with one fully connected layer at the bottom and a deconvolutional layers with fractional-stride convolution at rest of the upper layers. This makes the images gradually upscale as we move up to higher layers. Conversely, the function g(•) starts from convolutional layers and the fully connected layer at the top. The two functions, f (•) and g(•), are symmetric copies of one another, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. The overall network is trained via backpropagation through the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Evaluation: Battle between GANs</head><p>A problem with generative adversarial models is that there is no obvious way to evaluate them quantitatively. In the past, <ref type="bibr" target="#b3">[4]</ref> evaluated GANs by looking at nearest-neighbours in the training data. LAPGAN was evaluated in the same way, and in addition using human inspections <ref type="bibr" target="#b0">[1]</ref>. For these, volunteers were asked to judge whether given images are drawn from the dataset or generated by LAPGAN. In that case, the human acts as the discriminator, while the generator is a trained GAN. The problem with this approach is that human inspectors can be subjective to high variance, which makes it necessary to average over a large number of these, and the experimental setup is expensive and cumbersome. A third evaluation scheme, used recently by <ref type="bibr" target="#b14">[15]</ref> is based on classification performance. However, this approach is rather indirect and relies heavily on the choice of classifier. For example, in the work by Radford et al, they used nearest neighbor classifiers, which suffers from the problem that Euclidean distance is not a good dissimilarity measure for images.</p><p>Here, we propose an alternative way to evaluate generative adversarial models. Our approach is to directly compare two generative adversarial models by having them engage in a "battle" against each other. The naive intuition is that, since every generative adversarial models consists of a discriminator and a generator in pairs, we can exchange the pairs and have the models play the generative adversarial game against each other. <ref type="figure">Figure 5</ref> illustrates this approach <ref type="bibr" target="#b2">3</ref> .</p><p>The training and test stages are as follows. Consider two generative adversarial models, M 1 and M 2 .</p><p>Each model consists of a generator and a discriminator,</p><formula xml:id="formula_9">M 1 = {(G 1 , D 1 )} and M 2 = {(G 2 , D 2 )}.</formula><p>In the training phase, G 1 competes with D 1 in order to be trained for the battle in the test phase. Likewise for G 2 and D 2 . In the test phase, model M 1 plays against model M 2 by having G 1 try to fool D 2 and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 6: Model Comparison Metric for GANs</head><formula xml:id="formula_10">M1 M2 M1 D1(G1(z)) , D1(xtrain) D1(G2(z)) , D1(xtest) M2 D2(G1(z)) , D2(xtest) D2(G2(z)) , D2(xtrain)</formula><p>Accordingly, we end up with the combinations shown in <ref type="table">Table 6</ref>. Each entry in the table contains two scores, one from discriminating training or test data points, and the other from discriminating generated samples. At test time, we can look at the following ratios between the discriminative scores of the two models:</p><formula xml:id="formula_11">r test def = D 1 (x test ) D 2 (x test ) and r sample def = D 1 (G 2 (z)) D 2 (G 1 (z)) ,<label>(10)</label></formula><p>where (•) is the classification error rate, and x test is the predefined test set. These ratios allow us to compare the model performances.</p><p>The test ratio, r test , tells us which model generalizes better since it is based on discriminating the test data. Note that when the discriminator is overfitted to the training data, the generator will also be affected by this. This will increase the chance of producing biased samples towards the training data.</p><p>The sample ratio, r sample , tells us which model can fool the other model more easily, since the discriminators are classifying over the samples generated by their opponents. Strictly speaking, as our goal is to generate good samples, the sample ratio determines which model is better at generating good ("data like") samples.</p><p>We suggest using the sample ratio to determine the winning model, and to use the test ratio to determine the validity of the outcome. The reason for using the latter is due to the occasional possibility of the sample ratio being biased, in which case the battle is not completely fair when <ref type="figure">Figure 7</ref>: Cifar10 samples generated by GRAN <ref type="figure">Figure 8</ref>: LSUN samples generated by GRAN  the winner is solely determined by the sample ratio. It is possible that one of the discriminators is biased towards the training data more so than the other (i.e. overfitted on the training data). In order to address this issue, our proposed evaluation metric qualifies the sample ratio to be judged by the test ratio as follows:  <ref type="bibr" target="#b10">(11)</ref> This imposes a condition where r test 1, which assures that none of the discriminator is overfitted more than the other. If r test = 1, then this implies that r sample is biased, and thus, the sample ratio is no longer applicable.</p><formula xml:id="formula_12">winner =    M1 if r</formula><p>We call this evaluation measure Generative Adversarial Metric (GAM). GAM is not only able to compare generative adversarial models against each other, but also able to partially compare other models, such as the VAE or DRAW. This is done by observing the error rate of GRAN's discriminators based on the samples of the other generative model as an evaluation criterion. For example, in our experiments we report the error rates of the GRAN's discriminators with the samples of other generative models, i.e. err(D(z)) where z are the samples of other generative models and D(•) is the discriminator of GRAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In order to evaluate whether the extension of sequential generation enhances the perfomance, we assessed both quantitatively and qualitatively under three different image datasets. We conducted several empirical studies on GRAN under the model selection metrics discussed in Section 4. See Supplementary Materials for full experimental details.</p><p>The performance of GRAN is presented in <ref type="table" target="#tab_0">Table 1</ref>. We focused on comparing GRANs with 1, 3 and 5 time steps. For all three datasets, GRAN3 and GRAN5 outperformed GRAN1 as shown in <ref type="table" target="#tab_0">Table 1</ref>. Moreover, we present samples from GRAN for MNIST, cifar10 and LSUN in <ref type="figure">Figure 7</ref>, <ref type="figure">Figure 8</ref>, and Supp. <ref type="figure" target="#fig_0">Figure 12</ref>. Most of the MNIST and cifar10 samples shown in Supp. <ref type="figure" target="#fig_0">Figure  12</ref> and <ref type="figure">Figure 7</ref> appear to be discernible and reasonably classifiable by humans. Additionally, the LSUN samples from <ref type="figure">Figure 8</ref> seem to cover variety of church buildings and contain fine detailed textures. The "image statistics" of two real image datasets are embedded into both types of sample.</p><p>In the following, we analyze the results by answering a set of questions on our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q: How do GRAN and other GAN type of models perform compared to non generative adversarial models?</head><p>We compared our model to other generative models such as denoising VAE (DVAE) <ref type="bibr" target="#b7">[8]</ref> and DRAW on the MNIST dataset. Although this may not be the best way to assess the two models, since the generator of GRAN is not used, <ref type="table" target="#tab_1">Table 2</ref> presents the results of applying GAM as described at the end of Section 4. The error rates were all below 50%, and especially low for DVAE's samples. Surprisingly, even though samples from DRAW look very nice, the error rate on their samples were also quite low with GRAN3. This illustrates that the discriminators of generative adversarial models are good at discriminating the samples generated by DVAE and DRAW. Our hypothesis is that the samples look nicer due to the smoothing effect of having a mean squared error in their objective, but they do not capture all relevant aspects of the statistics of real handwritten images.  Since it is infeasible to naively examine the training data for similar looking images as GRAN's output, it is common (albeit somewhat questionable) to look at k-nearest neighbors to do a basic sanity check. As shown in Supp. <ref type="figure" target="#fig_0">Figure 13</ref>, Supp. <ref type="figure" target="#fig_0">Figure 16</ref>, and Supp. <ref type="figure" target="#fig_0">Figure 17</ref> and 18, one does not find any replicates of training data cases.</p><p>Empirically speaking, we did notice that GRAN tends to generate samples by interpolating between the training data. For example, Supp. <ref type="figure" target="#fig_0">Figure 11</ref> illustrates that the church buildings consist of similar structure of the entrance but the overall structure of the church has a different shape. Based on such examples, we hypothesize that the overfitting for GRAN in the worst case may imply that the model learns to interpolate sensibly between training examples. This is not the typical way of the term overfitting is used for generative models, which usually refers to memorizing the data. In fact, in adversarial training in general, the objective function is not based on mean squared error of the pixels which makes it not obvious how to memorize the training samples. However, this could mean that it is difficult for these models to generate images that are interpolated from training data.   well. This behaviour is somewhat similar to <ref type="bibr" target="#b0">[1]</ref>, as one might expect (although filling-in of color details suggest that the process is more complex than a simple coarse-to-fine generation). Note that this behaviour is not enforced in our case, since we use an identical architecture at every time step.</p><p>Q: What happens when we use a different noises for each step?</p><p>We sampled a noise vector z ∼ p(Z) and used the same noise for every time step. This is because z acts as a reference frame in <ref type="bibr" target="#b1">[2]</ref> as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. On the other hand, the role of the sample in DRAW is to inject noise at each step,</p><formula xml:id="formula_13">z 1 , z 2 , • • • z T , ∼ p(Z)</formula><p>, as prescribed by the variational autoencoding framework. We also experimented with both sampling z once in the beginning versus sampling z i at each time step. Here we describe the advantages and disadvantages to these two approaches.</p><p>The samples of cifar10 and LSUN generated by injecting different noises at each time step are shown in Supp. <ref type="figure" target="#fig_0">Figure 19</ref> and Supp. <ref type="figure" target="#fig_2">Figure 20</ref>. <ref type="figure">Figure 7</ref> and <ref type="figure">Figure 8</ref> are the output samples when injected using the same noise. The samples appear to be discernible and reasonably classifiable by humans as well. However, we observe a few samples that look very alike to one other. During the experiments, we found that when using different noise, it requires more effort to find a set of hyper-parameters that produce good samples. Furthermore, the samples tend to collapse when training for a long time. Hence, we had to carefully select the total number of iterations. This illustrates that the training became much more difficult and it provokes GRAN to "cheat" by putting a lot of probability mass on samples that the discriminator cannot classify, which produce samples that look very similar to each other.</p><p>On the other hand, when we look at the intermmediate time steps of samples generated using multiple noises, we find that there are more pronounced changes within each time step as demonstrated in Supp. <ref type="figure" target="#fig_0">Figure 19</ref> and Supp.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We proposed a new generative model based on adversarial training of a recurrent neural network inspired by <ref type="bibr" target="#b1">[2]</ref>. We showed the conditions under which the model performs well and also showed that it can produce higher quality visual samples than an equivalent single-step model. We also introduced a new metric for comparing adversarial networks quantitatively and presented that the recurrent generative model yields a superior performance over existing state-of-the-art generative models under this metric. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials Additional Notes on Convolutional Transpose</head><p>In the following, we describe the convolutional transpose procedure in detail. For simplicity, let us consider the case of a 1-dimensional convolutional operation with one kernel and stride of 1:</p><formula xml:id="formula_14">o = i * W,<label>(12)</label></formula><p>where i is an input, o is an output, and * is the convolutional operator. <ref type="figure" target="#fig_0">Figure 16</ref> and <ref type="figure" target="#fig_0">Figure 17</ref> show an illustration of the 1D convolution.</p><p>,QSXW,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2XWSXWR</head><p>&amp;RQYROXWLRQ <ref type="figure" target="#fig_0">Figure 18</ref>: The gradient of convolution at index k.  The gradient of Equation 15 wrt. the input takes the form</p><formula xml:id="formula_15">∂o ∂i = ∂i * W ∂i .<label>(13)</label></formula><p>Note that gradient of the convolution is a convolution itself. This can be seen in <ref type="figure" target="#fig_0">Figure 18</ref>. We can re-express the convolutional transposẽ</p><formula xml:id="formula_16">o =ĩ W (14)</formula><p>where is a convolutional transpose operator, andõ andĩ are just input and output of the function.</p><p>From <ref type="figure" target="#fig_0">Figure 19</ref>, we can observe that the gradient of convolutional formula in Equation 15 is just a transpose of the replicated input matrix. Since the convolutional gradient uses the convolutional transpose operator, the convolutional transpose can be implemented by using the gradient.</p><p>Now we consider the case of strided convolution. For simplicity, we assume that the stride is 2.</p><p>Similarly to before, we can write</p><formula xml:id="formula_17">o = i * W,<label>(15)</label></formula><p>where i is an input, o is an output, and * is the convolution operator. <ref type="figure" target="#fig_2">Figure 20</ref> and <ref type="figure" target="#fig_0">Figure 21</ref> illustrate the 1D convolution.</p><p>Input I   2. Thus, the gradient of the strided convolution is a convolutional operation on an upsampled version of the input i. This can be observed from <ref type="figure" target="#fig_2">Figure 22</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output o</head><p>Overall, the convolutional transpose with strides is expressed aŝ</p><formula xml:id="formula_18">o =î W (17)</formula><p>where is a convolutional transpose operator, andô andî are just input and output of the function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation between sequential modeling and backpropagation with respect to the input methods</head><p>We describe in more detail the relation between sequential modeling algorithms and backpropagation with respect to the input (BI). We will investigate their relation by examining a specific example.</p><p>The goal of BI is to find an optimal input by backpropagating an objective function with respect to the input. Let J(x) be a differentiable objective function that takes an input x. Then, depending on whether the objective function is non-linear function or not, we iterate</p><formula xml:id="formula_19">x opt = x (0) − T t=1 η t ∂J (t) ∂x (t−1)<label>(18)</label></formula><p>where t denotes time, and the chain rule yields</p><formula xml:id="formula_20">∂J (t) ∂x (t−1) = ∂J (t) ∂f (t) ∂f (t) ∂x (t−1) ,<label>(19)</label></formula><p>where f (•) is an intermediate function, which can be composed of many non-linear functions like neural networks, in the objective function J(•).</p><p>An example method that uses backpropagation with respect to the input is <ref type="bibr" target="#b1">[2]</ref>. We will consider with only the content based objective of this method, and compare it to one of the well-known sequential generators, DRAW <ref type="bibr" target="#b4">[5]</ref>. <ref type="figure" target="#fig_2">Figure 24</ref> presents unrolled version. The objective function of BI is defined as f x − f I 2 where f x is the hidden representation of the input x <ref type="bibr">(t)</ref> , and h I is the hidden representation of the reference content image of the convolutional network f (•). The network layers are shown as red blocks. Furthermore, the blue blocks (or the upper half) the diagram in <ref type="figure" target="#fig_2">Figure 24</ref> is the unrolled part of backpropagation gradient with respect to the input ∂f ∂x . The architecture of DRAW is shown in <ref type="figure" target="#fig_2">Figure 25 4</ref> . DRAW takes the input and the difference between the input and canvas at time t, and it propagates through the encoder and decoder. At the end of each time step, DRAW outputs the updated canvas C (t) by updating the previous canvas C (t <ref type="bibr">−1)</ref> with what will be drawn at time t, which is equivalent to change in the canvas ∆C <ref type="bibr">(t)</ref> .</p><p>The attention mechanism is omitted for clarity.</p><p>Algorithm 1 GRAN's sample generating process.</p><p>Initial hidden state:</p><formula xml:id="formula_21">h c,0 = 0. while t &lt; T do z t ∼ p(Z) h z = tanh(W z t + b) h c,t = g(∆C t−1 ) ∆C t = f ([h z , h c,t ]) end while C = σ( T t=1 ∆C t ).</formula><p>We can immediately notice the similarity between two architectures. The update procedure of draw, which is expressed as</p><formula xml:id="formula_22">C (t) = C (t−1) + ∆C (t) ,</formula><p>(20) resembles the update rule of BI in Equation 18. Moreover, the encoder of DRAW, enc(•), can be seen as some function f (•), which will be the convolutional neural network in BI. Similarly, the decoder of DRAW, dec(•), can be seen as the unrolled version of BI, which corresponds to ∂f <ref type="bibr">(t)</ref> ∂x (t−1) . The main difference is that BI takes the difference in the hidden representation space of f (•) and DRAW takes the difference from the original input space. Overall, we linked each components of two models by examining the abstraction as shown:</p><formula xml:id="formula_23">• ∆C (t) reflects ∂J (t) ∂x (t−1) . • enc(•) and dec(•) reflect f (•) and ∂f (t) ∂x (t−1)</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material to the Experiments</head><p>The MNIST dataset contains 60,000 images for training and 10,000 images for testing and each of the images is 28×28 pixels for handwritten digits from 0 to 9 (LeCun et al., 1998). Out of the 60,000 training examples, we used 10,000 examples as validation set to tune the hyper-parameters of our model. The CIFAR10 dataset consists of 60000 32×32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The LSUN Church dataset consists of high resolution natural scene images with 10 different classes <ref type="bibr" target="#b17">[18]</ref>. We considered training on outdoor church images, which contains 126,227 training, 300 validaiton, and 1000 test images. These images were downsampled to 64×64 pixels. The LSUN Living Room + Kitchen dataset consists of high resolution natural scene images with 10 different classes <ref type="bibr" target="#b17">[18]</ref>. We considered training on living room and kitchen images, which contains approx. 120,000 training, 300 validaiton, and 1000 test images. These images were downsampled to 64×64 pixels.  All datasets were normalized such that each pixel value ranges in between [0, 1]. For all of our results, we optimized the models with ADAM <ref type="bibr" target="#b9">[10]</ref>. The batch size was set to 100, and the learning rate was selected from a discrete range chosen based on the validation set. Importantly, we used different learning rates for the discriminative network and generative network. Throughout the experiments, we found that having different learning rates are useful to obtain succesfully trained generative adversarial models. As our proposed model is a sequential generator, we must select the number of steps, T , to run the model for generation. We compared the models in different number of timesteps, {1, 3, 5}. Note that GRAN is equivalent to DCGAN when T = 1 up to one extra fully connected layer. We denote this as GRAN1.</p><p>Throughout the experiments, we used a similar architecture for the generative and discriminative network as shown in <ref type="figure" target="#fig_1">Figure 3</ref> and <ref type="figure" target="#fig_2">Figure 2</ref>. The convolutional layers of the networks for the discriminator and generator are shared. The number of convolutional layers and the number of hidden units at each layer were varied based on the dataset. <ref type="table" target="#tab_1">Table 29</ref> shows the number of convolution kernels and the size of the filters at each convolutional layer. The numbers in the array from left to right corresponds to each bottom to top layer of the convolutional neural network. One can tie the weights of convolution and convolutional transpose in the encoder and decoder of GRAN to have the same number of parameters for both DCGAN and GRAN.</p><p>The quality of samples can depend on several tricks <ref type="bibr" target="#b14">[15]</ref>, including:</p><p>1. Removing fully connected hidden layers and replacing pooling layers with strided convolutions on the discriminator <ref type="bibr" target="#b16">[17]</ref> and fractional-strided convolutions (upsampling) on the generator.</p><p>2. Using batch-normalization <ref type="bibr" target="#b8">[9]</ref> on both generative and discriminative models.</p><p>3. Using ReLU activations in every layer of the generative model except the last layer, and using LeakyReLU activations <ref type="bibr" target="#b11">[12]</ref> in all layers of the discriminative model.</p><p>Overall, these architectural tricks make it easier to generate smooth and realistic samples. We rely on these tricks and incorporate them into GRAN.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Left: Unrolling the gradient-based optimization of pixels in Gatys et al. Right: The DRAW network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Depiction of single time step component of Generative Recurrent Adversarial Networks architecture layed out. (The numbers of the figures are used for modelling CIFAR10 dataset) (LAPGAN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Abstraction of Generative Recurrent Adversarial Networks. The function f serves as the decoder and the function g serves as the encoder of GRAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Training Phase of Generative Adversarial Networks. Training Phase and Test Phase of Generative Adversarial Networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2</head><label>2</label><figDesc>delineates the high-level abstraction of GRAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>At each time step, t, a sample z from the prior distribution is passed to a function f (•) along with the hidden states h c,t . Where h c,t represent the hidden state, or in other words, a current encoded status of the previous drawing ∆C t−1 . Here, ∆C t represents the output of the function f (•). (see Supp. Figure 11.) Henceforth, the function g(•) can be seen as a way to mimic the inverse of the function f (•).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Nearest Neighbour training examples for cifar10 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Nearest Neighbour training examples for lsun samples using GRAN3.Q: Does GRAN overfit to the training data?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Drawing at different time steps on ci-far10 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Drawing at different time steps on lsun samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Cifar10 samples generated by GRAN with injecting different noises at every time step LSUN samples generated by GRAN with injecting different noises at every time step Q: What do the samples look during the intermediate time steps? Supp. Figure 14, Supp. Figure 21, and Supp. Figure 22 present the intermediate samples when the total number of steps is 3. From the figures, we can observe the gradual development of the samples over time. The common observation from the intermediate samples is that images become more fine-grained and introduce details missing from the previous time step image. Intermediate samples for models with a total number of time steps of 5 can be found in the supplementary materials as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 20 .</head><label>20</label><figDesc>For example, the colour of the train in Supp.Figure 19changes, and a partial church is drawn in Supp.Figure 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>LSUN (living room +kitchen) samples generated by GRAN3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Figure 16: Applying convolution at index j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Figure 19: Convolution operation as a matrix operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 presents</head><label>16</label><figDesc>the naive visualization of convolution over the input centered at index j, and Figure 17 presents the convolution operation in terms of matrix operation. The latter figure will be useful for understanding the convolutional transpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Figure 20: Applying convolution at index j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 22 :</head><label>22</label><figDesc>The gradient of convolution at index k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 23</head><label>23</label><figDesc>: Convolution operation as a matrix operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 20 Figure 25 :</head><label>2025</label><figDesc>shows the visualization of 2-stride convolution over the input centered at index j and Figure 17 presents stride convolution operation in terms of a matrix operation. The gradient of Equation 15 takes the form ∂o ∂î = ∂î * W ∂i . (16) whereî is the upsampled input i such thatî = [i 1 , 0, i 2 , 0, i 3 , • • • , i M , 0] for stride size equal to Figure 24: The gradient of convolution at index k. Decoder Encoder The abstract view of DRAW architecture is delineated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 26 :</head><label>26</label><figDesc>MNIST Samples generated by GRAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 27 :</head><label>27</label><figDesc>Nearest Neighbour training examples for MNIST samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 28 :</head><label>28</label><figDesc>Drawing at different time steps on mnist samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 29 :</head><label>29</label><figDesc>The experimental hyper-parameters on different data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 30 :Figure 31 :</head><label>3031</label><figDesc>Example of three different churches (samples) with some similarities.Analysis on GRAN samplesThe following figure is to support the studies in the experiments, particularly it supports the description under Q: Does GRAN overfit the training data?Nearest Neighbours of samples from the training dataset Nearest Neighbour training examples for lsun churchsamples using GRAN5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 32 :Figure 33 :</head><label>3233</label><figDesc>Nearest Neighbour training examples for lsun (living room + kitchen) samples.Intermediate samples at time step for GRAN5 Drawing at different time steps on ci-far10 samples with injecting different noises at every time step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 34 :</head><label>34</label><figDesc>Drawing at different time steps on lsun samples with injecting different noises at every time step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 35 :</head><label>35</label><figDesc>Drawing at different time steps on ci-far10 samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 36 :</head><label>36</label><figDesc>Drawing at different time steps on lsun church samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 37 :</head><label>37</label><figDesc>Drawing at different time steps on lsun (living room + kitchen) samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 38 :</head><label>38</label><figDesc>LSUN (church) samples generated by GRAN5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 39 :</head><label>39</label><figDesc>LSUN (living room +kitchen) samples generated by GRAN5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Model Evaluation on various data sets.</figDesc><table><row><cell>Data set</cell><cell>Battler</cell><cell cols="2">rtest r sample</cell><cell>Winner</cell></row><row><cell>MNIST</cell><cell>GRAN1 vs. GRAN3 GRAN1 vs. GRAN5</cell><cell>0.79 0.95</cell><cell>1.75 1.19</cell><cell>GRAN3 GRAN5</cell></row><row><cell></cell><cell>GRAN1 vs. GRAN3</cell><cell>1.28</cell><cell>1.001</cell><cell>GRAN3</cell></row><row><cell>CIFAR10</cell><cell>GRAN1 vs. GRAN5</cell><cell>1.29</cell><cell>1.011</cell><cell>GRAN5</cell></row><row><cell></cell><cell>GRAN3 vs. GRAN5</cell><cell>1.00</cell><cell>2.289</cell><cell>GRAN5</cell></row><row><cell></cell><cell cols="2">GRAN1 VS. GRAN3 0.95</cell><cell>13.68</cell><cell>GRAN3</cell></row><row><cell>LSUN</cell><cell>GRAN1 vs. GRAN5</cell><cell>0.99</cell><cell>13.97</cell><cell>GRAN5</cell></row><row><cell></cell><cell>GRAN3 vs. GRAN5</cell><cell>0.99</cell><cell>2.38</cell><cell>GRAN5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">: Comparison between</cell></row><row><cell cols="2">GRAN and non-adversarial</cell></row><row><cell>models on MNIST.</cell><cell></cell></row><row><cell>Battler</cell><cell>Error</cell></row><row><cell>GRAN1 vs. DVAE</cell><cell>0.058</cell></row><row><cell>GRAN3 vs. DVAE</cell><cell>0.01</cell></row><row><cell cols="2">GRAN1 vs. DRAW 0.347</cell></row><row><cell cols="2">GRAN3 vs. DRAW 0.106</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>sample &lt; 1 and r test 1 M2 if r sample &gt; 1 and r test 1 Tie otherwise</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that we explore two scenarios of sampling z in the experiments. The first scenario is where z is sampled once in the beginning, then hz,t = hz as a consequence. In whe other scenario, z is sampled at every time step.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A larger figure is shown in the supplementary materials.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the members of the LISA Lab at Montreal, in particular Mohammed Pezeshki and Donghyun Lee, for helpful discussions.</p><p>This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions, and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet samples</head><p>We also trained GRAN on ImageNet dataset. ImageNet dataset (Deng et al., 2009) is a high resolution natural images. We rescaled the images to 64 × 64 pixels. The architecture that was used for ImangeNet is same as the architecture that was used for LSUN datset except that there are three times more kerns on both generator and discriminator.</p><p>The samples are shown in <ref type="figure">Figure 40</ref>. Unfortunately, the samples does not generate objects from ImageNet dataset. However, it also shows that they are not overfitting, because they do not show actual objects but they are quite artistic. We hypothesize that this is becasue the model does not have the capacity to model 1000 object classes. Hence, they stay as abstract objects. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems (NIPS)</title>
		<meeting>the Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets for convolutional face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Gauthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems (NIPS)</title>
		<meeting>the Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Natural Image Statistics: A Probabilistic Approach to Early Computational Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvrinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarmo</forename><surname>Hurri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated, 1st edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Denoising criterion for variational auto-encoding framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Daniel Jiwoong Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06406" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/pdf/1502.03167.pdf" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auto-encoding varational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems (NIPS)</title>
		<meeting>the Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Deep learning Workshop(NIPS)</title>
		<meeting>the Neural Information Processing Systems Deep learning Workshop(NIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-cooperative games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Nash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="295" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling natural images using gated mrfs. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2206" to="2222" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6806" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<imprint>
			<date type="published" when="2015-06-10" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Visio</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
