{"1000": "the focus of this work is on building pos taggers for foreign languages , assuming that we have an english pos tagger and some parallel text between the two languages. algorithm bilingual pos induction require: parallel english and foreign language data de and df, unlabeled foreign training data f; english tagger. ensure: f, a set of parameters learned using a constrained unsupervised model. this paper presents a novel approach to pos tag induction in languages that have no labeled resources . the model is based on a feature-hmm and a graph-based approach. in the feature-hmm model , a first order markov model defines a distribution : p ( x x, z) p ( z1 z1) x i1 p ( zi1 zi1 zizizizi) transition p ( xi xi zizi) emission in a traditional markov model, the emission distribution p ( xi xi zizizi) is a set of multinomials. this model replaces the emission distribution with a log-linear model, such that: p ( x x z z) exp f ( x, z) xval ( x, z) exp f ( x, z) f ( x, z) where val ( x, z) corresponds to the entire vocabulary. the number of latent hmm states for each language in our experiments was set to be the fine set of treebank tags. for unaligned words, we set the tag to the most frequent tag in the corresponding treebank, and trained a supervised feature-hmm.", "1001": "this paper presents a novel approach for recurrent neural network ( rnn) representation of sequences using a recurrent neural network ( rnn) . the rnn is trained to predict the next element of a sequence given the previous elements. given a sequence of vectors s with n vector elements x1, x1, ..., xn, we convert it to the input sequence x ( x0, x1, ..., xn) , where x0 xstart. this special element is used to denote the beginning of the input sequence, and the likelihood of any vector x being the next element of the sequence, given the previous elements x0, x1, ..., xi. the rnn can be seen as a generative model, and the likelihood of any vector x being the next element of the sequence, given the previous elements x0, x1, ..., xi. the rnn is trained to predict the next element of the sequence, given the previous elements x0, x1, ..., xi. in this paper , we present a new method for classifying sequences of images and words . the method is based on a convolutional neural network ( cnn) that is trained for regression with the mean square error ( mse) loss function. the cnn is trained for regression with the mse loss function. the gradient of the mse loss may serve as the sequence representation, even if the loss is not interpretable as a likelihood. the rnn is trained for regression with the mean square error ( mse) loss function. an improvement in recognition performance was noticed when the dropout rate was enlarged, up to a rate of 0.95 . this paper introduces a novel fv representation for sequences that is derived from rnns . the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs. the rnn-fv representation surpasses the state-of-the-art results for video action recognition on two challenging datasets. when used for representing sentences , the rnnfv representation achieves state-of-the-art or competitive results on image annotation and image search tasks. since the length of the sentences in these tasks is usually short and, therefore, the ordering is less crucial, we believe that using the rnn-fv representation for tasks that use longer text will provide an even larger gap between the conventional fv and the rnn-fv . a transfer learning result from the image annotation task to the video action recognition task was shown. the con-ceptual distance between these two tasks makes this result both interesting and surprising.", "1002": "the rate of publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research. recently, academic conferences started publishing videos of talks ( e.g., acl1, emnlp1, icml2, and more). in such talks, the presenter ( usually a co- author) must describe their paper coherently and concisely ( since there is a time limit), provid-ing a good basis for generating summaries. based on this idea, in this paper, we propose a new method, named talksumm ( acronym for talk- based summarization) , to automatically generate extractive content-based summaries for scientific papers based on video talks. our approach uti- lizes the transcripts of video talks, and treat them as spoken summaries of pa- pers. then , using algo- rithms, we map the transcripts to the text of the paper, and create extractive summaries. we make our dataset and related code publicly available. to our knowl- edge, this is the first approach to automatically cre- ate extractive summaries for scientific papers by utilizing the videos of conference talks. this paper proposes a novel automatic method to gener- ate training data for scientific papers summariza- tion, based on conference talks given by authors. we show that the a model trained on our dataset achieves competitive results compared to models trained on human generated summaries, and that the dataset quality satisfies human experts. in the future, we plan to study the effect of other video modalities on the alignment algorithm. on the original split, the vanilla seq2seq models outperform the best baseline of narayan et al. ( 2017) by up to bleu, without using the rdf triples. on the new split, the vanilla seq2seq models break completely, while copy-augmented models perform better. in parallel, an updated version of the dataset was released ( v1.0) , which is larger and features a train/test split protocol which is similar to our proposal.", "1003": "this work studied the use of pretrained word vectors to improve emotion detection from text. pretrained word vectors were used as an input to neural networks to improve emotion detection from text. in our setting, we used an svm classi er with a linear kernel and took m(e, t) ise, t) ise, t) m(e, t) m(e, t) mbow (d) (mbow (d) and mwe (d) are the ensemble models of bow and the corresponding embedded representations presented in section our ensemble methods outperformed the baseline for each document representation method. the best result, which is also signi - cantly better for each dataset, is of en-class model that achieved an average relative improvement of in f1-score over all datasets. the classes associated with the test sample are then taken to be the emotion with the highest decision function value or the set of emotions with a positive decision function value. for example, on average, cbow representation for glove source showed a improvement in f1-score relative to word2vec.", "1004": "the purpose of this paper is to detect emotions in customer support dialogues using the twitter platform and show the utility of such analyses. we extracted this data from december until june specifically, for each customer that posted a tweet to the customer support accounts, we searched for the previous , if any, turn to which it replied. given this method we traced back previous turns and reconstructed entire dialogues. this analysis reflects our ultimate goal: to enable a computer system to discern the emotions expressed by human customers, and to develop computerized tools that mimic the emotional technique used by a human customer service agent in a particular situation. this paper proposes a novel approach for classifying agent and customer turns in social media. in our approach, agent turn emotion prediction is performed before its content is known. thus, the baseline representation of an agent turn consisted of textual features extracted from its preceding customer turn. we evaluated each emotion s classification performance by using precision ( p) , recall ( r) and f1score ( f1score) , over the whole dataset ( for the two cus- tomer service data sources together) . for customer turn emotion detection , the svm dialogue model performed better than the svm-hmm dialogue model, and reached a macro and micro average f1-score improvements over all dialogues of and 11.7, respectively. furthermore, the macro and micro average f1-score results of the svm-hmm dialogue model ( 0.519 and 0.6, respectively) are satisfying given the moderate icc score between the judges ( 0.53 ) . for predicting the agent emotional technique, the svm dialogue model obtained slightly better results than svm-hmm dialogue model, and reached a macro and micro average f1-score improvements over all dialogues of and 43.5, respectively.", "1005": "few-shot learning ( fsc) is a relatively new field in the field of few-shot classification ( fsc ) . recently , there has been a lot of exciting progress in the field of few-shot learning in general, and in few-shot classification ( fsc) in particular. a popular method for approaching fsc is meta-learning, or learning-to-learn. in meta-learning, the inputs to both train and test phases are not images, but instead a set of few-shot tasks, ti, each k-shot / n-way task containing a small amount k ( usually) of labeled support images and some amount of unlabeled query images for each of the n categories of the task. the goal of meta-learning is to find a base model that is easily adapted to the specific task at hand, so that it will generalize well to tasks built from novel unseen categories and fulfill the goal of fsc. in this work, we show that adding small neural networks , metadapt controllers, that adapt the connections in the main network according to the given task improves performance. in this paper we propose a new architecture for few - shot architecture search that is adaptive to the task at hand. the architecture is based on a set of metadapt controllers that are used to predict the best way of adapting the mixing coefficients ( ( i, j) o for the corresponding edge operations. they are responsible for predicting , given a few-shot task, the best way of adapting the mixing coefficients ( ( i, j) o for the corresponding edge operations. they are responsible for predicting, given a few-shot task, the best way of adapting the task-adapted coefficients ( ( i, j) o for the corresponding edge operations.", "1006": "when a virtual agent starts losing the context of a conversation, the agent repeats the same ( or semantically similar) response over and over, despite the user s attempt to rephrase the same intent. such agent repetitions lead to an unnatural interaction ( kluwer, 2011) the goal of this work is to give developers of automated agents tools to detect and then solve problems cre- ated by exceptionally bad conversations. the goal of this work is to give developers of automated agents tools to detect and then solve problems cre- ated by exceptionally bad conversations.", "1007": "the problem of interpretability in machine learning can be divided into two concrete tasks: model interpretability aims to supply a structured explanation which captures what the model has learned. given a trained model and a single example, prediction interpretability aims to explain how the model arrived at its prediction. these can be further divided into white-box and black-box techniques. convolutional neural networks ( cnns) have seen multiple advances in interpretability when used for computer vision tasks. these techniques unfortunately do not trivially apply to discrete sequences, as they assume a continuous input space used to represent images. intuitions about how cnns work on an abstract level also may not carry over from image inputs to textfor example, pooling in cnns has been used to induce deformation invariance, which is likely different than the role it has when processing text. in this work, we examine and attempt to understand how cnns process text, and then use this information for the more practical goals of improving model-level and prediction-level explanations. this paper addresses the question of how to distinguish between the classes of ngrams that a given filter identifies and the classes of ngrams that it does not identify. in particular, we show that filters do not necessarily maximize activations at the word-level, but instead form slot activation patterns that give different types of ngrams similar activation strengths. by clustering high-scoring ngrams according to their slot activation patterns we can identify the groups of linguistic patterns captured by a filter. we also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words. by doing this for the topk scoring ngrams per cluster, we arrive at a comprehensive set of negative ngrams. each list item corresponds to a slot- activations cluster. for each cluster we present the top-k ngrams activating it , and for each ngram we specify its total activation, its slot-activation vector, and its list of bottom-k negative ngrams with their activations and slot activations.", "1008": "in this work, we suggest a new idea of editorial network ( editnet) a mixed extractive-abstractive summarization approach. a summary generated by editnet may include sentences that were either extracted, abstracted or of both types. moreover, per considered sentence, editnet may decide not to take either of these decisions and completely reject the sentence. the key idea behind editnet is to create an automatic editing process to enhance summary quality. compared to previous works, whose final summary is either entirely extracted or generated using an abstractive process, in this work, we suggest a new idea of editorial network ( editnet) a mixed extractive-abstractive summarization approach. this paper presents a novel alternative summarization approach that instead of solely applying extraction or abstraction, mixes both together. moreover, editnet implements a novel sentence rejection decision, allowing to correct initial selection decisions which are predicted to negatively effect summarization quality.", "1009": "this paper presents dimsim , a simple and effective algorithm to generate and rank phonetically similar chinese letters and words. the algorithm is based on a reli- able approach to generate and rank phonetically similar chinese letters and words. a simple and effective algorithm to generate and rank phonetically similar chinese letters and words. a simple and effective algorithm to generate and rank chinese letters and words. this paper addresses the problem of determining the distance between a pair of chinese pinyins and a pair of initial ( or final ) pinyins in the chinese language . the model aims to minimize the sum of the absolute differences between the euclidean distances of component pairs and the average distances obtained from the annotated training data across all pairs for initials ( or finals) c. we also incorporate a penalty function , p, for pairs deviating from the manually annotated distance so that more phonetically similar pairs are penalized more highly . for a given initial ( or final ) pair ( p1 , p2 ) , we first generate the all possible pinyins with a component of p1 such as bao and bing. for each pinyin py , we retrieve all the words with length two in the dictionary which also have first or second character with the same py. for each created word w, we change the initial ( or final ) from p1 to p2, retrieve the corresponding words from the dictionary and generate word pairs to compare. finally, from the full list we randomly select five word pairs vary the first character, and five word pairs that vary the second character. dimsim is a chinese phonetic similarity algorithm that generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final , and tone components. using a real world dataset , we demonstrate that dimsim effectively improves mrr by 7.5x , recall by 1.5x and precision by 1.4x over existing approaches.", "1010": "knowledge base question answering ( kbqa) systems answer questions by obtaining information from kb tuples. the main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the kbqa system. although general relation detection1 methods are well studied in the nlp community, such studies usually do not take the end task of kbqa into consideration. as a result, there is a significant gap between general relation detection studies and kb-specific relation detection. first, in most general relation detection tasks, the number of target relations is limited, normally smaller than in contrast, in kbqa even a small kb, like freebase2m, contains more than 6,000 relation types. second, relation detection for kbqa often becomes a zero-shot learning task, since some test instances may have unseen relations in the training data. third, for some kbqa tasks like webquestions, we need to predict a chain of relations instead of a single relation. this increases the number of target relation types and the sizes of candidate relation pools, further increasing the difficulty of kb relation detection. this paper improves kb relation detection to cope with the problems mentioned above. this paper proposes a new hierarchical residual model ( hr-bilstm) for hierarchical matching by adding shortcut connections between two bilstm layers. the hr-bilstm is used to predict the score of each relation r re : srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) srel ( r; q) kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks. we propose a novel kb relation detection model, hr-bilstm, that performs hierarchical matching between questions and kb relations. our model outperforms the previous methods on kb relation detection tasks and allows our kbqa system to achieve state-of-the-arts. for future work, we will investigate the integration of our hr-bilstm into end-to-end systems.", "1011": "in this work we propose a feature agnostic approach for dictionary extraction based on neural language models, such as word2 to prevent semantic drift during the dictionary expansion. the input is a large text corpus and a set of seed examples. given an input corpus and a set of seed examples, the proposed runs in two phases, the proposed runs in two phases, the exploit phase tries to identify similar instances to the dictionary explore and exploit. this paper proposes an interactive dictionary expansion tool using a lightweight neural language model. our algorithm is iterative and purely statistical, hence does not require any feature extraction beyond tokenization. it incorporates human feedback to improve performance and control semantic drift at every iteration cycle. the experiments showed high importance of tight huml integration on discovery efficiency.", "1012": "this paper presents an interpretable mechanism which additionally alleviates any issues arising due to a limited number of region proposals. our approach is based on a number of image concepts, such as semantic segmentations, fitnesss and priors for any number of objects of interest. we formulate our language grounding approach as an energy minimization over a large number of bounding boxes. the search over a large number of bounding boxes allows us to retrieve an accurate bounding-box prediction for a given phrase and an image. importantly, the search over a large number of bounding boxes allows us to find the global minimizer for a given energy function very effectively. to encode measurements dedicated to the image at hand, we take advantage of semantic segmentation and object detection techniques. the approach has been applied to very efficient deformable part models 43, for object class detection 26, for weakly supervised localization 5, indoor scene understanding 40, diverse object proposals and also for spatiotemporal object detection proposals we evaluate our approach on the referitgame and flickr 30k entities dataset 35, obtaining results like the ones visualized in fig. this paper proposes a novel approach for loss augmented inference which is based on the concept of a score map. the score map is a weighted sum of the feature maps over the region specified by a hypothesized bounding box. the score map can be computed by considering either the largest or the smallest possible bounding box in the bounding box hypothesis, depending on whether the weight in wt is positive or negative and whether the feature maps contain only positive or negative values. the score map can be split into two separate score maps, one with only positive values, and another with only negative values, while applying the same weight.", "1013": "the use of features from deep convolutional neural networks ( dcnns) pretrained on imagenet has led to important advances in computer vision. this paper proposes a new method to train a generative moment matching network autoencoder ( gfmn) that uses a dcgan-like architecture in the generator and a resnet-like architecture in the decoder. gfmn is trained with a dcgan-like architecture in the generator and a resnet-like architecture in the decoder. the objective of this paper is to show that the simple moving average ( ma) and the moving average of second order moments ( adam) are better than ma and adam for feature extraction in the context of gans. in this paper, we show that the simple moving average ( ma) and the moving average of second order moments ( adam) are better than ma and adam for feature extraction in the context of gans.", "1014": "to this end, we quantify purchase intention as a combined effect of form utility and time utility. to this end, we model a user s purchase intention as a combined effect of form utility and time utility. given purchase triplets ( user, item, time) and categories, the objective is to make recommendations based on users purchase intention and overall predicted combination of form utility and time utility. in this paper , we propose a demand-aware recommendation algorithm for general e- recommendation involving both durable and non - durable goods. the objective function is highly non - smooth with nested hinge losses, and a naive optimization algorithm will take at least o ( mnl) time, which is computationally infeasible when the data is large. to address this issue, we use proximal gradient descent to solve the problem. at each iteration, x is updated by x s ( x h ( x) ) where s ( ) is the soft-thresholding operator for singular values. since each user should make at least one purchase and each item should be purchased at least once to be included in p, n and m are smaller than p0. also, since k and t are usually very small, the time complexity to solve problem ( 4) is dominated by the term p0, which is a significant improvement over the naive approach with at least o ( mnl) complexity. since our problem has only two blocks d, x and each subproblem is convex, our optimization algorithm is guaranteed to converge to a stationary point indeed, it converges very fast in practice. in this paper , we examine the problem of demand-aware recommendation in settings when inter- purchase duration within categories affects users intention in combination with intrinsic properties of the items themselves. we formulate the problem of demand-aware recommendation in settings when inter- purchase duration within categories affects users intention in combination with intrinsic properties of the items themselves.", "1015": "many models of personality traits exist while the most common one is the big five model (digman, 1990), including: openness, conscientiousness, extraversion, agreeableness, and neuroticism. these traits were correlated with linguistic choices including lexicon and syntax (mairesse and walker, 2007)", "1016": "this paper proposes a fully neuromorphic event- based stereo disparity algorithm. a live- feed version of the system running on nine truenorth chips is shown to calculate disparity maps per second, and the ability to increase this up to 2,000 disparities per second ( subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as dvs. the proposed event- based stereo correspondence algorithm is implemented using a stereo pair of davis sensors ( a version of dvs) and nine truenorth ns1e boards however, the method is applicable to other spiking neuromorphic architectures, and it is also tested offline on larger models using a truenorth simulator. input rectification, spatiotemporal scaling , feature matching, search for best matches, morphological erosion and dilation, and bidirectional consistency check are all performed on truenorth , for a fully neuromorphic disparity solution. the winner-take-all ( wta) system is a feed-forward neural network that takes as input d thermometer code representations of the hadamard products for d distinct candidate disparity levels, and finds the disparity with the largest value, at every tick. for designing a scalable and compact wta system on a neuromorphic hardware, we introduced a novel encoding technique for inputs. in a binary eventbased system, numbers can be efficiently coded using base4 representation where each digit is encoded using a 3-bits thermometer code. note that a thermometer code of length 2n bits can be represented by a qtc of length n/2 bits. while it takes a few more bits than an bits binary code, it allows designing a feed-forward wta network comprising only four cascaded subnetworks, requiring fewer hardware resources as well as half the latency. latency is further improved with larger bases, but the growth in thermometer code length for each digit results in consuming more hardware resources. note that a thermometer code of length 2n bits in this paper we present a new event - based neuromorphic stereo system that is capable of running on live input event streams , using a fully graph-based computation model , where no frames, arrays or other such data -structures are used . in this paper \n we present a new event - based neuromorphic stereo system that is capable of running on live input event streams , using a fully graph-based computation model , where no frames, arrays or other such data -structures are used .", "1017": "we propose a characterization of i-markov equivalence between two causal graphs with latent variables for a given intervention set i that is based on a generalization of do-calculus rules to arbitrary subsets of interventions . we show a graphical characterization of i-markov equivalence of causal graphs with latents . we introduce a learning algorithm for inferring the graphical structure using a combination of observational and interventional data and utilizing the corresponding new constraints. this procedure comes with a new set of orientation rules. we call two causal graphs d1 , d2 i-markov equivalent if the set of distributions that are i-markov tod1 andd2 are the same. using the augmented graph , we identify a graphical condition that is necessary and sufficient for two cbns with latents to be i-markov equivalent. finally, we propose a sound algorithm for learning the augmented graph from interventional data. two causal graphs are said to be i-markov equivalent if their corresponding augmented mags satisfy the three conditions given in theorem for example, the two augmented mags in figures 2c and 2d satisfy the three conditions, hence the original causal graphs are in the same i-markov equivalence class. however, similar to the observational case, it is typically impossible to completely determine the causal graph from the available measured data, especially when latents are present. then, the objective is to learn a class of augmented mags consistent with data. algorithm presents a modification of the fci algorithm to learn augmented pags. algorithm follows a similar flow to that of the fci. creates the f-nodes by computing the set s of unique symmetric difference sets from all pairs of interventions in i sigma ( ) maps every f-node to a source pair of interventions, which is used later on to perform the do-tests. the algorithm starts by creating a complete graph of circle edges between v f then, it removes the edge between any two nodes x and y if a separating set exists. if the two nodes are f-nodes, then they are separated by the empty set by construction. otherwise, it calls the function do-constraints () in alg. to search for a separating set using the corresponding do-constraints.", "1018": "program induction has been used for decades, with rule induction and probabilistic program induction ( npi ) techniques that is able to answer complex queries by inducing programs of length up to 10, using atomic operators and variable types. this, to our knowledge, is the first npi system to be trained with only the gold answer as ( very distant) supervision for inducing such complex programs. although annotation affects a complete kbqa system, our focus here is on complex, multi-step program generation with only final answer as the distant supervision, with additional rewards in the final goal into a final goal to remove supervision, with a large number of steps needed to get any payoff. this renders complex program induction ( csqa) in the absence of gold programs extremely challenging. csqa is particularly suited to study the complex program induction ( cpi) challenge over other kbqa data sets because kbqa is particularly suited to study the complex program induction ( cpi) challenge over other kbqa data sets because kbqa is particularly suited to study the complex program induction ( cpi) challenge over other kbqa data sets because kbqa is particularly suited to study the complex program induction ( cpi) challenge over other kbqa data sets because kbqa this paper describes cipitr , a programming model that generates a program from a natural language question and a prepopulated variable memory table . the model generates a program from a natural language question and a prepopulated variable memory table . the model generates a program from a sequence of operators invoked with past instantiated variables as their arguments. the interpreter executes the generated program with the help of the kb and scratch memory and outputs the system answer. during training , the predicted answer is compared with the gold to obtain a reward, which is sent back to cipitr to update its model parameters through a reinforce objective. this is to isolate the programming performance of cipitr from the effect of imperfect linkage. a problem as complex as ours requires not only generic constraints for producing semantically correct programs, but also incorporation of prior knowledge, if the model permits. we present cipitr , an advanced npi framework that significantly pushes the frontier of complex program induction in absence of gold programs. cipitr uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic programming styles to constrain the combinatorial program space to only semantically correct programs. as future directions of work, cipitr can be further improved to handle the hardest question types by making the search more strategic, and can be further generalized to a diverse set of goals when training on all question categories together.", "1019": "the cross entropy ( ces) method is an unsupervised query-focused multi-document extractive summarizer ( extractive summarizer ) which uses the cross entropy method for selecting a subset of sentences extracted from input documents, whose combination is predicted to produce a good summary. in this work, we propose dual-ces an extended ces summarizer similar to ces , dual-ces is an unsupervised query-focused multi-document extractive summarizer. to this end, like ces , dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents, whose combination is predicted to produce a good summary. however, differently from ces , dual-ces does not attempt to address both saliency and focus goals in a single optimization step. instead, dual-ces implements a novel twostep dual-cascade optimization approach, which utilizes a novel twostep dual-cascade optimization approach. using such an approach, dual-ces tries to handle the tradeoff by gradually shifting from generating a long summary that is more salient in the first step to generating a short summary that is more focused in the second step. this paper presents a novel approach to summarization , which is based on two steps : ( 1 ) a pseudo-feedback distillation approach , ( 2 ) a saliency-based approach , and ( 3 ) a focus-driven approach . in the first step , each topic is expressed by one or more questions, and each topic is expressed by a set of english documents. in the second step , each topic is expressed by one or more questions, and each topic is expressed by a set of english documents.", "1020": "in this paper we show that we can get a considerable quality improvement by modifying a tts system that produced the world vocoder parameters to predict parameters for lpcnet as in the previous work 6, we conduct multiple adaptation experiments, applied on multiple vctk voices and show that the new system has much better quality and similarity to the target voices but can still run much faster than real-time in a single-cpu mode. the goal of this paper is to develop a system that produces high quality speech while operating at faster than real - time rate without an expensive gpu support. the system is built around three nn models for generating the prosody, acoustic features and the final speech signal. the system is tested using two proprietary tts voice datasets and demonstrated that its system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems. the task of creating a high-quality tts system out of a smaller set of audio data is even more challenging. we have shown that our system can perform well even with datasets as small as 5-20 minutes of audio. we demonstrated that when we reduce the size of the training data, there is some graceful degradation to the quality, but we are still able to maintain good similarity to the original speaker. for future work, we plan to allow voice modifications by adding control over voice parameters such as pitch, breathiness and vocal tract.", "1021": "in this paper we introduce the sobolev independence criterion ( sic) , a form of gradient regularized integral probability metric ( ipm) between the joint distribution and the product of marginalspy. sic relies on the statistics of the gradient of a witness function, or critic, for both ( 1 ) defining the ipm constraint and ( 2 ) finding the features that discriminate between the joint and the marginals. intuitively, the magnitude of the average gradient with respect to a feature gives an importance score for each feature. hence, promoting its sparsity is a natural constraint for feature selection in this paper two methods that provably control the false discovery rate ( fdr) in feature selection is an important problem for reproducible discoveries. in a nutshell , for a feature selection problem given the ground- truth set of features s, and a feature selection method such as sic that gives a candidate set s, our goal is to maximize the tpr ( true positive rate) or the power, and to keep the fdr under control. the architecture of the network can be problem dependent, but we focus here on a particular architecture: deep relu networks with biases removed. using our sparsity inducing gradient penalties with such networks, results in input sparsity at the level of the witness function f of sic. this is desirable since it allows for an interpretable model, similar to the effect of lasso with linear models, our sparsity inducing gradient penalties result in a nonlinear self - explainable witness function f 23, with explicit sparse dependency on the inputs. in this paper we propose a novel approach for unsupervised learning of multivariate features using a neural network model.. the neural network model is trained using a three - layer neural network with relu units and conditional batch normalization ( bcn) . in all of our tasks, one three - layer neural network with relu units and conditional batch normalization ( bcn) applied to all hidden layers serves as generator for all features j 1 , , p. a sample from p ( xj xj) is generated by giving as input to the network an index j indicating the feature to generate, and a sample xj p ( xj) , represented as a sample from the full joint distribution x p ( x1 , , xp) , with feature j being masked out. in particular, we used the adam optimizer with the default pytorch parameters and learning rate which is halved every epochs, and batch size of in order to recover the correct conditional independence we elected to use fdr control techniques to perform those dependent hypotheses testing ( btw coordinates) . by combining sic with hrt and fors we can guarantee that the correct dependency is recovered while the fdr is under control. in this paper , we propose a new network architecture called bigcritic ( see figure below ) . this network is trained with the same adam settings as above for updates ( liang ) and updates ( ccle ) ."}