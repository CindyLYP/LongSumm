{"1015": "we present a neural response generation model that generates responses conditioned on a target personality. the model learns high level features based on the target personality, and uses them to update its hidden state. our model achieves performance improvements in both perplexity and bleu scores over a baseline sequence-to-sequence model, and is validated by human judges. in this paper we propose a personality based model which is designed to generate responses conditioned on a target set of personality traits. the model is designed to generate responses with maximum likelihood which reflect the consensus of the agents that appear in the training data. this kind of response does not characterize a specific personality and thus can result in inconsistent or unwanted personality cues. the model is designed to generate responses conditioned on a target set of personality traits values which the responses should express. the target set of personality traits is represented as a vector p, where pi represents the desired value for the ith trait. this value encodes how strongly should this trait be expressed in the response. thus, the size of p depends on the selected personality model ( e.g., five traits for the big five model) . the model is trained end-to-end by maximizing p ( y x) nj1 p ( yj x, y1 : j1 ) . this model can not generate content for agents which do not appear in the training data, and thus, it is limited. this paper presents a model for generating linguistic responses that are adapted to the personality traits of the customer. the model is tested on a crowd-sourcing service and compared to the baseline seq2seq model in terms of perplexity and bleu score. in both cases, the model achieves a relative decrease in perplexity , and a relative improvement in bleu score. the experiments demonstrate that our model can better model the linguistic variation in agent responses by conditioning on target personality traits. in future work, we would like to generate responses adapted to the personality traits of the customer as well, and to apply our model to other tasks such as education systems. in this paper, we present a model for generating linguistic responses that are adapted to the personality traits of the customer. we experimented with a dataset of 87.5k real customer-agent utterance pairs from social media. we find that leveraging personality encoding improves relative performance up to in bleu score, compared to a baseline seq2seq model.", "1000": "###", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1007": "###", "1008": "###", "1009": "###", "1010": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1016": "###", "1017": "###", "1018": "###", "1019": "###", "1020": "###", "1021": "###"}