<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-05-11">11 May 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
							<email>rranjan1@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><forename type="middle">V M</forename><surname>Patel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<addrLine>5°,−3°,14°1°, −11°,1°− 6°,−1°,21°4°, −4°,10°6°, −10°, 7°− 9°,−11°,13°− 15°,−8°,−15°− 30°,1°,−15°1 2°,0°,−1°8°</addrLine>
									<postCode>8°, 12°F</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-05-11">11 May 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1603.01249v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face Detection</term>
					<term>Landmarks Localization</term>
					<term>Head Pose Estimation</term>
					<term>Gender Recognition</term>
					<term>Deep Convolutional Neural Networks</term>
					<term>Multi-task Learning</term>
				</keywords>
			</textClass>
			<abstract>
				<p>We present an algorithm for simultaneous face detection, landmarks localization, pose estimation and gender recognition using deep convolutional neural networks (CNN). The proposed method called, HyperFace, fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features. It exploits the synergy among the tasks which boosts up their individual performances. Extensive experiments show that the proposed method is able to capture both global and local information in faces and performs significantly better than many competitive algorithms for each of these four tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>D ETECTION and analysis of faces is a challenging problem in computer vision, and has been actively researched for applications such as face verification, face tracking, person identification, etc. Although recent methods based on deep Convolutional Neural Networks (CNN) have achieved remarkable results for the face detection task <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b49">[50]</ref>, it is still difficult to obtain facial landmark locations, head pose estimates and gender information from face images containing extreme poses, illumination and resolution variations. The tasks of face detection, landmark localization, pose estimation and gender classification have generally been solved as separate problems. Recently, it has been shown that learning correlated tasks simultaneously can boost the performance of individual tasks <ref type="bibr" target="#b57">[58]</ref> , <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this paper, we present a novel framework based on CNNs for simultaneous face detection, facial landmark localization, head pose estimation and gender recognition from a given image (see <ref type="figure">Figure 1</ref>). We design a CNN architecture to learn common features for these tasks and exploit the synergy among them. We exploit the fact that information contained in features is hierarchically distributed throughout the network as demonstrated in <ref type="bibr" target="#b52">[53]</ref>. Lower layers respond to edges and corners, and hence contain better localization properties. They are more suitable for learning landmark localization and pose estimation tasks. On the other hand, higher layers are class-specific and suitable for learning complex tasks such as face detection and gender recognition. It is evident that we need to make use of all the intermediate layers of a deep CNN in order to train different tasks under consideration. We refer the set of intermediate layer features as hyperfeatures. We borrow this term from <ref type="bibr" target="#b0">[1]</ref> which uses it to denote a stack of local histograms for multilevel image coding.</p><p>Since a CNN architecture contains multiple layers with hundreds of feature maps in each layer, the overall dimension of hyperfeatures is too large to be efficient for learning multiple tasks. Moreover, the hyperfeatures must be associated in a way that they efficiently encode the features common to the multiple tasks. This can be handled using feature fusion techniques. Features fusion aims to transform the features to a common subspace where they can be combined linearly or non-linearly. Recent advances in deep learning have shown that CNNs are capable of estimating any complex function. Hence, we construct a separate fusion-CNN to fuse the hyperfeatures. In order to learn the tasks, we train them simultaneously using multiple loss functions. In this way, the features get better at understanding faces, which leads to improvements in the performances of individual tasks. The deep CNN combined with the fusion-CNN can be learned together end-to-end.</p><p>We also study the performance of face detection, landmarks localization, pose estimation and gender recognition using off-the-shelf Region-based CNN (R-CNN <ref type="bibr" target="#b11">[12]</ref>) approach. Although R-CNN for face detection has been explored in DP2MFD <ref type="bibr" target="#b34">[35]</ref>, we provide a comprehensive study of all these tasks based on R-CNN. Furthermore, we study the multitask approach without fusing the intermediate layers of CNN. Detailed experiments show that multitask learning performs better compared to individual learning. Fusing the intermediate layers features provides additional performance boost. This paper makes the following contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>We propose a novel CNN architecture that performs face detection, landmarks localization, pose estimation and gender recognition by fusing the intermediate layers of the network. 2) We propose two post-processing methods: iterative region proposals and landmarks-based non-maximum suppression, which leverage the multitask information obtained from the CNN to improve the overall performance. 3) We study the performance of R-CNN based approaches for individual tasks and the multitask approach without intermediate layer fusion. 4) We achieve new state-of-the-art performances on challenging unconstrained datasets for all of these four tasks. This paper is organized as follows. Section 2 reviews related work. Section 3 describes the proposed HyperFace framework in detail. Section 4 describes the implementation of R-CNN based approach as well as Multitask Face. Section 5 provides the results of HyperFace along with R-CNN baselines on challenging datasets. Finally, Section 6 concludes the paper with a brief summary and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>One of the earlier approaches for jointly addressing the tasks of face detection, pose estimation, and landmark localization was proposed in <ref type="bibr" target="#b56">[57]</ref> and later extended in <ref type="bibr" target="#b57">[58]</ref>. This method is based on a mixture of trees with a shared pool of parts in the sense that every facial landmark is modeled as a part and uses global mixtures to capture the topological changes due to viewpoint changes. A joint cascade-based method was recently proposed in <ref type="bibr" target="#b4">[5]</ref> for simultaneously detecting faces and landmark points on a given image. This method yields improved detection performance by incorporating a face alignment step in the cascade structure. Multi-task learning using CNNs has also been studied recently in <ref type="bibr" target="#b54">[55]</ref>, which learns gender and other attributes to improve landmark localization, while <ref type="bibr" target="#b12">[13]</ref> trains a CNN for person pose estimation and action detection, using features only from the last layer. The intermediate layer features have been used for image segmentation <ref type="bibr" target="#b13">[14]</ref>, image classification <ref type="bibr" target="#b50">[51]</ref> and pedestrian detection <ref type="bibr" target="#b36">[37]</ref>.</p><p>Face detection: Viola-Jones detector <ref type="bibr" target="#b43">[44]</ref> is a classic method which uses cascaded classifiers on Haar-like features to detect faces. This method provides realtime face detection, but works best for full, frontal, and well lit faces. Deformable Parts Model (DPM) <ref type="bibr" target="#b10">[11]</ref>-based face detection methods have also been proposed in the literature where a face is essentially defined as a collection of parts <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b31">[32]</ref>. It has been shown that in unconstrained face detection, features like HOG or Haar wavelets do not capture the discriminative facial information at different illumination variations or poses. To overcome these limitations, various deep CNN-based face detection methods have been proposed in the literature <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b48">[49]</ref>. These methods have produced state-of-the-art results on many challenging publicly available face detection datasets. Some of the other recent face detection methods include NDFaces <ref type="bibr" target="#b29">[30]</ref>, PEP-Adapt <ref type="bibr" target="#b25">[26]</ref>, and <ref type="bibr" target="#b4">[5]</ref>.</p><p>Landmark localization: Fiducial point extraction or landmark localization is one of the most important steps in face recognition. Several approaches have been proposed in the literature. These include both regression-based <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b41">[42]</ref> and model-based <ref type="bibr" target="#b5">[6]</ref> , <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b28">[29]</ref> methods. While the former learns the shape increment given a mean initial shape, the latter trains an appearance model to predict the keypoint locations. CNN-based landmark localization methods have also been proposed in recent years <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b23">[24]</ref> and have achieved remarkable performance. Although a lot of work has been done for localizing landmarks for frontal faces, not much attention has been given to profile faces which occur often in real world scenarios. Recent methods like PIFA <ref type="bibr" target="#b19">[20]</ref>, CLVM <ref type="bibr" target="#b15">[16]</ref> and 3DDFA <ref type="bibr" target="#b55">[56]</ref> have attempted the landmark localization task on faces with varying pose angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose estimation:</head><p>The task of head pose estimation is to infer the orientation of person's head relative to the camera view. It is extremely useful in face verification for matching face similarity across different orientations. However, not much research has gone into pose estimation from unconstrained images. Non-linear manifold-based methods have been proposed in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b37">[38]</ref> to classify face images based on pose. A survey of various head pose estimation methods is provided in <ref type="bibr" target="#b33">[34]</ref>.  <ref type="bibr" target="#b24">[25]</ref> for each attribute such as male, long hair, white etc. Different features were computed for different features and they were used to train a different SVM for each attribute. CNN-based methods have also been proposed for learning attribute-based representations in <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HYPERFACE</head><p>We propose a single CNN model for simultaneous face detection, landmark localization, pose estimation and gender classification. The network architecture is deep in both vertical and horizontal directions and is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In this section, we provide a brief overview of the system and then discuss the different components in detail.</p><p>The proposed algorithm called HyperFace consists of three modules. The first one generates class independent region-proposals from the given image and scales them to 227 × 227 pixels. The second module is a CNN which takes in the resized candidate regions and classifies them as face or non-face. If a region gets classified as a face, the network additionally provides facial landmarks locations, estimated head pose and gender information. The third module is a post-processing step which involves iterative region proposals and landmarks-based non-maximum suppression (NMS) to boost the face detection score and improve the performance of individual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HyperFace Architecture</head><p>We start with Alexnet <ref type="bibr" target="#b22">[23]</ref> for image classification. The network consists of five convolutional layers along with three fully connected layers. We initialize the network with the weights of RCNN Face network trained for face detection task as described in Section 4. All the fully connected layers are removed as they encode image-classification specific information, which is not needed for pose estimation and landmark extraction. We exploit the following two observations to create our network. 1) The features in CNN are distributed hierarchically in the network. While the lower layer features are informative for landmark localization and pose estimation, the higher layer features are suitable for more complex tasks such as detection or classification <ref type="bibr" target="#b52">[53]</ref>. 2) Learning multiple correlated tasks simultaneously builds a synergy and improves the performance of individual tasks as shown in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b54">[55]</ref>. Hence, in order to simultaneously learn face detection, landmarks, pose and gender, we need to fuse the features from the intermediate layers of the network (hyperfeatures), and learn multiple tasks on top of it. Since the adjacent layers are highly correlated, we do not consider all the intermediate layers for fusion.</p><p>We fuse the max 1 , conv 3 and pool 5 layers of Alexnet, using a separate network. A naive way for fusion is directly concatenating the features. Since the feature maps for these layers have different dimensions 27 × 27 × 96, 13 × 13 × 384, 6×6×256, respectively, they cannot be concatenated directly. We therefore add conv 1a and conv 3a convolutional layers to pool 1 , conv 3 layers to obtain consistent feature maps of dimensions 6×6×256 at the output. We then concatenate the output of these layers along with pool 5 to form a 6 × 6 × 768 dimensional feature maps. The dimension is still quite high to train a multi-task framework. Hence, a 1 × 1 kernel convolution layer (conv all ) is added to reduce the dimensions <ref type="bibr" target="#b40">[41]</ref> to 6 × 6 × 192. We add a fully connected layer (f c all ) to conv all , which outputs a 3072 dimensional feature vector. At this point, we split the network into five separate branches corresponding to the different tasks. We add f c detection , f c landmarks , f c visibility , f c pose and f c gender fully connected layers, each of dimension 512, to f c all . Finally, a fully connected layer is added to each of the branch to predict the individual task labels. After every convolution or a fully connected layer, we deploy the Rectified Layer Unit (ReLU) non-linearity. We did not include any pooling operation in the fusion network as it provides local invariance which is not desired for the face landmark localization task. Taskspecific loss functions are then used to learn the weights of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>We use AFLW <ref type="bibr" target="#b21">[22]</ref> dataset for training the HyperFace network. It contains 25, 993 faces in 21, 997 real-world images with full pose, expression, ethnicity, age and gender variations. It provides annotations for 21 landmark points per face, along with the face bounding-box, face pose (yaw, pitch and roll) and gender information. We randomly selected 1000 images for testing, and keep the rest for training the network. Different loss functions are used for training the tasks of face detection, landmark localization, pose estimation and gender classification tasks.</p><p>Face Detection: We use the Selective Search <ref type="bibr" target="#b42">[43]</ref> algorithm in RCNN <ref type="bibr" target="#b11">[12]</ref> to generate region proposals for faces in an image. A region having an overlap of more than 0.5 with the ground truth bounding box is considered a positive sample (l = 1). The candidate regions with overlap less than 0.35 are treated as negative instance (l = 0). All the other regions are ignored. We use the softmax loss function given by (1) for training the face detection task.</p><formula xml:id="formula_0">loss D = −(1 − l) • log(1 − p) − l • log(p),<label>(1)</label></formula><p>where p is the probability that the candidate region is a face. The probability values p and 1 − p are obtained from the last fully connected layer for the detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Landmark Localization:</head><p>We use the 21 point markups for face landmark locations as provided in the AFLW <ref type="bibr" target="#b21">[22]</ref> dataset. Since the faces have full pose variations, some of the landmark points are invisible. The dataset provides the annotations for the visible landmarks. We consider regions with overlap greater than 0.35 with the ground truth for learning this task, while ignoring the rest. A region can be characterized by {x, y, w, h} where (x, y) are the coordinates of the center of the region and w,h are the width and height of the region respectively. Each visible landmark point is shifted with respect to the region center (x, y), and normalized by (w, h) as given by (2)</p><formula xml:id="formula_1">(a i , b i ) = x i − x w , y i − y h .<label>(2)</label></formula><p>where (x i , y i )'s are the given ground truth fiducial coordinates. The (a i , b i )'s are treated as labels for training the landmark localization task using the Euclidean loss weighted by the visibility factor. The labels for landmarks which are not visible are taken to be (0, 0). The loss in predicting the landmark location is computed from (3)</p><formula xml:id="formula_2">loss L = 1 2N N i=1 v i ((x i − a i ) 2 + ((ŷ i − b i ) 2 ),<label>(3)</label></formula><p>where (x i ,ŷ i ) is the i th landmark location predicted by the network, relative to a given region, N is the total number of landmark points (21 for AFLW <ref type="bibr" target="#b21">[22]</ref>). The visibility factor v i is 1 if the i th landmark is visible in the candidate region, else it is 0. This implies that there is no loss corresponding to invisible points and hence they do not take part during back-propagation. Learning Visibility: We also learn the visibility factor in order to test the presence of the predicted landmark. For a given region with overlap higher than 0.35, we use a simple Euclidean loss to train the visibility as shown in (4)</p><formula xml:id="formula_3">loss V = 1 N N i=1 (v i − v i ) 2 ,<label>(4)</label></formula><p>wherev i is the predicted visibility of i th landmark. The true visibility v i is 1 if the i th landmark is visible in the candidate region, else it is 0. Pose Estimation: We use the Euclidean loss to train the head pose estimates of roll (p 1 ), pitch (p 2 ) and yaw (p 3 ). We compute the loss for a candidate region having an overlap more than 0.5 with the ground truth, from (5)</p><formula xml:id="formula_4">loss P = (p 1 − p 1 ) 2 + (p 2 − p 2 ) 2 + (p 3 − p 3 ) 2 3 ,<label>(5)</label></formula><p>where (p 1 ,p 2 ,p 3 ) are the estimated pose labels. Gender Recognition: Predicting gender is a two class problem similar to face detection. For a candidate region with overlap of 0.5 with the ground truth, we compute the softmax loss given in <ref type="bibr" target="#b5">(6)</ref> </p><formula xml:id="formula_5">loss G = −(1 − g) • log(1 − p 0 ) − g • log(p 1 ),<label>(6)</label></formula><p>where g = 0 if the gender is male, or else g = 1. Here, (p 0 , p 1 ) is the two dimensional probability vector computed from the network. The total loss is computed as the weighted sum of the five individual losses as shown in (7)</p><formula xml:id="formula_6">loss f ull = t=5 t=1 λ t loss t ,<label>(7)</label></formula><p>where loss t is the individual loss corresponding to the t th task. The weight parameter λ t is decided based on the importance of the task in the overall loss. We choose (λ D = 1, λ L = 5, λ V = 0.5, λ P = 5, λ G = 2) for our experiments. Higher weights are assigned to landmark localization and pose estimation tasks as they need spatial accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Testing</head><p>From a given test image, we first extract the candidate region proposals using <ref type="bibr" target="#b42">[43]</ref>. For each of the regions, we predict the task labels by a forward-pass through the HyperFace network. Only regions with detection scores above a certain threshold are classified as face and processed for subsequent tasks. The predicted landmark points are scaled and shifted to the image co-ordinates using (8)</p><formula xml:id="formula_7">(x i , y i ) = (x i w + x,ŷ i h + y),<label>(8)</label></formula><p>where (x i ,ŷ i ) are the predicted locations of i th landmark from the network, and {x, y, w, h} are the region parameters defined in (2). Points obtained with predicted visibility less than a certain threshold are marked invisible. The pose labels obtained from the network are the estimated roll, pitch and yaw for the face region. The gender is assigned according to the label with maximum predicted probability. The fact that we obtain the landmark locations along with the detections, enables us to improve the postprocessing step so that all the tasks benefit from it. We propose two novel methods: Iterative Region Proposals (IRP) and Landmarks-based Non-Maximum Suppression (L-NMS) to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Region Proposals (IRP):</head><p>We use a fast version of Selective Search <ref type="bibr" target="#b42">[43]</ref> which extracts around 2000 regions from an image. We call this version F ast SS. It is quite possible that some faces with poor illumination or small size fail to get captured by any candidate region with a high overlap. The network would fail to detect that face due to low score. In these situations, it is desirable to have a candidate box which precisely captures the face. Hence, we generate a new candidate bounding box from the predicted landmark points using the FaceRectCalculator provided by <ref type="bibr" target="#b21">[22]</ref>, and pass it again through the network. The new region, being more localized yields a higher detection score and the corresponding tasks output, thus increasing the recall. This procedure can be repeated (say T time), so that boxes at a given step will be more localized to faces as compared to the previous step. From our experiments, we found that the localization component saturates in just one step (T = 1), which shows the strength of the predicted landmarks. The pseudo-code of IRP is presented in Algorithm 1. The usefulness of IRP can be seen in <ref type="figure" target="#fig_1">Figure 3</ref>, which shows a low-resolution face region cropped from the top-right image in <ref type="figure" target="#fig_3">Figure 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Landmarks-based Non-Maximum Suppression (L-NMS):</head><p>The traditional approach of non-maximum suppression involves selecting the top scoring region and discarding all the other regions with overlap more than a certain threshold. This method can fail in the following two scenarios: 1) If a region corresponding to the same detected face has less overlap with the highest scoring region, it can be detected as a separate face.</p><p>2) The highest scoring region might not always be localized well for the face, which can create some discrepancy if two faces are close together. To overcome these issues, we perform NMS on a new region whose bounding box is defined by the boundary co-ordinates as</p><formula xml:id="formula_8">[min i x i , min i y i , max i x i , max i y i ]</formula><p>of the landmarks for the given region. In this way, the candidate regions would get close to each other, thus decreasing the ambiguity of the overlap and improving localization. final bounding box ← F aceRectCalculator(final fids)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12: end</head><p>We apply landmarks-based NMS to keep the top-k boxes, based on the detection scores. The detected face corresponds to the region with maximum score. The landmark points, pose estimates and gender classification scores are decided by the median of the top k boxes obtained. Hence, the predictions do not rely only on one face region, but considers the votes from top-k regions for generating the final output. From our experiments, we found that the best results are obtained with the value of k being 5. The pseudocode for L-NMS is given in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NETWORK ARCHITECTURES</head><p>To emphasize the importance of multitask approach and fusion of the intermediate layers of CNN, we study the performance of simpler CNNs devoid of such features. We evaluate four R-CNN-based models, one for each task of face detection, landmark localization, pose estimation and gender recognition. We also build a separate Multitask Face model which performs multitask learning just like Hyper-Face, but doesn't fuse the information from the intermediate layers. These models are described as follows: RCNN Face: This model is used for face detection task. The network architecture is shown in <ref type="figure" target="#fig_3">Figure 4(a)</ref>. For training RCNN Face, we use the region proposals from AFLW <ref type="bibr" target="#b21">[22]</ref> training set, each associated with a face label based on the overlap with the ground truth. The loss is computed as per (1). The model parameters are initialized using the Alexnet <ref type="bibr" target="#b22">[23]</ref> weights trained on the Imagenet dataset <ref type="bibr" target="#b6">[7]</ref>. Once trained, the learned parameters from this network are used to initialize other models including Multitask Face and HyperFace as the standard Imagenet initialization doesn't converge well. We also perform a linear bounding box regression to localize the face co-ordinates.</p><p>RCNN Fiducial: This model is used for locating the landmarks. The network architecture is shown in <ref type="figure" target="#fig_3">Figure 4(b)</ref>. It simultaneously learns the visibility of the points to account for the invisible points at test time, and thus can be used as a standalone fiducial extractor. The loss functions for landmarks localization and visibility of points are computed using (3) and (4), respectively. Only region proposals which have an overlap&gt; 0.5 with the ground truth bounding box are used for training. The model parameters are initialized from RCNN Face.</p><p>RCNN Pose: This model is used for head pose estimation task. The outputs of the network are roll, pitch and yaw of the face. <ref type="figure" target="#fig_3">Figure 4(c)</ref> presents the network architecture. Similar to RCNN Fiducial, only region proposals with overlap&gt; 0.5 with the ground truth bounding box are used for training. The training loss is computed using (5). RCNN Gender: This model is used for face gender recognition task. The network architecture is shown in <ref type="figure" target="#fig_3">Figure 4(d)</ref>. It has the same training set as RCNN Fiducial and RCNN Pose. The training loss is computed using (6).</p><p>Multitask Face: Similar to HyperFace, this model is used to simultaneously detect face, localize landmarks, estimate pose and predict its gender. The only difference between Multitask Face and HyperFace is that HyperFace fuses the intermediate layers of the network whereas Multitask Face combines the tasks using the common fully connected layer at the end of the network as shown in <ref type="figure">Figure 5</ref>. Since it provides the landmarks and face score, it leverages iterative region proposals and landmark-based NMS post-processing algorithms during evaluation.</p><p>The performance of all the above models for their respective tasks are evaluated and discussed in details in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>We evaluated the proposed HyperFace method, along with Multask Face, RCNN Face, RCNN Fiducial, RCNN Pose and RCNN Gender on six challenging datasets:</p><p>• Annotated Face in-the-Wild (AFW) <ref type="bibr" target="#b56">[57]</ref> for evaluating face detection, landmark localization, and pose estimation tasks • Annotated Facial Landmarks in the Wild (AFLW) <ref type="bibr" target="#b21">[22]</ref> for evaluating landmark localization and pose estimation tasks • Face Detection Dataset and Benchmark (FDDB) <ref type="bibr" target="#b17">[18]</ref> and PASCAL faces <ref type="bibr" target="#b47">[48]</ref> for evaluating the face detection results • Large-scale CelebFaces Attributes (CelebA) <ref type="bibr" target="#b30">[31]</ref> and LFWA <ref type="bibr" target="#b16">[17]</ref> for evaluating gender recognition results. Our method was trained on randomly selected 20, 997 images from the AFLW dataset using Caffe <ref type="bibr" target="#b18">[19]</ref>. The remaining 1000 images were used for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Face Detection</head><p>We show face detection results for AFW, PASCAL and FDDB datasets. The AFW dataset <ref type="bibr" target="#b56">[57]</ref> was collected from Flickr and the images in this dataset contain large variations in appearance and viewpoint. In total there are 205 images with 468 faces in this dataset. The FDDB dataset <ref type="bibr" target="#b17">[18]</ref> consists of 2,845 images containing 5,171 faces collected from news articles on the Yahoo website. This dataset is the most widely used benchmark for unconstrained face detection. The PASCAL faces dataset <ref type="bibr" target="#b47">[48]</ref> was collected from the test set of PASCAL person layout dataset, which is a subset from PASCAL VOC <ref type="bibr" target="#b7">[8]</ref>. This dataset contains 1335 faces from 851 images with large appearance variations. For improved face detection performance, we learn a SVM classifier on top of f c detection features using the training splits from the FDDB dataset.</p><p>Some of the recent published methods compared in our evaluations include DP2MFD <ref type="bibr" target="#b34">[35]</ref>, Faceness <ref type="bibr" target="#b49">[50]</ref>, Head-Hunter <ref type="bibr" target="#b31">[32]</ref>, JointCascade <ref type="bibr" target="#b4">[5]</ref>, CCF <ref type="bibr" target="#b48">[49]</ref>, SquaresChnFtrs-5 <ref type="bibr" target="#b31">[32]</ref>, CascadeCNN <ref type="bibr" target="#b26">[27]</ref>, Structured Models <ref type="bibr" target="#b47">[48]</ref>, DDFD <ref type="bibr" target="#b9">[10]</ref>, NDPFace <ref type="bibr" target="#b29">[30]</ref>, PEP-Adapt <ref type="bibr" target="#b25">[26]</ref>, TSM <ref type="bibr" target="#b56">[57]</ref>, as well as three commercial systems Face++, Picasa and Face.com. The precision-recall curves of different detectors corresponding to the AFW and the PASCAL faces datasets are shown in <ref type="figure">Figures 6 (a)</ref> and (b), respectively. <ref type="figure" target="#fig_5">Figure 7</ref> compares the performance of different detectors using the Receiver Operating Characteristic (ROC) curves on the FDDB dataset. As can be seen from these figures, Hyper-Face outperforms all the reported academic and commercial detectors on the AFW and PASCAL datasets, with a high mean average precision (mAP ) of 97.9% and 92.46%, respectively. The FDDB dataset is very challenging for Hy-perFace and any other R-CNN-based face detection methods, as the dataset contains many small and blurred faces. Firstly, few of these faces do not get included in the region proposals from selective search. Secondly, re-sizing small faces to the input size of 227 × 227 adds distortion to the face resulting in low detection score. In spite of these issues, HyperFace performance is comparable to recently published deep learning-based face detection methods such as DP2MFD <ref type="bibr" target="#b34">[35]</ref> and Faceness <ref type="bibr" target="#b49">[50]</ref> on the FDDB dataset <ref type="bibr" target="#b0">1</ref> with mAP of 90.1%.</p><p>It is interesting to note the performance differences between RCNN Face, Multitask Face and HyperFace for the face detection tasks. <ref type="figure" target="#fig_5">Figures 6, and 7</ref> clearly show that multitask CNNs (Multitask Face and HyperFace) outperform RCNN Face by a wide margin. The boost in the performance gain is mainly due to the following two reasons. Firstly, multitask learning approach helps the network to learn improved features for face detection which is evident from their mAP values on the AFW dataset. Using just the linear bounding box regression and traditional NMS, HyperFace obtains a mAP of 94% ( <ref type="figure" target="#fig_0">Figure 12)</ref> while RCNN Face achieves a mAP of 90.3%. Secondly, having landmark information associated with detection boxes makes it easier to localize the bounding box to a face, by using IRP and L-NMS algorithms. On the other hand, the HyperFace and Multitask Face perform comparably to each other for all the face detection datasets which suggests that the network doesn't gain much by fusing intermediate layers for the face detection task.  <ref type="figure">Fig. 8</ref>. Cumulative error distribution curves for landmark localization on the AFW dataset. The numbers in the legend are the fraction of testing faces that have average error below (5%) of the face size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Landmark Localization</head><p>We evaluate the performance of different landmark localization algorithms on the AFW <ref type="bibr" target="#b56">[57]</ref> and AFLW <ref type="bibr" target="#b21">[22]</ref> datasets. Both of these datasets contain faces with full pose variations. Some of the methods compared include Multiview Active Appearance Model-based method (Multi. AAM) <ref type="bibr" target="#b56">[57]</ref>, Constrained Local Model (CLM) <ref type="bibr" target="#b35">[36]</ref>, Oxford facial landmark detector <ref type="bibr" target="#b8">[9]</ref>, Zhu <ref type="bibr" target="#b56">[57]</ref>, FaceDPL <ref type="bibr" target="#b57">[58]</ref>, JointCascade <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b0">1</ref>. http://vis-www.cs.umass.edu/fddb/results.html CDM <ref type="bibr" target="#b51">[52]</ref>, RCPR <ref type="bibr" target="#b2">[3]</ref>, ESR <ref type="bibr" target="#b3">[4]</ref>, SDM <ref type="bibr" target="#b45">[46]</ref> and 3DDFA <ref type="bibr" target="#b55">[56]</ref>. Although both of these datasets provide ground truth bounding boxes, we do not use them for evaluating on HyperFace, Multitask Face and RCNN Fiducial. Instead we use the respective algorithms to detect both the face and its fiducial points. Since, the RCNN Fiducial cannot detect faces, we provide it with the detections from the HyperFace. <ref type="figure">Figure 8</ref> compares the performance of different landmark localization methods on the AFW dataset using the protocol defined in <ref type="bibr" target="#b57">[58]</ref>. In this figure, (*) indicates that models that are evaluated on near frontal faces or use hand-initialization <ref type="bibr" target="#b56">[57]</ref>. The dataset provides six keypoints for each face which are: left eye center, right eye center, nose tip, mouth left, mouth center and mouth right. We compute the error as the mean distance between the predicted and ground truth keypoints, normalized by the face size. The plots for comparison were obtained from <ref type="bibr" target="#b57">[58]</ref>.   For the AFLW dataset, we calculate the error using all the visible keypoints. For AFW, we adopt the same protocol as defined in <ref type="bibr" target="#b55">[56]</ref>. The only difference is that our AFLW testset consists of only 1000 images with 1132 face samples, since we use the rest of the images for training. To be consistent with the protocol, we randomly create a subset of 450 samples from our testset whose absolute yaw angles within <ref type="figure" target="#fig_7">Figure 9</ref> compares the performance of different landmark localization methods. We obtain the comparison plots from <ref type="bibr" target="#b55">[56]</ref> where the evaluations for RCPR, ESR and SDM are carried out after adapting the algorithms to face profiling. <ref type="table" target="#tab_1">Table 1</ref> provides the Normalized Mean Error (NME) for AFLW dataset, for each of the pose group.</p><formula xml:id="formula_9">[0 • , 30 • ], [30 • , 60 • ] and [60 • , 90 • ] are 1/3 each.</formula><p>As can be seen from the figures, HyperFace outperforms many recent state-of-the-art landmark localization methods including FaceDPL <ref type="bibr" target="#b57">[58]</ref>, 3DDFA <ref type="bibr" target="#b55">[56]</ref> and SDM <ref type="bibr" target="#b45">[46]</ref>. <ref type="table" target="#tab_1">Table 1</ref> shows that HyperFace performs consistently accurate over all pose angles. This clearly suggests that while most of the methods work well on frontal faces, HyperFace is able to predict landmarks for faces with full pose variations. Moreover, we find that RCNN Fiducial and Multitask Face outperforms the earlier methods as well, performing comparably to each other. The HyperFace has an advantage over them as it uses the intermediate layers for fusion. The  local information is contained well in the lower layers of CNN and becomes invariant as depth increases. Fusing the layers brings out that hidden information which boosts the performance for the landmark localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Pose Estimation</head><p>We evaluate RCNN Pose, Multitask Face and HyperFace on the AFW <ref type="bibr" target="#b56">[57]</ref> and AFLW <ref type="bibr" target="#b21">[22]</ref> datasets for pose estimation task. The detection boxes used for evaluating the landmark localization task are used here as well for initialization. For the AFW dataset, we compare our approach with Multi. AAM <ref type="bibr" target="#b56">[57]</ref>, Multiview HoG <ref type="bibr" target="#b56">[57]</ref>, FaceDPL 2 <ref type="bibr" target="#b57">[58]</ref> and face.com. Note that multiview AAMs are initialized using the ground truth bounding boxes (denoted by *). <ref type="figure" target="#fig_9">Figure 11</ref> shows the cumulative error distribution curves on AFW dataset. The curve provides the fraction of faces for which the estimated pose is within some error tolerance. As can be seen from the figure, the HyperFace method achieves the best performance and beats FaceDPL by a large margin. For the AFLW dataset, we do not have pose estimation evaluation for any previous method. Hence, we show the performance of our method for different pose angles: roll, pitch and yaw in <ref type="figure" target="#fig_8">Figure 10</ref> (a), (b) and (c) respectively. It can be seen that the network is able to learn roll, and pitch information better than yaw.  HyperFace achieves a boosted performance due to the intermediate layers fusion. It shows that tasks which rely on the structure and orientation of the face work well with features from lower layers of the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Gender Recognition</head><p>We show the gender recognition performance on CelebA <ref type="bibr" target="#b30">[31]</ref> and LFWA <ref type="bibr" target="#b16">[17]</ref> datasets since these datasets come with gender information. The CelebA and LFWA datasets contain labeled images selected from the Celeb-Faces <ref type="bibr" target="#b38">[39]</ref> and LFW <ref type="bibr" target="#b16">[17]</ref> datasets, respectively <ref type="bibr" target="#b30">[31]</ref>. The CelebA dataset contains 10,000 identities and there are 200,000 images in total. The LFWA dataset has 13,233 images of 5,749 identities. We compare our approach with Face-Tracer <ref type="bibr" target="#b24">[25]</ref>, PANDA-w <ref type="bibr" target="#b53">[54]</ref>, PANDA-1 <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b27">[28]</ref> with ANet and <ref type="bibr" target="#b30">[31]</ref>. The gender recognition performance of different methods is reported in <ref type="table" target="#tab_6">Table 2</ref>. On the LFWA dataset, our method outperforms PANDA <ref type="bibr" target="#b53">[54]</ref> and FaceTracer <ref type="bibr" target="#b24">[25]</ref>, and is equal to <ref type="bibr" target="#b30">[31]</ref>. On the CelebA dataset, our method performs comparably to <ref type="bibr" target="#b30">[31]</ref>. Unlike <ref type="bibr" target="#b30">[31]</ref> which uses 180, 000 images for training and validation, we only use 20, 000 images from validation set of CelebA to fine-tune the network.</p><p>Similar to the face detection task, we find that gender recognition performs better for HyperFace and Multitask Face as compared to RCNN Gender proving that learning related tasks together improves the discriminating capability of the individual tasks. Again, we do not see  <ref type="figure" target="#fig_0">Figure 12</ref> provides an experimental analysis of the postprocessing methods: IRP and L-NMS, on face detection task on the AFW dataset. Fast SS denotes the quick version of selective search which produces around 2000 region proposals and takes 2s per image to compute. On the other hand, Quality SS refers to its slow version which outputs more than 10, 000 region proposals consuming more than 10s for one image. The HyperFace with a linear bounding box regression and traditional NMS achieves a mAP of 94%. Just by replacing them with L-NMS provides a boost of 1.2%. In this case, bounding-box is constructed using the landmarks information rather linear regression. Additionaly, we can see from the figure that although Quality SS generates more region proposals, it performs worse than Fast SS with ierative region proposals. IRP adds 300 new regions for a typical image consuming less than 0.5s which makes it highly efficient as compared to Quality SS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effect of Post-Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Runtime</head><p>The Hyperface method was tested on a machine with 8 cores and GTX TITAN-X GPU. The overall time taken to perform all the four tasks was 3s per image. The limitation was not because of CNN, but due to selective search which takes approximately 2s to generate candidate region proposals. One forward pass through the HyperFace network takes only 0.2s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>We discuss few crucial observations from our experiments. First, all the face related tasks benefit from using the multitask learning framework. The gain is mainly due to the network's ability to learn more discriminative features, and post-processing methods which can be leveraged by having landmarks as well as detection scores for a region. Secondly, fusing intermediate layers improves the performance for structure dependent tasks of pose estimation and landmarks localization, as the features become invariant to geometry in deeper layers of CNN. The HyperFace exploits these observations to improve the performance for all the four tasks.</p><p>We also visualize the features learned by the HyperFace network. <ref type="figure" target="#fig_1">Figure 13</ref> shows the network activation for a few selected feature maps out of 192 from the conv all layer. It can be seen that some feature maps are dedicated solely for a single task while others can be used to predict different tasks. For example, feature map 27 and 186 can be used for face detection and gender recognition, respectively. The former distinguishes the face and non-face regions whereas the latter outputs high activation for the female faces. Similarly, feature map 19 shows high activation near eyes and mouth regions, while feature map 96 gives a rough contour of the face orientation. Both of these features can be used for landmark localization and pose estimation tasks.</p><p>Several qualitative results of our method on the AFW, PASCAL and FDDB datasets are shown in <ref type="figure" target="#fig_3">Figure 14</ref>. As can be seen from this figure, our method is able to simultaneously perform all the four tasks on images containing extreme pose, illumination, and resolution variations with cluttered background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we presented a multi-task deep learning method called HyperFace for simultaneously detecting faces, localizing landmarks, estimating head pose and identifying gender. Extensive experiments using various publicly available unconstrained datasets demonstrate the effectiveness of our method on all four tasks. In future, we will evaluate the performance of our method on other applications such as simultaneous human detection and human pose estimation, object recognition and pedestrian detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence <ref type="figure" target="#fig_1">Fig. 13</ref>. Activations of selected feature maps from conv all layer of the HyperFace architecture. Green and yellow colors denote high activation whereas blue denotes low activation units. These features depict the distinguishable face traits for the tasks of face detection, landmarks localization, pose estimation and gender recognition.</p><p>Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of the proposed HyperFace. The network is able to classify a given image region as face or non-face, estimate the head pose, locate face landmarks and recognize gender.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Candidate face region (red box on left) obtained using Selective Search gives a low score for face detection, while landmarks are correctly localized. We generate a new face region (red box on right) using the landmarks information and feed it through the network to increase the detection score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 8 : 9 :</head><label>289</label><figDesc>Landmarks-based NMS 1: Get detected boxes from Algorithm 1 2: fids ← get hyperf ace f iducials(detected boxes) 3: precise boxes ← [min x , min y , max x , max y ](fids) 4: faces ← nms(precise boxes, overlap) 5: for each face in faces do 6: top-k boxes ← Get top-k scoring boxes 7: final fids ← median(f ids(top-k boxes)) final pose ← median(pose(top-k boxes)) final gender ← median(gender(top-k boxes)) 10: final visibility ← median(visibility(top-k boxes)) 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>R-CNN-based network architectures for (a) Face Detection (RCNN Face), (b) Landmark Localization (RCNN Fiducial), (c) Pose Estimation (RCNN Pose), and (d) Gender Recognition (RCNN Gender). The numbers on the left denote the kernel size and the numbers on the right denote the cardinality of feature maps for a given layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Network Architecture of Multitask Face. The numbers on the left denote the kernel size and the numbers on the right denote the cardinality of feature maps for a given layer. Performance evaluation on (a) the AFW dataset, (b) the PASCAL faces dataset. The numbers in the legend are the mean average precision for the corresponding datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Performance evaluation on the FDDB dataset. The numbers in the legend are the mean average precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Cumulative error distribution curves for landmark localization on the AFLW dataset. The numbers in the legend are the fraction of testing faces that have average error below (5%) of the face size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Performance evaluation of Pose Estimation on AFLW dataset for (a) roll (b) pitch and (c) yaw angles. The numbers in the legend are the mean error in degrees for the respective pose angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Cumulative error distribution curves for pose estimation on AFW dataset. The numbers in the legend are the percentage of faces that are labeled within ±15 • error tolerance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>LR (ap = 94%) Fast SS + L−NMS (ap = 95.2%) Quality SS + L−NMS (ap = 97.3%) Fast SS + L−NMS + IRP (ap = 97.9%) Variations in performance of HyperFace with respect to the Iterative Region Proposals and Landmarks-based NMS. The numbers in the legend are the mean average precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Qualitative results of our method. The blue boxes denote detected male faces, while pink boxes denote female faces. The green dots provide the landmark locations. Pose estimates for each face are shown on top of the boxes in the order of roll, pitch and yaw.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1</head><label>1</label><figDesc>Iterative Region Proposals 1: boxes ← selective search(image) 2: scores ← get hyperf ace scores(boxes) 3: detected boxes ← boxes(scores ≥ threshold) 4: new boxes ← detected boxes 5: for stage = 1 to T do 6: fids ← get hyperf ace f iducials(new boxes)</figDesc><table><row><cell>7:</cell><cell>new boxes ← F aceRectCalculator(fids)</cell></row><row><cell>8:</cell><cell>deteced boxes ← [deteced boxes|new boxes]</cell></row></table><note>9: end 10: final scores ← get hyperf ace scores(detected boxes)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 The</head><label>1</label><figDesc></figDesc><table><row><cell cols="6">NME(%) of face alignment results on AFLW test set with the best</cell></row><row><cell></cell><cell cols="3">results highlighted.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">AFLW Dataset (21 pts)</cell><cell></cell></row><row><cell>Method</cell><cell cols="4">[0, 30] [30, 60] [60, 90] mean</cell><cell>std</cell></row><row><cell>CDM</cell><cell>8.15</cell><cell>13.02</cell><cell>16.17</cell><cell>12.44</cell><cell>4.04</cell></row><row><cell>RCPR</cell><cell>5.43</cell><cell>6.58</cell><cell>11.53</cell><cell>7.85</cell><cell>3.24</cell></row><row><cell>ESR</cell><cell>5.66</cell><cell>7.12</cell><cell>11.94</cell><cell>8.24</cell><cell>3.29</cell></row><row><cell>SDM</cell><cell>4.75</cell><cell>5.55</cell><cell>9.34</cell><cell>6.55</cell><cell>2.45</cell></row><row><cell>3DDFA</cell><cell>5.00</cell><cell>5.06</cell><cell>6.74</cell><cell>5.60</cell><cell>0.99</cell></row><row><cell>3DDFA+SDM</cell><cell>4.75</cell><cell>4.83</cell><cell>6.38</cell><cell>5.32</cell><cell>0.92</cell></row><row><cell>RCNN Fiducial</cell><cell>4.49</cell><cell>4.70</cell><cell>5.09</cell><cell>4.76</cell><cell>0.30</cell></row><row><cell>Multitask Face</cell><cell>4.20</cell><cell>4.93</cell><cell>5.23</cell><cell>4.79</cell><cell>0.53</cell></row><row><cell>HyperFace</cell><cell>3.93</cell><cell>4.14</cell><cell>4.71</cell><cell>4.26</cell><cell>0.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The performance traits of RCNN Pose, Multitask Face and HyperFace for pose estimation task are similar to that of the landmarks localization task. RCNN Pose and Multitask Face perform comparable to each other whereas 2. Available at: http://www.ics.uci.edu/ ∼ dramanan/software/ face/face journal.pdf</figDesc><table><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fraction of Test Faces</cell><cell>0.4 0.6 0.8 0.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Multi. HoG (74.6%) Multi. AAMs (36.8%) face.com (64.3%) FaceDPL (89.4%) RCNN_Pose (95.0%) Multitask_Face (95.9%) HyperFace (97.7%)</cell></row><row><cell></cell><cell>0 0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Pose Estimation Error (in degrees)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 2</head><label>2</label><figDesc>Performance comparison (in %) of gender recognition on CelebA and LFWA datasets.</figDesc><table><row><cell>Method</cell><cell cols="2">CelebA LFWA</cell></row><row><cell>FaceTracer [25]</cell><cell>91</cell><cell>84</cell></row><row><cell>PANDA-w [54]</cell><cell>93</cell><cell>86</cell></row><row><cell>PANDA-1 [54]</cell><cell>97</cell><cell>92</cell></row><row><cell>[28]+ANet</cell><cell>95</cell><cell>91</cell></row><row><cell>LNets+ANet [31]</cell><cell></cell><cell>94</cell></row><row><cell>RCNN Gender</cell><cell>95</cell><cell>91</cell></row><row><cell>Multitask Face</cell><cell>97</cell><cell>93</cell></row><row><cell>HyperFace</cell><cell>97</cell><cell>94</cell></row><row><cell cols="3">much difference in the performance of Multitask Face and</cell></row><row><cell cols="3">HyperFace suggesting intermediate layers do not contribute</cell></row><row><cell cols="2">much for the gender recognition task.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multilevel image coding with hyperfeatures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="15" to="27" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Biased manifold embedding: A framework for person-independent head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR &apos;07. IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="109" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active shape models&amp;mdash;their training and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hello! my name is... buffy&quot; -automatic naming of characters in tv video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="92" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-view face detection using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Farfade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">R-cnns for pose estimation and action detection. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5212</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Head pose estimation by non-linear embedding and mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ranganath</surname></persName>
		</author>
		<idno>II- 342-5</idno>
	</analytic>
	<monogr>
		<title level="m">Image Processing, 2005. ICIP 2005. IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005-09" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down reasoning with convolutional latent-variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno>abs/1507.05699</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS- 2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pose-invariant 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. Burges, L. Bottou, and K. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Face alignment by local deep descriptor regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno>abs/1601.07950</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FaceTracer: A Search Engine for Large Collections of Images with Faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic elastic part model for unsupervised face detector adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="793" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning surf cascade for fast and accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3468" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face alignment via component-based discriminative search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>D. A. Forsyth, P. H. S. Torr, and A. Zisserman</editor>
		<imprint>
			<biblScope unit="volume">5303</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="85" />
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A fast and accurate unconstrained face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8692</biblScope>
			<biblScope unit="page" from="720" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Head pose estimation in computer vision: A survey. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="607" to="626" />
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A deep pyramid deformable part model for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics Theory, Applications and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deformable model fitting by regularized landmark mean-shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;13</title>
		<meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;13<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3626" to="3633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Head pose estimation using view based eigenspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 16th International Conference on</title>
		<meeting>16th International Conference on</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="302" to="305" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;13</title>
		<meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;13<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gauss-newton deformable part models for face alignment in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Computer Vision, ICCV &apos;11</title>
		<meeting>the 2011 International Conference on Computer Vision, ICCV &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Supervised descent method and its application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xuehan-Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename><surname>De La</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learn to combine multiple hypotheses for accurate face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Computer Vision Workshops, ICCVW &apos;13</title>
		<meeting>the 2013 IEEE International Conference on Computer Vision Workshops, ICCVW &apos;13<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Face detection by structural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-scale recognition with dag-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pose-free facial landmark fitting via optimized part mixtures and cascaded deformable shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1944" to="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1311.2901</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1637" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1511.07212</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">FaceDPL: Detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
