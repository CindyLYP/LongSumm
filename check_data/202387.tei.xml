<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Snorkel: Rapid Training Data Creation with Weak Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-28">28 Nov 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ratner</surname></persName>
							<email>ajratner@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
							<email>bach@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Ehrenberg</surname></persName>
							<email>henryre@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Fries</surname></persName>
							<email>jfries@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wu</surname></persName>
							<email>senwu@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Snorkel: Rapid Training Data Creation with Weak Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-28">28 Nov 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.14778/3157794.3157797</idno>
					<idno type="arXiv">arXiv:1711.10160v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train stateof-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8× faster and increase predictive performance an average 45.5% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8× speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the last several years, there has been an explosion of interest in machine-learning-based systems across industry, government, and academia, with an estimated spend this year of $12.5 billion <ref type="bibr" target="#b0">[1]</ref>. A central driver has been the <ref type="figure">Figure 1</ref>: In Example 1.1, training data is labeled by sources of differing accuracy and coverage. Two key challenges arise in using this weak supervision effectively. First, we need a way to estimate the unknown source accuracies to resolve disagreements. Second, we need to pass on this critical lineage information to the end model being trained.</p><p>advent of deep learning techniques, which can learn taskspecific representations of input data, obviating what used to be the most time-consuming development task: feature engineering. These learned representations are particularly effective for tasks like natural language processing and image analysis, which have high-dimensional, high-variance input that is impossible to fully capture with simple rules or handengineered features <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>. However, deep learning has a major upfront cost: these methods need massive training sets of labeled examples to learn from-often tens of thousands to millions to reach peak predictive performance <ref type="bibr" target="#b46">[47]</ref>.</p><p>Such training sets are enormously expensive to create, especially when domain expertise is required. For example, reading scientific papers, analyzing intelligence data, and interpreting medical images all require labeling by trained subject matter experts (SMEs). Moreover, we observe from our engagements with collaborators like research labs and major technology companies that modeling goals such as class definitions or granularity change as projects progress, necessitating re-labeling. Some big companies are able to absorb this cost, hiring large teams to label training data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b30">31]</ref>. However, the bulk of practitioners are increasingly turning to weak supervision: cheaper sources of labels that are noisier or heuristic. The most popular form is distant supervision, in which the records of an external knowledge base are heuristically aligned with data points to produce noisy labels <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref>. Other forms include crowdsourced labels <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b49">50]</ref>, rules and heuristics for labeling data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b51">52]</ref>, and others <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref>. While these sources are inexpensive, they often have limited accuracy and coverage.</p><p>Ideally, we would combine the labels from many weak supervision sources to increase the accuracy and coverage of our training set. However, two key challenges arise in doing so effectively. First, sources will overlap and conflict, and to resolve their conflicts we need to estimate their accuracies and correlation structure, without access to ground truth. Second, we need to pass on critical lineage information about label quality to the end model being trained. Example 1.1. In <ref type="figure">Figure 1</ref>, we obtain labels from a high accuracy, low coverage Source 1, and from a low accuracy, high coverage Source 2, which overlap and disagree (splitcolor points). If we take an unweighted majority vote to resolve conflicts, we end up with null (tie-vote) labels. If we could correctly estimate the source accuracies, we would resolve conflicts in the direction of Source 1.</p><p>We would still need to pass this information on to the end model being trained. Suppose that we took labels from Source where available, and otherwise took labels from Source 2. Then, the expected training set accuracy would be 60.3%only marginally better than the weaker source. Instead we should represent training label lineage in end model training, weighting labels generated by high-accuracy sources more.</p><p>In recent work, we developed data programming as a paradigm for addressing both of these challenges by modeling multiple label sources without access to ground truth, and generating probabilistic training labels representing the lineage of the individual labels. We prove that, surprisingly, we can recover source accuracy and correlation structure without hand-labeled training data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>. However, there are many practical aspects of implementing and applying this abstraction that have not been previously considered.</p><p>We present Snorkel, the first end-to-end system for combining weak supervision sources to rapidly create training data. We built Snorkel as a prototype to study how people could use data programming, a fundamentally new approach to building machine learning applications. Through weekly hackathons and office hours held at Stanford University over the past year, we have interacted with a growing user community around Snorkel's open source implementation. <ref type="bibr" target="#b0">1</ref> We have observed SMEs in industry, science, and government deploying Snorkel for knowledge base construction, image analysis, bioinformatics, fraud detection, and more. From this experience, we have distilled three principles that have shaped Snorkel's design:</p><p>1. Bring All Sources to Bear: The system should enable users to opportunistically use labels from all available weak supervision sources.</p><p>2. Training Data as the Interface to ML: The system should model label sources to produce a single, probabilistic label for each data point and train any of a wide range of classifiers to generalize beyond those sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Supervision as Interactive Programming:</head><p>The system should provide rapid results in response to user supervision. We envision weak supervision as the REPL-like interface for machine learning.</p><p>Our work makes the following technical contributions:</p><p>A Flexible Interface for Sources: We observe that the heterogeneity of weak supervision strategies is a stumbling block for developers. Different types of weak supervision http://snorkel.stanford.edu operate on different scopes of the input data. For example, distant supervision has to be mapped programmatically to specific spans of text. Crowd workers and weak classifiers often operate over entire documents or images. Heuristic rules are open ended; they can leverage information from multiple contexts simultaneously, such as combining information from a document's title, named entities in the text, and knowledge bases. This heterogeneity was cumbersome enough to completely block users of early versions of Snorkel.</p><p>To address this challenge, we built an interface layer around the abstract concept of a labeling function (LF). We developed a flexible language for expressing weak supervision strategies and supporting data structures. We observed accelerated user productivity with these tools, which we validated in a user study where SMEs build models 2.8× faster and increase predictive performance an average 45.5% versus seven hours of hand labeling.</p><p>Tradeoffs in Modeling of Sources: Snorkel learns the accuracies of weak supervision sources without access to ground truth using a generative model <ref type="bibr" target="#b37">[38]</ref>. Furthermore, it also learns correlations and other statistical dependencies among sources, correcting for dependencies in labeling functions that skew the estimated accuracies <ref type="bibr" target="#b4">[5]</ref>. This paradigm gives rise to previously unexplored tradeoff spaces between predictive performance and speed. The natural first question is: when does modeling the accuracies of sources improve predictive performance? Further, how many dependencies, such as correlations, are worth modeling?</p><p>We study the tradeoffs between predictive performance and training time in generative models for weak supervision. While modeling source accuracies and correlations will not hurt predictive performance, we present a theoretical analysis of when a simple majority vote will work just as well. Based on our conclusions, we introduce an optimizer for deciding when to model accuracies of labeling functions, and when learning can be skipped in favor of a simple majority vote. Further, our optimizer automatically decides which correlations to model among labeling functions. This optimizer correctly predicts the advantage of generative modeling over majority vote to within 2.16 accuracy points on average on our evaluation tasks, and accelerates pipeline executions by up to 1.8×. It also enables us to gain 60%-70% of the benefit of correlation learning while saving up to 61% of training time (34 minutes per execution).</p><p>First End-to-End System for Data Programming: Snorkel is the first system to implement our recent work on data programming <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>. Previous ML systems that we and others developed <ref type="bibr" target="#b51">[52]</ref> required extensive feature engineering and model specification, leading to confusion about where to inject relevant domain knowledge. While programming weak supervision seems superficially similar to feature engineering, we observe that users approach the two processes very differently. Our vision-weak supervision as the sole port of interaction for machine learning-implies radically different workflows, requiring a proof of concept.</p><p>Snorkel demonstrates that this paradigm enables users to develop high-quality models for a wide range of tasks. We report on two deployments of Snorkel, in collaboration with the U.S. Department of Veterans Affairs and Stanford Hospital and Clinics, and the U.S. Food and Drug Administration, where Snorkel improves over heuristic baselines by an average 110%. We also report results on four open-"causes", "induces", "linked to", "aggravates", … </p><formula xml:id="formula_0">Ontology(ctd, [A, B, -C]) Pattern("{{0}}causes{{1}}") CustomFn(x,y : heuristic(x,y)) Λ LABEL MATRIX MODELING OPTIMIZER Λ " Λ # Λ $ GENERATIVE MODEL PROBABILISTIC TRAINING DATA &amp; DISCRIMINATIVE MODEL</formula><p>We study a patient who became quadriplegic after parenteral magnesium administration for preeclampsia.</p><p>We study a patient who became quadriplegic after parenteral magnesium administration for preeclampsia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNLABELED DATA</head><p>We study a patient who became quadriplegic after parenteral magnesium administration for preeclampsia. SNORKEL <ref type="figure">Figure 2</ref>: An overview of the Snorkel system. (1) SME users write labeling functions (LFs) that express weak supervision sources like distant supervision, patterns, and heuristics. (2) Snorkel applies the LFs over unlabeled data and learns a generative model to combine the LFs' outputs into probabilistic labels. <ref type="formula" target="#formula_9">3</ref>Snorkel uses these labels to train a discriminative classification model, such as a deep neural network.</p><p>source datasets that are representative of other Snorkel deployments, including bioinformatics, medical image analysis, and crowdsourcing; on which Snorkel beats heuristics by an average 153% and comes within an average 3.60% of the predictive performance of large hand-curated training sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SNORKEL ARCHITECTURE</head><p>Snorkel's workflow is designed around data programming <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>, a fundamentally new paradigm for training machine learning models using weak supervision, and proceeds in three main stages ( <ref type="figure">Figure 2</ref>):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Writing Labeling Functions:</head><p>Rather than hand-labeling training data, users of Snorkel write labeling functions, which allow them to express various weak supervision sources such as patterns, heuristics, external knowledge bases, and more. This was the component most informed by early interactions (and mistakes) with users over the last year of deployment, and we present a flexible interface and supporting data model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Modeling Accuracies and Correlations: Next,</head><p>Snorkel automatically learns a generative model over the labeling functions, which allows it to estimate their accuracies and correlations. This step uses no ground-truth data, learning instead from the agreements and disagreements of the labeling functions. We observe that this step improves end predictive performance 5.81% over Snorkel with unweighted label combination, and anecdotally that it streamlines the user development experience by providing actionable feedback about labeling function quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training a Discriminative Model:</head><p>The output of Snorkel is a set of probabilistic labels that can be used to train a wide variety of state-of-the-art machine learning models, such as popular deep learning models. While the generative model is essentially a re-weighted combination of the user-provided labeling functions-which tend to be precise but low-coverage-modern discriminative models can retain this precision while learning to generalize beyond the labeling functions, increasing coverage and robustness on unseen data.</p><p>Next we set up the problem Snorkel addresses and describe its main components and design decisions.</p><p>Setup: Our goal is to learn a parameterized classification model h θ that, given a data point x ∈ X , predicts its label y ∈ Y, where the set of possible labels Y is discrete. For simplicity, we focus on the binary setting Y = {−1, 1}, though we include a multi-class application in our experiments. For example, x might be a medical image, and y a label indicating normal versus abnormal. In the relation extraction examples we look at, we often refer to x as a candidate. In a traditional supervised learning setup, we would learn h θ by fitting it to a training set of labeled data points. However, in our setting, we assume that we only have access to unlabeled data for training. We do assume access to a small set of labeled data used during development, called the development set, and a blind, held-out labeled test set for evaluation. These sets can be orders of magnitudes smaller than a training set, making them economical to obtain.</p><p>The user of Snorkel aims to generate training labels by providing a set of labeling functions, which are black-box functions, λ : X → Y ∪ {∅}, that take in a data point and output a label where we use ∅ to denote that the labeling functions abstains. Given m unlabeled data points and n labeling functions, Snorkel applies the labeling functions over the unlabeled data to produce a matrix of labeling function outputs Λ ∈ (Y ∪ {∅}) m×n . The goal of the remaining Snorkel pipeline is to synthesize this label matrix Λ-which may contain overlapping and conflicting labels for each data point-into a single vector of probabilistic training labelsỸ = (ỹ1, ...,ỹm), whereỹi ∈ [0, 1]. These training labels can then be used to train a discriminative model. Next, we introduce the running example of a text relation extraction task as a proxy for many real-world knowledge base construction and data analysis tasks: Example 2.1. Consider the task of extracting mentions of adverse chemical-disease relations from the biomedical literature (see CDR task, Section 4.1). Given documents with mentions of chemicals and diseases tagged, we refer to each co-occuring (chemical, disease) mention pair as a candidate extraction, which we view as a data point to be classified as either true or false. For example, in <ref type="figure">Figure 2</ref>, we would have two candidates with true labels y1 = True and y2 = False:</p><p>x = Causes ( " magnesium " , " quadriplegic " ) x = Causes ( " magnesium " , " preeclampsia " )  Labeling functions take as input a Candidate object, representing a data point to be classified. Each Candidate is a tuple of Context objects, which are part of a hierarchy representing the local context of the Candidate.</p><p>Data Model: A design challenge is managing complex, unstructured data in a way that enables SMEs to write labeling functions over it. In Snorkel, input data is stored in a context hierarchy. It is made up of context types connected by parent/child relationships, which are stored in a relational database and made available via an object-relational mapping (ORM) layer built with SQLAlchemy. Each context type represents a conceptual component of data to be processed by the system or used when writing labeling functions; for example a document, an image, a paragraph, a sentence, or an embedded table. Candidates-i.e., data points x-are then defined as tuples of contexts ( <ref type="figure" target="#fig_1">Figure 3</ref>). Example 2.2. In our running CDR example, the input documents can be represented in Snorkel as a hierarchy consisting of Documents, each containing one or more Sentences, each containing one or more Spans of text. These Spans may also be tagged with metadata, such as Entity markers identifying them as chemical or disease mentions <ref type="figure" target="#fig_1">(Figure 3)</ref>. A candidate is then a tuple of two Spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Language for Weak Supervision</head><p>Snorkel uses the core abstraction of a labeling function to allow users to specify a wide range of weak supervision sources such as patterns, heuristics, external knowledge bases, crowdsourced labels, and more. This higher-level, less precise input is more efficient to provide (see Section 4.2), and can be automatically denoised and synthesized, as described in subsequent sections.</p><p>In this section, we describe our design choices in building an interface for writing labeling functions, which we envision as a unifying programming language for weak supervision. These choices were informed to a large degree by our interactions-primarily through weekly office hours-with Snorkel users in bioinformatics, defense, industry, and other areas over the past year. <ref type="bibr" target="#b2">3</ref> For example, while we initially intended to have a more complex structure for labeling functions, with manually specified types and correlation structure, we quickly found that simplicity in this respect was critical to usability (and not empirically detrimental to our ability to model their outputs). We also quickly discovered that users wanted either far more expressivity or far less of it, compared to our first library of function templates. We thus trade off expressivity and efficiency by allowing users to write labeling functions at two levels of abstraction: custom Python functions and declarative operators.</p><p>https://www.sqlalchemy.org/ 3 http://snorkel.stanford.edu#users</p><p>Hand-Defined Labeling Functions: In its most general form, a labeling function is just an arbitrary snippet of code, usually written in Python, which accepts as input a Candidate object and either outputs a label or abstains. Often these functions are similar to extract-transform-load scripts, expressing basic patterns or heuristics, but may use supporting code or resources and be arbitrarily complex. Writing labeling functions by hand is supported by the ORM layer, which maps the context hierarchy and associated metadata to an object-oriented syntax, allowing the user to easily traverse the structure of the input data. We could also write this with Snorkel's declarative interface: Declarative Labeling Functions: Snorkel includes a library of declarative operators that encode the most common weak supervision function types, based on our experience with users over the last year. These functions capture a range of common forms of weak supervision, for example:</p><p>• Pattern-based: Pattern-based heuristics embody the motivation of soliciting higher information density input from SMEs. For example, pattern-based heuristics encompass feature annotations <ref type="bibr" target="#b50">[51]</ref> and pattern-bootstrapping approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref> (Example 2.3).</p><p>• Distant supervision: Distant supervision generates training labels by heuristically aligning data points with an external knowledge base, and is one of the most popular forms of weak supervision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>• Weak classifiers: Classifiers that are insufficient for our task-e.g., limited coverage, noisy, biased, and/or trained on a different dataset-can be used as labeling functions.</p><p>• Labeling function generators: One higher-level abstraction that we can build on top of labeling functions in Snorkel is labeling function generators, which generate multiple labeling functions from a single resource, such as crowdsourced labels and distant supervision from structured knowledge bases (Example 2.4).</p><p>Example 2.4. A challenge in traditional distant supervision is that different subsets of knowledge bases have different levels of accuracy and coverage. In our running example, we can use the Comparative Toxicogenomics Database (CTD) <ref type="bibr" target="#b3">4</ref> as distant supervision, separately modeling different subsets of it with separate labeling functions. For example, we might write one labeling function to label a candidate True if it occurs in the "Causes" subset, and another to label it False if it occurs in the "Treats" subset. We can write this using a labeling function generator, L F s C T D = Ontology ( ctd , {" Causes " : True , " Treats " : False })</p><p>which creates two labeling functions. In this way, generators can be connected to large resources and create hundreds of labeling functions with a line of code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative Model</head><p>The core operation of Snorkel is modeling and integrating the noisy signals provided by a set of labeling functions. Using the recently proposed approach of data programming <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>, we model the true class label for a data point as a latent variable in a probabilistic model. In the simplest case, we model each labeling function as a noisy "voter" which is independent-i.e., makes errors that are uncorrelated with the other labeling functions. This defines a generative model of the votes of the labeling functions as noisy signals about the true label.</p><p>We can also model statistical dependencies between the labeling functions to improve predictive performance. For example, if two labeling functions express similar heuristics, we can include this dependency in the model and avoid a "double counting" problem. We observe that such pairwise correlations are the most common, so we focus on them in this paper (though handling higher order dependencies is straightforward). We use our structure learning method for generative models <ref type="bibr" target="#b4">[5]</ref> to select a set C of labeling function pairs (j, k) to model as correlated (see Section 3.2).</p><p>Now we can construct the full generative model as a factor graph. We first apply all the labeling functions to the unlabeled data points, resulting in a label matrix Λ, where Λi,j = λj(xi). We then encode the generative model pw(Λ, Y ) using three factor types, representing the labeling propensity, accuracy, and pairwise correlations of labeling functions:</p><formula xml:id="formula_1">φ Lab i,j (Λ, Y ) = 1{Λi,j = ∅} φ Acc i,j (Λ, Y ) = 1{Λi,j = yi} φ Corr i,j,k (Λ, Y ) = 1{Λi,j = Λ i,k } (j, k) ∈ C</formula><p>For a given data point xi, we define the concatenated vector of these factors for all the labeling functions j = 1, ..., n and potential correlations C as φi(Λ, Y ), and the corresponding vector of parameters w ∈ R 2n+|C| . This defines our model:</p><formula xml:id="formula_2">pw(Λ, Y ) = Z −1 w exp m i=1 w T φi(Λ, yi) ,</formula><p>where Zw is a normalizing constant. To learn this model without access to the true labels Y , we minimize the negative log marginal likelihood given the observed label matrix Λ:</p><formula xml:id="formula_3">w = arg min w − log Y pw(Λ, Y ) .</formula><p>We optimize this objective by interleaving stochastic gradient descent steps with Gibbs sampling ones, similar to contrastive divergence <ref type="bibr" target="#b20">[21]</ref>; for more details, see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>. We use the Numbskull library, 5 a Python NUMBA-based Gibbs sampler. We then use the predictions,Ỹ = pŵ(Y |Λ), as probabilistic training labels. https://github.com/HazyResearch/numbskull</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Discriminative Model</head><p>The end goal in Snorkel is to train a model that generalizes beyond the information expressed in the labeling functions. We train a discriminative model h θ on our probabilistic labelsỸ by minimizing a noise-aware variant of the loss l(h θ (xi), y), i.e., the expected loss with respect toỸ :</p><formula xml:id="formula_4">θ = arg min θ m i=1 E y∼Ỹ [l(h θ (xi), y)] .</formula><p>A formal analysis shows that as we increase the amount of unlabeled data, the generalization error of discriminative models trained with Snorkel will decrease at the same asymptotic rate as traditional supervised learning models do with additional hand-labeled data <ref type="bibr" target="#b37">[38]</ref>, allowing us to increase predictive performance by adding more unlabeled data. Intuitively, this property holds because as more data is provided, the discriminative model sees more features that cooccur with the heuristics encoded in the labeling functions.</p><p>Example 2.5. The CDR data contains the sentence, "Myasthenia gravis presenting as weakness after magnesium administration." None of the 33 labeling functions we developed vote on the corresponding Causes(magnesium, myasthenia gravis) candidate, i.e., they all abstain. However, a deep neural network trained on probabilistic training labels from Snorkel correctly identifies it as a true mention.</p><p>Snorkel provides connectors for popular machine learning libraries such as TensorFlow <ref type="bibr" target="#b1">[2]</ref>, allowing users to exploit commodity models like deep neural networks that do not require hand-engineering of features and have robust predictive performance across a wide range of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WEAK SUPERVISION TRADEOFFS</head><p>We study the fundamental question of when-and at what level of complexity-we should expect Snorkel's generative model to yield the greatest predictive performance gains. Understanding these performance regimes can help guide users, and introduces a tradeoff space between predictive performance and speed. We characterize this space in two parts: first, by analyzing when the generative model can be approximated by an unweighted majority vote, and second, by automatically selecting the complexity of the correlation structure to model. We then introduce a two-stage, rulebased optimizer to support fast development cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling Accuracies</head><p>The natural first question when studying systems for weak supervision is, "When does modeling the accuracies of sources improve end-to-end predictive performance?" We study that question in this subsection and propose a heuristic to identify settings in which this modeling step is most beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Tradeoff Space</head><p>We start by considering the label density dΛ of the label matrix Λ, defined as the mean number of non-abstention labels per data point. In the low-density setting, sparsity of labels will mean that there is limited room for even an optimal weighting of the labeling functions to diverge much from the majority vote. Conversely, as the label density </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mid-Density (choose GM)</head><p>Low-Density Bound Optimizer (A * ) Optimal (A * ) Gen. Model (Aw) <ref type="figure">Figure 4</ref>: A plot of the modeling advantage, i.e., the improvement in label accuracy from the generative model, as a function of the number of labeling functions (equivalently, the label density) on a synthetic dataset. <ref type="bibr" target="#b6">7</ref> We plot the advantage obtained by a learned generative model (GM), Aw; by an optimal model A * ; the upper boundÃ * used in our optimizer; and the low-density bound (Proposition 1).</p><p>grows, known theory confirms that the majority vote will eventually be optimal <ref type="bibr" target="#b26">[27]</ref>. It is the middle-density regime where we expect to most benefit from applying the generative model. We start by defining a measure of the benefit of weighting the labeling functions by their true accuracies-in other words, the predictions of a perfectly estimated generative model-versus an unweighted majority vote: Definition 1. (Modeling Advantage) Let the weighted majority vote of n labeling functions on data point xi be denoted as fw(Λi) = n j=1 wjΛi,j, and the unweighted majority vote (MV) as f1(Λi) = n j=1 Λi,j, where we consider the binary classification setting and represent an abstaining vote as 0. We define the modeling advantage Aw as the improvement in accuracy of fw over f1 for a dataset:</p><formula xml:id="formula_5">Aw(Λ, y) = 1 m m i=1 (1 {yifw(Λi) &gt; 0 ∧ yif1(Λi) ≤ 0} − 1 {yifw(Λi) ≤ 0 ∧ yif1(Λi) &gt; 0})</formula><p>In other words, Aw is the number of times fw correctly disagrees with f1 on a label, minus the number of times it incorrectly disagrees. Let the optimal advantage A * = Aw * be the advantage using the optimal weights w * (WMV*).</p><p>To build intuition, we start by analyzing the optimal advantage for three regimes of label density (see <ref type="figure">Figure 6</ref>): Low Label Density: In this sparse setting, very few data points have more than one non-abstaining label; only a small number have multiple conflicting labels. We have observed this occurring, for example, in the early stages of application development. We see that with non-adversarial labeling functions (w * &gt; 0), even an optimal generative model (WMV*) can only disagree with MV when there are disagreeing labels, which will occur infrequently. We see that</p><p>We generate a class-balanced dataset of m = 1000 data points with binary labels, and n independent labeling functions with average accuracy 75% and a fixed 10% probability of voting. Proposition 1. (Low-Density Upper Bound) Assume that P (Λi,j = 0) = p l ∀i, j, and w * j &gt; 0 ∀j. Then, the expected label density isd = np l , and</p><formula xml:id="formula_6">EΛ,y,w * [A * ] = O d 2<label>(1)</label></formula><p>Proof Sketch: We bound the advantage above by computing the expected number of pairwise disagreements.</p><p>High Label Density: In this setting, the majority of the data points have a large number of labels. For example, we might be working in an extremely high-volume crowdsourcing setting, or an application with many highcoverage knowledge bases as distant supervision. Under modest assumptions-namely, that the average labeling function accuracy α * is greater than 50%-it is known that the majority vote converges exponentially to an optimal solution as the average label densityd increases, which serves as an upper bound for the expected optimal advantage as well:</p><p>Theorem 1. (High-Density Upper Bound <ref type="bibr" target="#b26">[27]</ref>) Assume that P (Λi,j = 0) = p l ∀i, j, and that</p><formula xml:id="formula_7">α * = 1 n n j=1 α * j = 1 n n j=1 1/(1 + exp(w * j )) &gt; 1 . Then: EΛ,y,w * [A * ] ≤ e −2p l( α * − 1 ) 2d (2)</formula><p>Proof: This follows from the result in <ref type="bibr" target="#b26">[27]</ref> for the symmetric Dawid-Skene model under constant probability sampling.</p><p>Medium Label Density: In this middle regime, we expect that modeling the accuracies of the labeling functions will deliver the greatest gains in predictive performance because we will have many data points with a small number of disagreeing labeling functions. For such points, the estimated labeling function accuracies can heavily affect the predicted labels. We indeed see gains in the empirical results using an independent generative model that only includes accuracy factors φ Acc i,j <ref type="table" target="#tab_2">(Table 1)</ref>. Furthermore, the guarantees in <ref type="bibr" target="#b37">[38]</ref> establish that we can learn the optimal weights, and thus approach the optimal advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Automatically Choosing a Modeling Strategy</head><p>The bounds in the previous subsection imply that there are settings in which we should be able to safely skip modeling the labeling function accuracies, simply taking the unweighted majority vote instead. However, in practice, the overall label density dΛ is insufficiently precise to determine the transition points of interest, given a user time-cost tradeoff preference (characterized by the advantage tolerance parameter γ in Algorithm 1). We show this in <ref type="table" target="#tab_2">Table 1</ref> using our application data sets from Section 4.1. For example, we see that the Chem and EHR label matrices have equivalent label densities; however, modeling the labeling function accuracies has a much greater effect for EHR than for Chem.</p><p>Instead of simply considering the average label density dΛ, we instead develop a best-case heuristic based on looking at the ratio of positive to negative labels for each data point. This heuristic serves as an upper bound to the true expected advantage, and thus we can use it to determine when we can safely skip training the generative model (see Algorithm 1). Let cy(Λi) = n j=1 1 {Λi,j = y} be the counts of labels of class y for xi, and assume that the true labeling function weights lie within a fixed range, wj ∈ [wmin, wmax] and have a meanw. <ref type="bibr" target="#b7">8</ref> Then, define:</p><formula xml:id="formula_8">Φ(Λi, y) = 1 {cy(Λi)wmax &gt; c−y(Λi)wmin} A * (Λ) = 1 m m i=1 y∈±1 1 {yf1(Λi) ≤ 0} Φ(Λi, y)σ(2fw(Λi)y)</formula><p>where </p><formula xml:id="formula_9">Ey,w * [A * | Λ] ≤Ã * (Λ)<label>(3)</label></formula><p>Proof Sketch: We upper-bound the modeling advantage by the expected number of instances in which WMV* is correct and MV is incorrect. We then upper-bound this by using the best-case probability of the weighted majority vote being correct given (wmin, wmax).</p><p>We applyÃ * to a synthetic dataset and plot in <ref type="figure">Figure 6</ref>. Next, we computeÃ * for the labeling matrices from experiments in Section 4.1, and compare with the empirical advantage of the trained generative models ( <ref type="table" target="#tab_2">Table 1)</ref>. We see that our approximate quantityÃ * serves as a correct guide in all cases for determining which modeling strategy to select, which for the mature applications reported on is indeed most often the generative model. However, we see that while EHR and Chem have equivalent label densities, our optimizer correctly predicts that Chem can be modeled with majority vote, speeding up each pipeline execution by 1.8×. We find in our applications that the optimizer can save execution time especially during the initial stages of iterative development (see full version).</p><p>We fix these at defaults of (w min ,w, wmax) = (0.5, 1.0, 1.5), which corresponds to assuming labeling functions have accuracies between 62% and 82%, and an average accuracy of 73%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Structure</head><p>In this subsection, we consider modeling additional statistical structure beyond the independent model. We study the tradeoff between predictive performance and computational cost, and describe how to automatically select a good point in this tradeoff space.</p><p>Structure Learning. We observe many Snorkel users writing labeling functions that are statistically dependent. Examples we have observed include:</p><p>• Functions that are variations of each other, such as checking for matches against similar regular expressions.</p><p>• Functions that operate on correlated inputs, such as raw tokens of text and their lemmatizations.</p><p>• Functions that use correlated sources of knowledge, such as distant supervision from overlapping knowledge bases.</p><p>Modeling such dependencies is important because they affect our estimates of the true labels. Consider the extreme case in which not accounting for dependencies is catastrophic:</p><p>Example 3.1. Consider a set of 10 labeling functions, where 5 are perfectly correlated, i.e., they vote the same way on every data point, and 5 are conditionally independent given the true label. If the correlated labeling functions have accuracy α = 50% and the uncorrelated ones have accuracy β = 99%, then the maximum likelihood estimate of their accuracies according to the independent model isα = 100% andβ = 50%. Specifying a generative model to account for such dependencies by hand is impractical for three reasons. First, it is difficult for non-expert users to specify these dependencies. Second, as users iterate on their labeling functions, their dependency structure can change rapidly, like when a user relaxes a labeling function to label many more candidates. Third, the dependency structure can be dataset specific, making it impossible to specify a priori, such as when a corpus contains many strings that match multiple regular expressions used in different labeling functions. We observed users of earlier versions of Snorkel struggling for these reasons to construct accurate and efficient generative models with dependencies. We therefore seek a method that can quickly identify an appropriate dependency structure from the labeling function outputs Λ alone.</p><p>Naively, we could include all dependencies of interest, such as all pairwise correlations, in the generative model and perform parameter estimation. However, this approach is impractical. For 100 labeling functions and 10,000 data points, estimating parameters with all possible correlations takes roughly 45 minutes. When multiplied over repeated runs of hyperparameter searching and development cycles, this cost greatly inhibits labeling function development. We therefore turn to our method for automatically selecting which dependencies to model without access to ground truth <ref type="bibr" target="#b4">[5]</ref>. It uses a pseudolikelihood estimator, which does not require any sampling or other approximations to compute the objective gradient exactly. It is much faster than maximum likelihood estimation, taking 15 seconds to select pairwise correlations to be modeled among 100 labeling functions with 10,000 data points. However, this approach relies on a selection threshold hyperparameter which induces a tradeoff space between predictive performance and computational cost. All User Study Labeling Functions <ref type="figure">Figure 5</ref>: Predictive performance of the generative model and number of learned correlations versus the correlation threshold . The selected elbow point achieves a good tradeoff between predictive performance and computational cost (linear in the number of correlations). Left: simulation of structure learning correcting the generative model. Middle: the CDR task. Right: all user study labeling functions for the Spouses task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Tradeoff Space</head><p>Such structure learning methods, whether pseudolikelihood or likelihood-based, crucially depend on a selection threshold for deciding which dependencies to add to the generative model. Fundamentally, the choice of determines the complexity of the generative model. We study the tradeoff between predictive performance and computational cost that this induces. We find that generally there is an "elbow point" beyond which the number of correlations selected-and thus the computational cost-explodes, and that this point is a safe tradeoff point between predictive performance and computation time.</p><p>Predictive Performance: At one extreme, a very large value of will not include any correlations in the generative model, making it identical to the independent model. As is decreased, correlations will be added. At first, when is still high, only the strongest correlations will be included. As these correlations are added, we observe that the generative model's predictive performance tends to improve. <ref type="figure">Figure 5</ref>, left, shows the result of varying in a simulation where more than half the labeling functions are correlated. After adding a few key dependencies, the generative model resolves the discrepancies among the labeling functions. <ref type="figure">Figure 5</ref>, middle, shows the effect of varying for the CDR task. Predictive performance improves as decreases until the model overfits. Finally, we consider a large number of labeling functions that are likely to be correlated. In our user study (described in Section 4.2), participants wrote labeling functions for the Spouses task. We combined all 125 of their functions and studied the effect of varying . Here, we expect there to be many correlations since it is likely that users wrote redundant functions. We see in <ref type="figure">Figure 5</ref>, right, that structure learning surpasses the best performing individual's generative model (50.0 F1).</p><p>Computational Cost: Computational cost is correlated with model complexity. Since learning in Snorkel is done with a Gibbs sampler, the overhead of modeling additional correlations is linear in the number of correlations. The dashed lines in <ref type="figure">Figure 5</ref> show the number of correlations included in each model versus . For example, on the Spouses task, fitting the parameters of the generative model at = 0.5 takes 4 minutes, and fitting its parameters with = 0.02 Specifically, is both the coefficient of the regularization term used to induce sparsity, and the minimum absolute weight in log scale that a dependency must have to be selected. takes 57 minutes. Further, parameter estimation is often run repeatedly during development for two reasons: (i) fitting generative model hyperparameters using a development set requires repeated runs, and (ii) as users iterate on their labeling functions, they must re-estimate the generative model to evaluate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Automatically Choosing a Model</head><p>Based on our observations, we seek to automatically choose a value of that trades off between predictive performance and computational cost using the labeling functions' outputs Λ alone. Including as a hyperparameter in a grid search over a development set is generally not feasible because of its large effect on running time. We therefore want to choose before other hyperparameters, without performing any parameter estimation. We propose using the number of correlations selected at each value of as an inexpensive indicator. The dashed lines in <ref type="figure">Figure 5</ref> show that as decreases, the number of selected correlations follows a pattern. Generally, the number of correlations grows slowly at first, then hits an "elbow point" beyond which the number explodes, which fits the assumption that the correlation structure is sparse. In all three cases, setting to this elbow point is a safe tradeoff between predictive performance and computational cost. In cases where performance grows consistently (left and right), the elbow point achieves most of the predictive performance gains at a small fraction of the computational cost. For example, on Spouses (right), choosing = 0.08 achieves a score of 56.6 F1-within one point of the best score-but only takes 8 minutes for parameter estimation. In cases where predictive performance eventually degrades (middle), the elbow point also selects a relatively small number of correlations, giving an 0.7 F1 point improvement and avoiding overfitting.</p><p>Performing structure learning for many settings of is inexpensive, especially since the search needs to be performed only once before tuning the other hyperparameters. On the large number of labeling functions in the Spouses task, structure learning for 25 values of takes 14 minutes. On CDR, with a smaller number of labeling functions, it takes 30 seconds. Further, if the search is started at a low value of and increased, it can often be terminated early, when the number of selected correlations reaches a low value. Selecting the elbow point itself is straightforward. We use the point with greatest absolute difference from its neighbors, but more sophisticated schemes can also be applied <ref type="bibr" target="#b42">[43]</ref>. Our full optimization algorithm for choosing a modeling strategy and (if necessary) correlations is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Modeling Strategy Optimizer</head><p>Input: Label matrix Λ ∈ (Y ∪ {∅}) m×n , advantage tolerance γ, structure search resolution η Output: Modeling strategy</p><formula xml:id="formula_10">ifÃ * (Λ) &lt; γ then return MV Structures ← [ ] for i from 1 to 1 2η do ← i • η C ← LearnStructure(Λ, ) Structures.append(|C|, ) ← SelectElbowPoint(Structures) return GM</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION</head><p>We evaluate Snorkel by drawing on deployments developed in collaboration with users. We report on two realworld deployments and four tasks on open-source data sets representative of other deployments. Our evaluation is designed to support the following three main claims:</p><p>• Snorkel outperforms distant supervision baselines.</p><p>In distant supervision <ref type="bibr" target="#b31">[32]</ref>, one of the most popular forms of weak supervision used in practice, an external knowledge base is heuristically aligned with input data to serve as noisy training labels. By allowing users to easily incorporate a broader, more heterogeneous set of weak supervision sources, Snorkel exceeds models trained via distant supervision by an average of 132%.</p><p>• Snorkel approaches hand supervision. We see that by writing tens of labeling functions, we were able to approach or match results using hand-labeled training data which took weeks or months to assemble, coming within 2.11% of the F1 score of hand supervision on relation extraction tasks and an average 5.08% accuracy or AUC on cross-modal tasks, for an average 3.60% across all tasks.</p><p>• Snorkel enables a new interaction paradigm. We measure Snorkel's efficiency and ease-of-use by reporting on a user study of biomedical researchers from across the U.S. These participants learned to write labeling functions to extract relations from news articles as part of a twoday workshop on learning to use Snorkel, and matched or outperformed models trained on hand-labeled training data, showing the efficiency of Snorkel's process even for first-time users.</p><p>We now describe our results in detail. First, we describe the six applications that validate our claims. We then show that Snorkel's generative modeling stage helps to improve the predictive performance of the discriminative model, demonstrating that it is 5.81% more accurate when trained on Snorkel's probabilistic labels versus labels produced by an unweighted average of labeling functions. We also validate that the ability to incorporate many different types of weak supervision incrementally improves results with an ablation study. Finally, we describe the protocol and results of our user study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Applications</head><p>To evaluate the effectiveness of Snorkel, we consider several real-world deployments and tasks on open-source datasets  <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Discriminative Models: One of the key bets in Snorkel's design is that the trend of increasingly powerful, open-source machine learning tools (e.g., models, pre-trained word embeddings and initial layers, automatic tuners, etc.) will only continue to accelerate. To best take advantage of this, Snorkel creates probabilistic training labels for any discriminative model with a standard loss function. In the following experiments, we control for end model selection by using currently popular, standard choices across all settings. For text modalities, we choose a bidirectional long short term memory (LSTM) sequence model <ref type="bibr" target="#b16">[17]</ref>, and for the medical image classification task we use a 50-layer ResNet <ref type="bibr" target="#b18">[19]</ref> pre-trained on the ImageNet object classification dataset <ref type="bibr" target="#b13">[14]</ref>. Both models are implemented in Tensorflow <ref type="bibr" target="#b1">[2]</ref> and trained using the Adam optimizer <ref type="bibr" target="#b23">[24]</ref>, with hyperparameters selected via random grid search using a small labeled development set. Final scores are reported on a held-out labeled test set. See full version for details.</p><p>A key takeaway of the following results is that the discriminative model generalizes beyond the heuristics encoded in the labeling functions (as in Example 2.5). In Section 4.1.1, we see that on relation extraction applications the discriminative model improves performance over the generative model primarily by increasing recall by 43.15% on average. In Section 4.1.2, the discriminative model classifies entirely new modalities of data to which the labeling functions cannot be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Relation Extraction from Text</head><p>We first focus on four relation extraction tasks on text data, as it is a challenging and common class of problems that are well studied and for which distant supervision is often considered. Predictive performance is summarized in <ref type="table">Table 3</ref>. We briefly describe each task.</p><p>Scientific Articles (Chem): With modern online repositories of scientific literature, such as PubMed for biomedical articles, research results are more accessible than ever before. However, actually extracting fine-grained pieces of information in a structured format and using this data to answer specific questions at scale remains a significant open challenge for researchers. To address this challenge in the <ref type="table">Table 3</ref>: Evaluation of Snorkel on relation extraction tasks from text. Snorkel's generative and discriminative models consistently improve over distant supervision, measured in F1, the harmonic mean of precision (P) and recall (R). We compare with hand-labeled data when available, coming within an average of 1 F1 point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distant Supervision</head><p>Snorkel (Gen.) Snorkel (Disc.) Hand Supervision <ref type="table">Task</ref>  Research, we used Snorkel to develop a system to extract structured data from unstructured EHR notes. Specifically, the system's task was to extract mentions of pain levels at precise anatomical locations from clinician notes, with the goal of using these features to automatically assess patient well-being and detect complications after medical interventions like surgery. To this end, our collaborators created a cohort of 5,800 patients from SHC EHR data, with visit dates between 1995 and 2015, resulting in 500K unstructured clinical documents. Since distant supervision from a knowledge base is not applicable, we compared against regular-expression-based labeling previously developed for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chemical-Disease Relations (CDR):</head><p>We used the 2015 BioCreative chemical-disease relation dataset <ref type="bibr" target="#b48">[49]</ref>, where the task is to identify mentions of causal links between chemicals and diseases in PubMed abstracts. We used all pairs of chemical and disease mentions co-occuring in a sentence as our candidate set. We used the Comparative Toxicogenomics Database (CTD) <ref type="bibr" target="#b32">[33]</ref> for distant supervision, and additionally wrote labeling functions capturing language patterns and information from the context hierarchy. To evaluate Snorkel's ability to discover previously unknown information, we randomly removed half of the relations in CTD and evaluated on candidates not contained in the remaining half.</p><p>Spouses: Our fourth task is to identify mentions of spouse relationships in a set of news articles from the Signal Media dataset <ref type="bibr" target="#b9">[10]</ref>. We used all pairs of person mentions (tagged with SpaCy's NER module 11 ) co-occuring in the same sentence as our candidate set. To obtain hand-labeled data for evaluation, we crowdsourced labels for the candidates via Amazon Mechanical Turk, soliciting labels from three workers for each example and assigning the majority vote. We then wrote labeling functions that encoded language patterns and distant supervision from DBpedia <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Cross-Modal: Images &amp; Crowdsourcing</head><p>In the cross-modal setting, we write labeling functions over one data modality (e.g., a text report, or the votes of crowdworkers) and use the resulting labels to train a classifier defined over a second, totally separate modality (e.g., an image or the text of a tweet). This demonstrates the flexibility of Snorkel, in that the labeling functions (and by extension, the generative model) do not need to operate over the same domain as the discriminative model being trained. Predictive performance is summarized in <ref type="table" target="#tab_6">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abnormality Detection in Lung Radiographs (Rad):</head><p>In many real-world radiology settings, there are large repositories of image data with corresponding narrative text reports, but limited or no labels that could be used for training an image classification model. In this application, in collaboration with radiologists, we wrote labeling functions over the text radiology reports, and used the resulting labels to train an image classifier to detect abnormalities in lung Xray images. We used a publicly available dataset from the OpenI biomedical image repository 12 consisting of 3,851 distinct radiology reports-composed of unstructured text and Medical Subject Headings (MeSH) 13 codes-and accompanying X-ray images. We trained a model to perform sentiment analysis using crowdsourced annotations from the weather sentiment task from Crowdflower. In this task, contributors were asked to grade the sentiment of oftenambiguous tweets relating to the weather, choosing between five categories of sentiment. Twenty contributors graded each tweet, but due to the difficulty of the task and lack of crowdworker filtering, there were many conflicts in worker labels. We represented each crowdworker as a labeling function-showing Snorkel's ability to subsume existing crowdsourcing modeling approaches-and then used the resulting labels to train a text model over the tweets, for making predictions independent of the crowd workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Effect of Generative Modeling</head><p>An important question is the significance of modeling the accuracies and correlations of the labeling functions on the end predictive performance of the discriminative model (versus in Section 3, where we only considered the effect on the accuracy of the generative model). We compare Snorkel with a simpler pipeline that skips the generative modeling stage and trains the discriminative model on an unweighted average of the labeling functions' outputs. <ref type="table" target="#tab_7">Table 5</ref> shows that the discriminative model trained on Snorkel's probabilistic labels consistently predicts better, improving 5.81% on average. These results demonstrate that the discriminative model effectively learns from the additional signal contained in Snorkel's probabilistic training labels over simpler modeling strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Labeling Function Type Ablation</head><p>We also examine the impact of different types of labeling functions on end predictive performance, using the CDR application as a representative example of three common categories of labeling functions:</p><p>• Text Patterns: Basic word, phrase, and regular expression labeling functions.</p><p>• Distant Supervision: External knowledge bases mapped to candidates, either directly or filtered by a heuristic.</p><p>• Structure-Based: Labeling functions expressing heuristics over the context hierarchy, e.g., reasoning about position in the document or relative to other candidates.</p><p>We show an ablation in <ref type="table" target="#tab_8">Table 6</ref>, sorting by stand-alone score. We see that distant supervision adds recall at the https://www.crowdflower.com/data/weather-sentiment/ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">User Study</head><p>We conducted a formal study of Snorkel to (i) evaluate how quickly SME users could learn to write labeling functions, and (ii) empirically validate the core hypothesis that writing labeling functions is more time-efficient than handlabeling data. Users were given instruction on Snorkel, and then asked to write labeling functions for the Spouses task described in the previous subsection.</p><p>Participants: In collaboration with the Mobilize Center <ref type="bibr" target="#b24">[25]</ref>, an NIH-funded Big Data to Knowledge (BD2K) center, we distributed a national call for applications to attend a two-day workshop on using Snorkel for biomedical knowledge base construction. Selection criteria included a strong biomedical project proposal and little-to-no prior experience using Snorkel. In total, 15 researchers were invited to attend out of 33 team applications submitted, with varying backgrounds in bioinformatics, clinical informatics, and data mining from universities, companies, and organizations around the United States. The education demographics included 6 bachelors, 4 masters, and 5 Ph.D. degrees. All participants could program in Python, with 80% rating their skill as intermediate or better; 40% of participants had little-to-no prior exposure to machine learning; and 53-60% had no prior experience with text mining or information extraction applications. See full version for details.</p><p>Protocol: The first day focused entirely on labeling functions, ranging from theoretical motivations to details of the Snorkel API. Over the course of 7 hours, participants were instructed in a classroom setting on how to use and evaluate models developed using Snorkel. Users were presented with 4 tutorial Jupyter notebooks providing skeleton code for evaluating labeling functions, along with a small labeled development candidate set, and were given 2.5 hours of dedicated development time in aggregate to write their labeling functions. All workshop materials are available online. <ref type="bibr" target="#b15">16</ref> Baseline: To compare our users' performance against models trained on hand-labeled data, we collected a large handlabeled dataset via Amazon Mechanical Turk (the same set used in the previous subsection). We then split this into 15 datasets representing 7 hours worth of hand-labeling time One participant declined to write labeling functions, so their score is not included in our analysis. <ref type="bibr" target="#b15">16</ref> https://github.com/HazyResearch/snorkel/tree/master/ tutorials/workshop each-based on the crowd-worker average of 10 seconds per label-simulating the alternative scenario where users skipped both instruction and labeling function development sessions and instead spent the full day hand-labeling data.</p><p>Results: Our key finding is that labeling functions written in Snorkel, even by SME users, can match or exceed a traditional hand-labeling approach. The majority (8) of subjects matched or outperformed these hand-labeled data models. The average Snorkel user's score was 30.4 F1, and the average hand-supervision score was 20.9 F1. The best performing user model scored 48.7 F1, 19.2 points higher than the best supervised model using hand-labeled data. The worst participant scored 12.0 F1, 0.3 points higher that the lowest hand-labeled model. The full distribution of scores by participant, and broken down by participant background, compared against the baseline models trained with handlabeled data is available in the full version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>This section is an overview of techniques for managing weak supervision, many of which are subsumed in Snorkel. We also contrast it with related forms of supervision.</p><p>Combining Weak Supervision Sources: The main challenge of weak supervision is how to combine multiple sources. For example, if a user provides two knowledge bases for distant supervision, how should a data point that matches only one knowledge base be labeled? Some researchers have used multi-instance learning to reduce the noise in weak supervision sources <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41]</ref>, essentially modeling the different weak supervision sources as soft constraints on the true label, but this approach is limited because it requires using a specific end model that supports multi-instance learning.</p><p>Researchers have therefore considered how to estimate the accuracy of label sources without a gold standard with which to compare-a classic problem <ref type="bibr" target="#b12">[13]</ref>-and combine these estimates into labels that can be used to train an arbitrary end model. Much of this work has focused on crowdsourcing, in which workers have unknown accuracy <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b52">53]</ref>. Such methods use generative probabilistic models to estimate a latent variable-the true class label-based on noisy observations. Other methods use generative models with hand-specified dependency structures to label data for specific modalities, such as topic models for text <ref type="bibr" target="#b3">[4]</ref> or denoising distant supervision sources <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b47">48]</ref>. Other techniques for estimating latent class labels given noisy observations include spectral methods <ref type="bibr" target="#b34">[35]</ref>. Snorkel is distinguished from these approaches because its generative model supports a wide range of weak supervision sources, and it learns the accuracies and correlation structure among weak supervision sources without ground truth data.</p><p>Other Forms of Supervision: Work on semi-supervised learning considers settings with some labeled data and a much larger set of unlabeled data, and then leverages various domain-and task-agnostic assumptions about smoothness, low-dimensional structure, or distance metrics to heuristically label the unlabeled data <ref type="bibr" target="#b8">[9]</ref>. Work on active learning aims to automatically estimate which data points are optimal to label, thereby hopefully reducing the total number of examples that need to be manually annotated <ref type="bibr" target="#b44">[45]</ref>. Transfer learning considers the strategy of repurposing models trained on different datasets or tasks where labeled training data is more abundant <ref type="bibr" target="#b33">[34]</ref>. Another type of supervision is self-training <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">44]</ref> and co-training <ref type="bibr" target="#b5">[6]</ref>, which involves training a model or pair of models on data that they labeled themselves. Weak supervision is distinct in that the goal is to solicit input directly from SMEs, however at a higher level of abstraction and/or in an inherently noisier form. Snorkel is focused on managing weak supervision sources, but combing its methods with these other types of supervision is straightforward.</p><p>Related Data Management Problems: Researchers have considered related problems in data management, such as data fusion <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref> and truth discovery <ref type="bibr" target="#b27">[28]</ref>. In these settings, the task is to estimate the reliability of data sources that provide assertions of facts and determine which facts are likely true. Many approaches to these problems use probablistic graphical models that are related to Snorkel's generative model in that they represent the unobserved truth as a latent variable, e.g., the latent truth model <ref type="bibr" target="#b53">[54]</ref>. Our setting differs in that labeling functions assign labels to userprovided data, and they may provide any label or abstain, which we must model. Work on data fusion has also explored how to model user-specified correlations among data sources <ref type="bibr" target="#b35">[36]</ref>. Snorkel automatically identifies which correlations among labeling functions to model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>Snorkel provides a new paradigm for soliciting and managing weak supervision to create training data sets. In Snorkel, users provide higher-level supervision in the form of labeling functions that capture domain knowledge and resources, without having to carefully manage the noise and conflicts inherent in combining weak supervision sources. Our evaluations demonstrate that Snorkel significantly reduces the cost and difficulty of training powerful machine learning models while exceeding prior weak supervision methods and approaching the quality of large, hand-labeled training sets. Snorkel's deployments in industry, research labs, and government agencies show that it has real-world impact, offering developers an improved way to build models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ADDITIONAL MATERIAL FOR SEC. 3.1 A.1 Minor Notes</head><p>Note that for the independent generative model (i.e., |C| = 0), the weight corresponding to the accuracy factor, w j , for labeling function j is just the log-odds of its accuracy:</p><formula xml:id="formula_11">α j = P (Λ i,j = 1 | Y i = 1, Λ i,j = 0) = P (Λ i,j = 1, Y i = 1, Λ i,j = 0) P (Y i = 1, Λ i,j = 0) = exp(w j ) exp(w j ) + exp(−w j ) =⇒ w j = 1 2 log α j 1 − α j</formula><p>Also note that the accuracy we consider is conditioned on the labeling function not abstaining, i.e.,:</p><formula xml:id="formula_12">P (Λ i,j = 1 | Y i = 1) = α j * P (Λ i,j = ∅)</formula><p>because a separate factor φ Lab i,j captures how often each labeling function votes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Proposition 1</head><p>In this proposition, our goal is to obtain a simple upper bound for the expected optimal advantage E Λ,y,w * [A * ] in the low label density regime. We consider a simple model where all the labeling functions have a fixed probability of emitting a non-zero label,</p><formula xml:id="formula_13">P (Λ i,j = ∅) = p l ∀i, j<label>(4)</label></formula><p>and that the labeling functions are all non-adversarial, i.e., they all have accuracies greater than 50%, or equivalently,</p><formula xml:id="formula_14">w * j &gt; 0 ∀j (5)</formula><p>First, we start by only counting cases where the optimal weighted majority vote (WMV*)-i.e., the predictions of the generative model with perfectly estimated weights-is correct and the majority vote (MV) is incorrect, which is an upper bound on the modeling advantage:</p><formula xml:id="formula_15">E Λ,y,w * [A w * (Λ, y)] = 1 m m i=1 E Λ i ,y i ,w * [1 {y i f w * (Λ i ) &gt; 0 ∧ y i f 1 (Λ i ) ≤ 0} − 1 {y i f w * (Λ i ) ≤ 0 ∧ y i f 1 (Λ i ) &gt; 0}]) ≤ 1 m m i=1 E Λ i ,y i ,w * [1 {y i f w * (Λ i ) &gt; 0 ∧ y i f 1 (Λ i ) ≤ 0}]</formula><p>Next, by (5), the only way that WMV* and MV could possibly disagree is if there is at least one disagreeing pair of labels:</p><formula xml:id="formula_16">E Λ,y,w * [A * (Λ, y)] ≤ 1 m m i=1 E Λ,y [1 {c 1 (Λ i ) &gt; 0 ∧ c −1 (Λ i ) &gt; 0}]</formula><p>where cy(Λ i ) = n j=1 {Λ i,j = y}, in other words, the counts of positive or negative labels for a given data point x i . Then, we can bound this by the expected number of disagreeing, non-abstaining pairs of labels:</p><formula xml:id="formula_17">E Λ,y,w * [A * (Λ, y)] ≤ 1 m m i=1 E Λ,y   n−1 j=1 n k=j+1 1 Λ i,j = Λ i,k ∧ Λ i,j , Λ i,k = 0   = 1 m m i=1 n−1 j=1 n k=j+1 E Λ,y 1 Λ i,j = Λ i,k ∧ Λ i,j , Λ i,k = 0 = 1 m m i=1 n−1 j=1 n k=j+1 y ∈±1 λ∈±1 P (Λ i,j = λ, Λ i,k = −λ, y i = y )</formula><p>Since we are considering the independent model, Λ i,j ⊥ Λ i,k =j | y i , we have that:</p><formula xml:id="formula_18">P (Λ i,j = λ, Λ i,k = −λ, y i = λ) = P (Λ i,j = λ | y i = λ)P (Λ i,k = −λ | y i = λ)P (y i = λ) = α j (1 − α k )p 2 l P (y i = λ)</formula><p>Thus we have:</p><formula xml:id="formula_19">E Λ,y,w * [A * (Λ, y)] ≤ n−1 j=1 n k=j+1 p 2 l (α j (1 − α k ) + (1 − α j )α k ) = n j=1 k =j p 2 l α j (1 − α k ) ≤ n j=1 n k=1 p 2 l α j (1 − α k ) = n 2 p 2 lᾱ (1 −ᾱ) =d 2ᾱ (1 −ᾱ)</formula><p>where we have defined the average labeling function accuracy as α, and where the label density is defined asd = p l n. Thus we have shown that the expected advantage scales at most quadratically in the label density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Explanation of Theorem 1</head><p>The Dawid-Skene model <ref type="bibr" target="#b12">[13]</ref> of crowd workers classically models each crowd worker as conditionally independent, and having some class-dependent but data point-independent probability of emitting a correct label. In our setting, considering the binary classification case (as Dawid-Skene treats), we can think of each crowd worker as a labeling function, in which case we have:</p><formula xml:id="formula_20">α + j = P (Λ i,j = 1 | y i = 1, Λ i,j = 0) α − j = P (Λ i,j = −1 | y i = −1, Λ i,j = 0)</formula><p>The symmetric Dawid-Skene setting is the one we consider, where α j ≡ α + j = α − j . Furthermore, we can refer to the matrix of probabilities of a worker j being given data point i to label (i.e., in our syntax, the probability of not abstaining) as the sampling probability matrix. If the entries are all the same, this is refered to as a constant probability sampling strategy, and is equivalent to our assumption P (Λ i,j = ∅) = p l ∀i, j.</p><p>In this setting, and assuming that the mean labeling function / crowd worker accuracyᾱ &gt; 1 (or equivalently,w * &gt; 0), then Corrollary 9 in <ref type="bibr" target="#b26">[27]</ref> provides us with the following upper bound on the mean error rate:</p><formula xml:id="formula_21">1 m P (f 1 (Λ i ) = y i ) ≤ e −2np 2 l (ᾱ * − 1 ) 2<label>(6)</label></formula><p>We then use (6) to upper bound the expected advantage E Λ,y,w * [A * ], substituting ind = np l for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Proposition 2</head><p>In this proposition, our goal is to find a tractable upper bound on the conditional modeling advantage, i.e., the modeling advantage given the observed label matrix Λ. This will be useful because, given our label matrix, we can compute this quantity and, when it is small, safely skip learning the generative model and just use an unweighted majority vote (MV) of the labeling functions. We assume in this proposition that the true weights of the labeling functions lie within a fixed range, w j ∈ [w min &gt; 0, wmax] and have a meanw. For notational convenience, let</p><formula xml:id="formula_22">y =      1 f 1 (Λ) &gt; 0 0 f 1 (Λ) = 0 −1 f 1 (Λ) &lt; 0</formula><p>We start with the expected advantage, and upper-bound by the expected number of instances in which WMV* is correct and MV </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choose</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MV Choose GM</head><p>Optimizer (A * ) Gen. Model (Aw) <ref type="figure">Figure 6</ref>: The advantage of using the generative labeling model (GM) over majority vote (MV) as predicted by our optimizer (Ã * ), and empirically (Aw), on the CDR application as the number of LFs is increased. We see that the optimizer correctly chooses MV during early development stages, and then GM in later ones. </p><formula xml:id="formula_23">E w * ,y [A * (Λ, y) | Λ] = E w * ,y∼P (• | Λ,w * ) [A w * (Λ, y)] ≤ 1 m m i=1 E w * ,y∼P (• | Λ i ,w * ) 1 y i = y i 1 y i f w * (Λ i ) ≤ 0 = 1 m m i=1 E w * E y∼P (• | Λ i ,w * ) 1 y i = y i 1 y i f w * (Λ i ) ≤ 0 = 1 m m i=1 E w * P (y i = y i | Λ i , w * )1 y i f w * (Λ i ) ≤ 0</formula><p>Next, define:</p><formula xml:id="formula_24">Φ(Λ i , y ) = 1 c y (Λ i )wmax − c −y (Λ i )w min</formula><p>i.e. this is an indicator for whether WMV* could possibly output y as a prediction under best-case circumstances. We use this in turn to upper-bound the expected modeling advantage again:</p><formula xml:id="formula_25">E w * ,y∼P (• | Λ,w * ) [A w * (Λ, y)] ≤ 1 m m i=1 E w * P (y i = y i | Λ i , w * )Φ(Λ i , −y i ) = 1 m m i=1 Φ(Λ i , −y i )E w * P (y i = y i | Λ i , w * ) ≤ 1 m m i=1 Φ(Λ i , −y i )P (y i = y i | Λ i ,w)</formula><p>Now, recall that, for y ∈ ±1:</p><formula xml:id="formula_26">P (y i = y | Λ i , w) = P (y i = y , Λ i | w) y ∈±1 P (y i = y , Λ i | w) = exp w T φ i (Λ i , y i = y ) y ∈±1 exp (w T φ i (Λ i , y i = y )) = exp w T Λ i y exp (w T Λ i ) + exp (−w T Λ i ) = σ 2fw(Λ i )y</formula><p>where σ(•) is the sigmoid function. Note that we are considering a simplified independent generative model with only accuracy factors; however, in this discriminative formulation the labeling propensity factors would drop out anyway since they do not depend on y, so their omission is just for notational simplicity. Putting this all together by removing the y i placeholder, simplifying notation to match the main body of the paper, we have:</p><formula xml:id="formula_27">E w * ,y [A * (Λ, y) | Λ] ≤ 1 m m i=1 y∈±1 1 {yf 1 (Λ i ) ≤ 0} Φ(Λ i , y)σ (2yfw(Λ i )) =Ã * (Λ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Modeling Advantage Notes</head><p>In <ref type="figure">Figure 6</ref>, we measure the modeling advantage of the generative model versus a majority vote of the labeling functions on random subsets of the CDR labeling functions of different sizes. We see that the modeling advantage grows as the number of labeling functions increases, indicating that the optimizer can save execution time especially during the initial stages of iterative development.</p><p>Note that in Section 4, due to known negative class imbalance in relation extraction problems, we count instances in which the generative model emits no label-i.e., a 0 label-as negatives, as is common practice (essentially, we are giving the generative model the benefit of the doubt given the known class imbalance). Thus our reported F1 score metric hides instances in which the generative model learns to apply a -1 label where majority vote applied 0. In computing the empirical modeling advantage, however, we do count such instances as improvements over majority vote, as these instances do have an effect on the training of the end discriminative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ADDITIONAL EVALUATION DETAILS B.1 Data Set Details</head><p>Additional information about the sizes of the datasets are included in <ref type="table" target="#tab_9">Table 7</ref>. Specifically, we report the size of the (unlabeled) training set and hand-labeled development and test sets, in terms of number of candidates. Note that the development and test sets can be orders of magnitude smaller that the training sets. Labeled development and test sets were either used when already available as part of a benchmark dataset, or labeled with the help of our SME collaborators, limited to several hours of labeling time maximum.   Interface Implementation. Snorkel's interface is designed to be accessible to SMEs without advanced programming skills. All components run in Jupyter (http://jupyter.org/) iPython notebooks, including writing labeling functions. Users can therefore write labeling functions as arbitrary Python functions for maximum flexibility <ref type="figure" target="#fig_7">(Figure 9</ref>). We also provide a library of labeling function primitives and generators to declaratively program weak supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 User Study</head><p>A key aspect of labeling function development is that the process is iterative. After developing an initial set of labeling functions, it is important for users to visualize the errors of the end model. Therefore, when the model is evaluated on the development data set, the candidates are separated into true positive, false positive, true negative, and false negative sets. Each of these buckets can be loaded into a viewer in a notebook ( <ref type="figure">Figure 10</ref>) so that SMEs can identify common patterns that are either not covered or misclassified by their current labeling functions. The viewer also supports labeling candidates directly in order to create or expand development and test sets.</p><p>Execution Model. Since labeling functions are self-contained and operate on discrete candidates, their execution is embarrassingly parallel. If Snorkel is connected to a relational database that supports simultaneous connections, e.g., PostgreSQL, then the master process (usually the notebook kernel) distributes the primary keys of the candidates to be labeled to Python worker processes. The workers independently read from the database to materialize the candidates via the ORM layer, then execute the labeling functions over them. The labels are returned to the master process which persists them via the ORM layer. Collecting the labels at the master is more efficient than having workers write directly to the database, due to table-level locking.</p><p>Snorkel includes a Spark (https://spark.apache.org/) integration layer, enabling labeling functions to be run across a cluster. Once the set of candidates is cached as a Spark data frame, only the closure of the labeling functions and the resulting labels need to be communicated to and from the workers. This is particularly helpful in Snorkel's iterative workflow. Distributing a large unstructured data set across a cluster is relatively expensive, but only has to be performed once. Then, as users refine their labeling functions, they can be rerun efficiently. This same execution model is supported for preprocessing utilities-such as natural language processing for text and candidate extraction-via a common class interface. Snorkel provides wrappers for Stanford CoreNLP (https://stanfordnlp.github. io/CoreNLP/) and SpaCy (https://spacy.io/) for text preprocessing, and supports automatically defining candidates using their named-entity recognition features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Labeling functions take as input a Candidate object, representing a data point to be classified. Each Candidate is a tuple of Context objects, which are part of a hierarchy representing the local context of the Candidate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L</head><label></label><figDesc>F c a u s e s = l f s e a r c h ( " {{1}}. * \ Wcauses \W . * {{2}} " , r e v e r s e a r g s = False )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figures 7 and 8</head><label>8</label><figDesc>show the distribution of scores by participant, and broken down by participant background, compared against the baseline models trained with hand-labeled data.Figure 8shows descriptive statistics of user factors broken down by their end model's predictive performance.C. IMPLEMENTATION DETAILSNote that all code is open source and available-with tutorials, blog posts, workshop lectures, and other material-at snorkel. stanford.edu.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Labeling functions which express patternmatching, distant supervision, and weak classifier heuristics, respectively, in Snorkel's Jupyter notebook interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :Figure 7 :F1Figure 8 :</head><label>1078</label><figDesc>The Viewer utility in Snorkel, showing candidate company-employee relation mentions from the ACE benchmark, composed of candidate person and company mention pairs.Table 8: Self-reported skill levels-beginner (Beg.), intermediate (Int.), and advanced (Adv.)-for all user study participants. Subject New Beg. Int. Adv. Predictive performance of our 14 user study participants. The majority of users matched or exceeded the performance of a model trained on 7 hours (2500 instances) of hand-labeled data. The profile of the best performing user by F1 score, was a MS or Ph.D. degree in any field, strong Python coding skills, and intermediate to advanced experience with machine learning. Prior experience with text mining added no benefit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Modeling advantage Aw attained using a generative model for several applications in Snorkel (Section 4.1), the upper boundÃ * used by our optimizer, the modeling strategy selected by the optimizer-either majority vote (MV) or generative model (GM)-and the empirical label density dΛ.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Aw (%)Ã  *  (%) Modeling Strategy dΛ</cell></row><row><cell>Radiology</cell><cell>7.0</cell><cell>12.4</cell><cell>GM</cell><cell>2.3</cell></row><row><cell>CDR</cell><cell>4.9</cell><cell>7.9</cell><cell>GM</cell><cell>1.8</cell></row><row><cell>Spouses</cell><cell>4.4</cell><cell>4.6</cell><cell>GM</cell><cell>1.4</cell></row><row><cell>Chem</cell><cell>0.1</cell><cell>0.3</cell><cell>MV</cell><cell>1.2</cell></row><row><cell>EHR</cell><cell>2.8</cell><cell>4.8</cell><cell>GM</cell><cell>1.2</cell></row><row><cell cols="5">the expected optimal advantage will have an upper bound</cell></row><row><cell cols="4">that falls quadratically with label density:</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>σ(•) is the sigmoid function, fw is majority vote with all weights set to the meanw, andÃ * (Λ) is the predicted modeling advantage used by our optimizer. Essentially, we are taking the expected counts of instances in which a weighted majority vote could possibly flip the incorrect predictions of unweighted majority vote under best case conditions, which is an upper bound for the expected advantage: Proposition 2. (Optimizer Upper Bound) Assume that the labeling functions have accuracy parameters (logodds weights) wj ∈ [wmin, wmax], and have E[w] =w. Then:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Number of labeling functions, fraction of positive labels (for binary classification tasks), number of training documents, and number of training candidates for each task.</figDesc><table><row><cell>Task</cell><cell cols="4"># LFs % Pos. # Docs # Candidates</cell></row><row><cell>Chem</cell><cell>16</cell><cell>4.1</cell><cell>1,753</cell><cell>65,398</cell></row><row><cell>EHR</cell><cell>24</cell><cell>36.8</cell><cell>47,827</cell><cell>225,607</cell></row><row><cell>CDR</cell><cell>33</cell><cell>24.6</cell><cell>900</cell><cell>8,272</cell></row><row><cell>Spouses</cell><cell>11</cell><cell>8.3</cell><cell>2,073</cell><cell>22,195</cell></row><row><cell>Radiology</cell><cell>18</cell><cell>36.0</cell><cell>3,851</cell><cell>3,851</cell></row><row><cell>Crowd</cell><cell>102</cell><cell>-</cell><cell>505</cell><cell>505</cell></row><row><cell cols="5">that are representative of other deployments in information</cell></row><row><cell cols="5">extraction, medical image classification, and crowdsourced</cell></row><row><cell cols="5">sentiment analysis. Summary statistics of the tasks are pro-</cell></row><row><cell>vided in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Evaluation on cross-modal experiments.</figDesc><table><row><cell cols="3">Labeling functions that operate on or represent one</cell></row><row><cell cols="3">modality (text, crowd workers) produce training la-</cell></row><row><cell cols="3">bels for models that operate on another modality</cell></row><row><cell cols="3">(images, text), and approach the predictive perfor-</cell></row><row><cell cols="3">mance of large hand-labeled training datasets.</cell></row><row><cell>Task</cell><cell cols="2">Snorkel (Disc.) Hand Supervision</cell></row><row><cell>Radiology (AUC)</cell><cell>72.0</cell><cell>76.2</cell></row><row><cell>Crowd (Acc)</cell><cell>65.6</cell><cell>68.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison between training the discriminative model on the labels estimated by the generative model, versus training on the unweighted average of the LF outputs. Predictive performance gains show that modeling LF noise helps.</figDesc><table><row><cell></cell><cell>Disc. Model on</cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell cols="2">Unweighted LFs Disc. Model</cell><cell>Lift</cell></row><row><cell>Chem</cell><cell>48.6</cell><cell>54.1</cell><cell>+5.5</cell></row><row><cell>EHR</cell><cell>80.9</cell><cell>81.4</cell><cell>+0.5</cell></row><row><cell>CDR</cell><cell>42.0</cell><cell>45.3</cell><cell>+3.3</cell></row><row><cell>Spouses</cell><cell>52.8</cell><cell>54.2</cell><cell>+1.4</cell></row><row><cell>Crowd (Acc)</cell><cell>62.5</cell><cell>65.6</cell><cell>+3.1</cell></row><row><cell>Rad. (AUC)</cell><cell>67.0</cell><cell>72.0</cell><cell>+5.0</cell></row><row><cell cols="2">Crowdsourcing (Crowd):</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Labeling function ablation study on CDR. Adding different types of labeling functions improves predictive performance.</figDesc><table><row><cell>LF Type</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>Lift</cell></row><row><cell>Text Patterns</cell><cell cols="3">42.3 42.4 42.3</cell><cell></cell></row><row><cell cols="5">+ Distant Supervision 37.5 54.1 44.3 +2.0</cell></row><row><cell>+ Structure-based</cell><cell cols="4">38.8 54.3 45.3 +1.0</cell></row><row><cell cols="5">cost of some precision, as we would expect, but ultimately</cell></row><row><cell cols="5">improves F1 score by 2 points; and that structure-based</cell></row><row><cell cols="5">labeling functions, enabled by Snorkel's context hierarchy</cell></row><row><cell cols="4">data representation, add an additional F1 point.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Number of candidates in the training, development, and test splits for each dataset.</figDesc><table><row><cell>Task</cell><cell cols="3"># Train. # Dev. # Test</cell></row><row><cell>Chem</cell><cell>65,398</cell><cell>1,292</cell><cell>1,232</cell></row><row><cell>EHR</cell><cell>225,607</cell><cell>913</cell><cell>604</cell></row><row><cell>CDR</cell><cell>8,272</cell><cell>888</cell><cell>4,620</cell></row><row><cell>Spouses</cell><cell>22,195</cell><cell>2,796</cell><cell>2,697</cell></row><row><cell>Radiology</cell><cell>3,851</cell><cell>385</cell><cell>385</cell></row><row><cell>Crowd</cell><cell>505</cell><cell>63</cell><cell>64</cell></row><row><cell cols="4">is incorrect (note that for tie votes, we simply upper bound by</cell></row><row><cell cols="4">trivially assuming an expected advantage of one):</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://ctdbase.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">https://spacy.io/ 12 http://openi.nlm.nih.gov/ 13 https://www.nlm.nih.gov/mesh/meshhome.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. Alison Callahan and Nigam Shah of Stanford, and Nicholas Giori of the US Dept. of Veterans Affairs developed the electronic health records application. Emily Mallory, Ambika Acharya, and Russ Altman of Stanford, and Roselie Bright and Elaine Johanson of the US Food and Drug Administration developed the scientific articles application. Joy Ku of the Mobilize Center organized the user study. Nishith Khandwala developed the radiograph application. We thank the contributors to Snorkel including Bryan He, Theodoros Rekatsinas, and Braden Hancock. We gratefully acknowledge the support of DARPA un- </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Worldwide semiannual cognitive/artificial intelligence systems spending guide</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>International Data Corporation</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning with a probabilistic teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Infomation Theory</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="373" to="379" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pattern learning for relation extraction with a hierarchical topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Delort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garrido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning the structure of generative models without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Computational Learning Theory (COLT)</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to extract relations from the Web using minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Billington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Fulcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Keseler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krummenacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Latendresse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Subhraveti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="471" to="480" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<title level="m">Semi-Supervised Learning. Adaptive Computation and Machine Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What do a million news articles look like?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Albakour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moussa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Recent Trends in News Information Retrieval</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aggregating crowdsourced binary ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International World Wide Web Conference</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A CTD-Pfizer collaboration: Manual curation of 88,000 scientific articles text mined for drug-disease and drug-phenotype interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society C</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Big Data Integration. Synthesis Lectures on Data Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Baidu&apos;s Andrew Ng on the future of artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Eadicicco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Time</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved pattern learning for bootstrapped entity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comprehensive and reliable crowd assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Mobilize center: an NIH big data to knowledge center to advance human movement research and improve mobility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Delp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1120" to="1125" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DBpedia -A large-scale, multilingual knowledge base extracted from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web Journal</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Error rate analysis of labeling by crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop: Machine Learning Meets Crowdsourcing</title>
		<meeting><address><addrLine>Atalanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on truth discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from measurements in exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generalized expectation criteria for semi-supervised learning with weakly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="955" to="984" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Google&apos;s hand-fed AI now gives answers, not just search results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Metz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-11" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The comparative toxicogenomics database: update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcmorran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ranking and combining multiple predictors without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the USA</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1253" to="1258" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fusing data with correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pochampally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meliou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Human computation: A survey and taxonomy of a growing field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Bederson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets, quickly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">HoloClean: Holistic data repairs with probabilistic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rekatsinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">F</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PVLDB</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1190" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SLiMFast: Guaranteed results for data fusion and source reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rekatsinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD International Conference on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combining generative and discriminative model scores for distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Finding a &quot;kneedle&quot; in a haystack: Detecting knee points in system behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Satopaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Distributed Computing Systems Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Probability of error of some adaptive pattern-recognition machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Scudder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Infomation Theory</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Active Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Label-free supervision of neural networks with physics and other domain knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02968</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Reducing wrong labels in distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Overview of the BioCreative V chemical disease relation (CDR) task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BioCreative Challenge Evaluation Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A survey of crowdsourcing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Privacy, Security, Risk and Trust (PASSAT) and Inernational Conference on Social Computing (SocialCom)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modeling annotators: A generative approach to learning from annotator rationales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">F</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">DeepDive: Declarative knowledge base construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="93" to="102" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spectral methods meet EM: A provably optimal algorithm for crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A Bayesian approach to discovering truth from conflicting sources for data integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gemmell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="550" to="561" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
