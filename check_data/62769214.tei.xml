<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face Attribute Prediction Using Off-the-Shelf CNN Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-06-21">21 Jun 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhong</surname></persName>
							<email>yzhong@kth.se</email>
							<affiliation key="aff0">
								<orgName type="institution">Computer Science and Communication KTH Royal Institute of Technology</orgName>
								<address>
									<postCode>100 44</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
							<email>sullivan@kth.se</email>
							<affiliation key="aff0">
								<orgName type="institution">Computer Science and Communication KTH Royal Institute of Technology</orgName>
								<address>
									<postCode>100 44</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Li</surname></persName>
							<email>haiboli@kth.se</email>
							<affiliation key="aff0">
								<orgName type="institution">Computer Science and Communication KTH Royal Institute of Technology</orgName>
								<address>
									<postCode>100 44</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Face Attribute Prediction Using Off-the-Shelf CNN Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-06-21">21 Jun 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1602.03935v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Predicting attributes from face images in the wild is a challenging computer vision problem. To automatically describe face attributes from face containing images, traditionally one needs to cascade three technical blocks-face localization, facial descriptor construction, and attribute classification-in a pipeline. As a typical classification problem, face attribute prediction has been addressed using deep learning. Current state-of-the-art performance was achieved by using two cascaded Convolutional Neural Networks (CNNs), which were specifically trained to learn face localization and attribute description. In this paper, we experiment with an alternative way of employing the power of deep representations from CNNs. Combining with conventional face localization techniques, we use off-the-shelf architectures trained for face recognition to build facial descriptors. Recognizing that the describable face attributes are diverse, our face descriptors are constructed from different levels of the CNNs for different attributes to best facilitate face attribute prediction. Experiments on two large datasets, LFWA and CelebA, show that our approach is entirely comparable to the state-of-the-art. Our findings not only demonstrate an efficient face attribute prediction approach, but also raise an important question: how to leverage the power of off-the-shelf CNN representations for novel tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent success achieved by the Convolutional Neural Networks (CNNs) has vastly driven the advances in many aspects of computer vision, such as image classification and object detection, and pushed the boundaries of understanding image content through computer vision. In face recognition, we have witnessed great improvements brought by CNNs in solving the challenging large-scale face verification and recognition tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Like recognizing identities, describing attributes from face images in the wild has been an active research topic for years. Being able to automatically describe face attributes from face images in the wild is very challenging but can be very helpful. For instance, one can not only build identifiers directly based on attributes <ref type="bibr" target="#b9">[10]</ref>, but also efficiently construct highly flexible large-scale hierarchical datasets, which can further benefit image classification and attribute-to-image generation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>The general process of predicting face attributes is to construct face representations and train domain classifiers for prediction. As summarized in <ref type="figure" target="#fig_0">Figure 1</ref>, traditional approaches (Pipeline 1) construct low-level descriptors, such as SIFT <ref type="bibr" target="#b10">[11]</ref> and LBP <ref type="bibr" target="#b0">[1]</ref>, through landmark detection. These descriptors are then utilized for building attribute classifiers. Similarly, by using CNNs one can also employ massive sentence and image training instances to construct end-to-end deep architectures (Pipeline 2) for learning semantic-visual correspondences as in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>. However, such approaches are rather resource demanding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-level Features Classi er</head><note type="other">Image</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pipeline 4</head><p>Classi er</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classi er</head><p>End-to-end training An intuitive alternative way (Pipeline 3) is to decompose the end-to-end network (by functionality) into a localization network, a feature construction network, and attribute classifiers, and build them individually as in <ref type="bibr" target="#b24">[25]</ref>. By cascading the trained components, such a pipeline can achieve stateof-the-art performance. However, the requirements of this approach on data and training efforts seem enormous. In addition, it appears that to reach the best performance, high-level features must be used in the concatenated deep networks; fine-tuning on the pre-trained off-the-shelf high-level abstraction yields significantly better performance.</p><p>However, given that many face attributes are locally orientated and different layers of CNN features encode different levels of visual information, we believe face attributes would not be best represented by merely high-level features from deep neural networks. Thus, in this paper, we alternatively tackle the face attribute prediction problem using a pipeline composed of a conventional localization component, an off-the-shelf CNN, and attribute classifiers (Pipeline 4 in <ref type="figure" target="#fig_0">Figure 1</ref>). Our focus is finding proper feature representations from pre-trained CNNs to boost attribute perdition. We use off-the-shelf architectures and a publicly accessible model intended for face recognition to do feature construction, and investigate what types of feature representations from the network can efficiently improve face attribute prediction.</p><p>Our investigations show that intermediate representations from pre-trained CNNs have distinct advantages over high-level features for the target face attribute prediction problem. By simply utilizing these features, we achieved very promising results on a par with the state-of-the-art, produced by the intensively trained two-stage CNN, on two recently released face attribute prediction datasets CelebA and LFWA <ref type="bibr" target="#b24">[25]</ref>. Our findings also suggest that off-the-shelf intermediate CNN representations could be easily utilized when transferring from the source problem to novel detection and classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditionally, face descriptors were built from hand-crafted features. These features were constructed either from the whole face area, or extracted from detected local components and concatenated into a train of descriptors <ref type="bibr" target="#b8">[9]</ref>. Classifiers were trained based on these features to recognize the presence and quantitative degree of the domain attributes. Recently, Liu. et al. <ref type="bibr" target="#b24">[25]</ref> proposed a cascaded learning framework to perform attribute prediction in the wild. By pre-training and finetuning on large object dataset and face datasets, it efficiently localizes faces and produces semantic attributes for arbitrary face sizes without alignment.</p><p>As a strong feature learner, CNN has been successfully applied in face recognition, especially for solving the challenging face recognition in the wild problem <ref type="bibr" target="#b4">[5]</ref>. Besides the DeepID series approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>, related efforts have also been made to pose correction <ref type="bibr" target="#b19">[20]</ref>, architectures design <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14]</ref> and data collection skills <ref type="bibr" target="#b11">[12]</ref>. With recently launched hardware platforms and the publicly accessible large-scale dataset <ref type="bibr" target="#b23">[24]</ref>, developing deep learning based face recognition approaches becomes feasible with less resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>To describe face appearances using CNN features, it is critical to first consider a proper face representation from the deep neural network. One natural way is to represent faces using the discriminatively learned features, from the high-level hidden layers, mostly used for representing identities in face recognition tasks, as in <ref type="bibr" target="#b17">[18]</ref>. In this case, appearance attributes are embedded in the activation of neurons in the discriminative feature.</p><p>However, to describe the appearance using deep representations from CNNs, it is easy to expect that the selected representation should preserve the variability to describe the appearance variations regarding facial physical characteristics, such as "big (eyes)" and "open (mouth)". While on the contrary, when attributes are identity correlated (e.g. gender and ethnicity), such representation should be robust with respect to non-identity related interference. Thus, the representation that most suitable to describe a certain attribute highly depends on the property (e.g. if subject to identity) of the attribute itself. Given that a CNN enables its intermediate representations to maintain both discriminality and rich spatial information <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>, it is therefore tenable to employ flexible selections of feature presentations for predicting face attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Procedures</head><p>To identify the most effective deep representations, our method explores the attribute prediction power of intermediate representations versus the final representation 1 from CNNs trained for face recognition. Therefore, we first trained a face classification CNN (or use a publicly available model), then we evaluated the prediction performance of the representations extracted from different levels of the CNN. The training of CNNs and the evaluation of prediction power were conducted separately on two independent datasets: the WebFace <ref type="bibr" target="#b23">[24]</ref> was used for CNN training, and the CelebA and LFWA for evaluation. We used two well known off-the-shelf architectures (configurations of filter stacks) in our experiments to benefit from the latest development in CNN architecture design.</p><p>Network architecture: The networks used in our experiments shared the same format: they were composed of off-the-shelf filter stacks followed by two Fully Connected (denoted by F C1 and F C2) layers. Considering the ease of training and efficient inference during test phase, we selected Google's FaceNet NN.1 <ref type="bibr" target="#b13">[14]</ref> (shortened as "FaceNet" in the following) and VGG's "very deep" model <ref type="bibr" target="#b14">[15]</ref> as the structure of convolutional (conv.) layers. The CNNs were trained in the most fundamental flat classification manner with a Softmax layer attached to the last FC layer during training. We used dropout regularization between FCs to prevent overfitting and the dropout rate was set to 0.5 for all FC layers in our networks. PReLU <ref type="bibr" target="#b3">[4]</ref> rectification was attached to each convolution and FC layer.</p><p>Training: Around 10, 000 identities with 350, 000 image instances of the WebFace dataset were used. Random mirroring, slight rotation and jittering were utilized as data augmentation. The learning rate was initially set to 0.015, and then decreased by a factor of 10 when the validation set accuracy stopped increasing. The networks were trained by 3 decreasing learning rates. Faces were segmented and normalized to a size of 120 × 120 and randomly cropped patches of 112 × 112 were fed into the network.</p><p>Feature Extraction: To extract face descriptors from CNNs, only the center patch (112 × 112) and its mirrored version of aligned face images were fed into the CNNs unless otherwise stated. We aligned faces using feature points detected by random forests <ref type="bibr" target="#b6">[7]</ref>. We took the averaged representations of the two fed-in patches at different levels of the network, i.e. "Spat.1 × 1", "Spat.3 × 3", "F C1", and "F C2" , as shown in <ref type="figure">Figure 2</ref>, and evaluated their attribute estimation performance to identify the most effective representation corresponding to each attribute.</p><p>The output of the last conv. filter stack was selected as the representative of the intermediate representations since it was shown to have the most discriminality and spatial information for recognition</p><p>The high level abstraction used for representing identity, which is often extracted from the last FC layer. Attribute prediction: The face attribute prediction performance was evaluated on the released version of CelebA and LFWA datasets 2 . The CelebA contains approximately 200, 000 images of 10, 000 identities and LFWA has 13, 233 images of 5, 749 identities. Each image in CelebA and LFWA is annotated with 40 binary attribute tags. We used the same procedure to build our attribute classifier as in <ref type="bibr" target="#b24">[25]</ref>: binary linear SVM <ref type="bibr" target="#b2">[3]</ref> classifiers were trained directly for all levels of representations (i.e. F Cs and Spat. s) to classify face attributes. On the CelebA, the training set for each attribute classifier had 20, 000 image instances (where available). Since this dataset and the data for training our CNN are independent (the learning targets are also different), we tested the attribute prediction accuracy of our classifiers across the whole dataset through random selection of training and testing face instances. On the LFWA, we took the training instances defined by the dataset. We report the prediction accuracy as the mean of True Acceptance Rate and True Rejection Rate for each attribute on both CelebA and LFWA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluations and Comparison:</head><p>The same evaluation protocol as in <ref type="bibr" target="#b24">[25]</ref> was used in our experiments. Since the features in our experiment was extracted from aligned face images and the alignment process was independent of the network, we selected the corresponding approach (" <ref type="bibr" target="#b16">[17]</ref>+ ANet" in <ref type="bibr" target="#b24">[25]</ref>) as the baseline method. The current state of the art in <ref type="bibr" target="#b24">[25]</ref> is denoted by "Two-stage CNN" and "LNet+ANet" in this paper.</p><p>The above mentioned procedures were used in the following experiments. We first employed our FaceNet to thoroughly study the discrepancy between different representation types for face attribute prediction. The identified best performing off-the-shelf features were utilized to challenge on the CelebA and the LFWA to compare with <ref type="bibr" target="#b24">[25]</ref>. We then extended our experiments by further investigating different configurations of the FaceNet, the VGG's "very deep" architecture, and the publicly available VGG-Face model 3 to ensure the discrepancy in attribute prediction power among the deep representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Performance Discrepancy between Deep Representations</head><p>Our intuition as stated above was that the intermediate face representations would be more suitable for describing diverse types of attributes regarding their physical characteristics and image conditions. To validate this, we trained a face recognition CNN with a structure of FaceNet. The length of both FC layers was set to 512 to reduce the risk of overfitting. The recognition rate of the trained FaceNet on the validation set was less than 98% and the face verification performance on the LFW <ref type="bibr" target="#b4">[5]</ref> was 97.5%. We then extracted the four types of face representations, Spat.1b1, Spat.3b3, F C1, and F C2, from our trained model and linear classifiers were constructed and evaluated respectively  on the training set. The prediction performance for each representation type on all attributes is shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>While it is intuitive that F C2, the identity discriminative feature, is unlikely to be the best choice for describing facial attributes all the time, it is still astonishing that F C2 was significantly outperformed ( 5% in prediction accuracy) by others on 13 attributes. Similar disadvantages can also be observed on the LFWA dataset. It is easy to find that:</p><p>1. Representations at different levels of the network feature quite diverse performance in attribute description.</p><p>2. Intermediate representations, especially Spat.3 × 3, are likely more effective in telling the weak identity-related attributes describing expressions and image conditions, which counts more on spatial information.</p><p>For instance, for attributes related to mouth and eyes which can produce dynamic facial expressions, such performance gaps are significant. For "Spat.3 × 3" which better preserved spatial information, it is more effective in specifying shape and motion of facial components (e.g. Attribute 20 and 34 on CelebA in <ref type="figure" target="#fig_1">Figure 3</ref>). This is natural since intermediate representations contain mid-level features composed by low-level ones, thus they are more suitable to describe local facial attributes.</p><p>Our investigations show that the best performing representations achieved attribute prediction accuracy of 86.6% on CelebA and 84.7% on LFWA, which is on a par with state-of-the-art "Two-stage CNN" approach which was trained with massive image classification and face data. The comparative results are listed in <ref type="table" target="#tab_4">Table 1</ref> and shown in <ref type="figure" target="#fig_2">Figure 4</ref>. One can see that:</p><p>1. By leveraging the intermediate deep representations from various levels of CNNs, the equivalent baseline approach is outperformed with a big margin.</p><p>2. Even without fine-tuning the pre-trained CNN, our average prediction performance is still comparable to the state-of-the-art on both datasets.</p><p>Here we noticed that the intermediate representations dominated the best representations of the attributes. This indicates that spatial information, i.e. location and magnitude of activation in conv. filter responses, is significant for describing attributes; if one wants to utilize high-level features, which implicitly embeds spatial information, fine-tuning must be conducted on the high-level abstractions to enhance such useful spatial information.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Further Validations</head><p>To further verify the potential utility of intermediate spatial representations for face attribute prediction, we also evaluated various network architectures trained by different configurations, which for each model are listed in <ref type="table" target="#tab_5">Table 2</ref>. CelebA was selected as the evaluation dataset due to its larger scale.</p><p>Specifically, we first evaluated two different networks of the FaceNet architecture. Model 1 was trained by the first 8, 000 identities that has the most images on the WebFace dataset (i.e. taking away the long-tail data). Since the length of the representing features plays a vital role in face representation <ref type="bibr" target="#b11">[12]</ref>, we then evaluated the influences of varying FC layer lengths in face attribute prediction with Model 2 by increasing the length of FC layers to 1024. The receptive field was kept the same (112 × 112).</p><p>We also cross-validated the utility of the deep representations with VGG type architectures in Model 3 and 4. Model 3 had filter stack as VGG fitler-Config.C <ref type="bibr" target="#b14">[15]</ref>, but with a duplicated conv. and pooling section appended to the fifth pooling so that it was even deeper. (Thus, for this configuration, the filter stack directly gave output with size of 3 × 3. It was then max. pooled to get 1 × 1 output.) To bring more divergence, we decreased the input size to 96 × 96 (still cropped from 120 × 120) and set FCs to 4096. Model 4 was the off-the-shelf VGG-Face network. The receptive area for Model 4 was 224 × 224. The corresponding results in terms of the averaged prediction accuracy are provided in <ref type="table" target="#tab_6">Table 3</ref>.</p><p>We observed that on average the spatial representations excelled on more than 75% of the 40 attributes. The spatial representation from the off-the-shelf VGG-Face model even dominated the best representations. We attribute it to the dramatic increase of the receptive area. The intermediate representations embedded more detailed spatial information also further boosted the performance of F C2, which was as effective as the Spat.3 × 3. The slightly worse performance of the features from Model 3 can be attributed to the lower receptive field and the 6th extra pooling, which caused transfer of prediction power from Spat.3 × 3 to Spat.1 × 1.</p><p>Through further analysis of the results, we found that the intermediate spatial representations predicted 5 attributes ("Bags Under Eyes" , "Blurry", "Mouth S. Open", "Pale Skin" and "Narrow Eyes") much better than the last FC representations.</p><p>We believe the reason intermediate spatial representations outperformed on so many attributes is that these human describable attributes are more likely to be identified from the spatial information captured by human brains. Considering these attributes are semantic concepts relating to specific domains and these domains by themselves alone can hardly be used to pin-point a specific identity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we address the problem of predicting face attributes using CNNs trained for face recognition. We employ CNNs with off-the-shelf architectures and publicly available models -Google's FaceNet and VGG's "very deep" model -with the conventional pipeline to study the prediction power of different representations from the trained CNNs. Our investigations present the correspondence diversity between the best performing representations and the human describable attributes. They also reveal that the intermediate representations from CNNs are very effective in predicting facial attributes in general. Although previous works have shown that fine-tuning the pretrained networks brought significant improvements when transferring to novel tasks, we empirically demonstrate that intermediate deep features from pre-trained networks can also form a promising alternative. By properly leveraging these off-the-shelf CNN representations, we achieved accurate attribute prediction on a par with current state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Potential pipelines of automatic attribute estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparing prediction performance of deep representations on CelebA and LFWA. Spat.1 × 1 was set as the reference for each attribute and the relative prediction power of each representation type is plotted based on its difference to the reference. The attributes are sorted based on the absolute prediction accuracy of the baseline method on each dataset. On CelebA, the absolute mean prediction accuracy of F C1 = 83% , F C2 = 82% , Spat.3×3 = 86% , Spat.1×1 = 82%, and on LFWA, F C1 = 83% , F C2 = 82% , Spat.3 × 3 = 85% , Spat.1 × 1 = 81%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparing attribute prediction results on CelebA and LFWA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Figure 2: Pipeline of extracting deep representations from trained CNN. Intermediate features (Spat.N × N , FC1) and final representation FC2 are extracted from the trained network. N for the side of deep feature map after extra pooling step. In total, 4 types of representations will be studied for face attribute prediction.</figDesc><table><row><cell>Conv. Filter Stack</cell><cell>FC 1</cell><cell>FC 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Spat.</cell></row><row><cell></cell><cell cols="2">Spatial</cell><cell>3*3</cell></row><row><cell></cell><cell>Pool.</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Spat.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>1*1</cell></row></table><note>and image retrieval [2]. Extra max. pooling steps were applied to reduce the dimension of interme- diate spatial representations. Then Spat.3 × 3 and Spat.1 × 1 were of 3 × 3 × K and 1 × 1 × K in our experiments regardless of the network, where K represents the channel depth of the employed network.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Comparing prediction accuracy (in %) on CelebA and LFWA: corresponding average values of our approaches are 86.6% and 84.7%; for the baseline method: 83%, 76%; for the current best LNet+ANet approach: 87% and 84%.</figDesc><table><row><cell></cell><cell></cell><cell>5 o Clock Shadow</cell><cell>Arched Eyebrows</cell><cell>Attractive</cell><cell>Bags Under Eyes</cell><cell>Bald</cell><cell>Bangs</cell><cell>Big Lips</cell><cell>Big Nose</cell><cell>Black Hair</cell><cell>Blond Hair</cell><cell>Blurry</cell><cell>Brown Hair</cell><cell>Bushy Eyebrows</cell><cell>Chubby</cell><cell>Double Chin</cell><cell>Eyeglasses</cell><cell>Goatee</cell><cell>Gray Hair</cell><cell>Heavy Makeup</cell><cell>High Cheekbones</cell></row><row><cell>CelebA</cell><cell>Baseline LNet+ANet Ours</cell><cell></cell><cell>75 83</cell><cell>82</cell><cell>77 79</cell><cell>96</cell><cell>94 94</cell><cell>70</cell><cell>74 79</cell><cell>87</cell><cell>86 93</cell><cell>87</cell><cell>74 79</cell><cell>87</cell><cell>86 88</cell><cell>89</cell><cell>96 99</cell><cell>94</cell><cell>95</cell><cell>91</cell><cell>85 87</cell></row><row><cell>LFWA</cell><cell>Baseline LNet+ANet Ours</cell><cell>84 77</cell><cell>66 82 83</cell><cell>75 83 79</cell><cell>72 83 83</cell><cell>86 88 91</cell><cell>84 88 91</cell><cell>70 75 78</cell><cell>73 81 83</cell><cell>82 90 90</cell><cell>90 97 97</cell><cell>75 74 88</cell><cell>71 77 76</cell><cell>69 82 83</cell><cell>68 73 75</cell><cell>70 78 80</cell><cell>88 95 91</cell><cell>68 78 83</cell><cell>82 84 87</cell><cell>89 95 95</cell><cell>79 88 88</cell></row><row><cell></cell><cell></cell><cell>Male</cell><cell>Mouth S. Open</cell><cell>Mustache</cell><cell>Narrow Eyes</cell><cell>No Beard</cell><cell>Oval Face</cell><cell>Pale Skin</cell><cell>Pointy Nose</cell><cell>Receding Hairline</cell><cell>Rosy Cheeks</cell><cell>Sideburns</cell><cell>Smiling</cell><cell>Straight Hair</cell><cell>Wavy Hair</cell><cell>Wearing Earrings</cell><cell>Wearing Hat</cell><cell>Wearing Lipstick</cell><cell>Wearing Necklace</cell><cell>Wearing Necktie</cell><cell>Young</cell></row><row><cell>CelebA</cell><cell>Baseline LNet+ANet Ours</cell><cell></cell><cell>92 92</cell><cell>95 93</cell><cell>81 78</cell><cell>95 94</cell><cell>66 67</cell><cell>91 85</cell><cell>72 73</cell><cell>89 87</cell><cell>90 88</cell><cell>96 95</cell><cell>92 92</cell><cell>73 73</cell><cell>80 79</cell><cell>82 82</cell><cell>99 96</cell><cell>93 93</cell><cell>71 73</cell><cell>93 91</cell><cell>87 86</cell></row><row><cell>LFWA</cell><cell>Baseline LNet+ANet Ours</cell><cell></cell><cell>76 82 81</cell><cell>79 92 94</cell><cell>74 81 81</cell><cell>69 79 80</cell><cell>66 74 75</cell><cell>68 84 73</cell><cell>72 80 83</cell><cell>70 85 86</cell><cell>71 78 82</cell><cell>72 77 82</cell><cell>82 91 90</cell><cell>72 76 77</cell><cell>65 76 77</cell><cell>87 94 94</cell><cell>82 88 90</cell><cell>86 95 95</cell><cell>81 88 90</cell><cell>72 79 81</cell><cell>79 86 86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>CNNs and training data used for further validations.</figDesc><table><row><cell>Model</cell><cell>Conv. Filter Stack Architecture</cell><cell>FC 1 Dim.</cell><cell>FC 2 Dim.</cell><cell>Training Dataset / Identities</cell></row><row><cell>1</cell><cell>FaceNet</cell><cell>512</cell><cell>512</cell><cell>WebFace, 8k</cell></row><row><cell>2</cell><cell>FaceNet</cell><cell cols="2">1024 1024</cell><cell>WebFace, 10k</cell></row><row><cell>3</cell><cell>VGG, Config. C</cell><cell cols="2">4096 4096</cell><cell>WebFace, 10k</cell></row><row><cell>4</cell><cell>VGG-Face</cell><cell cols="2">4096 4096</cell><cell>private, &gt;2.6k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Decomposition of the best representations of the architectures inTable 2. This table gives the number of each representation type that formed the best representation for each model and provides the average prediction accuracy achieved by the best representations and the F C2</figDesc><table><row><cell cols="2">representation. ("S." for "Spat.")</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="6"># of Best Rep. from S.3*3 S.1*1 FC1 FC2 Best Rep. FC2 ave. accuracy</cell></row><row><cell></cell><cell>33</cell><cell>0</cell><cell>6</cell><cell>1</cell><cell>86%</cell><cell>84%</cell></row><row><cell></cell><cell>31</cell><cell>0</cell><cell>6</cell><cell>3</cell><cell>86%</cell><cell>84%</cell></row><row><cell></cell><cell>28</cell><cell>11</cell><cell>1</cell><cell>0</cell><cell>85%</cell><cell>83%</cell></row><row><cell></cell><cell>37</cell><cell>1</cell><cell>0</cell><cell>2</cell><cell>86%</cell><cell>85%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html. Released in Oct. 2015.<ref type="bibr" target="#b2">3</ref> http://www.robots.ox.ac.uk/˜vgg/software/vgg_face/, accessed in Nov. 2015.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the support from NVIDIA Corporation for GPU donations. We have enjoyed discussions with Ali Sharif Razavian and Atsuto Maki.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Atsuto Maki, and Stefan Carlsson. From generic to specific deep representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5774</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2306</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahdat</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shree K</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Describable visual attributes for face verification and image search. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shree K</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1962" to="1977" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distinctive image features from scale-invariant keypoints. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
		</imprint>
	</monogr>
	<note>Face Attribute Prediction Using Off-the-Shelf CNN Features</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision</title>
		<meeting>the British Machine Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cnn features offthe-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Ali S Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
	<note>2014 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deepid3: Face recognition with very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Attribute2image: Conditional image generation from visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00570</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hd-cnn: Hierarchical deep convolutional neural network for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;15: Proc. IEEE 15th International Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang Wang Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
