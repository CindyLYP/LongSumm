<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RPCValet: NI-Driven Tail-Aware Balancing of µs-Scale RPCs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Daglis</surname></persName>
							<email>alexandros.daglis@cc.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sutherland</surname></persName>
							<email>mark.sutherland@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RPCValet: NI-Driven Tail-Aware Balancing of µs-Scale RPCs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3297858.3304070</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Modern online services come with stringent quality requirements in terms of response time tail latency. Because of their decomposition into fine-grained communicating software layers, a single user request fans out into a plethora of short, µs-scale RPCs, aggravating the need for faster inter-server communication. In reaction to that need, we are witnessing a technological transition characterized by the emergence of hardware-terminated user-level protocols (e.g., In-finiBand/RDMA) and new architectures with fully integrated Network Interfaces (NIs). Such architectures offer a unique opportunity for a new NI-driven approach to balancing RPCs among the cores of manycore server CPUs, yielding major tail latency improvements for µs-scale RPCs. We introduce RPCValet, an NI-driven RPC load-balancing design for architectures with hardware-terminated protocols and integrated NIs, that delivers near-optimal tail latency. RPCValet&apos;s RPC dispatch decisions emulate the theoretically optimal single-queue system, without incurring synchronization overheads currently associated with single-queue implementations. Our design improves throughput under tight tail latency goals by up to 1.4×, and reduces tail latency before saturation by up to 4× for RPCs with µs-scale service times, as compared to current systems with hardware support for RPC load distribution. RPCValet performs within 15% of the theoretically optimal single-queue system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern datacenters deliver a breadth of online services to millions of daily users. In addition to their huge scale, online services come with stringent Service Level Objectives (SLOs) to guarantee responsiveness. Often expressed in terms of tail latency, SLOs target the latency of the slowest requests, and thus bound the slowest interaction a user may have with the service. Tail-tolerant computing is one of the major ongoing challenges in the datacenter space, as long-tail events are rare and rooted in convoluted hardware-software interactions.</p><p>A key contributor to the well-known "Tail at Scale" challenge <ref type="bibr" target="#b13">[15]</ref> is the deployment of online services' software stacks in numerous communicating tiers, where the interactions between a service's tiers take the form of Remote Procedure Calls (RPCs). Large-scale software is often built in this fashion to ensure modularity, portability, and development velocity <ref type="bibr" target="#b24">[26]</ref>. Not only does each incoming request result in a wide fan-out of inter-tier RPCs <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b21">23]</ref>, each one lies directly on the critical path between the user and the online service <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b49">50]</ref>. The amalgam of the tail latency problem with the trend towards ephemeral and fungible software tiers has created a challenge to preserve the benefits of multi-tiered software while making it tail tolerant.</p><p>To lower communication overheads and tighten tail latency, there has been an intensive evolution effort in datacenter-scale networking hardware and software, away from traditional POSIX sockets and TCP/IP and towards lean userlevel protocols such as InfiniBand/RDMA <ref type="bibr" target="#b19">[21]</ref> or dataplanes such as IX and ZygOS <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b46">47]</ref>. Coupling protocol innovations with state-of-the-art hardware architectures such as Firebox <ref type="bibr" target="#b2">[4]</ref>, Scale-Out NUMA <ref type="bibr" target="#b42">[43]</ref> or Mellanox's BlueField Smart-NIC <ref type="bibr" target="#b36">[37]</ref>, which offer tight coupling of the network interface (NI) with compute logic, promises even lower communication latency. The net result of rapid advancements in the networking world is that inter-tier communication latency will approach the fundamental lower bound of speedof-light propagation in the foreseeable future <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b49">50]</ref>. The focus of optimization hence will completely shift to efficiently handling RPCs at the endpoints as soon as they are delivered from the network.</p><p>The growing number of cores on server-grade CPUs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref> exacerbates the challenge of distributing incoming RPCs to handler cores. Any delay or load imbalance caused by this initial stage of the RPC processing pipeline directly impacts tail latency and thus overall service quality. Modern NIC mechanisms such as Receive-Side Scaling (RSS) <ref type="bibr" target="#b41">[42]</ref> and Flow Direction <ref type="bibr" target="#b22">[24]</ref> offer load distribution and connection affinity, respectively. However, the key issue with these mechanisms, which apply static rules to split incoming traffic into multiple receive queues, is that they do not truly achieve load balancing across the server's cores. Any resulting load imbalance after applying these rules must be handled by system software, introducing unacceptable latency for the most latency-sensitive RPCs with µs-scale service times <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>In this paper, we propose RPCValet, a co-designed hardware and software system to achieve dynamic load balancing across CPU cores, based on the key insight that on-chip NIs offer the ability to monitor per-core load in real time and steer RPCs to lightly loaded cores. The enabler for this style of dynamic load balancing is tight CPU-NI integration, which allows fine-grained, nanosecond-scale communication between the two, unlike conventional PCIe-attached NIs. To demonstrate the benefits of our design, we first classify existing load-distribution mechanisms from both the hardware and software worlds as representative of different queuing models, and show how none of them is able to reach the performance of the theoretical best case. We then design a minimalistic set of hardware and protocol extensions to Scale-Out NUMA (soNUMA) <ref type="bibr" target="#b42">[43]</ref>, an architecture with on-chip integrated NIs, to show that a carefully architected system can indeed approach the best queuing model's performance, significantly outperforming prior load-balancing mechanisms. To summarize, our contributions include:</p><p>• RPCValet, an NI-driven dynamic load-balancing design that outperforms existing hardware mechanisms for load distribution, and approaches the theoretical maximum performance predicted by queuing models. • Hardware and protocol extensions to soNUMA for native messaging support, a required feature for efficient RPC handling. We find that, in contrast to prior judgment <ref type="bibr" target="#b42">[43]</ref>, native messaging support is not disruptive to the key premise of NI hardware simplicity, which such architectures leverage to enable on-chip NI integration. • An RPCValet implementation on soNUMA that delivers near-ideal RPC throughput under strict SLOs, attaining within 3-16% of the theoretically optimal queuing model. For µs-scale RPCs, RPCValet outperforms software-based and RSS-like hardware-driven load distribution by 2.3-2.7× and 29-76%, respectively. The paper is organized as follows: §2 outlines the performance differences between multi-and single-queue systems, highlighting the challenges in balancing incoming RPCs with short service times among cores. §3 presents RPCValet's design principles, followed by an implementation using soNUMA as a base architecture in §4. We detail our methodology in §5 and evaluate RPCValet in §6. Finally, we discuss related work in §7 and conclude in §8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background 2.1 Application and technology trends</head><p>Modern online services are decomposed into deep hierarchies of mutually reliant tiers <ref type="bibr" target="#b24">[26]</ref>, which typically interact using RPCs. The deeper the software hierarchy, the shorter each RPC's runtime, as short as a few µs for common software tiers such as data stores. Fine-grained RPCs exacerbate the tail latency challenge for services with strict SLOs, as accumulated µs-scale overheads can result in a long-tail event.</p><p>To mitigate the overheads of RPC-based communication, network technologies have seen renewed interest, with the InfiniBand fabric and protocol beginning to appear in datacenters <ref type="bibr" target="#b19">[21]</ref> due to its low latency and high IOPS. With networking latency approaching the fundamental limits of propagation delays <ref type="bibr" target="#b18">[20]</ref>, any overhead added to the raw RPC processing time at a receiving server critically impacts latency. For example, while InfiniBand significantly reduces latency compared to traditional TCP/IP over Ethernet, InfiniBand adapters still remain attached to servers over PCIe, which contributes an extra µs of latency to each message <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Efficiently handling µs-scale RPCs requires the elimination of these µs-scale overheads, which is the goal of fully integrated solutions (e.g., Firebox <ref type="bibr" target="#b2">[4]</ref>, soNUMA <ref type="bibr" target="#b42">[43]</ref>). Such architectures employ lean, hardware-terminated network stacks and integrated NIs to achieve sub-µs inter-server communication, representing the best fit for latency-sensitive RPC services. NI integration enables rapid fine-grained interaction between the CPU, NI, and memory hierarchy, a feature leveraged previously to accelerate performance-critical operations, such as atomic data object reads from remote memory <ref type="bibr" target="#b12">[14]</ref>. In this paper, we leverage NI integration to break existing tradeoffs in balancing RPCs across CPU cores and significantly improve throughput under SLO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Load Balancing: Theory</head><p>To study the effect of load balancing across cores on tail latency, we conduct a first-order analysis using basic queuing theory. We model a hypothetical 16-core server after a queuing system that features a variable number of input queues and 16 serving units. <ref type="figure" target="#fig_15">Fig. 1</ref> shows three different queuing system organizations. The notation Model Q × U denotes a queuing system with Q FIFOs where incoming messages arrive and U serving units per FIFO. The invariant across the three illustrated models is Q ×U = 16. The 16 × 1 system cannot perform any load balancing; incoming requests are uniformly distributed across 16 queues, each with a single serving unit. 1 × 16 represents the most flexible option that achieves the best load balancing: all serving units pull requests from a single FIFO. Finally, × 4 represents a middle ground: incoming messages are uniformly distributed across four FIFOs with four serving units each.</p><p>To evaluate different queuing organizations, we employ discrete event simulations modeling Poisson arrivals and four  <ref type="figure" target="#fig_15">Figure 1</ref>. Different queuing models for 16 serving units (CPU cores). P(λ) stands for Poisson arrival distribution. different service time distributions: fixed, uniform, exponential, and generalized extreme value (GEV). Poisson arrivals are commonly used to model the independent nature of incoming requests. §5 details each distribution's parameters. <ref type="figure" target="#fig_2">Fig. 2a</ref> shows the performance of five queuing systems Q × U with (Q</p><p>, for an exponential service time distribution. The system's achieved performance is directly connected to its ability to assign requests to idle serving units. As expected, performance is proportional to U . The best and worst performing configurations are 1 × 16 and 16 × 1 respectively, while 2 × 8, 4 × 4 and 8 × 2 lie in between these two. <ref type="figure" target="#fig_2">Fig. 2b and 2c</ref> show the relation of throughput and 99th percentile latency for the two extreme queuing system configurations, namely 1 × 16 and 16 × 1. As seen in <ref type="figure" target="#fig_2">Fig. 2a</ref>, 1 × 16 significantly outperforms 16×1. 16×1's inability to assign requests to idle cores results in higher tail latencies and a peak throughput 25-73% lower than × 16 under a tail latency SLO at 10× the mean service timeS. In addition, the degree of performance degradation is affected by the service time distribution. For both queuing models, we observe that the higher a distribution's variance, the higher the tail latency (TL) before the saturation point is reached, hence T L f ix ed &lt; T L uni &lt; T L exp &lt; T L G EV . Also, the higher the distribution's variance, the more dramatic the performance gap between × 16 and 16 × 1, as is clearly seen for GEV. The application's service time distribution is beyond an architect's control, as it is affected by numerous software and hardware factors. However, they can control the queuing model that the underlying system implements. The theoretical results suggest that systems should implement a queuing configuration that is as close as possible to a single-queue (1 × 16) configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Load Balancing: Practice</head><p>A subtlety not captured by our queuing models is the practical overhead associated with sharing resources (i.e., the input queue). In a manycore CPU, allowing all the cores to pull incoming network messages from a single queue requires synchronization. We refer to this RPC dispatch mode as "pull-based". Especially for short-lived RPCs, with service times of a few µs, such synchronization represents significant overhead. Architectures that share a pool of connections between cores have this pitfall; common examples include using variants of Linux's poll system call, or locked event queues supported by libevent.</p><p>An alternative approach for distributing load to multiple cores, advocated by recent research, is dedicating a private queue of incoming network messages to each core <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b44">45]</ref>. Although this design choice corresponds to a rigid N × 1 queuing model (N being the number of cores), it completely eschews overheads related to sharing (i.e., synchronization and coherence), delivering significant throughput gains. By leveraging RSS <ref type="bibr" target="#b41">[42]</ref> inside the NI, messages are consistently distributed at arrival time to one of the N input queues. This ultimately results in a different mode of communication: instead of the cores pulling messages from a single queue, the NI hardware actively pushes messages into each core's queue. We refer to this load distribution mode as "push-based".</p><p>FlexNIC <ref type="bibr" target="#b28">[30]</ref> extends the push-based model by proposing a P4-inspired domain-specific language, allowing software to install match-action rules into the NI. Despite their many differences, both FlexNIC and RSS completely rely on decisions based on the RPC packets' header content. Whether configured statically or by the application, push-based load distribution still fundamentally embodies a multi-queue system vulnerable to load imbalance, as no information pertaining to the system's current load is taken into account. §2.2's queuing models demonstrate the effect of this imbalance as compared to a system with balanced queues.</p><p>The two aforementioned approaches to load distribution, pull-and push-based, represent a tradeoff between synchronization and load imbalance. In this paper, we leverage the onchip NI logic featured in emerging fully integrated architectures such as soNUMA <ref type="bibr" target="#b42">[43]</ref> to introduce a novel push-based NI-driven load-balancing mechanism capable of breaking that tradeoff by making dynamic load-balancing decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RPCValet Load-Balancing Design</head><p>This section describes the insights and foundations guiding RPCValet's design. Our goal is to achieve a synchronizationfree system that behaves like the theoretical best singlequeue model. We begin by setting forth our basic assumptions about the underlying hardware and software, then explain the roadblocks to achieving dynamic load balancing, and conclude with the principles of RPCValet's design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Architecture</head><p>We design RPCValet for emerging architectures featuring fully integrated NIs and hardware-terminated transport protocols. We target these architectures for two reasons. First, an important class of online services exhibits RPCs with service times that are frequently only a few µs long. For example,   the average service time for Memcached <ref type="bibr" target="#b1">[2]</ref> is ∼ 2µs <ref type="bibr" target="#b46">[47]</ref>. Even software with functionality richer than simple data retrieval can exhibit µs-scale service times: the average TPC-C query service time on the Silo in-memory database <ref type="bibr" target="#b52">[53]</ref> is only 33µs <ref type="bibr" target="#b46">[47]</ref>. Software tiers with such short service times necessitate network architectures optimized for the lowest possible latency, using techniques such as kernel bypass and polling rather than receiving interrupts.</p><p>Second, unpredictable tail-inducing events for these shortlived RPCs often disrupt application execution for periods of time that are comparable to the RPCs themselves <ref type="bibr" target="#b4">[6]</ref>. For example, the extra latency imposed by TLB misses or context switches spans from a few hundred ns to a few µs. At such fine granularities, any load-balancing policy implemented at the distal end of an I/O-attached NI is simply too far from the CPU cores to adjust its load dispatch decisions appropriately. Therefore, we argue that mitigating load imbalance at the µs level requires µs-optimized hardware.</p><p>The critical feature of our µs-optimized hardware is a fully integrated NI with direct access to the server's memory hierarchy, eliminating costly roundtrips over traditional I/O fabrics such as PCIe. Each server registers a part of its DRAM in advance with a particular context that is then exported to all participating servers, creating a partitioned global address space (PGAS) where every server can read/write remote memory in RDMA fashion. The architecture's programming model is a concrete instantiation of the Virtual Interface Architecture (VIA) <ref type="bibr" target="#b16">[18]</ref>, where each CPU core communicates with the NI through memory-mapped queue pairs (QPs). Each QP consists of a Work Queue (WQ) where the core writes entries (WQEs) to be processed by the NI, and a Completion Queue (CQ), where the NI writes entries (CQEs) to indicate that the cores' WQEs were completed. For more details, refer to the original VIA <ref type="bibr" target="#b16">[18]</ref> and soNUMA <ref type="bibr" target="#b42">[43]</ref> work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NI Integration: The Key Enabler</head><p>The NI's integration on the same piece of silicon as the CPU is the key enabler for handling µs-scale events. By leveraging the fact that such integration enables fine-grained real-time (nanosecond-scale) information to be passed back and forth between the NI and the server's CPU, the NI has the ability to respond to rapidly changing load levels and make dynamic load-balancing decisions. To illustrate the importance of ns-scale interactions, consider a data serving tier such as Redis [3], maintaining a sorted array in memory. Since the implementation of its sorted list container uses a skip list to provide add/remove operations in O (loд (N )) time, an RPC to add a new entry may incur multiple TLB misses, stalling the core for a few µs while new translations are installed. While this core is stalled on the TLB miss(es), it is best to dispatch RPCs to other available cores on the server.</p><p>An integrated NI can, with proper hardware support, monitor each core's state and steer RPCs to the least loaded cores. Such monitoring is implausible without NI integration, as the latency of transferring load information over an I/O bus (e.g., ∼ 1.5µs for a 3-hop posted PCIe transaction) would mean that the NI will make delayed-hence sub-optimal, or even wrong-decisions until the information arrives.</p><p>The active feedback of information from the server's compute units (which are not restricted to CPU cores) to the NI can take many forms, ranging from monitoring memory hierarchy events to metadata directly exposed by the application. Regardless of the exact policy, the underlying enabler for RPCValet's ability to handle µs-scale load imbalance is that load dispatch decisions are driven by an integrated NI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Design Principles</head><p>Our design goal is to break the tradeoff between the load imbalance inherent in multi-queue systems and the synchronization associated with pulling load from a single queue. To begin, we retain the VIA's design principle of allocating a single virtual interface (identical to a QP in IB/soNUMA terminology) to each participating thread, which is critically important for handling µs-scale RPCs. Registering independent QPs with the NI helps us achieve the goal of eliminating synchronization, as each thread polls on its own QP and waits for the arrival of new RPCs. This simplifies the load-balancing problem to simply choosing the correct QP to dispatch the RPC to. By allowing the NI to choose the QP at message arrival time, based on one of the many possible heuristics for estimating per-core load, our design achieves the goal of synchronization-free push-based load balancing.  Unfortunately, realizing such a design with our baseline architecture ( § 3.1) is not possible, as existing primitives are not expressive enough for push-based dispatch. In particular, architectures with on-chip NIs such as soNUMA <ref type="bibr" target="#b42">[43]</ref> do not provide native support for messaging operations, favoring RDMA operations for hardware simplicity that facilitates NI integration. These RDMA operations (a.k.a. "one-sided" ops) enable direct read/write access of remote memory locations, without involving a CPU at the remote end. Hence, a reception of a one-sided op is not associated with a creation of a CPU notification event by the NI.</p><p>Messaging can be emulated on top of one-sided ops by allocating shared bounded buffers in the PGAS <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b42">43]</ref>, into which threads directly place messages using one-sided writes. <ref type="figure" target="#fig_4">Fig. 3a</ref> illustrates the high-level operation of emulated messaging. As emulated messaging is performed in a connection-oriented fashion from thread to thread, each RPC-handling thread allocates N bounded buffers, each with S message slots; N is the number of nodes that can send messages. Each of the C cores polls at the head slots of its corresponding N buffers for incoming RPCs.</p><p>The fundamental drawback of such emulated messaging is that the sending thread implicitly determines which thread at the remote end will process its RPC request, because the memory location the RPC is written to is tied to a specific thread. The result is a multi-queue system, vulnerable to load imbalance. Although it may be possible to implement some form of load-aware messaging (e.g., per-thread client-server flow control), such mechanisms will have little to no benefit due to the relatively high network round-trip time for load information to diffuse between the two endpoints, especially when serving short-lived RPCs.</p><p>A key reason why, in the case of emulated messaging, the NI at the destination cannot affect the a priori assignment of an incoming RPC to a thread is that the protocol does not enable the NI to distinguish a "message" (i.e., a one-sided write triggering two-sided communication) from a default one-sided op. Protocol support for native messaging with innate semantics of two-sided operations overcomes this limitation and enables the NI at the message's destination node  to perform push-based load balancing. <ref type="figure" target="#fig_4">Fig. 3b</ref> demonstrates RPCValet's high-level operation. The NI first writes every incoming message into a single PGAS-resident message buffer of N × S slots, as in the case of emulated messaging. Then, the NI uses a selected core's QP to notify it to process the incoming RPC request. In effect, RPCValet decouples a message's arrival and location in memory from its assignment to a core for processing, thus achieving the best of both worlds: the load-balancing flexibility of a single-queue system, and the synchronization-free, zero-copy behavior of partitioned multi-queue architectures. <ref type="figure" target="#fig_4">Fig. 3b</ref> demonstrates how NI-driven dynamic dispatch decisions result in balanced load, in contrast to <ref type="figure" target="#fig_4">Fig. 3a</ref>'s example.</p><p>In conclusion, RPCValet requires extensions to both the on-chip NI hardware and the networking protocol, to first provide support for native messaging and, second, realize dynamic load-balancing decisions. In the following section, we describe an implementation of an architecture featuring both of these mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RPCValet Implementation</head><p>In this section, we describe our RPCValet implementation as an extension of the soNUMA architecture <ref type="bibr" target="#b42">[43]</ref>, including a lightweight extension of the baseline protocol for native messaging and support for NI-driven load balancing.</p><p>4.1 Scale-Out NUMA with Manycore NI soNUMA enables rapid remote memory access through a lean hardware-terminated protocol and on-chip NI integration. soNUMA deploys a QP interface for CPU-NI interaction ( §3.1) and leverages on-chip cache coherence to accelerate QP-entry transfers between the CPU and NI. <ref type="figure" target="#fig_6">Fig. 4</ref> shows soNUMA's scalable NI architecture for manycore CPUs <ref type="bibr" target="#b11">[13]</ref>. The conventionally monolithic NI is split into two heterogeneous parts, a frontend and a backend. The frontend is the "control" component, and is collocated with each core to drastically accelerate QP interactions. The backend is replicated across the chip's edge, to scale the NI's capability with growing network bandwidth, and handles all data and network packets. Pairs of frontend and backend entities, which together logically comprise a complete  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lightweight Native Messaging</head><p>We devise a lightweight implementation of native messaging as a required building block for dynamic load-balancing decisions at the NI. A key difficulty to overcome is support for multi-packet messages, that must be reassembled by the destination NI. This goal conflicts with soNUMA's stateless request-response protocol, which unrolls large requests into independent packets each carrying a single cache block payload. Emulated messaging (see §3.3) does not require any reassembly at the destination, because all packets are directly written to the bounded buffer specified by the sender.</p><p>One workaround to avoid message reassembly complications would be to limit the maximum message size to the link layer's MTU. Prior work has adopted this approach to build an RPC framework on an IB cluster <ref type="bibr" target="#b25">[27]</ref>. Such a design choice may be an acceptable limitation for IB networks which have a relatively large MTU of 4KB. However, fully integrated solutions with on-chip NIs will likely feature small MTUs (e.g., a single cache line in soNUMA), so limiting the maximum message size to the link-layer MTU is impractical.</p><p>Our approach to avoiding the hardware overheads associated with message reassembly is keeping the buffer provisioning of the emulated messaging mechanism, which allows the sender to determine the memory location the message will be written to. Therefore, soNUMA's request-response protocol can still handle the message as a series of independent cache-block-sized writes to the requester-specified memory location. While this mechanism may seem identical to one-sided operations, we introduce a new pair of send and replenish operations which expose the semantics of multi-packet messages to the NI-it can then distinguish true one-sided operations from messaging operations, which are eligible for load balancing. The NI keeps track of packet receptions belonging to a send, deduces when it has been fully received, and then hands it off to a core for processing. Buffer provisioning. We introduce the notion of a messaging domain, which includes N nodes that can exchange messages and is defined by a pair of buffers allocated in each node's memory, the send buffer and the receive buffer. The send buffer comprises N × S slots, as described in §3.3. <ref type="figure" target="#fig_8">Fig. 5a</ref> illustrates a send buffer with S=3 and different shades of gray distinguishing the send slots per participating node. Each send slot contains bookkeeping information for the local cores to keep track of their outstanding messages. It contains a valid bit, indicating whether the send slot is currently being used, a pointer to a buffer in local memory containing the message's payload, and a field indicating the size of the payload to be sent. A separate in-memory data structure maintains the head pointer for each of the N sets of send slots, which the cores use to atomically enqueue new send requests (not shown).</p><p>The receive buffer, shown in <ref type="figure" target="#fig_8">Fig. 5b</ref>, is the dual of the send buffer, where incoming send messages from remote nodes end up, and is sized similarly (N × S receive slots). Unlike send slots, receive slots are sized to accommodate message payloads. Each receive slot also contains a counter field, used to determine whether all of a message's packets have arrived. The counter field should provide enough bits to represent the number of cache blocks comprising the largest message; we overprovision by allocating a full cache block (64B), to avoid unaligned accesses for incoming payloads.</p><p>Overall, the messaging mechanism's memory footprint is 32 × N × S + (max_msд_size + 64) × N × S bytes. We expect that for current deployments, that number should not exceed a few tens of MBs. Systems adopting fully integrated solutions will likely be of contained scale (e.g., rack-scale systems), featuring a few hundred nodes, hence bounding the N parameter. In addition, most communication-intensive latency-sensitive applications send small messages, bounding max_msд_size. For instance, the vast majority of objects in object stores like Memcached are &lt;500B <ref type="bibr" target="#b3">[5]</ref>, while 90% of all packets sent within Facebook's datacenters are smaller than 1KB <ref type="bibr" target="#b48">[49]</ref>. Finally, given the low network latency fully integrated solutions like soNUMA deliver, the number of concurrent outstanding requests S required to sustain peak throughput per node pair would be modest (a few tens). Dynamic buffer management mechanisms to reduce memory footprint are possible, but beyond the scope of this paper.</p><p>Importantly, a fixed max_msд_size does not preclude the exchange of larger messages altogether. A rendezvous mechanism <ref type="bibr" target="#b50">[51]</ref> can be used, where the sending node's initial message specifies the location and size of the data, and the receiving node uses a one-sided read operation to directly pull the message's payload from the sending node's memory.</p><p>Send operation. Sending a message to a remote node involves the following steps. First, the core writes the message in a local core-private buffer <ref type="figure" target="#fig_8">(Fig. 5a, 1 )</ref>, updates the tail entry of the send buffer set corresponding to the target node (e.g., Node 1) 2 and enqueues a send operation in its private WQ 3 . The send operation specifies a messaging domain, the target node id, the remote receive buffer slot's address, a pointer to the local buffer containing the outgoing message, and the message's size. The target receive buffer slot's address can be trivially computed, as the number of nodes in the messaging domain, the number of send/receive slots per node, and the max_msд_size are all defined at the messaging domain's setup time. The NI polls on the WQ , parses the command, reads the message from the local memory buffer , and sends it to the destination node. At the destination, the NI writes each send packet directly in the local memory hierarchy, into the specified receive slot, and increments that receive slot's counter <ref type="figure" target="#fig_8">(Fig. 5b, 6</ref> ). When the counter matches the send operation's total packet count (contained in each packet's header), the NI writes a message arrival notification entry in a shared CQ 7 . The shared CQ is a memory-mapped and cacheable FIFO where the NI enqueues pointers to received send requests. When it is time for a dispatch decision, the NI selects a core and assigns the head entry of the shared CQ to it by writing the receive slot's index, contained in the shared CQ entry, into that core's corresponding CQ 8 . This is a crucial step that enables RPCValet's NI-driven dynamic load balancing, which we expand in §4.3. Finally, the core receives the new send request 9 polling the head of its private CQ, then directly reads the message from the receive buffer and processes it.</p><p>Replenish operation. A replenish operation always follows the receipt of a send operation as a form of end-toend flow control: a replenish notifies the send operation's source node that the request has been processed and hence its corresponding send buffer slot is free and can be reused. In <ref type="figure" target="#fig_8">Fig. 5b</ref>'s example, when core 3 is done processing the send request, it enqueues a replenish in its private WQ A . The replenish only contains the target node and the target send buffer slot's address, trivially deduced from the receive buffer index the corresponding send was retrieved from. The NI, which is polling at the head of core 3's WQ, reads the new replenish request B and sends the message to node 0. When the replenish message arrives at node 0, the NI invalidates the corresponding send buffer slot by resetting its valid field <ref type="figure" target="#fig_8">(Fig. 5a, C )</ref>, indicating its availability to be reused. In practice, a replenish operation is syntactic sugar for a special remote write operation, which resets the valid field of a send buffer slot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NI-driven Dynamic Load Balancing</head><p>With the NI's newly added ability to recognize and manage message arrivals, we now proceed to introduce NI-driven dynamic load balancing. Load-balancing policies implemented by the NIs can be sophisticated and can take various affinities and parameters into account (e.g., certain types of RPCs serviced by specific cores, or data-locality awareness). Implementations can range from simple hardwired logic to microcoded state machines. However, we opt to keep a simple proof-of-concept design, to illustrate the feasibility and effectiveness of load-balancing decisions at the NIs and demonstrate that we can achieve the load-balancing quality of a single-queue system without synchronization overheads. <ref type="figure" target="#fig_8">Fig. 5b</ref>'s step 8 is the crucial step that determines the balancing of incoming requests to cores. In RPCValet, the receiving node's NI keeps track of the number of outstanding send requests assigned to each core. Receiving a replenish operation from a core implies that the core is done processing a previously assigned send. Allowing only one outstanding request per core and dispatching a new request only after receiving a notification of the previous one's completion corresponds to true single-queue system behavior, but leaves a small execution bubble at the core. The bubble can be eliminated by setting the number of outstanding requests per core to two. We found that introducing a small multiqueue effect is offset by eliminating the bubble, resulting in marginal performance gains for ultra-fast RPCs with service times of a few 100s of nanoseconds.</p><p>A challenge that emerges from the distributed nature of a Manycore NI architecture is that the otherwise independent NI backends, each of which is handling send message arrivals from the network, need to coordinate to balance incoming load across cores. Our proposed solution is simple, yet effective: centralize the last step of message reception and dispatch. One of the NI backends-henceforth referred to as the NI dispatcher-is statically assigned to handle message dispatch to all the available cores. Network packet and data handling still benefit from the parallelism offered by the Manycore NI architecture, as all NI backends still independently handle incoming network packets and access memory directly. However, once an NI backend writes all packets comprising a message in their corresponding receive buffer slots, it creates a special message completion packet and forwards it to the NI dispatcher over the on-chip interconnect. Once the NI dispatcher receives the message completion packet, it enqueues the information in the shared CQ, from which it dispatches messages to cores in FIFO order as soon as it receives a replenish operation. As all the incoming messages are dispatched from a single queue to all available cores, RPCValet behaves like a true single-queue queuing system.</p><p>Having a single NI dispatcher eschews software synchronization, but raises scalability questions. However, for modern server processor core counts, the required dispatch throughput should be easily sustainable by a single centralized hardware unit, while the additional latency due to the indirection from any NI backend to the NI dispatcher is negligible. From the throughput perspective, even an RPC service time as low as 500ns corresponds to a new dispatch decision every ∼31/8ns for a 16/64-core chip, respectively. Both dispatch frequencies are modest enough for a single hardware dispatch component to handle, especially for our simple greedy dispatch implementation. The same observation also holds for more sophisticated dispatch policies if their hardware implementation can be pipelined. Latency-wise, the indirection from any NI backend to the NI dispatcher costs a couple of on-chip interconnect hops, adding just a few ns to the end-to-end message delivery latency. In case of exotic system deployments where the above assumptions do not hold, an intermediary design point is possible where each NI backend can dispatch to a limited subset of cores on the chip. As an example of this design point, we also implement and evaluate a 4 × 4 queuing system in §6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">soNUMA Extensions for RPCValet</head><p>We now briefly summarize the modifications to soNUMA's hardware to enable RPCValet, including the necessary protocol extensions for messaging and load balancing. Load balancing itself is transparent to the protocol and only affects a pipeline stage in the NI backends.</p><p>Additional hardware state. Most of the state required for messaging (i.e., send/receive buffers) is allocated in host memory. The only metadata kept in dedicated SRAM are the send and receive buffers' location and size, as they require constant fast access. On each node, the maintained state per registered soNUMA context includes a memory address range per node and a QP per local core. In total, we add 20B of stored state per context, including: the base virtual addresses for the send/receive buffers, the maximum message size (max_msд_size), the # of nodes (N ) in the messaging domain, and the # of messaging slots (S) per node.</p><p>Hardware logic extensions. soNUMA's NI features three distinct pipelines for handling Request Generations, Request Completions, and Remote Request Processing, respectively <ref type="bibr" target="#b42">[43]</ref>. We extend these pipelines to support the new messaging primitives and load-balancing functionality. Receiving a new send or replenish request is very similar to the reception of a remote write operation in the original soNUMA design. To support our native messaging design, we add a field containing the total message size to the network layer header; this is necessary so the NI hardware can identify when all of a message's packets have been received.</p><p>We add five new stages to the NI pipelines in total. A new stage in Request Generation differentiates between send and replenish operations, and operates on the messaging domain metadata. All other modifications are limited to the Remote Request Processing Pipeline, which is only replicated across NI backends. When a send is received, the pipeline performs a fetch-and-increment operation to the corresponding counter field of the target receive buffer slot ( §4.2, "Send operation"). The next stage checks if the counter's new value matches the message's length, carried in each packet header. If all of the send operation's packets have arrived, the next stage enqueues a pointer to the corresponding receive buffer slot in the shared CQ.</p><p>The final stage added to the Remote Request Processing pipeline, Dispatch, keeps track of the number of outstanding requests assigned to each core and determines when and to which core to dispatch send requests to from the shared CQ. A core is "available" when its number of outstanding requests is below the threshold defined; in our implementation, this number is two. Whenever there is an available core, the Dispatch stage dequeues the shared CQ's first entry and sends it to the target core's NI frontend, where the Request Completion pipeline writes it into the core's private CQ. The complexity of the Dispatch stage is very simple for our greedy algorithm, but varies based on the logic and algorithm involved in making load-balancing decisions. Finally, after completing the request, the core signals its availability by enqueuing a replenish operation in its WQ, which is propagated by the core's NI frontend to the NI backend that originally dispatched the request.</p><p>In summary, the additional hardware complexity is modest, thus compatible with architectures featuring ultra-lightweight protocols and on-chip integrated NIs, such as soNUMA. Given the on-chip NI's fast access to its local memory hierarchy, it is possible to virtualize most of the bulky state required for the messaging mechanism's send and receive buffers in the host's memory. The dedicated hardware requirements are limited to a small increase in SRAM capacity, while the NI logic extensions are contained and straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>We now detail our methodology for evaluating RPCValet's effectiveness in balancing load transparently in hardware.</p><p>System organization. We model a single tiled 16-core chip implementing soNUMA with a Manycore NI, as illustrated  <ref type="table">Table 1</ref>. Flexus simulation parameters.</p><p>in <ref type="figure" target="#fig_6">Fig. 4</ref>. The modeled chip is part of a 200-node cluster, with remote nodes emulated by a traffic generator which creates synthetic send requests following Poisson arrival rates, from randomly selected nodes of the cluster. The traffic generator also generates synthetic replies to the modeled chip's outgoing requests. We use Flexus <ref type="bibr" target="#b53">[54]</ref> cycle-accurate simulation with <ref type="table">Table 1</ref>'s parameters.</p><p>Microbenchmark. We use a multithreaded microbenchmark that emulates different service time distributions, where each thread executes the following actions in a loop: (i) spins on its CQ, until a new send request arrives; (ii) emulates the execution of an RPC by spending processing time X , where X follows a given distribution as detailed below; (iii) generates a synthetic RPC reply, which is sent back to the requester using a send operation with a 512B payload; and (iv) issues a replenish corresponding to the processed send request, marking the end of of the incoming RPC's processing. The overall service time for an emulated RPC (i.e., the total time a core is occupied) is the sum of steps (ii) to (iv).</p><p>RPC processing time distributions. To evaluate RPCValet on a range of RPC profiles, we utilize processing time distributions generated with three different methods. First, we develop an RPC processing time generator that samples values from a selected distribution. We experiment with four different distributions: fixed, uniform, exponential, and GEV. Fixed represents the ideal case, where all requests take the same processing time. GEV represents a more challenging case with infrequent long tails, which may arise from events like page faults or interrupts. Uniform and exponential distributions fall between fixed and GEV in terms of impact on load balancing, as established in <ref type="figure" target="#fig_2">Fig. 2</ref>. For our synthetic processing time distributions, we use 300ns as a base latency and add an extra 300ns on average, following one of the four distributions. The parameters we use for GEV are (location, scale, shape) = (363, 100, 0.65), which result in a mean of 600 cycles (i.e., 300ns at 2GHz) <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure">Fig. 6a</ref> illustrates the PDFs of the four resulting processing time distributions. Second, we run the HERD <ref type="bibr" target="#b25">[27]</ref> key-value store and collect the distribution of the RPCs' processing times. We use a dualsocket Xeon E5-2680 Haswell server and pin 12 threads on an equal number of a single socket's physical cores. The second socket's cores generate load. Our parameters for HERD are:  <ref type="figure">Figure 6</ref>. Modeled RPC processing time distributions.</p><p>95/5% read/write query mix, uniform key popularity, and a 4GB dataset (256MB per thread). <ref type="figure">Fig. 6b</ref> displays a histogram of HERD's RPC processing times after the request has exited the network, which have a mean of 330ns. Finally, we evaluate the Masstree data store <ref type="bibr" target="#b39">[40]</ref>, which stores key-value pairs in a trie-like structure and supports ordered scans in addition to put/get operations. Ordered scans are common in database/analytics applications and compete with latency-critical operations for CPU time when accessing the same data store. To collect RPC processing times, we use the same platform and dataset we used for HERD and load the server with 99% single-key gets, interleaved with 1% long-running scans which return 100 consecutive keys. The resulting distribution for gets is shown in <ref type="figure">Figure 6c</ref> and has an average of 1.25 µs. The runtime of scans is 60-120 µs (not shown in <ref type="figure">Fig. 6c</ref> due to the X-axis bounds).</p><p>Load-balancing implementations. We first compare the performance of two RPCValet variants, 1 × 16 and the less flexible 4 × 4. In 4 × 4, each NI backend is limited to balancing load across the four cores corresponding to its on-chip network row. We also consider a × 1 system, representing partitioned dataplanes where every incoming message is assigned to a core at arrival time without any rebalancing. 16×1 is the only currently existing NI-driven load distribution mechanism. Next, we compare the best-performing hardware load-balancing implementation, 1 × 16, to a software-based counterpart. In our software implementation, NIs enqueue incoming send requests into a single CQ from which all 16 threads pull requests in FIFO order. We use an MCS queuebased lock <ref type="bibr" target="#b40">[41]</ref> for the shared request queue.</p><p>We assume a 99th percentile Service Level Objective (SLO) of ⩽ 10× the mean service timeS we measure in each experiment and evaluate all configurations in terms of throughput under SLO. We measure each request's latency as the time from the reception of a send message until the thread that services the request posts a replenish operation.    <ref type="figure" target="#fig_12">Fig. 7a</ref> shows the performance of HERD with each of the three evaluated NI-driven load-balancing configurations. With a resultingS of ∼ 550ns, 1 × 16 delivers 29MRPS, 1.16× and 1.18× higher throughput than 4 × 4 and 16 × 1, respectively. 1 × 16 consistently delivers the best performance, thanks to its superior flexibility in dynamically balancing load across all 16 available cores. In comparison, × 4 offers limited flexibility, while 16 × 1 offers none at all. The flexibility to balance load from a single queue to multiple cores not only results in higher peak throughput under SLO, but also up to 4× lower tail latency before reaching saturation load. Conversely, lower tail means that the throughput gap between RPCValet and 1 × 16 would be larger for SLOs stricter than the assumed 10 ×S. Note that data points appearing slightly lower at mid load as compared to low load in <ref type="figure" target="#fig_12">Fig. 7a</ref> is a measurement artifact: for low arrival rates, the relatively small number of completed requests during our simulation's duration results in reduced tail calculation accuracy. <ref type="figure" target="#fig_12">Fig. 7b</ref> shows the tail latency of Masstree's get operations with each queuing configuration. We set the SLO for Masstree at 10× the service time of the get operations, equalling 12.5µs; we do not consider the scan operations latency critical. Due to interference from the scans, 16 × 1 cannot meet the SLO even for the lowest arrival rate of 2MRPS, while even 4 × 4 quickly violates the SLO at 3MRPS. 1 × 16 delivers 4.1MRPS at SLO, outperforming 4 × 4 by 37%. Under a more relaxed SLO of 75µs, RPCValet's 1 × 16 configuration delivers 54% higher throughput than 16 × 1 and 20% higher than 4×4. In the presence of long-running scans that occupy cores for many µs, RPCValet leverages occupancy feedback from the cores to eliminate excess queuing of latency-critical gets and improve throughput under SLO. <ref type="figure" target="#fig_12">Fig. 7c</ref> shows the results for two of our synthetic service time distributions, fixed and GEV. The results for uniform and exponential distributions fall between these two, are omitted for brevity, and are available in <ref type="bibr" target="#b10">[12]</ref>. The results follow the expectations set in §2.2. For the fixed distribution, × 16 delivers 1.13× and 1.2× higher throughput than 4 × 4 and 16 × 1 under SLO, respectively. For GEV, the throughput improvement grows to 1.17× and 1.4×, respectively. Similar to HERD results, in addition to throughput gains, RPCValet also delivers up to 4× lower tail latency before saturation. In all of <ref type="figure" target="#fig_12">Fig. 7</ref>'s experiments we set RPCValet's number of outstanding requests per core to two (see §4.3). Reducing this to one marginally degrades HERD's throughput, because of its short sub-µs service times, but has no measurable performance difference in the rest of our experiments.</p><p>In conclusion, RPCValet significantly improves system throughput under tight tail latency goals. Implementations that enable request dispatch to all available cores (i.e., × 16) deliver the best performance. However, even implementations with limited balancing flexibility, such as 4 × 4, are competitive. As realizing a true single-queue system incurs additional design complexity, such limited-flexibility alternatives introduce viable options for system designers willing to sacrifice some performance in favor of simplicity. <ref type="figure">Fig. 8</ref> compares the performance of RPCValet to a software implementation, both of which implement the same theoretically optimal queuing system (i.e., <ref type="bibr">1 × 16)</ref>. The difference between the two is how load is dispatched to a core. Software requires a synchronization primitive (in our case, an MCS lock) for cores to atomically pull incoming requests from the queue. In contrast, RPCValet does not incur any synchronization costs, as dispatch is driven by the NI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Hardware Versus Software Load Balancing</head><p>The software implementation is competitive with the hardware implementation at low load, but because of contention on the single lock, it saturates significantly faster. As a result, our hardware implementation delivers 2.3-2.7× higher throughput under SLO, depending on the request processing time distribution. A comparison between <ref type="figure" target="#fig_12">Fig. 7b and 8</ref> reveals that the 1×16 software implementation is not only inferior to the 1 × 16 hardware implementation, but to all of the evaluated hardware implementations. The fact that even the × 1 hardware implementation is superior to the software × 16 implementation indicates that the software synchronization costs outweigh the dispatch flexibility they provide, a direct consequence of the µs-scale RPCs we focus on. In addition, we corroborate the findings of prior work on dataplanes <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b46">47]</ref>-which effectively build a 16 × 1 system using RSS-showing that elimination of software synchronization from the critical path offsets the resulting load imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison to Queuing Model</head><p>Our results in §6.1 qualitatively meet the expectations set by the queuing analysis presented in §2.2. We now quantitatively compare the obtained results to the ones expected from purely theoretical models, to determine the performance gap between RPCValet and the theoretical 1 × 16 system. To make RPCValet measurements comparable to the theoretical queuing results, we devise the following methodology. We measure the mean service timeS on our implementation; a part D of this service time is synthetically generated to follow one of the distributions in §5, and the rest,S − D, is spent on the rest of the microbenchmark's code (e.g., event loop, executing send for the RPC response and replenish to free the RPC slot). We conservatively assume that this S − D part of the service time follows a fixed distribution. Using discrete-event simulation, we model and evaluate the performance of theoretical queuing systems with a service timeS, where D S of the service time follows a certain distribution (fixed, uniform, exponential, GEV) andS −D S of the service time is fixed. <ref type="figure" target="#fig_16">Fig. 9</ref> compares RPCValet to the theoretical 1 × 16. The graphs show the 99th percentile latency as a function of offered load, with four different distributions for the D part of the service time. RPCValet performs as close as 3% to 1 × 16, and within 15% in the worst case (GEV). We attribute the gap between the implementation and the model to contention that emerges under high load in the implemented systems, which is not captured by the model. Furthermore, assuming a fixed service time distribution for theS − D part of the service time is a conservative simplifying assumption: modeling variable latency for this component would have a detrimental effect on the performance predicted by the model, thus shrinking the gap between the model and the implementation. In conclusion, RPCValet leaves no significant room for improvement; neither centralizing dispatch nor maintaining private request queues per core introduces performance concerns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Other Techniques to Reduce Tail Latency. Prior work aiming to control the tail latency of Web services deployed at datacenter scale introduced techniques that duplicate/hedge requests across multiple servers hosting replicated data <ref type="bibr" target="#b13">[15]</ref>. The goal of such replication is to shrink the probability of an RPC experiencing a long-latency event and consequently affecting the response latency of its originating request. A natural side-effect of replication is the execution of more requests than strictly necessary, also necessitating extra serverside logic to reduce the load added by duplicated requests. As compared to ms-scale applications where the network RTT is a negligible latency contributor, applying the same technique for µs-scale applications requires a more aggressive duplication of requests, further increasing the generation of unnecessary server load. In contrast to such client-side techniques, RPCValet's server-side operation offers an alternative that does not increase the global server load.</p><p>RPCValet improves tail latency by minimizing the effect of queuing. Queuing is only one of many sources of tail latency, which lie in all layers of the server's software stack. Therefore, no single solution can wholly address the tail challenge; a synergy of many techniques is necessary, each targeting specific issues in particular layers (e.g., IX <ref type="bibr" target="#b5">[7]</ref> targets protocol and interrupt processing). However, despite the complex nature of the problem, managing on-server queuing is a universal approach that helps mitigate all sources of tail latency. Our work does not prevent straggler RPCs, but eliminates the chance that such stragglers will cascadingly impact the latency of other queued RPCs by providing a true single-queue system on each RPC-handling server. RPCValet is synergistic with techniques on both clients and servers to address specific sources of tail latency in the workflow of serving RPCs.</p><p>A range of prior work also leverages queuing insights to balance web requests within a datacenter, by mainly focusing on algorithmic aspects of load distribution among backend servers rather than a single server's cores. Examples of such algorithms are Join-Shortest-Queue <ref type="bibr" target="#b20">[22]</ref>, Power-of-d <ref type="bibr" target="#b7">[9]</ref>, and Join-Idle-Queue <ref type="bibr" target="#b38">[39]</ref>. Pegasus <ref type="bibr" target="#b33">[34]</ref> is a rack-scale solution where the ToR switch applies load-aware request scheduling by either estimating per-server load, or by leveraging load statistics reported directly by the servers. In the context of balancing µs-scale RPCs among a single server's cores, challenges such as dispatcher-to-core latency are of minor importance, because of the integrated NI's proximity. Our results show that single-queue behavior is feasible by deferring dispatch until a core is free, which is unattainable at cluster scale due to the latency of the off-chip network.</p><p>Load Distribution Frameworks. Most modern NICs distribute load between multiple hardware queues, which can be privately assigned to cores, through Receive Side Scaling (RSS) <ref type="bibr" target="#b41">[42]</ref> or Flow Director <ref type="bibr" target="#b22">[24]</ref>. Systems like IX <ref type="bibr" target="#b5">[7]</ref> and  MICA <ref type="bibr" target="#b34">[35]</ref> leverage these mechanisms to significantly boost their throughput under tail latency constraints. The disadvantage of RSS/Flow Director is that they blindly spread load across multiple receive queues based on specific network packet header fields, and are oblivious to load imbalance. ZygOS <ref type="bibr" target="#b46">[47]</ref> ameliorates the shortcomings of partitioned dataplanes, which suffer from increased tail latency under load imbalance. ZygOS introduces an intermediate shuffling layer where idle CPU threads can perform work stealing from other input queues. Due to the added synchronization overhead of work stealing, there is a measurable performance gap between ZygOS and the best single-queue system, inversely proportional to the RPC service times. RPCValet achieves the best of both worlds, offering single-queue performance without synchronization; instead of adding layers to rebalance load, we co-design hardware and software to implement a single-queue system. The Shinjuku operating system <ref type="bibr" target="#b23">[25]</ref> improves throughput under SLO by preempting long-running RPCs instead of running every RPC to completion. Their approach is particularly effective for workloads with extreme service time variability and CPUs with limited core count. Shinjuku preempts requests every 5-15µs, which is higher than the vast majority of our evaluated RPC runtimes. A system combining Shinjuku and RPCValet would rigorously handle RPCs of a broad runtime range, from hundreds of ns to hundreds of µs.</p><p>Programmable Network Interfaces. Offloading compute to programmable network processors is an old idea that has seen rekindled interest; FLASH <ref type="bibr" target="#b30">[32]</ref> and Typhoon <ref type="bibr" target="#b47">[48]</ref> integrated general-purpose processors with the NI, enabling custom handler execution upon message reception. NI-controlled message dispatch to cores has been proposed in the context of parallel protocol handler execution for DSMs to eschew software synchronization overheads <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b45">46]</ref>. Programming abstractions such as PDQ <ref type="bibr" target="#b17">[19]</ref> could be deployed as loadbalancing decisions in RPCValet's NI dispatch pipeline.</p><p>Today's commercial "SmartNICs" target protocol processing or high-level application acceleration, with the goal of reducing CPU load; they integrate either CPU cores (e.g., Mellanox's BlueField <ref type="bibr" target="#b36">[37]</ref>), or FPGAs (e.g., Microsoft's Catapult <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b32">33]</ref>). FlexNIC <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b29">31]</ref> draws inspiration from SDN switches <ref type="bibr" target="#b6">[8]</ref>, deploying a match-action pipeline for line-rate header processing. The programmable logic in these Smart-NICs could be leveraged to implement non-static load balancing, adding flexibility to RSS or Flow Director. However, our dynamic load balancing scheme relies on ns-scale interaction between the NI and CPU logic, which is only attainable through tight NI integration and CPU-NI co-design.</p><p>RPC Layers on InfiniBand NICs. Latency-critical software systems for key-value storage <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b26">28]</ref>, distributed transaction processing <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b26">28]</ref>, distributed durable data storage <ref type="bibr" target="#b43">[44]</ref>, and generalized datacenter RPCs <ref type="bibr" target="#b51">[52]</ref>, have already begun using RDMA NICs due to their low latency and high IOPS. All of these systems are fine-tuned to maximize RPC throughput given the underlying limitations of their discrete NICs and the IB verbs specification. We distinguish RPCValet from these software-only systems by our focus on balancing the load of incoming RPCs across the CPU cores. Furthermore, all of the above proposals are adversely affected by the shortcomings of PCIe-attached NICs, and use specific optimizations to ameliorate their inherent latency bottlenecks; this strengthens our insight that NI integration is the key enabler for handling RPCs in true single-queue fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduced RPCValet, an NI-driven dynamic load-balancing mechanism for µs-scale RPCs. RPCValet behaves like a singlequeue system, without incurring the synchronization overheads typically associated with single-queue implementations. RPCValet performs within 3-15% of the ideal singlequeue system and significantly outperforms current RPC load-balancing approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Model 1 × 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Model 16 × 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Tail latency as a function of throughput for different queuing systems and service time distributions. Y-axis values are shown as multiples of the mean service timeS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Emulated messaging versus RPCValet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>w o r k R o u t e r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Manycore NI architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Receiver (destination).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Messaging illustration. Node 0 (sender) sends a message to node 1 (receiver).NI, communicate with special packets over the chip's interconnect. Our RPCValet implementation relies on such a Manycore NI architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5</head><label>5</label><figDesc>shows the delivery of a message from Node 0 to Node 1 in steps. Completing the message delivery requires the execution of a send operation on Node 0 and a replenish operation on Node 1.Fig. 5only shows NI backends; NI frontends are collocated with every core. We start with the required buffer provisioning associated with messaging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 .</head><label>7</label><figDesc>Load balancing with three different hardware queuing implementations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Evaluation 6 . 1</head><label>61</label><figDesc>Load Balancing: Hardware Queuing Systems</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 . 1 ×</head><label>81</label><figDesc><ref type="bibr" target="#b14">16</ref> load balancing: hardware vs. software.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>1 ×</head><label>1</label><figDesc> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 9 .</head><label>9</label><figDesc>RPCValet comparison to theoretical 1 × 16 queuing model. Y-axis values shown as multiples of the avg service timeS.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Edouard Bugnion, James Larus, Dmitrii Ustiugov, Virendra Marathe, Dionisios Pnevmatikatos, Mario Drumond, Arash Pourhabibi, Marios Kogias and the anonymous reviewers for their precious feedback. This work was partially funded by Huawei Technologies, the Nano-Tera YINS project, the Oracle Labs Accelarating Distributed Systems with Advanced One-Sided Operations grant, and the SNSF's Memory-Centric Server Architecture for Datacenters project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generalized Extreme Value distribution</title>
		<ptr target="https://www.wolframalpha.com/input/?i=MaxStableDistribution%5B363" />
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="0" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memcached</surname></persName>
		</author>
		<ptr target="http://memcached.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">Krste Asanović. A Hardware Building Block for 2020 Warehouse-Scale Computers. USENIX FAST Keynote</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Workload analysis of a large-scale key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Paleczny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 2012 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attack of the killer microseconds. Commun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Luiz André Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parthasarathy</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranganathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM</publisher>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">IX: A Protected Dataplane Operating System for High Throughput and Low Latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Klimovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Symposium on Operating System Design and Implementation (OSDI)</title>
		<meeting>the 11th Symposium on Operating System Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">P4: programming protocol-independent packet processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Bosshart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Gibb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Izzard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Rexford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cole</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Talayco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randomized load balancing with general service time distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maury</forename><surname>Bramson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 2010 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="275" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TAO: Facebook&apos;s Distributed Data Store for the Social Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Bronson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Amsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Ferris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Giardullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Marchukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lovro</forename><surname>Puzar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Jiun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkateshwaran</forename><surname>Venkataramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2013 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A cloud-scale acceleration architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalin</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sitaram</forename><surname>Lanka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<meeting>the 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Network-Compute Co-Design for Distributed In-Memory Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Daglis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
	<note type="report_type">EPFL PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Manycore network interfaces for in-memory rackscale computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 42nd International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SABRes: Atomic object reads for in-memory rack-scale computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Ustiugov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<meeting>the 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The tail at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz André</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamo: amazon&apos;s highly available key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madan</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunavardhan</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pilchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminathan</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vosshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 21st ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FaRM: Fast Remote Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Dragojevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyanth</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orion</forename><surname>Hodson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 11th Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="401" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Virtual Interface Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">J</forename><surname>Regnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><forename type="middle">L</forename><surname>Mcalpine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Shubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">Marie</forename><surname>Merritt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Gronke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dodd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="66" to="76" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel Dispatch Queue: A Queue-Based Programming Abstraction to Parallelize Fine-Grain Communication Protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th IEEE Symposium on High-Performance Computer Architecture (HPCA)</title>
		<meeting>the 5th IEEE Symposium on High-Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Network Requirements for Resource Disaggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Peter Xiang Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangjin</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachit</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Symposium on Operating System Design and Implementation (OSDI)</title>
		<meeting>the 12th Symposium on Operating System Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="249" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RDMA over Commodity Ethernet at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitu</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Lipshteyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2016 Conference</title>
		<meeting>the ACM SIGCOMM 2016 Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="202" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analysis of join-the-shortest-queue routing for web server farms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Sigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ward</forename><surname>Whitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perform. Eval</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1062" to="1081" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SVE: Distributed Video Processing at Facebook Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petchean</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Nykiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iaroslav</forename><surname>Tverdokhlib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Yajurvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Dapolito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Bykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wyatt</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 26th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Intel</forename><surname>Corp</surname></persName>
		</author>
		<ptr target="http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/intel-ethernet-flow-director.pdf" />
		<title level="m">Introduction to Intel Ethernet Flow Director and Memcached Performance</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shinjuku: Preemptive Scheduling for µsecond-scale Tail Latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostis</forename><surname>Kaffes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">Tigar</forename><surname>Humphries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mazieres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Service fabric: a distributed platform for building microservices in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopal</forename><surname>Kakivaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hasha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shegufta</forename><surname>Bakht Ahsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Pfleiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Tarta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fussell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipul</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mansoor</forename><surname>Mohsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chacko</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Mastrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aprameya</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnav</forename><surname>Kidambi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EuroSys Conference</title>
		<editor>Meng Lin, Jeffrey Chen, Abhay Balkrishna Mhatre, Preetha Subbarayalu, Mert Coskun, and Indranil Gupta</editor>
		<meeting>the 2018 EuroSys Conference<address><addrLine>Randy Wang, Abhishek Ram, Sumukh Shivaprakash, Rajeet Nair, Alan Warwick, Bharat S. Narasimman</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>Anmol Ahuja, Oana Platon</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using RDMA efficiently for key-value services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2014 Conference</title>
		<meeting>the ACM SIGCOMM 2014 Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="295" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">FaSST: Fast, Scalable and Simple Distributed Transactions with Two-Sided (RDMA) Datagram RPCs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Symposium on Operating System Design and Implementation (OSDI)</title>
		<meeting>the 12th Symposium on Operating System Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="185" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svilen</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><forename type="middle">M</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tipp</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu-Yeon</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 42nd International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="158" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FlexNIC: Rethinking Network DMA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 15th Workshop on Hot Topics in Operating Systems (HotOS-XV)</title>
		<meeting>The 15th Workshop on Hot Topics in Operating Systems (HotOS-XV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High Performance Packet Processing with FlexNIC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXI)</title>
		<meeting>the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XXI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
	<note>Anderson, and Arvind Krishnamurthy</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Kuskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ofelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Heinlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Simoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kourosh</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Chapin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nakahira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mendel</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">The</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stanford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Multiprocessor</surname></persName>
		</author>
		<title level="m">Proceedings of the 21st International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 21st International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="302" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanwei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 26th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="137" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pegasus: Load-Aware Selective Replication with an In-Network Coherence Directory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UW CSE Technical Report</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MICA: A Holistic Approach to Fast In-Memory Key-Value Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 11th Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="429" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Linley Group. Epyc Relaunches AMD Into Servers. Microprocessor Report</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Linley Group. Mellanox Accelerates BlueField SoC. Microprocessor Report</title>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">X-Gene 3 Up and Running</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linley</forename><surname>Group</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Join-Idle-Queue: A novel load balancing algorithm for dynamically scalable web services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaomin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Geller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">G</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perform. Eval</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1056" to="1071" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cache craftiness for fast multicore key-value storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert Tappan</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 EuroSys Conference</title>
		<meeting>the 2012 EuroSys Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="65" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Receive Side Scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Corp</surname></persName>
		</author>
		<ptr target="http://msdn.microsoft.com/library/windows/hardware/ff556942.aspx" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scale-out NUMA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XIX)</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-XIX)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankita</forename><surname>Kejriwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Montazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mendel</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Stutsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<idno>7:1-7:55</idno>
	</analytic>
	<monogr>
		<title level="j">The RAMCloud Storage System. ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Arrakis: The Operating System Is the Control Plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Woos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Roscoe</surname></persName>
		</author>
		<idno>11:1-11:30</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Address Partitioning in DSM Clusters with Parallel Coherence Controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilanthiraiyan</forename><surname>Pragaspathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Parallel Architecture and Compilation Techniques (PACT)</title>
		<meeting>the 9th International Conference on Parallel Architecture and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ZygOS: Achieving Low Tail Latency for Microsecond-scale Networked Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Prekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Kogias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Bugnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 26th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Tempest and Typhoon: User-Level Shared Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Larus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 21st International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="325" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Inside the Social Network&apos;s (Datacenter) Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmeet</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2015 Conference</title>
		<meeting>the ACM SIGCOMM 2015 Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="123" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">It&apos;s Time for Low Latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mendel</forename><surname>Stutsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 13th Workshop on Hot Topics in Operating Systems (HotOS-XIII)</title>
		<meeting>The 13th Workshop on Hot Topics in Operating Systems (HotOS-XIII)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">RDMA read based rendezvous protocol for MPI over InfiniBand: design alternatives and benefits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayantan</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun-Wook</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhabaleswar</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</title>
		<meeting>the 11th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">LITE Kernel RDMA Support for Datacenter Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeh</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 26th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="306" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Speedy transactions in multicore in-memory databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Liskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 24th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SimFlex: Statistical Sampling of Computer System Simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><forename type="middle">E</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastassia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">C</forename><surname>Hoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
