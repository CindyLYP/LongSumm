{"1000": "tldr; the authors present a novel approach for inducing unsupervised part of speech taggers for languages that have no labeled training data, but have translated text in a resourcerich language. their method does not assume any knowledge about the target language across eight european languages, their approach results in an average absolute improvement of over a stateoftheart baseline, and over vanilla hidden markov models induced with the expectation maximization algorithm. the authors evaluate their approach on eight european languages, and considerably bridges the gap to fully supervised pos tagging performance (96.6). tldr; the authors propose an unsupervised pos tagging system that uses a graph over trigram types. the vertices of the graph correspond to word types. the vertices of the graph are extracted from the different sides of a parallel corpus (de, df) and an additional unlabeled monolingual foreign corpus f, which will be used for training. we use two different similarity functions to define the edge weights among the foreign vertices and between vertices from different languages. the similarity between two trigram types is given by summing over the pm tldr; the authors use a graph with vertices connected to other vertices in the graph and vertices connected to other vertices in the graph. the authors use a supervised english tagger to label the english side of the bitext.7 to initialize the graph for label propagation we use a supervised english tagger to label the english side of the bitext.7 to initialize the graph for label propagation we use a supervised english tagger to label the english side of the bitext.7 to initialize the graph for label propagation we use a supervised english tagger to label the english side of the bitext.7 tldr; the authors propose a new unsupervised tagger based on the featurebased hmm of bergkirkpatrick et al. the authors use a squared loss to penalize neighboring vertices that have different label distributions, and additionally regularize the label distributions towards the uniform distribution u over all possible labels y the objective is convex in q. the first term in the objective function is the graph smoothness regularizer which encourages the distributions of similar vertices (large wij) to be similar. the second term is a regularizer and encourages all type marginals to be uniform to the extent that is allowed by the first two terms (cf. maximum entropy what they suggest a graphbased approach to pos tagging. they use the language specific pos tagset of petrov et al. they train their model on parallel data from the europarl corpus and the ods united nations dataset (un, 2006). they use the universal pos tagset of petrov et al. they train their model on parallel data from the europarl corpus and the tldr; the authors propose to perform label propagation (lp) in order to extract the constraint features for all foreign word types. their model outperforms the hitherto stateofthe art unsupervised hmm baseline, and performs better than no lp baseline, when we macroaverage the accuracy over all languages. their full model outperforms the no lp setting because it has better vocabulary coverage and allows the extraction of a larger set of constraint features. for comparison, the completely unsupervised feature-hmm baseline accuracy on the universal pos tags for english is 79.4, and goes up to with a treebank dictionary. the paper proposes a graph based approach for predicting part of speech (pse) labels across languages. graph based label propagation (gbp) is a graph based approach for predicting part of speech (pse) labels across languages. because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.", "1001": "this paper presents a novel approach for fv representation of sequences using a recurrent neural network (rnn). the rnn is trained to predict the next element of a sequence given the previous elements. it is applied to two different and challenging tasks: video action recognition and image annotation by sentences. state of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. a surprising boost in performance in the video action recognition task was achieved by using a transfer learning approach from the image annotation task. specifically, the vgg image embedding of a frame is projected using a linear transformation which was the paper presents a transfer learning approach for image annotation and image search tasks. specifically, the vgg image embedding of a frame is projected using a linear transformation which is learned on matching images and sentences by the canonical correlation analysis (cca) algorithm the proposed rnn-fv method achieves state of the art results in action recognition on the hmdb51 and ucf101 datasets. in image annotation and image search tasks, the rnn-fv method is used for the representation of sentences and achieves state of the art results on the flickr8k dataset tldr; the authors propose rnn fvs for image annotation and classification. the rnn is trained to predict the next element in a sequence, given the previous elements. given a sequence of vectors with n vector elements x1, ..., xn, we convert it to the input sequence x (x0, x1, ..., xn) where x0 xstart. this special element is trained to predict, at each time step i, the next element xi1 of the sequence, given the previous elements. given a sequence of vectors with n vector elements x1, ..., xn, we convert it tldr; the authors train a rnn to predict the next element in a sequence, given the previous elements of the sequence. the output layer of the network is a softmax layer with m units. given a sequence, the gradient of the rnn loss may serve as the sequence representation, even if the loss is not interpretable as a likelihood. the classification application is applicable for predicting a sequence of symbols w1, w2, ..., wn that have matching vector representations tldr; the authors propose a new generative model that can be used to predict the sequence of images in a video. the proposed model is a convolutional neural network (cnn) with three layers: a fullyconnected layer with leaky activation, a fullyconnected layer with leaky activation, and a 500d linear fullyconnected layer. the model is trained for regression with the mean square error (mse) loss function. weight decay and dropouts are also applied. the rnn is capable of encoding the sequence properties, and as underlying features, we rely on video encodings that are based on single frames or on fixed length blocks tldr; the authors train the network to predict the next word in the input sequence, given the previous words in the input sequence. they train the network to predict the next word in the input sequence, given the previous words in the input sequence. this paper introduces a new pooling method for rnns. the idea is to use the convolutional layers of the rnn to generate a vector representation of the image, which can then be used as the input to a regularized cca (regularized cca) algorithm to generate a vector representation of the sentence, which can then be used as the input to a multiclass svm (multiclass linear svm) to generate a vector representation of the image, which can then be used as the input to a regularized cca algorithm to generate a vector representation of the sentence, which can then be used as the input to a tldr; the authors propose a neural network architecture for bidirectional image and sentence retrieval. in particular, they propose a neural network architecture for bidirectional image and sentence retrieval. the network architecture is based on the following architecture: the network is trained on the input image and the output sentence. the network is trained on the input image and the output sentence. this paper introduces a novel fv representation for sequences that is derived from rnns. the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs. when used for representing sentences, the rnnfv representation achieves state of the art or competitive results on image annotation and image search tasks. since the length of the sentences in these tasks is usually short and, therefore, we believe that using the rnnfv for tasks that use longer text will provide an even larger gap between the conventional fv and the rnnfv", "1002": "the rate of publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research. we hypothesize that such talks constitute a coherent and concise description of the papers, content, and can form the basis for good summaries. we collected papers and their corresponding videos, and created a dataset of paper summaries. a model trained on this dataset achieves similar performance as models trained on a dataset of summaries created manually. in addition, we validated the quality of our summaries by human experts. in this work, we focused on nlp and ml conferences, and analyzed video talks from acl, naacl, emnlp, sigdial, and icml ( format). we downloaded the 4www.cleo conference.org igem.org/videos/videos videos and extracted the speech data. then, via a publicly available asr service6, we extracted transcripts of the speech, and based on the video metadata (e.g., title), we retrieved the correspond- ing paper (in pdf format). scienceparse7 to extract the text of the paper, and applied a simple given the paper, the speaker generates words for describing ver bally sentences from the paper, one word at each time step. thus, at each time step, the speaker has a single sentence from the paper in mind, and produces a word that constitutes a part of its ver- bal description. then, at the next timestep, the speaker either stays with the same sentence, or moves on to describing another sentence, and so on. thus, given the transcript, we aim to retrieve those source sentences and use them as the sum mary. the number of words uttered to describe each sentence can serve as importance score, in- dic tldr; the authors propose a method for generating a summary of a paper, based on the output of a speaker. the model is trained on a dataset of scisumms, and the authors evaluate the model on a human generated dataset of scisumms summaries. the authors evaluate the model on a dataset of scisumms summaries, and the authors evaluate the model on a human generated dataset of scisumms summaries.", "1003": "emotion detection from text has become a popular task due to the key role of emotions in human-machine interaction. currently, two types of datasets labeled with emotions are publicly available: manually labeled, and pseudolabeled. as a result, the quality of these datasets is usually high. however, the task is tedious, time consuming, and expensive 4, and thus, these datasets are usually small (in the order of thousands of annotated samples). manual annotations are usually applied to domain speci c datasets (e.g., news headlines). to overcome these limitations, pseudolabeled datasets are gathered from social media platforms where social media posts are explicitly tagged tldr; the authors propose an ensemble approach that combines both a linear model based on bow, and a nonlinear model based on the pretrained word vectors. in order to utilize the high quality but small size datasets, we propose an ensemble approach that combines both a linear model based on bow, and a nonlinear model based on the pretrained word vectors. in addition, we propose a new method for realizing a sentence level representation from the single words vectors. ve emotion detection is the task of identifying emotions in sentences that have high annotation agreement between the annotators and the emotion label. in this paper, the authors propose an ensemble based approach for ve emotion detection, where each emotion is represented as a di erent document. the authors evaluate the proposed ensemble approach on two datasets for emotion detection: isear and semeval. the results show that the proposed ensemble approach achieves stateoftheart performance on both datasets. tldr; the authors propose an ensemble of bow and embedded document representations for emotion detection in text. the ensemble is composed of bow and embedded document representations. the authors evaluate the proposed ensemble on a set of emotion detection datasets, and show that the proposed ensemble outperforms the baseline for each dataset. the best result, which is also signi - cantly better for each dataset, is of enclass model that achieved an average relative improvement of in f1-score over all datasets. these results indicate the advantage in combining both bow and embedded document representations for emotion detection from text.", "1004": "in this paper, we analyze customer support dialogues using the twitter platform and show the utility of such analyses. in such a context, providing automatic detection and analysis of the emotions expressed by customers is important, as is identification of the emotional techniques (e.g., apology, empathy, etc.) in the responses of customer service agents. result of such an analysis can help assess the quality of such a service, help and inform agents about desirable responses, and help develop automated service agents for social media interactions. a recent study shows that one in five ( 23) customers in the u.s. say they have used social media for customer tldr; the paper presents a method to automatically detect emotions in social media dialogues and predict the emotional technique that is likely to be used by a human customer service agent in a given situation. this analysis reflects our ultimate goal: to enable a computer system to discern the emotions expressed by human customers, and to develop computerized tools that mimic the emotional technique used by a human customer service agent in a particular situation. for example, in each situation, should they apologize, should they thank the customer, etc.) another interesting trend in customer service, in addition to the use of social media, is the automation of various functions of customer interaction. in the present work, we define a dialogue to be a sequence of turns between a specific customer and an agent, where the customer initiates the first turn. consecutive posts of the same party ( customer or agent) uninterrupted by the other party, are considered as a single turn (even if there are several tweets). given the nature of customer support services, we assume the last turn in the dialogue is an agent turn. thus, we expect an even number of turns in the dialogue. we filtered out dialogues in which more than one customer or one agent are involved. given this method we traced back previous turns and reconstructed entire dialogues. tldr; the authors propose a model to classify emotions in a written dialogue between a customer and a support agent. the paper proposes to use the context of the dialogue to extract informative features that we refer to as dialogue features. this difference stems from the fact that in order to train an automated service agent to respond based on customer input, the agent s emotional technique needs to be computed before the agent generates its response sentence. content is available at classification time (as well as the history of the dialogue) meaning, the customer has already provided her input and the system must now understand what is the emotion being expressed. in addition, a tldr; the authors train a recurrent neural network (rnn) to classify agent turns and customer turns. the agent turn is represented as a feature vector, and the customer turn is represented as a feature vector. the authors train a recurrent neural network (rnn) to classify agent turns and customer turns. tldr; the paper presents a novel approach for emotion detection in dialogue. the paper proposes to use an svm to predict the emotion of a dialogue turn, and a hmm to predict the emotion of a sequence of dialogue turns. the model is trained on a dataset of turk dialogues, where it is compared to an svm that uses only the textual features extracted from the preceding customer turn. the model is evaluated on a dataset of turk dialogues, where it is compared to an svm that uses only the textual features extracted from the preceding customer turn. the model is evaluated on a dataset of turk dialogues, where it tldr; the authors propose svm dialogue and svm hmm models to predict the emotional technique used by a customer service agent. in particular, the svm model predicts the emotional technique of the agent, while the svm hmm model predicts the emotional technique of the customer. in experiments, svm dialogue and svm hmm models outperform baseline models in terms of macro and micro average f1-scores for both tasks. svm hmm models outperform svm models in terms of macro and micro average f1-scores for both tasks.", "1005": "tldr; the authors propose to use differentiable neural architecture search (d-nas) to optimize the architecture for fewshot classification (fsc) without overfitting. this is due to incorporating the architecture as an additional set of neural network parameters to be optimized, and solving this optimization using sgd. it is apparent that larger architectures increase fsc performance, up to a certain size, where performance seems to saturate or even degrade. it seems the overall performance of the fsc techniques can not continue to grow by simply expanding the backbone size. it seems the overall performance of the fsc techniques can not continue to grow by tldr; the authors propose a neural network architecture search (nas) framework for fewshot classification (fsc) tasks. the proposed architecture search framework consists of two parts: architecture search and metaadaptation. the architecture search part of the framework consists of two parts: architecture search and metaadaptation. the metaadaptation part of the framework consists of two parts: architecture search and metaadaptation. tldr; the authors train a cnn on a fewshot task. the cnn is trained endtoend with an unrolled convex optimization solution of an optimal classifier, such as svm. in metaoptnet a cnn backbone is trained endtoend with an unrolled convex optimization solution of an optimal classifier, such as svm. in bf3s auxiliary selfsupervision tasks are added, such as predicting image rotation or patch location. in robustdist first an ensemble of up to models is learned. in metaoptnet a cnn backbone is trained endtoend with an unrolled convex optimization solution of an tldr; the authors propose a method to adapt the architecture of a neural network to the task at hand. the architecture of a neural network is defined as a directed acyclic graph (dag). the architecture of the adaptable block is built from feature maps vxi that are linked by mixtures of operations. the input feature map to the block is x0 and its output is xv a mixed operation, o(i, j) is defined as o(i, j) o(i, j) oo exp((i, j) oo exp((i, j) oo exp((i, j what they suggest a new architecture for a neural network to adapt to a given task. they suggest a new architecture for a neural network to adapt to a given task. tldr; the authors propose a method to learn an adaptive architecture for fewshot classification tasks. the key idea is to learn an adaptive architecture for a given training set, and then transfer the learned architecture to a larger training set. the authors evaluate their method on the miniimagenet and fc100 fewshot benchmarks and show that it outperforms stateofthe art methods. the authors also show that it is possible to transfer the learned architecture from fc100 to miniimagenet. tldr; the authors propose a bilevel optimization method, called metadaptation, to improve the performance of fc100 and resnet. the authors test the method on cifar100, fc100, and resnet12 and show that it improves the performance of fc100 and resnet12 by up to and sometimes more. the authors also test the method on fewshot snas and show that it improves the performance of snas and resnet by up to and sometimes more. tldr; the authors present metadapt, a fewshot learning approach that enables metalearning network architecture that is adaptive to novel fewshot tasks. in this paper, the authors propose metadapt, a fewshot learning approach that enables metalearning network architecture that is adaptive to novel fewshot tasks.", "1006": "in this paper, we outline an approach to detecting such egregious conversations, using behavioral cues from the user, patterns in agent responses, and useragent interaction. using logs of two commercial systems, we show that using these features improves the detection f1-score by around over using textual features alone. in addition, we show that those features are common across two quite different domains and , arguably, universal. as an aid to the improvement, analysis of egregious conversations can often point to problems in training data or system logic that can be repaired. if detected in real time, a human agent can be pulled in to salvage the conversation. the goal of this paper is to study conversational features that lead to egregious conversations. specifically, we consider customer inputs throughout a whole conversation, and detect cues such as rephrasing, the presence of heightened emotions, and queries about whether the is a human or requests to speak to an actual human. in addition, we analyze the responses, looking for repetitions (e.g. from loops that might be due to flow problems, and the presence of not trained responses. finally, we analyze the larger conversational context exploring, for example, where the presence of a not trained response might be especially problematic. tldr; the authors propose a method to detect egregious conversations between a human and a virtual agent. to do this, they extract features from both the customer and the agent responses, together with features related to the combination of specific inputs and responses. in addition, some of these features are contextual, meaning that they are dependent on where in the conversation they appear. using this set of features for detecting egre- gious conversations is novel, and as our experimental results show, improves performance compared to a model based solely on features extracted from the conversation text. tldr; the authors extracted data from two commercial systems, each embedded in a larger system with its own logic. company a s system deals with sales support during an online purchase. company b s system deals with technical support for purchased software products. another similarity feature is between two customer s subsequent turns when the agent was not trained with a linear kernel. a feature with a linear kernel is generated using the scores calculated for the described features, where each feature value is a number between 0,1. egr classification model outputs a label or nonegregious as a prediction for the conversation. the paper proposes an egr model to classify conversations between a virtual agent and a customer. in particular, the model is trained on a dataset of conversations between a virtual agent and a customer. the model is trained on a dataset of conversations between a virtual agent and a customer. the goal of this work is to give developers of automated agents tools to solve problems cre- ated by exceptionally bad conversations. those conversations were particularly prevalent with the agent s difficulty to identify correctly the user s intent due to nlu errors or lg limitation. we did not encounter any unsupported intent leading to customer rephrasing, which affected the ability of the model to classify those conversations. in addition, the customer intents that appeared in those conversations were very diverse. while customer rephrasing was captured by the egr model, for the textbased model some of the intents were new ( did not appear in the", "1007": "the paper presents an analysis into the inner workings of convolutional neural networks (cnns) for processing text. we aim to understand the method by which the networks process and classify text. the ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model the paper proposes a method to improve interpretability and prediction interpretability of cnns by focusing on informative ngrams and taking into account also the negative cues. the paper proposes a method to improve interpretability and prediction interpretability of cnns by focusing on informative ngrams and taking into account also the negative cues. the paper proposes a method for separating informative and uninformative ngrams in a text classification task. to this end, the paper proposes to use a threshold for each filter to separate informative and uninformative ngrams. the paper also proposes a method for calculating the threshold for each filter, based on a shared purity value. to assess the quality of threshold obtained by the proposed method, the authors discard values that do not pass the threshold for each filter and observe the performance of the model. in essence, the more filters we utilize in the network, the less correlation there is between each filter s s and the final classification, as the decision is being made by a greater consensus. this means that demanding a higher purity will be accompanied by lower coverage, relative to other experiments, and more ngrams will be discarded. we challenge this view and show that filters often specialize in multiple distinctly different semantic classes by utilizing activation patterns which are not necessarily maximized. we also show that filters may not only identify good ngrams, but may also actively supress bad ones. the paper investigates the relationship between the activations of ngrams and their corresponding word embeddings. specifically, the authors show that the activations of ngrams can be decomposed into two parts: the activations of the word embeddings, and the activations of the ngram embeddings. the first part is the activations of the ngram embeddings. the second part is the activations of the ngram embeddings, and the activations of the ngram embeddings themselves. the second part is the activations of the ngram embeddings themselves. the paper presents two theories to explain the discrepancy between the activations of naturally occurring and possible ngrams. our second theory to explain the discrepancy between the activations of naturally occurring and possible ngrams is that certain filter slots are not used to detect a class of highly activating words, but rather to rule out a class of highly negative words. we refer to these as negative ngrams. in order to identify case negative ngrams, we heuristically test whether the changed words  scores directly influence the status of the activation relative to the threshold: given an already identified negative ngram, if the ngram scoresans the bottom-k negative slot cnns have been shown to perform well on text classification tasks, but their interpretability is still an open question. in this paper, the authors study the interpretability of cnns for text. the authors show that cnns perform well on text classification tasks, but their interpretability is still an open question.", "1008": "the paper presents editnet, an automatic editing process that generates a summary of extracted sentences. the key idea of editnet is to create an automatic editing process to enhance summary quality. for each sentence in s, the editor may make three possible decisions. the first decision is to keep the extracted sentence untouched ( represented by label e). the second alternative is to rephrase the sentence ( represented by label a). such a decision, for example, may represent the editor s wish to simplify or compress the original source sentence. the last possible decision is to completely reject the sentence ( represented by label r). for each sentence si s (in order) the editor makes one of the three possible decisions: extract, abstract or reject si. therefore, the editor may modify summary s by paraphrasing or rejecting some of its sentences, resulting in a mixed extractive-abstractive summary. in each step i 1, 2, , l, in order to make an educated decision, the editor considers both sentence representations esi and asi as its input, together with two additional auxiliary representations. the first auxiliary representation is that of the whole document d itself. such a representation provides a global context for decision making. in each step i, therefore, tldr; the authors propose a novel softattention mechanism that can be used to generate an abstractive representation of a given document. the authors evaluate their approach on the cnn/dailymail dataset, and show that it is competitive with stateoftheart softattention in terms of performance and accuracy. the authors also test their approach on the sgdm dataset, and show that it is competitive with stateoftheart softattention in terms of performance and accuracy. the authors also evaluate their approach on the sgdm dataset, and show that it is competitive with softattention in terms of performance and accuracy. tldr; the authors propose editnet, a summarization approach that mixes both extraction and abstraction. the authors evaluate editnet against several stateofthe art summarization models, including neusum, dca and bertsum. editnet: a summarization approach that mixes both extraction and abstraction. the authors evaluate editnet against several stateofthe art summarization models, including neusum", "1009": "dimsim is a chinese phonetic similarity algorithm that can be used for nlp tasks such as named entity identification, text normalization and spelling correction. dimsim is a supervised learning approach to learn ndimensional encodings for finals and initials where n can be easily extended from one to two or higher dimensions. the learning model derives accurate encodings by jointly considering pinyin linguistic characteristics, such as place of articulation and pronunciation methods, as well as high quality annotated training data sets. dimsim outperforms double metaphone (dm), minimum edit distance ( med) and aline demonstrating that dimsim outperforms these algorithms by 7.5x on mean reciprocal rank, 1.4x on precision and 1.5x on recall on a realworld dataset. tldr; the authors propose a supervised machine learning approach that uses pinyin linguistic characteristics combined with manually labeled data sets of phonetic similarity. the latter consists of word pairs, with specific pairs of initials or finals manually annotated for phonetic similarity. learning pinyin encodings, the next task is to generate phonetically similar candidates. the set of annotated pairs between initials and finals are then used to learn the ndimensional encodings of initials and finals, which will in turn be used for generating phonetically similar candidates. this is done by grouping pinyin components into initial clusters and only annotating pairs within each cluster, and represent tive cluster pairs. figure partitions initials nto clusters, consisti g of bp, dt, gk, hf, nl, r, jqx, zcs, zhchsh, m, y and w, based on the pronunciatio method and the place of rticulation. we then eliminate the comparison of pairs that are highly similar or highly dissimilar. for example, as the semivowel initials y and w are dissimilar to all other initials, we label every initial for each pinyin py, we retrieve all the words with length two in the dictionary which also have first or second character with the same py. for each created word w, we change the initial (or final) from p1 to p2, retrieve the corresponding words from the dictionary and generate the word pairs to compare. finally, from the full list we randomly select five word pairs that vary the first character, and five word pairs that vary the second character. we invite three native chinese speakers to perform the annotations. for each word pair, the annotators give a label on a point scale, where the labels range from completely disagree what they suggest a new phonetic similarity metric based on the difference between the tones of two pinyins and the difference between the tones of two words. they use a chinese pinyin dictionary to generate a list of words with the same pinyin in pys and the same number of characters as w. they calculate the similarity of each candidate word with w using equation and filter out candidates that fall outside the similarity threshold. thus, th is a parameter that affects the precision and recall of the generated candidates. a larger th generates more candidates, increasing recall while decreasing precision.3 tldr; the authors propose dimsim, a new generative model that encodes the phonetic distance between two words into a two dimensional space. dimsim is able to encode the phonetic distance between two words into a two dimensional space, and thus outperforms all other baselines in terms of precision and average mrr. dimsim is able to encode the phonetic distance between two words into a two dimensional space, and thus outperforms all other baselines in terms of precision and dimsim is a chinese named entity translation system that learns to generate phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components. dimsim learns separate high dimensional encodings for initials and finals, and uses them to calculate and rank the distances between pinyin representations of chinese word pairs. these tasks all share a dependency on reliable phonetic similarity as an intermediate step, especially for languages such as chinese where incorrect homophones and synophones abound. dimsim improves mrr by 7.5x, recall by 1.5x and precision by 1.4x over existing approaches.", "1010": "given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the kbqa process: reranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model. this step is important to deal with the ambiguities normally present in entity linking results. given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the kbqa process: reranking the entity candidates according to whether they connect to high given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the kbqa process: (i) reranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model. (ii) finding the core relation (chains) for each topic entity2 selection from a much smaller candidate entity set after reranking. the above steps are followed by an optional constraint detection step, when the question can not be answered problem with this approach is that it suffers from the low relation coverage due to limited amount of training data, thus can not generalize well to large number of opendomain relations. we hope the question representations could also comprise vectors that summarize various lengths of phrase information (different levels of abstraction) in order to match relation representations of different granularity. we deal with this problem by applying deep bilstms on questions. the firstlayer of bilstm works on the word embeddings of question words q1,, , qn and gets hidden representations ( 1: n(1, 1); ( 1, 1) n) the paper proposes a hierarchical residual bilstm (hrb) model to perform hierarchical matching between different levels of relation/question representations. this is important for our task since each relation token can correspond to phrases of different lengths, mainly because of syntactic variations. for example in table 1, the relation word written could be matched to either the same single word in the question or a much longer phrase be the writer of. this raises the difficulty of matching between different levels of relation/question representations. we could perform the above hierarchical matching by computing the similarity between each layer of and hr separately and doing the (weighted) sum between the two scores in this step, for each candidate entity e and its associated relations re, given the original entity linker score slinker, and the score of the most confident relation r rlq \u2047 re, we sum these two scores to rerank the entities: srerank (e; q) slinker (e; q) srel (r; e; q) srel (r; e; q) srel (r; e; q) srel (r; e; q) srel (r; e; q) tldr; the authors propose an lstm model that detects whether a given question contains a relation between the answer and the topic entity. given a question, the model generates a kb subgraph, which contains all the nodes connected to the question entity with any relation, and a subgraph associated to the original query. given the top scored query generated by the previous steps, for each node v (answer node or cvt node), we collect all the nodes c connecting to v (with relation rc) with any relation, and generate a subgraph associated to the original query. given a question, the model computes a bilstms can be divided into two types: residual and deep. residual bilstms can be thought of as a combination of two bilstms. deep bilstms can be thought of as a combination of two bilstms.", "1011": "in this paper, we propose a humanintheloop ( huml) dictionary expansion approach that employs a lightweight neural language model coupled with tight huml supervision to assist the user in building and maintaining a domainspecific dictionary from an input text corpus. the approach is based on the explore/exploit paradigm to effectively discover new instances ( explore) from the text corpus as well as predict new unseen terms not currently in the corpus using the accepted dictionary entries ( exploit). the evaluation shows that high promptness of the huml ( tighter computer/human partnership) results in nearly perfect performance of the system. tldr; the authors propose an interactive dictionary expansion approach where the tuning is an integral part of the process, i.e. the human works in partnership with the statistical method to drive the semantic of the task effectively and efficaciously. combining the explore and exploit approaches in an unsupervised fashion (or an infrequently supervised fashion) is not particularly effective. it tends to generate many spurious results that the human subject matter expert needs to wade through. close supervision, however, results in a much more performant system. the evaluation shows that high promptness of the huml ( tighter computer/human partnership) results in nearly perfect performance of the system for a surveillance application (e.g., drug side effects mentioned on twitter) it reduces how frequently a human needs to tune up the lexicon to make sure it is catching all relevant entity instances. in the first approach, we first break each instance in to a set of single terms t t1, t2, ..., tn, then for each term ti in t we identify a set of similar terms tsti ts1, ts2, ..., tss in the vocabulary vtc using equation in the next step, we build new phrases by replacing ti with a for example, given the instance clotting problems in the input dictionary the approach first tries to identify related terms in the text corpus for clotting. for example, given the instance clotting problems in the input dictionary the approach first tries to identify related terms in the text corpus for clotting. in the next iteration the phrase can be further extended, by identifying new related words.", "1012": "textual grounding is an important but challenging task for humancomputer interaction, robotics and knowledge mining. grounding of textual phrases, i.e., finding bounding boxes in images which relate to textual phrases, is an important problem for human-computer interaction, robotics and mining of knowledge bases, three applications that are of increasing importance when considering autonomous systems, augmented and virtual reality environments. for example, we may want to guide the bottle on your left, or the plate in the top shelf. while those phrases are easy to interpret for a human, they pose significant challenges for present day textual grounding algorithms, as interpretation of those phrases requires an understanding of objects tldr; the authors propose an approach to grounding natural language in images and video. in particular, given an image and a phrase, the authors suggest using an efficient subwindow search to find the bounding box containing the highest accumulated score over the combined score map. all those concepts come in the form of score maps which we combine linearly before searching for the bounding box containing the highest accumulated score over the combined score map. moreover, linear combination of score maps reveals importance of score maps for specific queries as well as similarity between queries such as skier and snowboarder. hence the search over a large number of bounding boxes allows us to retrieve an tldr; the authors propose a new neural network architecture for bounding boxes. in particular, they introduce a new energy function, which is a linear combination of the score maps accumulated within a bounding box. this energy function results in a sparse wt, which increases the speed of inference. the authors evaluate their model on a number of datasets and show that it is competitive with stateoftheart models. the paper presents a method for bounding box inference based on a branch and bound based search algorithm. given a product space y, the algorithm computes a lower bound e (x, yj, w) for the energy of all possible bounding boxes within the respective subspace. the algorithm proceeds by choosing the subspace with lowest lower bound until this subspace consists of a single element, i.e., until y. given such a repetitive decomposition strategy for the output space, and since the energy e (x, yj, w) for a bounding box y is obtained using a linear combination of word priors and accumulated segmentation masks tldr; the authors train a neural machine that can bound the energy of a single bounding box. the authors train a neural machine that can bound the energy of a single bounding box. what they suggest a method to predict bounding boxes in images. their method is based on a combination of semantic segmentation, object detection, and pose estimation. semantic segmentation is used to extract the probability maps for categories. object detection is used to extract the body part location, then postprocess to get the head, upper body, lower body, and hand regions. their method outperforms competing methods by around on the flickr 30k entities dataset and by around on the referitgame dataset. they also provide an ablation study of the word and image information as shown in the figure below. the paper proposes a mechanism for grounding of textual phrases which provides interpretability, is easy to extend, and permits globally optimal inference. in contrast to existing approaches which are generally based on a small set of bounding box proposals, we efficiently search over all possible bounding boxes. we think interpretability, i.e., linking of word and image concepts, is an important concept, particularly for textual grounding, which deserves more attention. interpretability, i.e., linking of word and image concepts, is an important concept, particularly for textual grounding, which deserves more attention.", "1013": "generative feature matching network (gfmn) is a new moment matching based approach to train implicit generative models by performing mean and covariance matching of features extracted from all convolutional layers of pretrained deep convnets. some interesting properties of gfmns include: (a) the loss function is directly correlated to the generated image quality; (b) mode collapsing is not an issue; and (c) the same pretrained feature extractor can be used across different datasets. our extensive quantitative and qualitative experimental results demonstrate that pretrained autoencoders and dcnn classifiers can be effectively used as (crossdomain) feature extractors tldr; the authors propose to use pretrained autoencoders and dcnns as feature extractors for gfmn training. autoencoder features: a natural choice of unsupervised method to train a feature extractor is the autoencoder (ae) framework. the decoder part of an ae consists exactly of an image generator that uses features extracted by the encoder. therefore, by design, the encoder network should be a good feature extractor for the purpose of generation. we use diagonal covariance matrices as computing full covariance matrices is impractical for large numbers of features. our hypothesis is that imagenet-based pfs are tldr; the authors propose a new approach for image generation based on mmd and mmd based generative models (gan). mmd is a well defined metric for universal kernels. given a kernel k, let p, q be two distributions, their mmd is: mmd2 (k, p, q) p q2hk, where p expkx is the mean embedding. mmd2 (k, p, q) p q2hk, where p expkx is the mean embedding. if k is universal then mmd2 (k, p, q) if and only if p q. generative moment matching network autoencoder (gmmnae) gmmnae uses a gaussian kernel to perform moment matching using the latent code learned by a pretrained ae, and then uses the frozen pretrained decoder to map back to image space. the authors argue that this is universal in the image domain, and that it is sufficient for learning implicit generative models. generative moment matching network autoencoder (gmmnae) gmmnae uses a gaussian kernel to perform moment matching using the latent code learned by a pretrained ae, and then uses the frozen pretrained decoder to map back to image propose a method to extract feature extractors from a generator network (g). the generator network (g) is used to generate a new image. the generator network (g) is used to extract feature extractors from the generator network (g). the generator network (g) is used to extract feature propose a new feature extractor, called adam moving average (ama) for image generation. the adam moving average (am) is a feature extractor that can be used to generate images from dcnns. the adam moving average (am) is a feature extractor that tldr; the authors propose generalized moment matching (gfmn) which is a nonadversarial way of training an implicit generative model. the key idea is to train an implicit generative model by using a cross domain feature extractor. the main contribution of this paper is the use of mean covariance feature matching (mcm) in the context of gans. specifically, they propose a method for performing mean covariance feature matching (mcm) in the context of gans. the method is based on the idea of using the first and second moments to perform feature matching. in experiments, they show that mcm improves the performance of gans in terms of both accuracy and regret. they also show that mcm improves the performance of gans in terms of both accuracy and regret. what they suggest a new architecture for gans, which they call wgans. the architecture is based on the idea of wgans, where the generator is trained on imagenet, and the discriminator is trained on imagenet. the discriminator is trained on imagenet, and the generator is trained on imagenet. the disc", "1014": "recommender systems aim to present items with high utility to the consumers utility may be decomposed into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time 28. economists define items to be either durable goods or nondurable goods based on how long they are intended to last before being replaced a key characteristic of durable goods is the long duration of time between successive purchases within item categories whereas this duration for nondurable goods is much shorter, or even negligible. thus, durable and nondurable goods have differing time utility characteristics which lead to differing demand characteristics. although we have witnessed great success the paper presents a novel approach to demandaware recommendation that is able to make recommendations based on users overall predicted combination of form utility and time utility. given purchase triplets (user, item, time) and categories, the objective is to make recommendations based on users overall predicted combination of form utility and time utility. we purchases by the sparse binary tensor p to model implicit feedback, we assume that p is obtained by thresholding an underlying real-valued utility tensor to a binary tensor y and then revealing a subset of y s positive entries. to this end, we quantify a user s time utility for an item by comparing the time t given a set of m users, n items, and l time slots, we construct a thirdorder binary tensor p 0, 1mnl to represent the purchase history. specifically, entry pijk indicates that user i has purchased item j in time slot k. we denote p0 as the number of nonzero entries in tensor p since p is usually very sparse, we have p0 mnl. given p and c, we further generate a tensor t rmrl where ticjk denotes the number of time slots between user i s most recent purchase within item category cj until time k. if user i has not purchased tldr; the authors propose a model to predict the purchase intentions of a user. the model is based on a binary tensor, where the positive entries of the form are the purchase intentions, and the negative entries are actual purchases. given this observation, we follow and include a labeldependent loss trading the relative cost of positive and unlabeled samples: l (x,p) ijk: pijk1 max1 (xijk max ( 0, dcj ticjk) ijk: pijk1 max ( 0, dcj ticjk) ijk: pijk1 max ( 0, dcj ticjk tldr; in this paper, the authors propose a method to compute the top singular vectors of a neural network (nnn) using proximal gradient descent (pgd). in particular, they propose to compute the top singular vectors of a nnn using the softthresholding operator (s). the softthresholding operator (s) is defined as follows: the softthresholding operator (s) is defined as follows: the softthresholding operator (s) is defined as follows: the softthresholding operator (s) is defined as follows: the softthresholding operator (s) is defined as follows: the softthresholding the paper presents a demandaware recommendation algorithm that can be used to recommend items to users, based on their consumption patterns. the proposed algorithm is based on a lowrank matrix x wht, where w rm10 and h rn10 are random gaussian matrices with entries drawn from n ( 1, 0.5) and then normalize x to the range of 0, we randomly assign all the n items to r categories, with their inter purchase durations equaling 10, 20, we then construct the high purchase intension set (i, j, k) ticjk dcj and xij 0.5, and sample a subset of tldr; the authors evaluate seven recommendation algorithms on two realworld datasets: tmall and amazon review. for each user, we randomly sample her purchase records as the training data, and use the remaining as the test data. for each purchase record (u, i, t) in the test set, we evaluate all the algorithms on two tasks: (i) category prediction, and (ii) purchase time prediction. ideally, good recommendations should have both small category rankings and small time errors. the algorithms m3f, pmf, and wr-mf are excluded from the purchase time prediction task since they are static in this paper, we examine the problem of demandaware recommendation in settings when inter purchase durations within item categories affects intrinsic s intention in combination with intrinsic properties of the items themselves. we formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the utility tensor and a vector of inter purchase durations, and propose a scalable optimization algorithm with a tractable time complexity. our empirical studies show that the proposed approach can yield perfect recovery of duration vectors in noiseless settings; it is robust to noise and scalable as analyzed theoretically.", "1015": "the paper presents a neural response generation model that generates responses conditioned on a target personality. the model learns high level features based on the target personality, and uses them to update its hidden state. the model achieves performance improvements in both perplexity and bleu scores over a baseline seq2seq model, and is validated by human judges. tldr; the authors propose a model that generates responses conditioned on a target set of personality traits values which the responses should express. each personality feature is a weighted sum of the targeted traits values (following a sigmoid activation). at each token generation, the decoder updates the hidden state conditioned on the personality traits features hp, as well as on the previous hidden state, the output token and the context. this resulted in 87.5k conversation pairs in total including different agents (138160 pairs per agent on average) bleu scores as an indicator of model capability tldr; the authors propose a personality based response generation model that can be used to generate customer responses that are tailored to the personality of the customer. in particular, the model is able to generate responses that are tailored to the personality of the customer. the model is tested on a large dataset of customer service tasks, where it is shown to outperform the baseline seq2seq model in terms of both perplexity and bleu score. in addition, the model is evaluated on a crowdsourcing service, where it is shown to outperform the baseline model in terms of both perplexity and bleu score.", "1016": "this is the first time that an end to end stereo pipeline from image acquisition and rectification, multiscale spatiotemporal stereo correspondence, winnertakeall, to disparity regularization is implemented fully on eventbased hardware. using a cluster of truenorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by dynamic vision sensors ( dvs) at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. experiments on realworld sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature tldr; the authors propose a fully neuromorphic event based event based stereo disparity algorithm. when the data in a cycle is sparse, as is the case with a dvs sensor, most neurons would not compute for most of the time, resulting in low active power this processing differs from traditional architectures that use frame-buffers and other conventional data structures; where same memory fetching and computation is repeated for each pixel every frame, independent of scene activity. the proposed event based disparity method is implemented using a stereo pair of davis sensors and nine truenorth ns1e boards however, the method is applicable to other spiking neuromorphic architectures truenorth is a reconfigurable, nonvon neumann neuromorphic chip containing million spiking neurons and million synapses distributed across parallel, eventdriven, neurosynaptic cores. the chip consumes 70mw when operating at a ms computation tick and normal workloads. depending on event dynamics and network architecture, faster tick period is possible, which we take advantage of in this work to achieve as low as ms per tick, thus doubling the maximum throughput achievable. each neurosynaptic core connects inputs to neurons using a crossbar of binary synapses with a lookup table of weights for bits of precision, plus a sign bit. a neuron state variable, called membrane the paper presents a new neural network architecture that can be used to generate 3d images. the architecture is composed of two modules: a memory cell module and a ring buffer module. the memory cell module is a recurrent neuron which accepts as input either its own output (so that it does not lose its stored value whenever the neuron is queried for its stored value), input axons to set the neuron value and control axons for resetting and querying the memory cell. the ring buffer is implemented by storing events in membrane potentials of memory cell neurons in a circular buffer, and through the use of control neurons which spike periodically to polar the wta system is a feedforward neural network that takes as input d thermometer code representations of the hadamard products for d distinct candidate disparity levels, and finds the disparity with the largest value, at every tick. for designing a scalable and compact wta system on a neuromorphic hardware, we introduced a novel encoding technique for inputs. in a binary eventbased system, numbers can be efficiently coded using base4 representation where each digit is encoded using a 3-bits thermometer code. note that a thermometer code of length 2n bits can be represented by a qtc of length n/2 bits. while it takes a few more bits than a what they suggest a method to learn a membrane potential from a dataset. the membrane potential is a function of all the events in the dataset. tldr; the authors test their model on a synthetic dataset of 3d objects, and on a real world dataset of objects that are rapidly rotating in a circular plane approximately perpendicular to the yaxis. the synthetic dataset is generated by assigning to each left sensor pixel a random event with a probability per polarity. similarly, each right sensor pixel is assigned a value by projecting it to the 3d scene and reprojecting the corresponding datapoint to the left camera coordinate frame to find the closest pixel value. for the nonsynthetic dataset, a kinect is used to extract ground truth of the scene structure. this also entails a calibration process for this paper introduces a fully event based stereo system that can compute disparity maps for a large number of events. the event based disparity map is computed using a spiking neural network with low precision weights. the event based disparity map is computed using a fully graph based computation model, where no frames, arrays or other such data structures are used.", "1017": "tldr; the authors introduce a novel notion of interventional equivalence class of causal graphs with latent variables based on these invariances. given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions, where the elements of the family are indistinguishable using the invariances obtained from a direct application of the calculus rules. we provide a formal graphical characterization of this equivalence. finally, we extend the fci algorithm, which was originally designed to operate based on cis, to combine observational and interventional datasets, including new orientation rules particular to this setting. in this paper, we start by noting something very simple, albeit powerful, that happens when a combination of observational and experimental distributions are available: there are constraints over the graphical structure that emerge by comparing these different distributions, and which are not of conditional independence (ci) type2. remarkably, and unknown until our work, the converse of the causal calculus developed by pearl offers a systematic way of reading these constraints and tying them back to the underlying graphical structure. in reference to their connection to the do-calculus rules (or a generalization), we call these constraints the do-constraints. broadly speaking, do-constraint given a set of interventional distributions, we construct an augmented graph by introducing an f-node for every unique set difference between pairs of controlled intervention sets. without the controlled experiment assumption, our machinery can still be used if one knows which mechanism changes are identical and by constructing f-nodes to reflect and capture the mechanism difference across two interventions. for simplicity of presentation, however, we restrict ourselves to the controlled experiment setting and pursue this route explicitly. for example, if px, z (y), for some y, we can read that fz y fx, fx, z accordingly, given a set of interventional distributions, we construct an tldr; the authors propose fci, a method to learn causal graphs from a combination of observational and interventional data. fci is a standard sound and complete method to learn such an object related work: learning causal graphs from a combination of observational and interventional data has been studied in the literature. for causally sufficient systems, the notion and characterization of interventional markov equivalence has been introduced in 9, more recently, showed that the same characterization can be used for both hard and soft interventions. for causally insufficient systems, uses sat solvers to learn a summary graph over the observed variables given data from different experimental conditions. introduces an tldr; the authors present two tests to distinguish between causal relations between variables in a causal graph. the first test is called the do-do test, and the second is called the do-separation test. the authors show that these two tests can be used to distinguish between causal relations between variables in a causal graph. in particular, the authors show that these two tests can be used to distinguish between causal relations between variables in a causal graph that are dependent (or not independent) and consequently dconnected in the graph, while no claim can be made about the causal relation between them. tldr; the authors define the notion of interventional markov equivalence as follows: given two causal graphsd1 (vl1, e1) andd2 (vl2, e2), and an intervention set i 2v, d1 andd2 are called i-markov equivalent if pi (d1, v) pi (d2, v) the set of all tuples that satisfy the i-markov property with respect tod are denoted by pi (d, v) the construction of the augmented graph goes as follows. first, initialize the augmented graph to the input causal graph. then, for every distinct symmetric set tldr; the authors propose a method to learn a causal graph from a combination of observational and interventional data. given a causal graphd and an intervention set i, the augmented mag is the mag constructed over v from augi (d), i.e., mag (augi) (d). the authors also provide a characterization for two causal graphs to be i-markov equivalent. tldr; the authors propose fci (function learnaugmentednodes) an algorithm to learn the skeleton of augmented pags (augmented pags). the algorithm initializes a complete graph with circle edges, then it removes the edge between any pair of nodes if a separating set between the pair exists and records the set. in phase ii, the algorithm identifies unshielded triples a, b, c and orients the edges into b if b is not in the separating set of a and c. only one of the rules uses separating sets while the rest use mag properties, and soundness and completeness tldr; the authors propose a method for learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data. in particular, they consider the problem of learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data. the paper proposes a method for learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data.", "1018": "the paper presents terminal rewards (cipitr) a neural program induction (npi) system for complex kbqa. terminal rewards (cipitr) is a neural program induction (npi) system for complex kbqa. cipitr solves complex kbqa considerably more accurately than keyvalue memory networks and neural symbolic machines. for moderately complex queries requiring 2to 5-step programs, cipitr scores at least higher f1 than the competing systems. on one of the hardest class of programs ( comparative reasoning) with steps, cipitr outperforms nsm by a factor of and memory networks by times.1 cipitr reduces the combinatorial program space to only semantically correct programs by incorporating symbolic constraints guided by kb schema and inferred answer type, and by adopting pragmatic programming techniques by decomposing the final goal into a hierarchy of subgoals, thereby mitigating the sparse reward problem. on even moderately complex programs of length 25, cipitr outperformed nsm by a factor of and kvmnet by a factor of further. on one of the hardest class of programs of around steps (i.e. comparative reasoning), cipitr outperformed nsm by a factor of and kvmnet by a factor of further. whereas tldr; the authors propose a framework for complex program induction (cpi) in kbqa data sets. in particular, the authors propose a framework for cpi in kbqa data sets. the cpi problem is the following: given a natural language query (e.g., answerable from the freebase kb) and a kb subgraph (e.g., a set of kb entities, a kb type, a rel relation, or an integer). given a kb subgraph (e.g., a set of kb entities, a rel relation, or cipitr takes as input the natural language question, the kb, and the prepopulated variable memory tables to generate a program (i.e., a sequence of operators invoked with past instantiated variables as their arguments and generating new variables in memory). the interpreter executes the generated program with the help of the kb and scratch memory and outputs the system answer. during training, the predicted answer is compared with the gold to obtain a reward, which is sent back to cipitr to update its model parameters through a reinforce to investigate robustness of cipitr to linkage errors may be of future interest cipitr takes a natural language query and generates an output program in a number of steps. a program is composed of actions, which are operators applied over variables, cipitr uses a beam search to obtain multiple candidate programs to provide feedback to the model from a single training instance. algorithm shows the pseudocode of the program induction algorithm (with beam size b as for simplicity) which goes over t time steps, each time sampling np feasible operators conditional to the program state. cipitr is a reinforcement learning framework that can learn to generate semantically correct programs. the key idea is to use a phase change network (pcn) to learn to avoid the curse of dimensionality in exponential action spaces. the first phase is to gather the information from the preprocessed program. the second phase is to operate on all the generated variables in order to reach the answer. the predicted answer type helps in the direction of the correct answer type by biasing the sampling towards feasible operators that can produce the desired answer type by biasing the sampling towards feasible operators that can produce the correct answer type by biasing the sampling towards feasible cipitr is evaluated on complex kbqa and compares favorably against stateoftheart kbqa and other stateoftheart kbqa models in terms of performance and accuracy. in terms of performance, cipitr outperforms stateoftheart kbqa and other stateoftheart kbqa models. in terms of accuracy, cipitr outperforms stateoftheart kbqa and other stateoftheart kbqa models. in terms of performance, cipitr outperforms stateoftheart kbqa and other stateoftheart kbqa models in terms of performance and accuracy. cipitr is a multistep inference model that is able to learn the rules behind the multistep inference process simply from the distance supervision provided by the question pairs and even performs slightly better in some of the query classes. the query categories next in order of complexity are quantitative and quantitative count, which translate to an average of lined programs. the hardest types are quantitative and quantitative count, which translate to an average of lined programs. for the train and valid splits, a rulebased query type classifier with accuracy was used to bucket queries into the classes listed in table for each of these three systems, we also train and evaluate one single model over all cipitr uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic pragmatic programming styles to the extreme reward sparsity and incorporates generic programming styles to only semantically correct programs. the key idea of cipitr is to use auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporate generic pragmatic programming styles to only semantically correct programs.", "1019": "tldr; the authors propose a novel unsupervised, queryfocused, multidocument extractive summarizer (ces). the summarizer then needs to produce a textual summary that captures the most salient (general and informative) content parts within input documents. the summarizer may also be required to satisfy a specific user information need, expressed by one or more queries. therefore, the summarizer will need to produce a focused summary which includes the most relevant information to that need. while both saliency and focus goals should be considered within a queryfocused summarization setting, these goals may be actually conflicting with each other higher saliency usually the cross entropy method (ce) is a global policy search optimization framework for solving the sentence subset selection problem. instead, following 6, q (sq, d) is surrogated by several summary quality prediction measures qi (sq, d) (i 1, 2, each predictor qi (sq, d) is designed to estimate the level of saliency or focus of a given candidate summary s and is presumed to correlate ( up to some extent) with actual summarization quality, e.g. rouge for simplicity, similar to ces, various predictions are assumed to be independent and are combined into a single optimization objective tldr; the authors propose a twostep dual saliency and focus optimization approach. in the first step, the authors relax the summary length constraint, aiming at producing a longer and more salient summary. in the second step, the authors relax the summary length constraint, aiming at producing a longer and more salient summary. this summary is then treated as a pseudoeffective reference summary from which saliency-based pseudofeedback is distilled. such pseudofeedback is then utilized in the second step of the cascade for setting an additional auxiliary saliency-driven goal. at the second step, the primary goal is actually to produce tldr; the authors propose a cascade approach to improve saliency of unigram language models. the first step of the cascade consists of the same set of documents d, summary length constraint lmax and the pseudoreference summary sl that was generated in the previous step. the second step of the cascade consists of the same set of documents d, summary length constraint lmax and the pseudoreference summary sl that was generated in the previous step. the target measure qfoc (q, d) guides the optimization towards the production of a focused summary, while still keeping high saliency as much as possible. to achieve that tldr; the authors evaluate their model on document understanding conference (duc) 2005 and document understanding conference ( benchmarks) and show that it is competitive with stateoftheart methods. the authors suggest an extension to dualces that adaptively adjusts the value of hyperparameter l. to this end, they introduce a new learning parameter lt which defines the maximum length limit for summary production (sampling) that is allowed at iteration t of the ce-method. we now assume that summary lengths have a poisson (lt) distribution of word occurrences with mean lt. using importance sampling, this parameter is estimated at iteration t as tldr; the authors evaluate two variants of the ces summarizer: the original ces summarizer and the dualces summarizer. dualces is a variant of the original ces summarizer. the authors evaluate dualces on a variety of summarization tasks, including duc, recall and fmeasure. on recall, dualces has achieved between and better rouge-2 and rouge-1, respectively. on f-measure, dualces significantly improves over the two ces variants in all benchmarks. on recall, dualces has achieved at least between and better rouge-2 and rouge-1, respectively. tldr; the authors propose dualces, an unsupervised, queryfocused, extractive multidocument summarizer. the authors evaluate dualces on the duc benchmark, and show that it provides the best summarization quality compared to other alternative stateoftheart unsupervised summarizers. also find this summary at [davidstutz.de]([url]/).", "1020": "this paper presents a tts system that can synthesize speech with high quality while running times faster than realtime on a standard cpu. the system is composed of three separate neural network blocks: prosody prediction, acoustic feature prediction and linear prediction coding net as a neural vocoder. prosody generator emits a sequence of subphoneme elements, including duration, pitch and intensity values. each subphoneme element represents either a heading, a middle or a trailing part of a phoneme. each subphoneme element represents either a heading, a a prosody generator generates a prosody vector per tts unit, comprising the unit s logduration, initial log pitch, final log pitch and logenergy. it generates a sequence of subphoneme elements, including duration, pitch and intensity values. each subphoneme element represents either a heading, a middle or a trailing part of phoneme. the synthesizer represents each subphoneme element by several consecutive frames according to the element s duration and generates an acoustic feature vector for each frame. finally, an lpcnet block is used to convert the stream of the acoustic feature vectors to a speech signal. each block has its own tldr; the authors train an lpcnet model to predict the vocal source signal and then apply it an lpc filter calculated from the cepstrum. this has the advantages of better control over the output of the spectral shape since it depends directly on the lpc filter shape. the model is also more robust to the predicted residual errors since any high frequency noise is also shaped by the lpc filter. the model is trained on a small target voice. a held out validation set is used as a stop criterion to avoid overfitting. the lpcnet model was reported to perform well in speaker independent setting, when trained on multi tldr; the authors present a new tts system that can produce high quality speech while operating at faster than realtime rate without an expensive gpu support. the system is built around three nn models for generating the prosody, acoustic features and the final speech signal. we tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems. the task of creating a high quality tts system out of a smaller set of audio data is even more challenging. we have shown that our system can perform well even with datasets as small as", "1021": "feature selection is an important problem in statistics and machine learning for interpretable predictive modeling and scientific discoveries. the mutual information between two random variables x and y is the most commonly used dependency measure. the mutual information is defined as the kullbackleibler divergence between the joint distribution of x, y and the product of their marginals. mutual information is however challenging to estimate from samples, which motivated the introduction of dependency measures based on other f -divergences or integral probability metrics than the kl divergence. for instance, the hilbert-schmidt independence criterion (hsic) uses the maximum mean discrepancy (mmd) to assess the dependency this paper proposes a method for feature selection based on gradient regularization. in particular, the authors propose a method for selecting the features that maintain maximum dependency between the input and the response. the key idea is to use a gradient regularizer that takes into account the nonlinearity of the interaction between the input and the response. the authors show that this regularizer can be used to control the false discovery rate (fdr) of the model. in particular, they show that the fdr can be controlled by using the holdout randomization test. sobolev independence criterion the sobolev independence criterion (sic) is defined as follows: sic (l1 ) 2 ( pxy, pxpy) sup ff epxyf (x, y) epxpyf (x, y) s (f) sic (l1 ) 2 ( pxy, pxpy) sup ff epxyf (x, y) s (f) sic (l1 ) 2 ( pxy, pxpy) sup ff epxyf (x, y) s (f) sic (l1 ) in this paper, the authors propose a method to estimate the nonlinear sparsity of a function space. specifically, they propose to estimate the nonlinear sparsity of a function space based on the mean and covariance of the joint distribution and product of the marginals of the function space. in particular, they propose to estimate the nonlinear sparsity of a function space based on the mean and covariance of the joint distribution and product of the marginals of the function space. in this paper, the authors propose a method to estimate the nonlinear sparsity of a function space based on the mean and covariance of the joint distribution and this paper introduces a new dependency measure, called the empirical nonconvex sic (sic) measure, which can be used to quantify dependencies between features. the empirical nonconvex sic (sic) measure is defined as follows: f(x, y) xf(x, y) xf(x, y) xf(x, y) xf(x, y) xf(x, y) xf(x, y) xf(x, y) xf(x, y) xf(x, y) xf(x, y) xf(x, y) this paper proposes a method to control the false discovery rate (fdr) of a feature selection algorithm. in particular, the authors propose to use the holdout randomization test (hrt) as a method to control the fdr. sic is a feature selection method that can be seen as a sparse gradient regularized mmd. sic has the advantage of fitting one critic that has interpretable feature scores. related to the mmd is the hilbert schmidt independence criterion (hsic) and other variants of kernel dependency measures introduced in 2, none of those criteria has a nonparametric sparsity constraint on its witness function this paper introduces a method for feature selection (sic) based on a convolutional neural network (cnn) architecture. a convolutional neural network (cnn) is a network where the input is a vector representation of the input and the output is a vector representation of the output. the main idea of sic is to use a convolutional neural network (cnn) architecture where the input is a vector representation of the input and the output is a vector representation of the output. the main idea of sic is to use a convolutional neural network (cnn) architecture where the input is a vector representation of the input and the"}