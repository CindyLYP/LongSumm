{
    "1000": "the authors present a novel approach for inducing unsupervised part of speech taggers for languages that have no labeled training data, but have translated text in a resourcerich language. their method does not assume any knowledge about the target language across eight european languages, their approach results in an average absolute improvement of over a stateoftheart baseline, and over vanilla hidden markov models induced with the expectation maximization algorithm. the authors evaluate their approach on eight european languages, and considerably bridges the gap to fully supervised pos tagging performance (96.6). the vertices of the graph are extracted from the different sides of a parallel corpus (de, df) and an additional unlabeled monolingual foreign corpus f, which will be used for training. we use two different similarity functions to define the edge weights among the foreign vertices and between vertices from different languages. the authors use a supervised english tagger to label the english side of the bitext.7 to initialize the graph for label propagation we use a supervised english tagger to label the english side of the bitext.7 to initialize the graph for label propagation we use a supervised english tagger to label the english side of the bitext.7 to initialize the graph for label propagation we use a supervised english tagger to label the english side of the bitext.7 tldr; the authors propose a new unsupervised tagger based on the featurebased hmm of bergkirkpatrick et al. the authors use a squared loss to penalize neighboring vertices that have different label distributions, and additionally regularize the label distributions towards the uniform distribution u over all possible labels y the objective is convex in q. the first term in the objective function is the graph smoothness regularizer which encourages the distributions of similar vertices (large wij) to be similar. the second term is a regularizer and encourages all type marginals to be uniform to the extent that is allowed by the first two terms (cf. they train their model on parallel data from the europarl corpus and the tldr; the authors propose to perform label propagation (lp) in order to extract the constraint features for all foreign word types. their full model outperforms the no lp setting because it has better vocabulary coverage and allows the extraction of a larger set of constraint features. for comparison, the completely unsupervised feature-hmm baseline accuracy on the universal pos tags for english is 79.4, and goes up to with a treebank dictionary. the paper proposes a graph based approach for predicting part of speech (pse) labels across languages. because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs. unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for train.",
    "1001": "this paper presents a novel approach for fv representation of sequences using a recurrent neural network (rnn). the rnn is trained to predict the next element of a sequence given the previous elements. it is applied to two different and challenging tasks: video action recognition and image annotation by sentences. state of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. a surprising boost in performance in the video action recognition task was achieved by using a transfer learning approach from the image annotation task. specifically, the vgg image embedding of a frame is projected using a linear transformation which was the paper presents a transfer learning approach for image annotation and image search tasks. specifically, the vgg image embedding of a frame is projected using a linear transformation which is learned on matching images and sentences by the canonical correlation analysis (cca) algorithm the proposed rnn-fv method achieves state of the art results in action recognition on the hmdb51 and ucf101 datasets. in image annotation and image search tasks, the rnn-fv method is used for the representation of sentences and achieves state of the art results on the flickr8k dataset tldr; the authors propose rnn fvs for image annotation and classification. given a sequence of vectors with n vector elements x1, ..., xn, we convert it tldr; the authors train a rnn to predict the next element in a sequence, given the previous elements of the sequence. given a sequence, the gradient of the rnn loss may serve as the sequence representation, even if the loss is not interpretable as a likelihood. the classification application is applicable for predicting a sequence of symbols w1, w2, ..., wn that have matching vector representations tldr; the authors propose a new generative model that can be used to predict the sequence of images in a video. the rnn is capable of encoding the sequence properties, and as underlying features, we rely on video encodings that are based on single frames or on fixed length blocks tldr; the authors train the network to predict the next word in the input sequence, given the previous words in the input sequence. they train the network to predict the next word in the input sequence, given the previous words in the input sequence. the idea is to use the convolutional layers of the rnn to generate a vector representation of the image, which can then be used as the input to a regularized cca (regularized cca) algorithm to generate a vector representation of the sentence, which can then be used as the input to a multiclass svm (multiclass linear svm) to generate a vector representation of the image, which can then be used as the input to a regularized cca algorithm to generate a vector representation of the sentence, which can then be used as the input to a tldr; the authors propose a neural network architecture for bidirectional image and sentence retrieval. the network architecture is based on the following architecture: the network is trained on the input image and the output sentence. the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs. when used for representing sentences, the rnnfv representation achieves state of the art or competitive results on image annotation and image search tasks. since the length of the sentences in these tasks is usually short and, therefore, we believe that using the rnnfv for tasks that use longer text will provide an even larger gap between the conventional fv and the rnnfv this paper introduces a novel fv representation for sequences that is derived from rnns.",
    "1002": "the rate of publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research. we hypothesize that such talks constitute a coherent and concise description of the papers, content, and can form the basis for good summaries. we collected papers and their corresponding videos, and created a dataset of paper summaries. in addition, we validated the quality of our summaries by human experts. then, via a publicly available asr service6, we extracted transcripts of the speech, and based on the video metadata (e.g., title), we retrieved the correspond- ing paper (in pdf format). scienceparse7 to extract the text of the paper, and applied a simple given the paper, the speaker generates words for describing ver bally sentences from the paper, one word at each time step. thus, at each time step, the speaker has a single sentence from the paper in mind, and produces a word that constitutes a part of its ver- bal description. thus, given the transcript, we aim to retrieve those source sentences and use them as the sum mary. the number of words uttered to describe each sentence can serve as importance score, in- dic tldr; the authors propose a method for generating a summary of a paper, based on the output of a speaker. currently, no largescale training data is available for the task of scientific paper summarization. in this paper, we propose a novel method that automatically generates summaries for scientific papers by utilizing videos of talks at scientific conferences.",
    "1003": "emotion detection from text has become a popular task due to the key role of emotions in human-machine interaction. to overcome these limitations, pseudolabeled datasets are gathered from social media platforms where social media posts are explicitly tagged tldr; the authors propose an ensemble approach that combines both a linear model based on bow, and a nonlinear model based on the pretrained word vectors. in order to utilize the high quality but small size datasets, we propose an ensemble approach that combines both a linear model based on bow, and a nonlinear model based on the pretrained word vectors. in addition, we propose a new method for realizing a sentence level representation from the single words vectors. in this paper, the authors propose an ensemble based approach for ve emotion detection, where each emotion is represented as a di erent document. the authors evaluate the proposed ensemble approach on two datasets for emotion detection: isear and semeval. tldr; the authors propose an ensemble of bow and embedded document representations for emotion detection in text. the authors evaluate the proposed ensemble on a set of emotion detection datasets, and show that the proposed ensemble outperforms the baseline for each dataset. these results indicate the advantage in combining both bow and embedded document representations for emotion detection from text. generating high quality word vectors requires large-scale data and computing power. when generation is not an option, one can utilize pre-trained representations; the most popular pre-trained representations are based on word2vec and glove algorithms, and were trained on large corpora. to our knowledge, this is the rst research that shows how to utilize pre-trained representations for emotion detection.",
    "1004": "in this paper, we analyze customer support dialogues using the twitter platform and show the utility of such analyses. in such a context, providing automatic detection and analysis of the emotions expressed by customers is important, as is identification of the emotional techniques (e.g., apology, empathy, etc.) result of such an analysis can help assess the quality of such a service, help and inform agents about desirable responses, and help develop automated service agents for social media interactions. a recent study shows that one in five ( 23) customers in the u.s. say they have used social media for customer tldr; the paper presents a method to automatically detect emotions in social media dialogues and predict the emotional technique that is likely to be used by a human customer service agent in a given situation. this analysis reflects our ultimate goal: to enable a computer system to discern the emotions expressed by human customers, and to develop computerized tools that mimic the emotional technique used by a human customer service agent in a particular situation. another interesting trend in customer service, in addition to the use of social media, is the automation of various functions of customer interaction. in the present work, we define a dialogue to be a sequence of turns between a specific customer and an agent, where the customer initiates the first turn. given the nature of customer support services, we assume the last turn in the dialogue is an agent turn. thus, we expect an even number of turns in the dialogue. we filtered out dialogues in which more than one customer or one agent are involved. tldr; the authors propose a model to classify emotions in a written dialogue between a customer and a support agent. this difference stems from the fact that in order to train an automated service agent to respond based on customer input, the agent s emotional technique needs to be computed before the agent generates its response sentence. content is available at classification time (as well as the history of the dialogue) meaning, the customer has already provided her input and the system must now understand what is the emotion being expressed. the model is evaluated on a dataset of turk dialogues, where it tldr; the authors propose svm dialogue and svm hmm models to predict the emotional technique used by a customer service agent. in this paper, we show that it is possible to automatically detect emotions being expressed and, second that it is possible to predict the emotional technique that is likely to be used by a human agent in a given situation.",
    "1005": "tldr; the authors propose to use differentiable neural architecture search (d-nas) to optimize the architecture for fewshot classification (fsc) without overfitting. this is due to incorporating the architecture as an additional set of neural network parameters to be optimized, and solving this optimization using sgd. it seems the overall performance of the fsc techniques can not continue to grow by simply expanding the backbone size. it seems the overall performance of the fsc techniques can not continue to grow by tldr; the authors propose a neural network architecture search (nas) framework for fewshot classification (fsc) tasks. the proposed architecture search framework consists of two parts: architecture search and metaadaptation. in metaoptnet a cnn backbone is trained endtoend with an unrolled convex optimization solution of an tldr; the authors propose a method to adapt the architecture of a neural network to the task at hand. they suggest a new architecture for a neural network to adapt to a given task. tldr; the authors propose a method to learn an adaptive architecture for fewshot classification tasks. the key idea is to learn an adaptive architecture for a given training set, and then transfer the learned architecture to a larger training set. the authors evaluate their method on the miniimagenet and fc100 fewshot benchmarks and show that it outperforms stateofthe art methods. the authors also show that it is possible to transfer the learned architecture from fc100 to miniimagenet. tldr; the authors propose a bilevel optimization method, called metadaptation, to improve the performance of fc100 and resnet. tldr; the authors present metadapt, a fewshot learning approach that enables metalearning network architecture that is adaptive to novel fewshot tasks. the goal of meta-learning is to find a base model that is easily adapted to the specific task at hand, so that it will generalize well to tasks built from novel unseen categories and fulfill the goal of fsc. to this end, we employ tools inspired by the differentiable neural architecture search (d-nas) literature in order to optimize the architecture for fsl without over-fitting. additionally, to make the architecture task adaptive, we propose the concept of metadapt controller modules. these modules are added to the model and are meta-trained to predict the optimal network connections for a given novel task. to avoid over-fitting, a bi-level (two-fold) optimization is performed where first the operation layers weights are trained on one fold of the data and then the connections weights are trained on the other fold.",
    "1006": "in this paper, we outline an approach to detecting such egregious conversations, using behavioral cues from the user, patterns in agent responses, and useragent interaction. using logs of two commercial systems, we show that using these features improves the detection f1-score by around over using textual features alone. in addition, we show that those features are common across two quite different domains and , arguably, universal. as an aid to the improvement, analysis of egregious conversations can often point to problems in training data or system logic that can be repaired. specifically, we consider customer inputs throughout a whole conversation, and detect cues such as rephrasing, the presence of heightened emotions, and queries about whether the is a human or requests to speak to an actual human. finally, we analyze the larger conversational context exploring, for example, where the presence of a not trained response might be especially problematic. to do this, they extract features from both the customer and the agent responses, together with features related to the combination of specific inputs and responses. using this set of features for detecting egre- gious conversations is novel, and as our experimental results show, improves performance compared to a model based solely on features extracted from the conversation text. tldr; the authors extracted data from two commercial systems, each embedded in a larger system with its own logic. another similarity feature is between two customer s subsequent turns when the agent was not trained with a linear kernel. the goal of this work is to give developers of automated agents tools to solve problems cre- ated by exceptionally bad conversations. we did not encounter any unsupported intent leading to customer rephrasing, which affected the ability of the model to classify those conversations. while customer rephrasing was captured by the egr model, for the textbased model some of the intents were new ( did not appear in the virtual agents are becoming a prominent channel of interaction in customer service. in such instances, a human agent might need to step in and salvage the conversation. detecting bad conversations is important since disappointing customer service may threaten customer loyalty and impact revenue.",
    "1007": "we aim to understand the method by which the networks process and classify text. the ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model the paper proposes a method to improve interpretability and prediction interpretability of cnns by focusing on informative ngrams and taking into account also the negative cues. to this end, the paper proposes to use a threshold for each filter to separate informative and uninformative ngrams. the paper also proposes a method for calculating the threshold for each filter, based on a shared purity value. to assess the quality of threshold obtained by the proposed method, the authors discard values that do not pass the threshold for each filter and observe the performance of the model. in essence, the more filters we utilize in the network, the less correlation there is between each filter s s and the final classification, as the decision is being made by a greater consensus. we challenge this view and show that filters often specialize in multiple distinctly different semantic classes by utilizing activation patterns which are not necessarily maximized. we also show that filters may not only identify good ngrams, but may also actively supress bad ones. the paper presents two theories to explain the discrepancy between the activations of naturally occurring and possible ngrams. our second theory to explain the discrepancy between the activations of naturally occurring and possible ngrams is that certain filter slots are not used to detect a class of highly activating words, but rather to rule out a class of highly negative words. in order to identify case negative ngrams, we heuristically test whether the changed words  scores directly influence the status of the activation relative to the threshold: given an already identified negative ngram, if the ngram scoresans the bottom-k negative slot cnns have been shown to perform well on text classification tasks, but their interpretability is still an open question. the authors show that cnns perform well on text classification tasks, but their interpretability is still an open question. we examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. we show that filters do not maximize activations at the word-level, but instead form slot activation patterns that give different types of ngrams similar activation strengths. by clustering high-scoring ngrams according to their slot activation patterns we can identify the groups of linguistic patterns captured by a filter. we also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words. finally, we use these findings to suggest improvements to model-based and predictionbased interpretability of cnns for text.",
    "1008": "the key idea of editnet is to create an automatic editing process to enhance summary quality. for each sentence in s, the editor may make three possible decisions. the first decision is to keep the extracted sentence untouched ( represented by label e). such a decision, for example, may represent the editor s wish to simplify or compress the original source sentence. the last possible decision is to completely reject the sentence ( represented by label r). for each sentence si s (in order) the editor makes one of the three possible decisions: extract, abstract or reject si. in each step i 1, 2, , l, in order to make an educated decision, the editor considers both sentence representations esi and asi as its input, together with two additional auxiliary representations. such a representation provides a global context for decision making. in each step i, therefore, tldr; the authors propose a novel softattention mechanism that can be used to generate an abstractive representation of a given document. moreover, per considered sentence, editnet may decide not to take either of these decisions and completely reject the sentence. moreover, editnet implements a novel sentence rejection decision, allowing to correct initial sentence selection decisions which are predicted to negatively effect summarization quality.",
    "1009": "dimsim is a supervised learning approach to learn ndimensional encodings for finals and initials where n can be easily extended from one to two or higher dimensions. the learning model derives accurate encodings by jointly considering pinyin linguistic characteristics, such as place of articulation and pronunciation methods, as well as high quality annotated training data sets. tldr; the authors propose a supervised machine learning approach that uses pinyin linguistic characteristics combined with manually labeled data sets of phonetic similarity. learning pinyin encodings, the next task is to generate phonetically similar candidates. the set of annotated pairs between initials and finals are then used to learn the ndimensional encodings of initials and finals, which will in turn be used for generating phonetically similar candidates. for example, as the semivowel initials y and w are dissimilar to all other initials, we label every initial for each pinyin py, we retrieve all the words with length two in the dictionary which also have first or second character with the same py. for each created word w, we change the initial (or final) from p1 to p2, retrieve the corresponding words from the dictionary and generate the word pairs to compare. finally, from the full list we randomly select five word pairs that vary the first character, and five word pairs that vary the second character. for each word pair, the annotators give a label on a point scale, where the labels range from completely disagree what they suggest a new phonetic similarity metric based on the difference between the tones of two pinyins and the difference between the tones of two words. they use a chinese pinyin dictionary to generate a list of words with the same pinyin in pys and the same number of characters as w. they calculate the similarity of each candidate word with w using equation and filter out candidates that fall outside the similarity threshold. a larger th generates more candidates, increasing recall while decreasing precision.3 tldr; the authors propose dimsim, a new generative model that encodes the phonetic distance between two words into a two dimensional space. dimsim is able to encode the phonetic distance between two words into a two dimensional space, and thus outperforms all other baselines in terms of precision and average mrr. dimsim is able to encode the phonetic distance between two words into a two dimensional space, and thus outperforms all other baselines in terms of precision and dimsim is a chinese named entity translation system that learns to generate phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components. dimsim learns separate high dimensional encodings for initials and finals, and uses them to calculate and rank the distances between pinyin representations of chinese word pairs. this paper presents dimsim, a learned ndimensional phonetic encoding for chinese along with a phonetic similarity algorithm, which uses the encoding to generate and rank phonetically similar chinese words. dimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components.",
    "1010": "given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the kbqa process: reranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model. (ii) finding the core relation (chains) for each topic entity2 selection from a much smaller candidate entity set after reranking. the above steps are followed by an optional constraint detection step, when the question can not be answered problem with this approach is that it suffers from the low relation coverage due to limited amount of training data, thus can not generalize well to large number of opendomain relations. we hope the question representations could also comprise vectors that summarize various lengths of phrase information (different levels of abstraction) in order to match relation representations of different granularity. the firstlayer of bilstm works on the word embeddings of question words q1,, , qn and gets hidden representations ( 1: n(1, 1); ( 1, 1) n) the paper proposes a hierarchical residual bilstm (hrb) model to perform hierarchical matching between different levels of relation/question representations. this is important for our task since each relation token can correspond to phrases of different lengths, mainly because of syntactic variations. for example in table 1, the relation word written could be matched to either the same single word in the question or a much longer phrase be the writer of. this raises the difficulty of matching between different levels of relation/question representations. given a question, the model computes a bilstms can be divided into two types: residual and deep. relation detection is a key step in kbqa and is significantly different from general relation extraction tasks.",
    "1011": "the approach is based on the explore/exploit paradigm to effectively discover new instances ( explore) from the text corpus as well as predict new unseen terms not currently in the corpus using the accepted dictionary entries ( exploit). the evaluation shows that high promptness of the huml ( tighter computer/human partnership) results in nearly perfect performance of the system for a surveillance application (e.g., drug side effects mentioned on twitter) it reduces how frequently a human needs to tune up the lexicon to make sure it is catching all relevant entity instances. in the first approach, we first break each instance in to a set of single terms t t1, t2, ..., tn, then for each term ti in t we identify a set of similar terms tsti ts1, ts2, ..., tss in the vocabulary vtc using equation in the next step, we build new phrases by replacing ti with a for example, given the instance clotting problems in the input dictionary the approach first tries to identify related terms in the text corpus for clotting. for example, given the instance clotting problems in the input dictionary the approach first tries to identify related terms in the text corpus for clotting. given an input text corpus and a set of seed examples, the proposed approach runs in two phases, explore and exploit, to identify new potential dictionary entries. the explore phase tries to identify similar instances to the dictionary entries that are present in the input corpus, using term vectors from the neural language model to calculate a similarity score. the exploit phase tries to construct more complex multiterm phrases by analyzing the single terms of the instances in the input dictionary.",
    "1012": "grounding of textual phrases, i.e., finding bounding boxes in images which relate to textual phrases, is an important problem for human-computer interaction, robotics and mining of knowledge bases, three applications that are of increasing importance when considering autonomous systems, augmented and virtual reality environments. while those phrases are easy to interpret for a human, they pose significant challenges for present day textual grounding algorithms, as interpretation of those phrases requires an understanding of objects tldr; the authors propose an approach to grounding natural language in images and video. in particular, given an image and a phrase, the authors suggest using an efficient subwindow search to find the bounding box containing the highest accumulated score over the combined score map. all those concepts come in the form of score maps which we combine linearly before searching for the bounding box containing the highest accumulated score over the combined score map. hence the search over a large number of bounding boxes allows us to retrieve an tldr; the authors propose a new neural network architecture for bounding boxes. in particular, they introduce a new energy function, which is a linear combination of the score maps accumulated within a bounding box. the paper presents a method for bounding box inference based on a branch and bound based search algorithm. given a product space y, the algorithm computes a lower bound e (x, yj, w) for the energy of all possible bounding boxes within the respective subspace. the algorithm proceeds by choosing the subspace with lowest lower bound until this subspace consists of a single element, i.e., until y. given such a repetitive decomposition strategy for the output space, and since the energy e (x, yj, w) for a bounding box y is obtained using a linear combination of word priors and accumulated segmentation masks tldr; the authors train a neural machine that can bound the energy of a single bounding box. they also provide an ablation study of the word and image information as shown in the figure below. in contrast to existing approaches which are generally based on a small set of bounding box proposals, we efficiently search over all possible bounding boxes. existing algorithms generally formulate the task as selection from a set of bounding box proposals obtained from deep net based systems. in this work, we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes.",
    "1013": "generative feature matching network (gfmn) is a new moment matching based approach to train implicit generative models by performing mean and covariance matching of features extracted from all convolutional layers of pretrained deep convnets. some interesting properties of gfmns include: (a) the loss function is directly correlated to the generated image quality; (b) mode collapsing is not an issue; and (c) the same pretrained feature extractor can be used across different datasets. our extensive quantitative and qualitative experimental results demonstrate that pretrained autoencoders and dcnn classifiers can be effectively used as (crossdomain) feature extractors tldr; the authors propose to use pretrained autoencoders and dcnns as feature extractors for gfmn training. autoencoder features: a natural choice of unsupervised method to train a feature extractor is the autoencoder (ae) framework. therefore, by design, the encoder network should be a good feature extractor for the purpose of generation. if k is universal then mmd2 (k, p, q) if and only if p q. generative moment matching network autoencoder (gmmnae) gmmnae uses a gaussian kernel to perform moment matching using the latent code learned by a pretrained ae, and then uses the frozen pretrained decoder to map back to image space. the authors argue that this is universal in the image domain, and that it is sufficient for learning implicit generative models. the generator network (g) is used to generate a new image. the generator network (g) is used to extract feature propose a new feature extractor, called adam moving average (ama) for image generation. the adam moving average (am) is a feature extractor that can be used to generate images from dcnns. the adam moving average (am) is a feature extractor that tldr; the authors propose generalized moment matching (gfmn) which is a nonadversarial way of training an implicit generative model. the key idea is to train an implicit generative model by using a cross domain feature extractor. the main contribution of this paper is the use of mean covariance feature matching (mcm) in the context of gans. the method is based on the idea of using the first and second moments to perform feature matching. the disc the use of perceptual features (pfs) in the context of learning implicit generative models through moment matching (mm) is not well studied. our proposed approach improves upon existing mm methods by: breaking away from adversarial learning; avoiding online learning of kernel functions; avoiding online learning of kernel functions; and being efficient with respect to both number of used moments and required minibatch size.",
    "1014": "recommender systems aim to present items with high utility to the consumers utility may be decomposed into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time 28. economists define items to be either durable goods or nondurable goods based on how long they are intended to last before being replaced a key characteristic of durable goods is the long duration of time between successive purchases within item categories whereas this duration for nondurable goods is much shorter, or even negligible. although we have witnessed great success the paper presents a novel approach to demandaware recommendation that is able to make recommendations based on users overall predicted combination of form utility and time utility. given purchase triplets (user, item, time) and categories, the objective is to make recommendations based on users overall predicted combination of form utility and time utility. to this end, we quantify a user s time utility for an item by comparing the time t given a set of m users, n items, and l time slots, we construct a thirdorder binary tensor p 0, 1mnl to represent the purchase history. given p and c, we further generate a tensor t rmrl where ticjk denotes the number of time slots between user i s most recent purchase within item category cj until time k. if user i has not purchased tldr; the authors propose a model to predict the purchase intentions of a user. the model is based on a binary tensor, where the positive entries of the form are the purchase intentions, and the negative entries are actual purchases. given this observation, we follow and include a labeldependent loss trading the relative cost of positive and unlabeled samples: l (x,p) ijk: pijk1 max1 (xijk max ( 0, dcj ticjk) ijk: pijk1 max ( 0, dcj ticjk) ijk: pijk1 max ( 0, dcj ticjk tldr; in this paper, the authors propose a method to compute the top singular vectors of a neural network (nnn) using proximal gradient descent (pgd). the softthresholding operator (s) is defined as follows: the softthresholding operator (s) is defined as follows: the softthresholding operator (s) is defined as follows: the softthresholding operator (s) is defined as follows: the softthresholding operator (s) is defined as follows: the softthresholding the paper presents a demandaware recommendation algorithm that can be used to recommend items to users, based on their consumption patterns. for each purchase record (u, i, t) in the test set, we evaluate all the algorithms on two tasks: (i) category prediction, and (ii) purchase time prediction. the algorithms m3f, pmf, and wr-mf are excluded from the purchase time prediction task since they are static in this paper, we examine the problem of demandaware recommendation in settings when inter purchase durations within item categories affects intrinsic s intention in combination with intrinsic properties of the items themselves. recommender systems aim to present items with high utility to the consumers utility may be into form utility: the item is desired as it is manifested, and time utility: the item is desired at the given point in time 28. in particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession.",
    "1015": "the paper presents a neural response generation model that generates responses conditioned on a target personality. the model learns high level features based on the target personality, and uses them to update its hidden state. tldr; the authors propose a model that generates responses conditioned on a target set of personality traits values which the responses should express. at each token generation, the decoder updates the hidden state conditioned on the personality traits features hp, as well as on the previous hidden state, the output token and the context. this resulted in 87.5k conversation pairs in total including different agents (138160 pairs per agent on average) bleu scores as an indicator of model capability tldr; the authors propose a personality based response generation model that can be used to generate customer responses that are tailored to the personality of the customer. in particular, the model is able to generate responses that are tailored to the personality of the customer. personality is a set of traits which represent durable characteristics of a person. in this paper we study how to encode personality traits as part of neural response generation for conversational agents. our approach builds upon a sequence-to-sequence by adding an additional layer that represents the target set of personality traits, and a hidden layer that learns high-level personality based features.",
    "1016": "using a cluster of truenorth neurosynaptic processors, we demonstrate their ability to process bilateral event-based inputs streamed live by dynamic vision sensors ( dvs) at up to 2,000 disparity maps per second, producing high fidelity disparities which are in turn used to reconstruct, at low power, the depth of events produced from rapidly changing scenes. experiments on realworld sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature tldr; the authors propose a fully neuromorphic event based event based stereo disparity algorithm. when the data in a cycle is sparse, as is the case with a dvs sensor, most neurons would not compute for most of the time, resulting in low active power this processing differs from traditional architectures that use frame-buffers and other conventional data structures; where same memory fetching and computation is repeated for each pixel every frame, independent of scene activity. the proposed event based disparity method is implemented using a stereo pair of davis sensors and nine truenorth ns1e boards however, the method is applicable to other spiking neuromorphic architectures truenorth is a reconfigurable, nonvon neumann neuromorphic chip containing million spiking neurons and million synapses distributed across parallel, eventdriven, neurosynaptic cores. a neuron state variable, called membrane the paper presents a new neural network architecture that can be used to generate 3d images. the ring buffer is implemented by storing events in membrane potentials of memory cell neurons in a circular buffer, and through the use of control neurons which spike periodically to polar the wta system is a feedforward neural network that takes as input d thermometer code representations of the hadamard products for d distinct candidate disparity levels, and finds the disparity with the largest value, at every tick. while it takes a few more bits than a what they suggest a method to learn a membrane potential from a dataset. similarly, each right sensor pixel is assigned a value by projecting it to the 3d scene and reprojecting the corresponding datapoint to the left camera coordinate frame to find the closest pixel value. this also entails a calibration process for this paper introduces a fully event based stereo system that can compute disparity maps for a large number of events. the event based disparity map is computed using a spiking neural network with low precision weights. the event based disparity map is computed using a fully graph based computation model, where no frames, arrays or other such data structures are used. we introduce a stereo correspondence system implemented fully on event-based digital hardware, using a fully graph-based non von-neumann computation model, where no frames, arrays, or any other such data-structures are used. experiments on real- world sequences demonstrate the ability of the system to take full advantage of the asynchronous and sparse nature of dvs sensors for low power depth reconstruction, in environments where conventional frame-based cameras connected to synchronous processors would be inefficient for rapidly moving objects.",
    "1017": "given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions, where the elements of the family are indistinguishable using the invariances obtained from a direct application of the calculus rules. finally, we extend the fci algorithm, which was originally designed to operate based on cis, to combine observational and interventional datasets, including new orientation rules particular to this setting. in this paper, we start by noting something very simple, albeit powerful, that happens when a combination of observational and experimental distributions are available: there are constraints over the graphical structure that emerge by comparing these different distributions, and which are not of conditional independence (ci) type2. remarkably, and unknown until our work, the converse of the causal calculus developed by pearl offers a systematic way of reading these constraints and tying them back to the underlying graphical structure. broadly speaking, do-constraint given a set of interventional distributions, we construct an augmented graph by introducing an f-node for every unique set difference between pairs of controlled intervention sets. without the controlled experiment assumption, our machinery can still be used if one knows which mechanism changes are identical and by constructing f-nodes to reflect and capture the mechanism difference across two interventions. for example, if px, z (y), for some y, we can read that fz y fx, fx, z accordingly, given a set of interventional distributions, we construct an tldr; the authors propose fci, a method to learn causal graphs from a combination of observational and interventional data. fci is a standard sound and complete method to learn such an object related work: learning causal graphs from a combination of observational and interventional data has been studied in the literature. for causally sufficient systems, the notion and characterization of interventional markov equivalence has been introduced in 9, more recently, showed that the same characterization can be used for both hard and soft interventions. for causally insufficient systems, uses sat solvers to learn a summary graph over the observed variables given data from different experimental conditions. introduces an tldr; the authors present two tests to distinguish between causal relations between variables in a causal graph. in particular, the authors show that these two tests can be used to distinguish between causal relations between variables in a causal graph that are dependent (or not independent) and consequently dconnected in the graph, while no claim can be made about the causal relation between them. then, for every distinct symmetric set tldr; the authors propose a method to learn a causal graph from a combination of observational and interventional data. the authors also provide a characterization for two causal graphs to be i-markov equivalent. in phase ii, the algorithm identifies unshielded triples a, b, c and orients the edges into b if b is not in the separating set of a and c. only one of the rules uses separating sets while the rest use mag properties, and soundness and completeness tldr; the authors propose a method for learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data. in particular, they consider the problem of learning the causal structure underlying a phenomenon of interest from a combination of observational and experimental data. the task of learning the causal structure underlying a certain phenomenon is undertaken by connecting the set of conditional independences (cis) readable from the observational data, on the one side, with the set of corresponding constraints implied over the graphical structure, on the other, which are tied through a graphical criterion known as d-separation. in this paper, we investigate the more general setting where multiple observational and experimental distributions are available. we start with the simple observation that the invariances given by cis/dseparation are just one special type of constraints, which follow from the careful comparison of the different distributions available.",
    "1018": "for moderately complex queries requiring 2to 5-step programs, cipitr scores at least higher f1 than the competing systems. on one of the hardest class of programs ( comparative reasoning) with steps, cipitr outperforms nsm by a factor of and memory networks by times.1 cipitr reduces the combinatorial program space to only semantically correct programs by incorporating symbolic constraints guided by kb schema and inferred answer type, and by adopting pragmatic programming techniques by decomposing the final goal into a hierarchy of subgoals, thereby mitigating the sparse reward problem. whereas tldr; the authors propose a framework for complex program induction (cpi) in kbqa data sets. given a kb subgraph (e.g., a set of kb entities, a rel relation, or cipitr takes as input the natural language question, the kb, and the prepopulated variable memory tables to generate a program (i.e., a sequence of operators invoked with past instantiated variables as their arguments and generating new variables in memory). during training, the predicted answer is compared with the gold to obtain a reward, which is sent back to cipitr to update its model parameters through a reinforce to investigate robustness of cipitr to linkage errors may be of future interest cipitr takes a natural language query and generates an output program in a number of steps. a program is composed of actions, which are operators applied over variables, cipitr uses a beam search to obtain multiple candidate programs to provide feedback to the model from a single training instance. algorithm shows the pseudocode of the program induction algorithm (with beam size b as for simplicity) which goes over t time steps, each time sampling np feasible operators conditional to the program state. the key idea is to use a phase change network (pcn) to learn to avoid the curse of dimensionality in exponential action spaces. the second phase is to operate on all the generated variables in order to reach the answer. the predicted answer type helps in the direction of the correct answer type by biasing the sampling towards feasible operators that can produce the desired answer type by biasing the sampling towards feasible operators that can produce the correct answer type by biasing the sampling towards feasible cipitr is evaluated on complex kbqa and compares favorably against stateoftheart kbqa and other stateoftheart kbqa models in terms of performance and accuracy. cipitr is a multistep inference model that is able to learn the rules behind the multistep inference process simply from the distance supervision provided by the question pairs and even performs slightly better in some of the query classes. the query categories next in order of complexity are quantitative and quantitative count, which translate to an average of lined programs. for the train and valid splits, a rulebased query type classifier with accuracy was used to bucket queries into the classes listed in table for each of these three systems, we also train and evaluate one single model over all cipitr uses auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporates generic pragmatic programming styles to the extreme reward sparsity and incorporates generic programming styles to only semantically correct programs. the key idea of cipitr is to use auxiliary rewarding techniques to mitigate the extreme reward sparsity and incorporate generic pragmatic programming styles to only semantically correct programs. neural program induction (npi) is a pragmatic approach toward modularizing reasoning process by translating a complex natural language query into a multistep executable program. while npi has been commonly trained with the gold program or its sketch, for realistic kbqa applications such programs are expensive to obtain there, practically only natural language answers can be provided for training. we present complex imperative program induction from terminal rewards (cipitr), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, kb schema, and inferred answer type. on one of the hardest class of programs (comparative reasoning) with steps, cipitr outperforms nsm by a factor of and kvmnet by a factor of further diverse program classes.",
    "1019": "the summarizer then needs to produce a textual summary that captures the most salient (general and informative) content parts within input documents. the summarizer may also be required to satisfy a specific user information need, expressed by one or more queries. therefore, the summarizer will need to produce a focused summary which includes the most relevant information to that need. while both saliency and focus goals should be considered within a queryfocused summarization setting, these goals may be actually conflicting with each other higher saliency usually the cross entropy method (ce) is a global policy search optimization framework for solving the sentence subset selection problem. instead, following 6, q (sq, d) is surrogated by several summary quality prediction measures qi (sq, d) (i 1, 2, each predictor qi (sq, d) is designed to estimate the level of saliency or focus of a given candidate summary s and is presumed to correlate ( up to some extent) with actual summarization quality, e.g. rouge for simplicity, similar to ces, various predictions are assumed to be independent and are combined into a single optimization objective tldr; the authors propose a twostep dual saliency and focus optimization approach. in the first step, the authors relax the summary length constraint, aiming at producing a longer and more salient summary. at the second step, the primary goal is actually to produce tldr; the authors propose a cascade approach to improve saliency of unigram language models. the first step of the cascade consists of the same set of documents d, summary length constraint lmax and the pseudoreference summary sl that was generated in the previous step. the target measure qfoc (q, d) guides the optimization towards the production of a focused summary, while still keeping high saliency as much as possible. to achieve that tldr; the authors evaluate their model on document understanding conference (duc) 2005 and document understanding conference ( benchmarks) and show that it is competitive with stateoftheart methods. the authors suggest an extension to dualces that adaptively adjusts the value of hyperparameter l. to this end, they introduce a new learning parameter lt which defines the maximum length limit for summary production (sampling) that is allowed at iteration t of the ce-method. we now assume that summary lengths have a poisson (lt) distribution of word occurrences with mean lt. using importance sampling, this parameter is estimated at iteration t as tldr; the authors evaluate two variants of the ces summarizer: the original ces summarizer and the dualces summarizer. the authors evaluate dualces on the duc benchmark, and show that it provides the best summarization quality compared to other alternative stateoftheart unsupervised summarizers. the vast amounts of textual data end users need to consume motivates the need for automatic summarization an automatic summarizer gets as an input one or more documents and possibly also a limit on summary length (e.g., maximum number of words). while both saliency and focus goals should be considered within a query-focused summarization setting, these goals may be actually conflicting with each other higher saliency usually comes at the expense of lower focus and viceversa.",
    "1020": "this paper presents a tts system that can synthesize speech with high quality while running times faster than realtime on a standard cpu. finally, an lpcnet block is used to convert the stream of the acoustic feature vectors to a speech signal. each block has its own tldr; the authors train an lpcnet model to predict the vocal source signal and then apply it an lpc filter calculated from the cepstrum. this has the advantages of better control over the output of the spectral shape since it depends directly on the lpc filter shape. the model is also more robust to the predicted residual errors since any high frequency noise is also shaped by the lpc filter. the lpcnet model was reported to perform well in speaker independent setting, when trained on multi tldr; the authors present a new tts system that can produce high quality speech while operating at faster than realtime rate without an expensive gpu support. the system is built around three nn models for generating the prosody, acoustic features and the final speech signal. we tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems. the task of creating a high quality tts system out of a smaller set of audio data is even more challenging. we have shown that our system can perform well even with datasets as small as in this paper we present a system that can synthesize speech with close to natural quality while running times faster than real-time on a standard cpu. the modular setup of the system allows for simple adaptation to new voices with a small amount of data.",
    "1021": "the mutual information between two random variables x and y is the most commonly used dependency measure. the mutual information is defined as the kullbackleibler divergence between the joint distribution of x, y and the product of their marginals. mutual information is however challenging to estimate from samples, which motivated the introduction of dependency measures based on other f -divergences or integral probability metrics than the kl divergence. for instance, the hilbert-schmidt independence criterion (hsic) uses the maximum mean discrepancy (mmd) to assess the dependency this paper proposes a method for feature selection based on gradient regularization. in particular, the authors propose a method for selecting the features that maintain maximum dependency between the input and the response. the key idea is to use a gradient regularizer that takes into account the nonlinearity of the interaction between the input and the response. the authors show that this regularizer can be used to control the false discovery rate (fdr) of the model. specifically, they propose to estimate the nonlinear sparsity of a function space based on the mean and covariance of the joint distribution and product of the marginals of the function space. in this paper, the authors propose a method to estimate the nonlinear sparsity of a function space based on the mean and covariance of the joint distribution and this paper introduces a new dependency measure, called the empirical nonconvex sic (sic) measure, which can be used to quantify dependencies between features. related to the mmd is the hilbert schmidt independence criterion (hsic) and other variants of kernel dependency measures introduced in 2, none of those criteria has a nonparametric sparsity constraint on its witness function this paper introduces a method for feature selection (sic) based on a convolutional neural network (cnn) architecture. the main idea of sic is to use a convolutional neural network (cnn) architecture where the input is a vector representation of the input and the in this paper we introduce the sobolev independence criterion (sic) , an interpretable dependency measure between a high dimensional random variable x and a response variable y sic decomposes to the sum of feature importance scores and hence can be used for nonlinear feature selection. sic can be seen as a gradient regularized integral probability metric (ipm) between the joint distribution of the two random variables and the product of their marginals."
}