<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-view Face Detection Using Deep Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-04-20">20 Apr 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><forename type="middle">Sudhakar</forename><surname>Farfade</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saberian</surname></persName>
							<email>saberian@yahoo-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Multi-view Face Detection Using Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-04-20">20 Apr 2015</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2671188.2749408</idno>
					<idno type="arXiv">arXiv:1502.02766v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face Detection</term>
					<term>Convolutional Neural Network</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this paper we consider the problem of multi-view face detection. While there has been significant research on this problem, current state-of-the-art approaches for this task require annotation of facial landmarks, e.g. TSM [25], or annotation of face poses [28, 22]. They also require training dozens of models to fully capture faces in all orientations, e.g. 22 models in HeadHunter method [22]. In this paper we propose Deep Dense Face Detector (DDFD), a method that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model based on deep convolutional neural networks. The proposed method has minimal complexity; unlike other recent deep learning object detection methods [9], it does not require additional components such as segmentation, bounding-box regression, or SVM classifiers. Furthermore, we analyzed scores of the proposed face detector for faces in different orientations and found that 1) the proposed method is able to detect faces from different angles and can handle occlusion to some extent, 2) there seems to be a correlation between distribution of positive examples in the training set and scores of the proposed face detector. The latter suggests that the proposed method&apos;s performance can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. Evaluations on popular face detection benchmark datasets show that our single-model face detector algorithm has similar or better performance compared to the previous methods, which are more complex and require annotations of either different poses or facial landmarks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the wide spread use of smartphones and fast mobile networks, millions of photos are uploaded everyday to the cloud storages such as Dropbox or social networks such as Facebook, Twitter, Instagram, Google+, and Flicker. Organizing and retrieving relevant information from these photos is very challenging and directly impact user experience on those platforms. For example, users commonly look for photos that were taken at a particular location, at a particular time, or with a particular friend. The former two queries are fairly straightforward, as almost all of today's cameras embed time and GPS location into photos. The last query, i.e. contextual query, is more challenging as there is no explicit signal about the identities of people in the photos. The key for this identification is the detection of human faces. This has made low complexity, rapid and accurate face detection an essential component for cloud based photo sharing/storage platforms.</p><p>For the past two decades, face detection has always been an active research area in the vision community. The seminal work of Viola and Jones <ref type="bibr" target="#b40">[40]</ref> made it possible to rapidly detect up-right faces in real-time with very low computational complexity. Their detector, called detector cascade, consists of a sequence of simple-to-complex face classifiers and has attracted extensive research efforts. Moreover, detector cascade has been deployed in many commercial products such as smartphones and digital cameras. While cascade detectors can accurately find visible up-right faces, they often fail to detect faces from different angles, e.g. side view or partially occluded faces. This failure can significantly impact the performance of photo organizing software/applications since user generated content often contains faces from different angles or faces that are not fully visible; see for example <ref type="figure">Figure 1</ref>. This has motivated many works on the problem of multi-view face detection over the past two decades. Current solutions can be summarized into three categories:</p><p>• Cascade-based: These methods extend the <ref type="bibr">Viola and</ref> Jones detector cascade. For example, <ref type="bibr" target="#b41">[41]</ref> proposed to train a detector cascade for each view of the face and combined their results at the test time. Recently, <ref type="bibr" target="#b22">[22]</ref> combined this method with integral channel features <ref type="bibr" target="#b2">[3]</ref> and soft-cascade <ref type="bibr" target="#b0">[1]</ref>, and showed that by using 22 cascades, it is possible to obtain state-of-the-art performance for multi-view face detection. This approach, however, requires face orientation annotations. Moreover its complexity in training and testing increases linearly with the number of models. To address the computational complexity issue, Viola and Jones <ref type="bibr" target="#b39">[39]</ref> proposed to first estimate the face pose using a tree classifier and then run the cascade of corresponding face pose to verify the detection. While improving the detection speed, this method degrades the accuracy because mistakes of the initial tree classifier are irreversible. This method is further improved by <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b12">12]</ref> where, instead of one detector cascade, several detectors are used after the initial classifier. Finally, <ref type="bibr" target="#b35">[35]</ref> and <ref type="bibr" target="#b28">[28]</ref> combined detector cascade with multiclass boosting and proposed a method for multiclass/multi-view object detection.</p><p>• DPM-based: These methods are based on the deformable part models technique <ref type="bibr" target="#b4">[5]</ref> where a face is defined as a collection of its parts. The parts are defined via unsupervised or supervised training, and a classifier, latent SVM, is trained to find those parts and their geometric relationship. These detectors are robust to partial occlusion because they can detect faces even when some of the parts are not present. These methods are, however, computationally intensive because 1) they require solving a latent SVM for each candidate location and 2) multiple DPMs have to be trained and combined to achieve the state-of-the-art performance <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b25">25]</ref>. Moreover, in some cases DPM-based models require annotation of facial landmarks for training, e.g <ref type="bibr" target="#b25">[25]</ref>.</p><p>• Neural-Network-based: There is a long history of using neural networks for the task of face detection <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b23">23]</ref>. In particular, <ref type="bibr" target="#b38">[38]</ref> trained a two-stage system based on convolutional neural networks. The first network locates rough positions of faces and the second network verifies the detection and makes more accurate localization. In <ref type="bibr" target="#b27">[27]</ref>, the authors trained multiple face detection networks and combined their output to improve the performance. <ref type="bibr" target="#b7">[8]</ref> trained a single multi-layer network for face detection. The trained network is able to partially handle different poses and rotation angles. More recently, <ref type="bibr" target="#b23">[23]</ref> proposed to train a neural network jointly for face detection and pose estimation. They showed that this joint learning scheme can significantly improve performance of both detection and pose estimation. Our method follows the works in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">23]</ref> but constructs a deeper CNN for face detection.</p><p>The key challenge in multi-view face detection, as pointed out by Viola and Jones <ref type="bibr" target="#b39">[39]</ref>, is that learning algorithms such as Boosting or SVM and image features such as HOG or Haar wavelets are not strong enough to capture faces of different poses and thus the resulted classifiers are hopelessly inaccurate. However, with recent advances in deep learning and GPU computation, it is possible to utilize the high capacity of deep convolutional neural networks for feature extraction/classification, and train a single model for the task of multi-view face detection.</p><p>Deep convolutional neural network has recently demonstrated outstanding performance in a variety of vision tasks such as face recognition <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b30">30]</ref>, object classification <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b31">31]</ref>, and object detection <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b32">32]</ref>. In particular <ref type="bibr" target="#b19">[19]</ref> trained an 8-layered network, called AlexNet, and showed that deep convolutional neural networks can significantly outperform other methods for the task of large scale image classification. For the task of object detection, <ref type="bibr" target="#b9">[9]</ref> proposed R-CNN method that uses an image segmentation technique, selective search <ref type="bibr" target="#b36">[36]</ref>, to find candidate image regions and classify those candidates using a version of AlexNet that is finetuned for objects in the PASCAL VOC dataset. More recently, <ref type="bibr" target="#b33">[33]</ref> improved R-CNN by 1) augmenting the selective search proposals with candidate regions from multibox approach <ref type="bibr" target="#b3">[4]</ref>, and 2) replacing 8-layered AlexNet with a much deeper CNN model of GoogLeNet <ref type="bibr" target="#b31">[31]</ref>. Despite state-of-theart performance, these methods are computationally suboptimal because they require evaluating a CNN over more than 2, 000 overlapping candidate regions independently. To address this issue, <ref type="bibr" target="#b18">[18]</ref> recently proposed to run the CNN model on the full image once and create a feature pyramid. The candidate regions, obtained by selective search, are then mapped into this feature pyramid space. <ref type="bibr" target="#b18">[18]</ref> then uses spatial pyramid pooling <ref type="bibr" target="#b20">[20]</ref> and SVM on the mapped regions to classify candidate proposals. Beyond region-based methods, deep convolutional neural networks have also been used with sliding window approach, e.g. OverFeat <ref type="bibr" target="#b29">[29]</ref> and deformable part models <ref type="bibr" target="#b10">[10]</ref> for object detection and <ref type="bibr" target="#b17">[17]</ref> for human pose estimation. In general, for object detection these methods still have an inferior performance compared to region-based methods such as R-CNN <ref type="bibr" target="#b9">[9]</ref> and <ref type="bibr" target="#b33">[33]</ref>. However, in our face detection experiments we found that the region-based methods are often very slow and result in relatively weak performance.</p><p>In this paper, we propose a method based on deep learning, called Deep Dense Face Detector (DDFD), that does not require pose/landmark annotation and is able to detect faces in a wide range of orientations using a single model. The proposed method has minimal complexity because unlike recent deep learning object detection methods such as <ref type="bibr" target="#b9">[9]</ref>, it does not require additional components for segmentation, bounding-box regression, or SVM classifiers. Compared to previous convolutional neural-network-based face detectors such as <ref type="bibr" target="#b7">[8]</ref>, our network is deeper and is trained on a significantly larger training set. In addition, by analyzing detection confidence scores, we show that there seems to be a correlation between the distribution of positive examples in the training set and the confidence scores of the proposed detector. This suggests that the performance of our method can be further improved by using better sampling strategies and more sophisticated data augmentation techniques. In our experiments, we compare the proposed method to a deep learning based method, R-CNN, and several cascade and DPM-based methods. We show that DDFD can achieve similar or better performance even without using pose annotation or information about facial landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED METHOD</head><p>In this section, we provide details of the algorithm and training process of our proposed face detector, called Deep Dense Face Detector (DDFD). The key ideas are 1) leverage the high capacity of deep convolutional networks for classification and feature extraction to learn a single classifier for detecting faces from multiple views and 2) minimize the computational complexity by simplifying the architecture of the detector.</p><p>We start by fine-tuning AlexNet <ref type="bibr" target="#b19">[19]</ref> for face detection. For this we extracted training examples from the AFLW dataset <ref type="bibr" target="#b21">[21]</ref>, which consists of 21K images with 24K face annotations. To increase the number of positive examples, we randomly sampled sub-windows of the images and used them as positive examples if they had more than a 50% IOU (intersection over union) with the ground truth. For further data augmentation, we also randomly flipped these training examples. This resulted in a total number of 200K positive and and 20 millions negative training examples. These examples were then resized to 227 × 227 and used to finetune a pre-trained AlexNet model <ref type="bibr" target="#b19">[19]</ref>. For fine-tuning, we used 50K iterations and batch size of 128 images, where each batch contained 32 positive and 96 negative examples.</p><p>Using this fine-tuned deep network, it is possible to take either region-based or sliding window approaches to obtain the final face detector. In this work we selected a sliding window approach because it has less complexity and is independent of extra modules such as selective search. Also, as discussed in the experiment section, this approach leads to better results as compared to R-CNN.</p><p>Our face classifier, similar to AlexNet <ref type="bibr" target="#b19">[19]</ref>, consists of 8 layers where the first 5 layers are convolutional and the last 3 layers are fully-connected. We first converted the fullyconnected layers into convolutional layers by reshaping layer parameters <ref type="bibr" target="#b14">[14]</ref>. This made it possible to efficiently run the CNN on images of any size and obtain a heat-map of the face classifier. An example of a heat-map is shown in <ref type="figure" target="#fig_1">Figure 2</ref>right. Each point in the heat-map shows the CNN response, the probability of having a face, for its corresponding 227 × 227 region in the original image. The detected regions were then processed by non-maximal suppression to accurately localize the faces. Finally, to detect faces of different sizes, we scaled the images up/down and obtained new heat-maps. We tried different scaling schemes and found that rescaling image 3 times per octave gives reasonably good performance. This is interesting as many of the other methods such as <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b1">2]</ref> requires a significantly larger number of resizing per octave, e.g. 8. Note that, unlike R-CNN <ref type="bibr" target="#b9">[9]</ref>, which uses SVM classifier to obtain the final score, we removed the SVM module and found that the network output are informative enough for the task of face detection.</p><p>Face localization can be further improved by using a boundingbox regression module similar to <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b9">9]</ref>. In our experiment, however, adding this module degraded the performance. Therefore, compared to the other methods such as R-CNN <ref type="bibr" target="#b9">[9]</ref>, which uses selective search, SVM and boundingbox regression, or DenseNet <ref type="bibr" target="#b10">[10]</ref>, which is based on the deformable part models, our proposed method (DDFD) is fairly simple. Despite its simplicity, as shown in the experiments section, DDFD can achieve state-of-the-art performance for face detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Detector Analysis</head><p>In this section, we look into the scores of the proposed face detector and observe that there seems to be a correlation between those scores and the distribution of positive examples in the training set. We can later use this hypothesis to obtain better training set or to design better data augmentation procedures and improve performance of DDFD.</p><p>We begin by running our detector on a variety of faces with different in-plane and out-of-plane rotations, occlusions and lighting conditions (see for example <ref type="figure">Figure 1</ref>, <ref type="figure" target="#fig_1">Figure 2</ref>left and <ref type="figure">Figure 3</ref>). First, note that in all cases our detector is able to detect the faces except for the two highly occluded ones in <ref type="figure">Figure 1</ref>. Second, for almost all of the detected faces, the detector's confidence score is pretty high, close to 1. Also as shown in the heat-map of <ref type="figure" target="#fig_1">Figure 2</ref>-right, the scores are close to zero for all other regions. This shows that DDFD has very strong discriminative power, and its output can be used directly without any post-processing steps such as SVM, which is used in R-CNN <ref type="bibr" target="#b9">[9]</ref>. Third, if we compare the detector scores for faces in <ref type="figure" target="#fig_1">Figure 2</ref>-left, it is clear that the up-right frontal face in the bottom has a very high score of 0.999 while faces with more in-plane rotation have less score. Note that these scores are output of a sigmoid function, i.e. probability (soft-max) layer in the CNN, and thus small changes in them reflects much larger changes in the output of the previous layer. It is interesting to see that the scores decrease as the in-plane rotation increases. We can see the same trend for out-of-plane rotated faces and occluded faces in <ref type="figure">Figures 1 and 3</ref>. We hypothesize that this trend in the scores is not because detecting rotated face are more difficult but it is because of lack of good training examples to represent such faces in the training process.</p><p>To examine this hypothesis, we looked into the face annotations for AFLW dataset <ref type="bibr" target="#b21">[21]</ref>. <ref type="figure" target="#fig_3">Figure 4</ref> shows the distribution of the annotated faces with regards to their in-plane, pitch (up and down) and yaw (left to right) rotations. As shown in this figure, the number of faces with more than 30 degrees out-of-plane rotation is significantly lower than the faces with less than 30 degree rotation. Similarly, the number of faces with yaw or pitch less than 50 degree is significantly larger than the other ones. Given this skewed training set, it not surprising that the fine-tuned CNN is more confident about up-right faces. This is because the CNN is trained to minimize the risk of the soft-max loss function</p><formula xml:id="formula_0">R = x i ∈B log [prob(yi|xi)] ,<label>(1)</label></formula><p>where B is the example batch that is used in an iteration of stochastic gradient descent and yi is the label of example  significantly hurt performance of the final detector. In an extreme case if B never contains any example of a certain class, the CNN classifier will never learn the attributes of that class. In our implementation |B| = 128 and it is collected by randomly sampling the training set. However, since the number of negative examples are 100 times more than the number of positive examples, a uniform sampling will result in only about 2 positive examples per batch. This significantly degrades the chance of the CNN to distinguish faces from nonfaces. To address this issue, we enforced one quarter of each batch to be positive examples, where the positive examples are uniformly sampled from the pool of positive training samples. But, as illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, this pool is highly skewed in different aspects, e.g. in-plane and out-of-plane rotations. The CNN is therefore getting exposed with more up-right faces; it is thus not surprising that the fine-tuned CNN is more confident about the up-right faces than the rotated ones. This analysis suggests that the key for improving performance of DDFD is to ensure that all categories of the training examples have similar chances to contribute in optimizing the CNN. This can be accomplished by enforcing population-based sampling strategies such as increasing selection probability for categories with low population.</p><p>Similarly, as shown in <ref type="figure">Figure 1</ref>, the current face detector still fails to detect faces with heavy occlusions. Similar to the issue with rotated faces, we believe that this problem can also be addressed through modification of the training set. In fact, most of the face images in the AFLW dataset <ref type="bibr" target="#b21">[21]</ref> are not occluded, which makes it difficult for a CNN to learn that faces can be occluded. This issue can be addressed by using more sophisticated data augmentation techniques such as occluding parts of positive examples. Note that simply covering parts of positive examples with black/white or noise blocks is not useful as the CNN may learn those artificial patterns.</p><p>To summarize, the proposed face detector based on deep CNN is able to detect faces from different angles and handle occlusion to some extent. However, since the training set is skewed, the network is more confident about up-right faces and better results can be achieved by using better sampling strategies and more sophisticated data augmentation techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We implemented the proposed face detector using the Caffe library <ref type="bibr" target="#b16">[16]</ref> and used its pre-trained Alexnet <ref type="bibr" target="#b19">[19]</ref> model for fine-tuning. For further details on the training process of our proposed face detector please see section 2. After converting fully-connected layers to convolutional layers <ref type="bibr" target="#b14">[14]</ref>, it is possible to get the network response (heat-map) for the whole input image in one call to Caffe code. The heat-map shows the scores of the CNN for every 227 × 227 window with a stride of 32 pixels in the original image. We directly used this response for classifying a window as face or background. To detect faces of smaller or larger than 227 × 227, we scaled the image up or down respectively.</p><p>We tested our face detection approach on PASCAL Face <ref type="bibr" target="#b42">[42]</ref>, AFW <ref type="bibr" target="#b25">[25]</ref> and FDDB <ref type="bibr" target="#b15">[15]</ref> datasets. For selecting and tuning parameters of the proposed face detector we used the PASCAL Face dataset. PASCAL Face dataset consists of 851 images and 1341 annotated faces, where annotated faces can be as small as 35 pixels. AFW dataset is built using Flickr images. It has 205 images with 473 annotated faces, and its images tend to contain cluttered background with large variations in both face viewpoint and appearance (aging, sunglasses, make-ups, skin color, expression etc.). Similarly, FDDB dataset <ref type="bibr" target="#b15">[15]</ref> consists of 5171 annotated faces with 2846 images and contains occluded, out-of-focus, and low resolution faces. For evaluation, we used the toolbox provide by <ref type="bibr" target="#b22">[22]</ref> with corrected annotations for PASCAL Face and AFW datasets and the original annotations of FDDB dataset.</p><p>We started by finding the optimal number of scales for the proposed detector using PASCAL dataset. We upscaled images by factor of 5 to detect faces as small as 227/5 = 45 pixels. We then down scaled the image with by a factor, fs, and repeated the process until the minimum image dimension is less than 227 pixels. For the choice of fs, we chose fs ∈ { √ 0.5 = 0.7071, 3 √ 0.5 = 0.7937, 5 √ 0.5 = 0.8706, <ref type="bibr" target="#b6">7</ref> √ 0.5 = 0.9056}; <ref type="figure" target="#fig_4">Figure 5</ref> shows the effect of this parameter on the precision and recall of our face detector (DDFD). Decreasing fs allows the detector to scan the image finer and increases the computational time. According to <ref type="figure" target="#fig_4">Figure 5</ref>, it seems that these choices of fs has little impact on the performance of the detector. Surprisingly, fs = 3 √ 0.5 seems to have slightly better performance although it does <ref type="figure">Figure 3</ref>: A set of faces with different out-of-plane rotations and occlusions. The figure also shows output of our proposed face detector after NMS along with the corresponding confidence score for each detection.</p><p>not scan the image as thorough as fs = 5 √ 0.5 or fs = 7 √ 0.5. Based on this experiment we use fs = 3 √ 0.5 for the rest of this paper.</p><p>Another component of our system is the non-maximum suppression module (NMS). For this we evaluated two different strategies:</p><p>• NMS-max: we find the window of the maximum score and remove all of the bounding-boxes with an IOU (intersection over union) larger than an overlap threshold.</p><p>• NMS-avg: we first filter out windows with confidence lower than 0.2. We then use groupRectangles function of OpenCV to cluster the detected windows according to an overlap threshold. Within each cluster, we then removed all windows with score less than 90% of the maximum score of that cluster. Next we averaged the locations of the remaining bounding-boxes to get the detection window. Finally, we used the maximum score of the cluster as the score of the proposed detection.</p><p>We tested both strategies and <ref type="figure" target="#fig_5">Figure 6</ref> shows the performance of each strategy for different overlap thresholds. As shown in this figure, performance of both methods vary significantly with the overlap threshold. An overlap threshold of 0.3 gives the best performance for NMS-max while, for NMS-avg 0.2 performs the best. According to this figure, NMS-avg has better performance compared to NMS-max in terms of average precision.  Finally, we examined the effect of a bounding-box regression module for improving detector localization. The idea is to train regressors to predict the difference between the locations of the predicted bounding-box and the ground truth. At the test time these regressors can be used to estimate the location difference and adjust the predicted bounding-boxes accordingly. This idea has been shown to improve localization performance in several methods including <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b3">4]</ref>. To train our bounding-box regressors, we followed the algorithm of <ref type="bibr" target="#b9">[9]</ref> and <ref type="figure" target="#fig_6">Figure 7</ref> shows the performance of our detector with and without this module. As shown in this figure, surprisingly, adding a bounding-box regressors degrades the performance for both NMS strategies. Our analysis revealed that this is due to the mismatch between the annotations of the training set and the test set. This mismatch is mostly for side-view faces and is illustrated in <ref type="figure" target="#fig_7">Figure 8</ref>. In addition to degrading performance of bounding-box regression module, this mismatch also leads to false miss-detections in the evaluation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison with R-CNN</head><p>R-CNN <ref type="bibr" target="#b9">[9]</ref> is one of the current state-of-the-art methods for object detection. In this section we compare our proposed detector with R-CNN and its variants.</p><p>We started by fine-tuning AlexNet for face detection using the process described in section 2. We then trained a SVM  classifier for face classification using output of the seventh layer (f c7 features). We also trained a bounding-box regression unit to further improve the results and used NMS-max for final localization. We repeated this experiment on a version of AlexNet that is fine-tuned for PASCAL VOC 2012 dataset and is provided with R-CNN code. <ref type="figure" target="#fig_9">Figure 9</ref> compares the performance of our detector with different NMS strategies along with the performance of R-CNN methods with and without bounding-box regression. As shown in this figure, it is not surprising that performance of the detectors with AlexNet fine-tuned for faces (Face-FT) are better than the ones that are fine-tuned with PASCAL-VOC objects (VOC-FT). In addition, it seems that bounding-box regression can significantly improve R-CNN performance. However, even the best R-CNN classifier has significantly inferior performance compared to our proposed face detector independent of the NMS strategy. We believe the inferior performance of R-CNN are due to 1) the loss of recall since selective search may miss some of face regions and 2) loss in localization since bounding-box regression is not perfect and may not be able to fully align the segmentation bounding-  boxes, provided by selective search <ref type="bibr" target="#b36">[36]</ref>, with the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparisons with state-of-the-art</head><p>In this section we compare the performance of our proposed detector with other state-of-the-art face detectors using publicly available datasets of PASCAL faces <ref type="bibr" target="#b42">[42]</ref>, AFW <ref type="bibr" target="#b25">[25]</ref> and FDDB <ref type="bibr" target="#b15">[15]</ref>. In particular, we compare our method with 1) DPM-based methods such as structural model <ref type="bibr" target="#b42">[42]</ref> and TSM <ref type="bibr" target="#b25">[25]</ref> and 2) cascade-based method such as head hunter <ref type="bibr" target="#b22">[22]</ref>. <ref type="figure" target="#fig_8">Figures 10 and 11</ref> illustrate this comparison. Note that these comparison are not completely fair as most of the other methods such as DPM or HeadHunter use extra information of view point annotation during the training. As shown in these figures our single model face detector was able to achieve similar or better results compared to the other state-of-the-art methods, without using pose annotation or information about facial landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we proposed a face detection method based on deep learning, called Deep Dense Face Detector (DDFD). The proposed method does not require pose/landmark annotation and is able to detect faces in a wide range of ori-  entations using a single model. In addition, DDFD is independent of common modules in recent deep learning object detection methods such as bounding-box regression, SVM, or image segmentation. We compared the proposed method with R-CNN and other face detection methods that are developed specifically for multi-view face detection e.g. cascadebased and DPM-based. We showed that our detector is able to achieve similar or better results even without using pose annotation or information about facial landmarks. Finally, we analyzed performance of our proposed face detector on a variety of face images and found that there seems to be a correlation between distribution of positive examples in the training set and scores of the proposed detector. In future we are planning to use better sampling strategies and more sophisticated data augmentation techniques to further improve performance of the proposed method for detecting occluded and rotated faces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>xi. The sampling method for selecting examples in B can</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>left) an example image with faces in different in-plane rotations. It also shows output of our proposed face detector after NMS along with corresponding confidence score for each detection. right) heat-map for the response of DDFD scores over the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Histogram of faces in AFLW dataset based on their top) in-plane, middle) pitch (up and down) and bottom) yaw(left to right) rotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Effect of scaling factor on precision and recall of the detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Effect of different NMS strategies and their overlap thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Performance of the proposed face detector with and without bounding-box regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Annotation of a side face in left) training set and right) test set. The red bounding-box is the predicted bounding-box by our proposed detector. This detection is counted as a false positive as its IOU with ground truth is less than 50%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Comparison of different face detectors on left) PASCAL faces and right) AFW dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Comparison of our face detector, DDFD, with different R-CNN face detectors.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comparison of different face detectors on FDDB dataset. Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Neural Architecture for Fast and Robust Face Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE-IAPR International Conference on Pattern Recognition</title>
		<meeting>IEEE-IAPR International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training Convolutional Filters for Robust Face Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Workshop of Neural Networks for Signal Processing</title>
		<meeting>IEEE International Workshop of Neural Networks for Signal Processing</meeting>
		<imprint>
			<date type="published" when="2003-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional face finder: a neural architecture for fast and robust face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delakis</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A high-performance face detection system using openmp. Concurrency and Computation: Practice and Experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hadjidoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Dimakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vector boosting for rotation invariant multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-performance rotation invariant multiview face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Densenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.1869</idno>
		<title level="m">Implementing efficient convnet descriptor pyramids</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M R</forename><surname>Martin Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<meeting>IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Synergistic face detection and pose estimation with energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Synergistic face detection and pose estimation with energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Embedded convolutional face finder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Multimedia and Expo</title>
		<meeting>IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-resolution cascades for multiclass object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scalable, high-quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Scalable, high-quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sharing visual features for multiclass and multiview object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An original approach for the localisation of objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monrocq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Neural Networks</title>
		<meeting>International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Original approach for the localisation of objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monrocq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image, and Signal Processing</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>IEE Proc on Vision</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast multi-view face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast rotation invariant multi-view face detection based on real adaboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Face detection by structural models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
