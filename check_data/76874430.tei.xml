<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Let&apos;s Talk About Storage &amp; Recovery Methods for Non-Volatile Memory Database Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Arulraj</surname></persName>
							<email>jarulraj@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Carnegie Mellon University Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Pavlo</surname></persName>
							<email>pavlo@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Carnegie Mellon University Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subramanya</forename><forename type="middle">R</forename><surname>Dulloor</surname></persName>
							<email>subramanya.r.dulloor@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Carnegie Mellon University Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Let&apos;s Talk About Storage &amp; Recovery Methods for Non-Volatile Memory Database Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2723372.2749441</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>The advent of non-volatile memory (NVM) will fundamentally change the dichotomy between memory and durable storage in database management systems (DBMSs). These new NVM devices are almost as fast as DRAM, but all writes to it are potentially persistent even after power loss. Existing DBMSs are unable to take full advantage of this technology because their internal architectures are predicated on the assumption that memory is volatile. With NVM, many of the components of legacy DBMSs are unnecessary and will degrade the performance of data intensive applications. To better understand these issues, we implemented three engines in a modular DBMS testbed that are based on different storage management architectures: (1) in-place updates, (2) copy-on-write updates, and (3) log-structured updates. We then present NVMaware variants of these architectures that leverage the persistence and byte-addressability properties of NVM in their storage and recovery methods. Our experimental evaluation on an NVM hardware emulator shows that these engines achieve up to 5.5× higher throughput than their traditional counterparts while reducing the amount of wear due to write operations by up to 2×. We also demonstrate that our NVM-aware recovery protocols allow these engines to recover almost instantaneously after the DBMS restarts.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Changes in computer trends have given rise to new on-line transaction processing (OLTP) applications that support a large number of concurrent users and systems. What makes these modern applications unlike their predecessors is the scale in which they ingest information <ref type="bibr" target="#b38">[41]</ref>. Database management systems (DBMSs) are the critical component of these applications because they are responsible for ensuring transactions' operations execute in the correct order and that their changes are not lost after a crash. Optimizing the DBMS's performance is important because it determines how quickly an application can take in new information and how quickly it can use it to make new decisions. This performance is affected by how fast the system can read and write data from storage.</p><p>DBMSs have always dealt with the trade-off between volatile and non-volatile storage devices. In order to retain data after a loss of power, the DBMS must write that data to a non-volatile device, such as a SSD or HDD. Such devices only support slow, bulk data transfers as blocks. Contrast this with volatile DRAM, where a DBMS can quickly read and write a single byte from these devices, but all data is lost once power is lost.</p><p>In addition, there are inherent physical limitations that prevent DRAM from scaling to capacities beyond today's levels <ref type="bibr" target="#b43">[46]</ref>. Using a large amount of DRAM also consumes a lot of energy since it requires periodic refreshing to preserve data even if it is not actively used. Studies have shown that DRAM consumes about 40% of the overall power consumed by a server <ref type="bibr" target="#b39">[42]</ref>.</p><p>Although flash-based SSDs have better storage capacities and use less energy than DRAM, they have other issues that make them less than ideal. For example, they are much slower than DRAM and only support unwieldy block-based access methods. This means that if a transaction updates a single byte of data stored on an SSD, then the DBMS must write the change out as a block (typically 4 KB). This is problematic for OLTP applications that make many small changes to the database because these devices only support a limited number of writes per address <ref type="bibr" target="#b63">[66]</ref>. Shrinking SSDs to smaller sizes also degrades their reliability and increases interference effects. Stop-gap solutions, such as battery-backed DRAM caches, help mitigate the performance difference but do not resolve these other problems <ref type="bibr" target="#b8">[11]</ref>.</p><p>Non-volatile memory (NVM) <ref type="bibr" target="#b0">1</ref> offers an intriguing blend of the two storage mediums. NVM is a broad class of technologies, including phase-change memory <ref type="bibr" target="#b52">[55]</ref>, memristors <ref type="bibr" target="#b57">[60]</ref>, and STT-MRAM <ref type="bibr" target="#b23">[26]</ref> that provide low latency reads and writes on the same order of magnitude as DRAM, but with persistent writes and large storage capacity like a SSD <ref type="bibr" target="#b10">[13]</ref>. <ref type="table" target="#tab_1">Table 1</ref> compares the characteristics of NVM with other storage technologies.</p><p>It is unclear at this point, however, how to best leverage these new technologies in a DBMS. There are several aspects of NVM that make existing DBMS architectures inappropriate for them <ref type="bibr" target="#b11">[14,</ref><ref type="bibr" target="#b20">23]</ref>. For example, disk-oriented DBMSs (e.g., Oracle RDBMS, IBM DB2, MySQL) are predicated on using block-oriented devices for durable storage that are slow at random access. As such, they maintain an in-memory cache for blocks of tuples and try to maximize the amount of sequential reads and writes to storage. In the case of memory-oriented DBMSs (e.g., VoltDB, MemSQL), they contain certain components to overcome the volatility of DRAM. Such components may be unnecessary in a system with byte-addressable NVM with fast random access.</p><p>In this paper, we evaluate different storage and recovery methods for OLTP DBMSs from the ground-up, starting with an NVM-only storage hierarchy. We implemented three storage engine architectures in a single DBMS: <ref type="bibr" target="#b0">(1)</ref> in-place updates with logging, (2) copyon-write updates without logging, and (3) log-structured updates.</p><p>NVM is also referred to as storage-class memory or persistent memory.   <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b51">54,</ref><ref type="bibr" target="#b46">49]</ref>: phase-change memory (PCM) <ref type="bibr" target="#b52">[55]</ref>, memristors (RRAM) <ref type="bibr" target="#b57">[60]</ref>, and STT-MRAM (MRAM) <ref type="bibr" target="#b23">[26]</ref>.</p><p>We then developed optimized variants for these approaches that reduce the computational overhead, storage footprint, and wear-out of NVM devices. For our evaluation, we use a hardware-based emulator where the system only has NVM and volatile CPU-level caches (i.e., no DRAM). Our analysis shows that the NVM-optimized storage engines improve the DBMS's throughput by a factor of 5.5× while reducing the number of writes to NVM in half. These results also suggest that NVM-optimized in-place updates is the ideal method as it has lowest overhead, causes minimal wear on the device, and allows the DBMS to restart almost instantaneously. Our work differs from previous studies because we evaluate a DBMS using a single-tier storage hierarchy. Others have only employed NVM for logging <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b62">65]</ref> or used a two-level hierarchy with DRAM and NVM <ref type="bibr" target="#b50">[53]</ref>. We note that it is possible today to replace DRAM with NV-DIMM <ref type="bibr" target="#b3">[6]</ref>, and run an NVM-only DBMS unmodified on this storage hierarchy. This enables us to achieve performance similar to that obtained on a DRAM and NVM storage hierarchy, while avoiding the overhead of dealing with the volatility of DRAM. Further, some NVM technologies, such as STT-RAM <ref type="bibr" target="#b23">[26]</ref>, are expected to deliver lower read and write latencies than DRAM. NVM-only DBMSs would be a good fit for these technologies.</p><p>The remainder of this paper is organized as follows. We begin in Section 2 with a discussion of NVM and why it necessitates a re-evaluation of DBMS storage architectures. Next, in Section 3 we describe our DBMS testbed and its storage engines that we developed for this study. We then present in Section 4 our optimizations for these engines that leverage NVM's unique properties. We then present our experimental evaluation in Section 5. We conclude with a discussion of related work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>We now provide an overview of emerging NVM technologies and discuss the hardware emulator platform that we use in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation</head><p>There are essentially two types of DBMS architectures: diskoriented and memory-oriented systems <ref type="bibr" target="#b20">[23]</ref>. The former is exemplified by the first DBMSs, such as IBM's System R <ref type="bibr" target="#b4">[7]</ref>, where the system is predicated on the management of blocks of tuples on disk using an in-memory cache; the latter by IBM's IMS/VS Fast Path <ref type="bibr" target="#b29">[32]</ref>, where the system performs updates on in-memory data and relies on the disk to ensure durability. The need to ensure that all changes are durable has dominated the design of systems with both types of architectures. This has involved optimizing the layout of data for each storage layer depending on how fast it can perform random accesses <ref type="bibr" target="#b28">[31]</ref>. Further, updates performed on tuples stored in memory need to be propagated to an on-disk representation for durability. Previous studies have shown that the overhead of managing this data movement for OLTP workloads is considerable <ref type="bibr" target="#b32">[35]</ref>.</p><p>NVM technologies, like phase-change memory <ref type="bibr" target="#b52">[55]</ref> and memristors <ref type="bibr" target="#b57">[60]</ref>, remove these tuple transformation and propagation costs through byte-addressable loads and stores with low latency. This means that they can be used for efficient architectures that are used in memory-oriented DBMSs <ref type="bibr" target="#b22">[25]</ref>. But unlike DRAM, all writes to the NVM are potentially durable and therefore a DBMS can access the tuples directly in the NVM after a restart or crash without needing to reload the database first.</p><p>Although the advantages of NVM are obvious, making full use of them in an OLTP DBMS is non-trivial. Previous work that compared disk-oriented and memory-oriented DBMSs for NVM showed that the two architectures achieve almost the same performance when using NVM because of the overhead of logging <ref type="bibr" target="#b20">[23]</ref>. This is because current DBMSs assume that memory is volatile, and thus their architectures are predicated on making redundant copies of changes on durable storage. Thus, we seek to understand the characteristics of different storage and recovery methods from the ground-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NVM Hardware Emulator</head><p>NVM storage devices are currently prohibitively expensive and only support small capacities. For this reason, we use a NVM hardware emulator developed by Intel Labs <ref type="bibr" target="#b24">[27]</ref> in this paper. The emulator supports tunable read latencies and read/write bandwidths. This enables us to evaluate multiple hardware profiles that are not specific to a particular NVM technology. Unlike NVM simulators, like PCM-SIM <ref type="bibr" target="#b41">[44]</ref>, this emulator enables us to better understand the impact of cache evictions, prefetching, and speculative execution on long-running workloads. The hardware emulator is based on a dualsocket Intel Xeon platform. Each CPU supports four DDR3 channels with two DIMMs per channel. Half of the memory channels on each processor are reserved for the emulated NVM while the rest are used for regular memory. The emulator's custom BIOS firmware partitions the physical memory address space into separate address spaces for DRAM and emulated NVM.</p><p>NVM technologies have higher read and write latency than DRAM. We are able to emulate the latency for the NVM partition by using custom CPU microcode. The microcode estimates the additional cycles that the CPU would have to wait if DRAM is replaced by slower NVM and then stalls the CPU for those cycles. The accuracy of the latency emulation model was validated by comparing the performance of several applications on emulated NVM and slower NUMA memory <ref type="bibr" target="#b24">[27]</ref>. Since the CPU uses a write-back cache for NVM, the high latency of writes to NVM is not observed on every write but the sustainable write bandwidth of NVM is lower compared to DRAM <ref type="bibr" target="#b35">[38]</ref>. The emulator allows us to control the write bandwidth by limiting the number of DDR operations performed per microsecond. It is currently not able to independently vary the read and write bandwidths as this throttling feature affects all DDR operations. This limitation is not an issue for this evaluation as OLTP workloads are not bandwidth intensive <ref type="bibr" target="#b62">[65]</ref>.</p><p>The emulator exposes two interfaces to access the NVM storage:</p><p>Allocator Interface: The emulator uses a NVM-aware memory allocator that provides the POSIX malloc interface. Applications obtain and use NVM directly using this interface. Internally, libnuma library <ref type="bibr" target="#b2">[4]</ref> is used to ensure that the application allocates memory only from emulated NVM and not regular DRAM. When the DBMS writes to a memory location, the written (dirty) data may reside indefinitely in the volatile CPU caches and not propagate immediately to NVM. We use a hardware write barrier primitive (SFENCE) to guarantee the durability of writes to NVM when they are flushed from CPU caches.</p><p>Filesystem Interface: The emulator exposes a NVM-backed volume to the OS through a special filesystem that is optimized for non-volatile memory <ref type="bibr" target="#b24">[27]</ref>. This allows applications to use the POSIX filesystem interface to read/write data to files stored on  NVM. Normally, in a block-oriented filesystem, file I/O requires two copies; one involving the block device and another involving the user buffer. The emulator's optimized filesystem, however, requires only one copy between the file and the user buffers. This improves the file I/O performance by 7-10× compared to block-oriented filesystems like EXT4. The filesystem interface allows existing DBMSs to make use of NVM for durable storage.</p><p>Both of the above interfaces use memory from the emulated NVM. The key difference, however, is that the filesystem interface supports a naming mechanism that ensures that file offsets are valid after the system restarts. The downside of the filesystem interface is that it requires the application's writes to go through the kernel's virtual filesystem (VFS) layer. In contrast, when the application uses the allocator interface, it can write to and read from NVM directly within userspace. However, the allocator interface does not automatically provide a naming mechanism that is valid after a system restart. We use a memory allocator that is designed for NVM to overcome this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">NVM-aware Memory Allocator</head><p>An NVM-aware memory allocator for a DBMS needs to satisfy two key requirements. The first is that it should provide a durability mechanism to ensure that modifications to data stored on NVM are persisted. This is necessary because the changes made by a transaction to a location on NVM may still reside in volatile CPU caches when the transaction commits. If a power failure happens before the changes reach NVM, then these changes are lost. The allocator exposes a special API call to provide this durability mechanism. Internally, the allocator first writes back the cache lines containing the data from any level of the cache hierarchy to NVM using CLFLUSH <ref type="bibr" target="#b1">[2]</ref> instruction. Then, it issues a SFENCE instruction to ensure that the data flushed from the CPU caches becomes durable. Otherwise, this data might still be buffered in the memory controller and lost in case of a power failure. From here on, we refer to the above mentioned durability mechanism as the sync primitive.</p><p>The second requirement is that it should provide a naming mechanism for allocations so that pointers to locations in memory are valid even after the system restarts. The allocator ensures that the virtual memory addresses assigned to a memory-mapped region never change. With this mechanism, a pointer to a NVM location is mapped to the same virtual location after the OS or DBMS restarts. We refer to these pointers as non-volatile pointers <ref type="bibr" target="#b48">[51]</ref>.</p><p>The NVM allocator that we use in our evaluation is based on the open-source NVM-related libpmem library <ref type="bibr" target="#b55">[58]</ref>. We extended this allocator to follow a rotating best-fit allocation policy and to support multi-threaded usage. The allocator directly maps the NVM to its address space. Unlike the filesystem interface, accessing a region of memory obtained from this allocator does not require copying data to user buffers. After an OS restart, the allocator reclaims memory that has not been persisted and restores its internal metadata to a  consistent state. This recovery mechanism is required only after the OS restarts and not after the DBMS restarts, because the allocator handles memory management for all applications.</p><p>To show that accessing NVM through the allocator interface is faster than using the filesystem interface, we compare them using a micro-benchmark. In this experiment, the application performs durable writes to NVM using the two interfaces with sequential and random access patterns. The application performs durable writes using the filesystem's fsync system call and the allocator's sync primitive. We vary the size of the data chunk that the application writes from 1 to 256 bytes. The results in <ref type="figure" target="#fig_1">Fig. 1</ref> show that NVMaware allocator delivers 10-12× higher write bandwidth than the filesystem. The performance gap is more evident when the application writes out small data chunks sequentially. We also note that the gap between sequential and random write bandwidth is lower than that observed in other durable storage technologies.</p><p>We now describe the DBMS testbed that we built to evaluate storage and recovery methods for DBMSs running on NVM. As we discuss in subsequent sections, we use non-volatile pointers to build non-volatile data structures that are guaranteed to be consistent after the OS or DBMS restarts. These data structures, along with the allocator interface, are used to build NVM-aware storage engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DBMS TESTBED</head><p>We developed a lightweight DBMS to evaluate different storage architecture designs for OLTP workloads. We did not use an existing DBMS as that would require significant changes to incorporate the storage engines into a single system. Although some DBMSs support a pluggable storage engine back-end (e.g., MySQL, MongoDB), modifying them to support NVM would still require significant changes. We also did not want to taint our measurements with features that are not relevant to our evaluation.</p><p>The architecture of the testbed running on the hardware emulator is depicted in <ref type="figure" target="#fig_3">Fig. 2</ref>. The DBMS's internal coordinator receives incoming transaction requests from the application and then invokes the target stored procedure. As a transaction executes in the system, it invokes queries to read and write tuples from the database. These requests are passed through a query executor that invokes the necessary operations on the DBMS's active storage engine.</p><p>The DBMS uses pthreads to allow multiple transactions to run concurrently in separate worker threads. It executes as a single process with the number of worker threads equal to the number of cores, where each thread is mapped to a different core. Since we do not want the DBMS's concurrency control scheme to interfere with our evaluation, we partition the database and use a lightweight   <ref type="figure">Figure 3</ref>: Architectural layout of the three traditional storage engines supported in the DBMS testbed. The engine components accessed using the allocator interface and those accessed using the filesystem interface are bifurcated by the dashed line.</p><p>locking scheme where transactions are executed serially at each partition based on timestamp ordering <ref type="bibr" target="#b56">[59]</ref>. Using a DBMS that supports a pluggable back-end allows us to compare the performance characteristics of different storage and recovery methods on a single platform. We implemented three storage engines that use different approaches for supporting durable updates to a database: (1) inplace updates engine, (2) copy-on-write updates engine, and (3) log-structured updates engine. Each engine also supports both primary and secondary indexes. We now describe these engines in detail. For each engine, we first discuss how they apply changes made by transactions to the database and then how they ensure durability after a crash. All of these engines are based on the architectures found in state-of-the-art DBMSs. That is, they use memory obtained using the allocator interface as volatile memory and do not exploit NVM's persistence. Later in Section 4, we present our improved variants of these engines that are optimized for NVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">In-Place Updates Engine (InP)</head><p>The first engine uses the most common storage engine strategy in DBMSs. With in-place updates, there is only a single version of each tuple at all times. When a transaction updates a field for an existing tuple, the system writes the new value directly on top of the original one. This is the most efficient method of applying changes, since the engine does not make a copy of the tuple first before updating it and only the updated fields are modified. The design of this engine is based on VoltDB [5], which is a memoryoriented DBMS that does not contain legacy disk-oriented DBMS components like a buffer pool. The InP engine uses the STX B+tree library for all of its indexes <ref type="bibr" target="#b7">[10]</ref>.</p><p>Storage: <ref type="figure">Fig. 3a</ref> illustrates the architecture of the InP engine. The storage area for tables is split into separate pools for fixed-sized blocks and variable-length blocks. Each block consists of a set of slots. The InP engine stores the table's tuples in fixed-size slots. This ensures that the tuples are byte-aligned and the engine can easily compute their offsets. Any field in a table that is larger than 8 bytes is stored separately in a variable-length slot. The 8-byte location of that slot is stored in that field's location in the tuple.</p><p>The tables' tuples are unsorted within these blocks. For each table, the DBMS maintains a list of unoccupied tuple slots. When a transaction deletes a tuple, the deleted tuple's slot is added to this pool. When a transaction inserts a tuple into a table, the engine first checks the table's pool for an available slot. If the pool is empty, then the engine allocates a new fixed-size block using the allocator interface. The engine also uses the allocator interface to maintain the indexes and stores them in memory.</p><p>Recovery: Since the changes made by transactions committed after the last checkpoint are not written to "durable" storage, the InP engine maintains a durable write-ahead log (WAL) in the filesystem to assist in recovery from crashes and power failures. WAL records the transactions' changes before they are applied to DBMS <ref type="bibr" target="#b26">[29]</ref>. As transactions execute queries that modify the database, the engine appends a new entry to the WAL for those changes. Each entry contains the transaction identifier, the table modified, the tuple identifier, and the before/after tuple images depending on the operation.</p><p>The most well-known recovery protocol for in-place updates is ARIES <ref type="bibr" target="#b44">[47]</ref>. With ARIES, the engine periodically takes checkpoints that are stored on the filesystem to bound recovery latency and reduce the storage space consumed by the log. In our implementation, we compress (gzip) the checkpoints on the filesystem to reduce their storage footprint on NVM. During recovery, the engine first loads the last checkpoint. It then replays the log to ensure that the changes made by transactions committed since the checkpoint are present in the database. Changes made by uncommitted transactions at the time of failure are not propagated to the database. The InP engine uses a variant of ARIES that is adapted for main memory DBMSs with a byte-addressable storage engine <ref type="bibr" target="#b42">[45]</ref>. As we do not store physical changes to indexes in this log, all of the tables' indexes are rebuilt during recovery because they may have been corrupted <ref type="bibr" target="#b42">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Copy-on-Write Updates Engine (CoW)</head><p>The second storage engine performs copy-on-write updates where instead of modifying the original tuple, it creates a copy of the tuple and then modifies that copy. As the CoW engine never overwrites committed data, it does not need to record changes in a WAL for recovery. The CoW engine instead uses different look-up directories for accessing the versions of tuples in the database. With this approach, known as shadow paging in IBM's System R <ref type="bibr" target="#b30">[33]</ref>, the DBMS maintains two look-up directories at all times: (1) the current directory, and (2) the dirty directory. The current directory points to the most recent versions of the tuples and only contains the effects of committed transactions. The dirty directory points to the versions of tuples being modified by active transactions. To ensure that the transactions are isolated from the effects of uncommitted transactions, the engine maintains a master record that always points to the current directory. <ref type="figure">Fig. 3b</ref> presents the architecture of the CoW engine. After applying the changes on the copy of the tuple, the engine updates the dirty directory to point to the new version of the tuple. When the transaction commits, the engine updates the master record atomically to point to the dirty directory. The engine maintains an internal page cache to keep the hot pages in memory.</p><p>System R implements shadow paging by copying the current directory to create the new dirty directory after every commit operation. But, creating the dirty directory in this manner incurs high I/O overhead. The CoW engine uses LMDB's copy-on-write B+trees <ref type="bibr" target="#b53">[56,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b33">36]</ref> to implement shadow paging efficiently. <ref type="figure">Fig. 3b</ref> illustrates an update operation on a CoW B+tree. When the engine modifies the leaf node 4 in the current directory, it only needs to make a copy of the internal nodes lying along the path from that leaf node up to the root of the current version. The current and dirty directories of the copy-on-write B+tree share the rest of the tree. This significantly reduces the I/O overhead of creating the dirty directory as only a fraction of the B+tree is copied. To further reduce the overhead of shadow paging, the CoW engine uses a group commit mechanism that batches the changes made by a group of transactions before committing the dirty directory.</p><p>Storage: The CoW engine stores the directories on the filesystem. The tuples in each table are stored in a HDD/SSD-optimized format where all the tuple's fields are inlined. This avoids expensive random accesses that are required when some fields are not inlined. Each database is stored in a separate file and the master record for the database is located at a fixed offset within the file. It supports secondary indexes as a mapping of secondary keys to primary keys.</p><p>The downside of the CoW engine is that it creates a new copy of tuple even if a transaction only modifies a subset of the tuple's fields. The engine also needs to keep track of references to tuples from different versions of the copy-on-write B+tree so that it can reclaim the storage space consumed by old unreferenced tuple versions <ref type="bibr" target="#b6">[9]</ref>. As we show in Section 5.3, this engine has high write amplification (i.e., the amount of data written to storage is much higher compared to the amount of data written by the application). This increases wear on the NVM device thereby reducing its lifetime.</p><p>Recovery: If the DBMS crashes before the master record is updated, then the changes present in the dirty directory are not visible after restart. Hence, there is no recovery process for the CoW engine. When the DBMS comes back on-line, the master record points to the current directory that is guaranteed to be consistent. The dirty directory is garbage collected asynchronously, since it only contains the changes of uncommitted transactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Log-structured Updates Engine (Log)</head><p>Lastly, the third storage engine uses a log-structured update policy. This approach originated from log-structured filesystems <ref type="bibr" target="#b54">[57]</ref>, and then it was adapted to DBMSs as log-structured merge (LSM) trees <ref type="bibr" target="#b47">[50]</ref> for write-intensive workloads. The LSM tree consists of a collection of runs of data. Each run contains an ordered set of entries that record the changes performed on tuples. Runs reside either in volatile memory (i.e., MemTable) or on durable storage (i.e., SSTables) with their storage layout optimized for the underlying storage device. The LSM tree reduces write amplification by batching the updates in MemTable and periodically cascading the changes to durable storage <ref type="bibr" target="#b47">[50]</ref>. The design for our Log engine is based on Google's LevelDB <ref type="bibr" target="#b19">[22]</ref>, which implements the log-structured update policy using LSM trees.</p><p>Storage: <ref type="figure">Fig. 3c</ref> depicts the architecture of the Log engine. The Log engine uses a leveled LSM tree <ref type="bibr" target="#b37">[40]</ref>, where each level in the tree contains the changes for a single run. The data starts from the MemTable stored in the topmost level and propagates down to SSTables stored in lower parts of the tree over time. The size of the run stored in a given level is k times larger than that of the run stored in its parent, where k is the growth factor of the tree. The Log engine allows us to control the size of the MemTable and the growth factor of the tree. It first stores the tuple modifications in a memoryoptimized format using the allocator interface in the MemTable. The MemTable contains indexes to handle point and range queries efficiently. When the size of the MemTable exceeds a threshold, the engine flushes it to the filesystem as an immutable SSTable stored in a separate file. The Log engine also constructs a Bloom filter <ref type="bibr" target="#b9">[12]</ref> for each SSTable to quickly determine at runtime whether it contains entries associated with a tuple to avoid unnecessary index look-ups.</p><p>The contents of the MemTable are lost after system restart. Hence, to ensure durability, the Log engine maintains a WAL in the filesystem. The engine first records the changes in the log and then applies the changes to the MemTable. The log entry contains the transaction identifier, the table modified, the tuple identifier, and the before/after images of the tuple depending on the type of operation. To reduce the I/O overhead, the engine batches log entries for a group of transactions and flushes them together.</p><p>The log-structured update approach performs well for writeintensive workloads as it reduces random writes to durable storage. The downside of the Log engine is that it incurs high read amplification (i.e., the number of reads required to fetch the data is much higher than that actually needed by the application). To retrieve a tuple, the Log engine first needs to look-up the indexes of all the runs of the LSM tree that contain entries associated with the desired tuple in order to reconstruct the tuple <ref type="bibr" target="#b0">[1]</ref>. To reduce this read amplification, the Log engine performs a periodic compaction process that merges a subset of SSTables. First, the entries associated with a tuple in different SSTables are merged into one entry in a new SSTable. Tombstone entries are used to identify purged tuples. Then, the engine builds indexes for the new SSTable.</p><p>Recovery: During recovery, the Log engine rebuilds the MemTable using the WAL, as the changes contained in it were not written onto durable storage. It first replays the log to ensure that the changes made by committed transactions are present. It then removes any changes performed by uncommitted transactions, thereby bringing the MemTable to a consistent state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NVM-AWARE ENGINES</head><p>All of the engines described above are derived from existing DBMS architectures that are predicated on a two-tier storage hierarchy comprised of volatile DRAM and a non-volatile HDD/SSD. These storage devices have distinct hardware constraints and performance properties <ref type="bibr" target="#b51">[54]</ref>. First, the read and write latency of nonvolatile storage is several orders of magnitude higher than DRAM. Second, the DBMS accesses data on non-volatile storage at blockgranularity, while with DRAM it accesses data at byte-granularity. Third, the performance gap between sequential and random accesses is greater for non-volatile storage compared to DRAM.</p><p>The traditional engines were designed to account for and reduce the impact of these differences. For example, they maintain two layouts of tuples depending on the storage device. Tuples stored in memory can contain non-inlined fields because DRAM is byteaddressable and handles random accesses efficiently. In contrast, fields in tuples stored on durable storage are inlined to avoid random accesses because they are more expensive. To amortize the overhead for accessing durable storage, these engines batch writes and flush them in a deferred manner.</p><p>Many of these techniques, however, are unnecessary in a system with a NVM-only storage hierarchy <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b46">49]</ref>. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the access latencies of NVM are orders of magnitude shorter than that of HDDs and SSDs. Further, the performance gap between sequential and random accesses on NVM is comparable to that of DRAM. We therefore adapt the storage and recovery mechanisms of these traditional engines to exploit NVM's characteristics. We refer to these optimized storage engines as the NVM-aware engines. As we show in our evaluation in Section 5, these engines deliver higher throughput than their traditional counterparts while still ensuring durability. They reduce write amplification using NVM's persistence thereby expanding the lifetime of the NVM device. These engines use only the allocator interface described in Section 2.3 with NVM-optimized data structures <ref type="bibr" target="#b46">[49,</ref><ref type="bibr" target="#b59">62]</ref>. <ref type="table" target="#tab_4">Table 2</ref> presents an overview of the steps performed by the NVMaware storage engines, while executing the primitive database operations. We note that the engine performs these operations within the context of a transaction. For instance, if the transaction aborts while executing an operation, it must undo the effects of any earlier operation performed by the transaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">In-Place Updates Engine (NVM-InP)</head><p>One of the main problems with the InP engine described in Section 3.1 is that it has high rate of data duplication. When a transaction inserts a tuple, the engine records the tuple's contents in the WAL and then again in the table storage area. The InP engine's logging infrastructure also assumes that the system's durable storage device has orders of magnitude higher write latency compared to DRAM. It therefore batches multiple log records and flushes them periodically to the WAL using sequential writes. This approach, however, increases the mean response latency as transactions need to wait for the group commit operation.</p><p>Given this, we designed the NVM-InP engine to avoid these issues. Now when a transaction inserts a tuple, rather than copying the tuple to the WAL, the NVM-InP engine only records a nonvolatile pointer to the tuple in the WAL. This is sufficient because both the pointer and the tuple referred to by the pointer are stored on NVM. Thus, the engine can use the pointer to access the tuple after the system restarts without needing to re-apply changes in the WAL. It also stores indexes as non-volatile B+trees that can be accessed immediately when the system restarts without rebuilding.</p><p>Storage: The architecture of the NVM-InP engine is shown in <ref type="figure" target="#fig_5">Fig. 4a</ref> and <ref type="table" target="#tab_4">Table 2</ref> presents an overview of the steps to perform different operations. The engine stores tuples and non-inlined fields using fixed-size and variable-length slots, respectively. To reclaim the storage space of tuples and non-inlined fields inserted by uncommitted transactions after the system restarts, the NVM-InP engine maintains durability state in each slot's header. A slot can be in one of three states -unallocated, allocated but not persisted, or persisted. After the system restarts, slots that are allocated but not persisted transition back to unallocated state.</p><p>The NVM-InP engine stores the WAL as a non-volatile linked list. It appends new entries to the list using an atomic write. Each entry contains the transaction identifier, the table modified, the tuple identifier, and pointers to the operation's changes. The changes include tuple pointers for insert operation and field pointers for update oper-ations on non-inlined fields. The engine persists this entry before updating the slot's state as persisted. If it does not ensure this ordering, then the engine cannot reclaim the storage space consumed by uncommitted transactions after the system restarts, thereby causing non-volatile memory leaks. After all of the transaction's changes are safely persisted, the engine truncates the log.</p><p>The engine supports primary and secondary indexes using nonvolatile B+trees that it maintains using the allocator interface. We modified the STX B+tree library so that all operations that alter the index's internal structure are atomic <ref type="bibr" target="#b46">[49,</ref><ref type="bibr" target="#b59">62]</ref>. For instance, when adding an entry to a B+tree node, instead of inserting the key in a sorted order, it appends the entry to a list of entries in the node. This modification is necessary because if the entry crosses cache line boundaries, the cache line write-backs required to persist the entry need not happen atomically. Our changes to the library ensure that the engine can safely access the index immediately after the system restarts as it is guaranteed to be in a consistent state.</p><p>Recovery: The effects of committed transactions are durable after the system restarts because the NVM-InP engine immediately persists the changes made by a transaction when it commits. The engine therefore does not need to replay the log during recovery. But the changes of uncommitted transactions may be present in the database because the memory controller can evict cache lines containing those changes to NVM at any time <ref type="bibr" target="#b45">[48]</ref>. The NVM-InP engine therefore needs to undo those transactions using the WAL.</p><p>To undo an insert operation, the engine releases the tuple's storage space using the pointer recorded in the WAL entry and then removes entries associated with the tuple in the indexes. In case of an update operation, the engine restores the tuple's state using the before image. If the after image contains non-inlined tuple fields, then the engine frees up the memory occupied by those fields. For a delete operation, it only needs to update the indexes to point to the original tuple. To handle transaction rollbacks and DBMS recovery correctly, the NVM-InP engine releases storage space occupied by tuples or non-inlined fields only after it is certain that they are no longer required. As this recovery protocol does not include a redo process, the NVM-InP engine has a short recovery latency that only depends on the number of uncommitted transactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Copy-on-Write Updates Engine (NVM-CoW)</head><p>The original CoW engine stores tuples in self-containing blocks without pointers in the copy-on-write B+tree on the filesystem. The problem with this engine is that the overhead of propagating modifications to the dirty directory is high; even if a transaction only modifies one tuple, the engine needs to copy the entire block to the filesystem. When a transaction commits, the CoW engine uses the filesystem interface to flush the dirty blocks and updates the master record (stored at a fixed location in the file) to point to the root of  the dirty directory <ref type="bibr" target="#b13">[16]</ref>. These writes are expensive as they need to switch the privilege level and go through the kernel's VFS path.</p><p>The NVM-CoW engine employs three optimizations to reduce these overheads. First, it uses a non-volatile copy-on-write B+tree that it maintains using the allocator interface. Second, the NVM-CoW engine directly persists the tuple copies and only records non-volatile tuple pointers in the dirty directory. Lastly, it uses the lightweight durability mechanism of the allocator interface to persist changes in the copy-on-write B+tree.</p><p>Storage: <ref type="figure" target="#fig_5">Fig. 4b</ref> depicts the architecture of the NVM-CoW engine. The storage area for tuples is spread across separate pools for fixed-sized and variable-length data. The engine maintains the durability state of each slot in both pools similar to the NVM-InP engine. The NVM-CoW engine stores the current and dirty directory of the non-volatile copy-on-write B+tree using the allocator interface. We modified the B+tree from LMDB <ref type="bibr" target="#b13">[16]</ref> to handle modifications at finer granularity to exploit NVM's byte addressability. The engine maintains the master record using the allocator interface to support efficient updates. When the system restarts, the engine can safely access the current directory using the master record because that directory is guaranteed to be in a consistent state. This is because the data structure is append-only and the data stored in the current directory is never overwritten.</p><p>The execution steps for this engine are shown in <ref type="table" target="#tab_4">Table 2</ref>. The salient feature of this engine's design is that it avoids the transformation and copying costs incurred by the CoW engine. When a transaction updates a tuple, the engine first makes a copy and then applies the changes to that copy. It then records only the nonvolatile tuple pointer in the dirty directory. The engine also batches transactions to amortize the cost of persisting the current directory. To commit a batch of transactions, it first persists the changes performed by uncommitted transactions. It then persists the contents of the dirty directory. Finally, it updates the master record using an atomic durable write to point to that directory. The engine orders all of these writes using memory barriers to ensure that only committed transactions are visible after the system restarts.</p><p>Recovery: As the NVM-CoW engine never overwrites committed data, it does not have a recovery process. When the system restarts, it first accesses the master record to locate the current directory. After that, it can start handling transactions. The storage space consumed by the dirty directory at the time of failure is asynchronously reclaimed by the NVM-aware allocator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Log-structured Updates Engine (NVM-Log)</head><p>The Log engine batches all writes in the MemTable to reduce random accesses to durable storage <ref type="bibr" target="#b47">[50,</ref><ref type="bibr" target="#b40">43]</ref>. The benefits of this approach, however, are not as evident for a NVM-only storage hierarchy because the performance gap between sequential and random accesses is smaller. The original log-structured engine that we described in Section 3.3 incurs significant overhead from periodically flushing MemTable to the filesystem and compacting SSTables to bound read amplification. Similar to the NVM-InP engine, the NVM-Log engine records all the changes performed by transactions on a WAL stored on NVM.</p><p>Our NVM-Log engine avoids data duplication in the MemTable and the WAL as it only records non-volatile pointers to tuple modifications in the WAL. Instead of flushing MemTable out to the filesystem as a SSTable, it only marks the MemTable as immutable and starts a new MemTable. This immutable MemTable is physically stored in the same way on NVM as the mutable MemTable. The only difference is that the engine does not propagate writes to the immutable MemTables. We also modified the compaction process to merge a set of these MemTables to generate a new larger MemTable. The NVM-Log engine uses a NVM-aware recovery protocol that has lower recovery latency than its traditional counterpart.</p><p>Storage: As shown in <ref type="figure" target="#fig_5">Fig. 4c</ref>, the NVM-Log engine uses an LSM tree to store the database. Each level of the tree contains a sorted run of data. Similar to the Log engine, this engine first stores all the changes performed by transactions in the MemTable which is the topmost level of the LSM tree. The changes include tuple contents for insert operation, updated fields for update operation and tombstone markers for delete operation. When the size of the MemTable exceeds a threshold, the NVM-Log engine marks it as immutable and starts a new MemTable. We modify the periodic compaction process the engine performs for bounding read amplification to merge a set of immutable MemTables and create a new MemTable. The engine constructs a Bloom filter <ref type="bibr" target="#b9">[12]</ref> for each immutable MemTable to minimize unnecessary index look-ups.</p><p>Similar to the Log engine, the NVM-Log engine maintains a WAL. The purpose of the WAL is not to rebuild the MemTable, but rather to undo the effects of uncommitted transactions from the MemTable. An overview of the operations performed by the NVM-Log engine is shown in <ref type="table" target="#tab_4">Table 2</ref>. Like the NVM-InP engine, this new engine also stores the WAL as a non-volatile linked list of entries. When a transaction inserts a tuple, the engine first flushes the tuple to NVM and records the non-volatile tuple pointer in a WAL entry. It then persists the log entry and marks the tuple as persisted. Finally, it adds an entry in the MemTable indexes. After the transaction commits, the engine truncates the relevant log entry because the changes recorded in MemTable are durable. Its logging overhead is lower than the Log engine as it records less data and maintains the WAL using the allocator interface. The engine uses non-volatile B+trees <ref type="bibr" target="#b46">[49,</ref><ref type="bibr" target="#b59">62]</ref>, described in Section 4.1, as MemTable indexes. Hence, it does not need to rebuild its indexes upon restarting.</p><p>Recovery: When the transaction commits, all the changes performed by the transaction are persisted in the in-memory component. During recovery, the NVM-Log engine only needs to undo the effects of uncommitted transactions on the MemTable. Its recovery latency is therefore lower than the Log engine as it no longer needs to rebuild the MemTable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL ANALYSIS</head><p>In this section, we present our analysis of the six different storage engine implementations. Our DBMS testbed allows us to evaluate the throughput, the number of reads/writes to the NVM device, the storage footprint, and the time that it takes to recover the database after restarting. We also use the perf toolkit to measure additional, lower-level hardware metrics of the system for each experiment <ref type="bibr">[3]</ref>.</p><p>The experiments were all performed on Intel Lab's NVM hardware emulator <ref type="bibr" target="#b24">[27]</ref>. It contains a dual-socket Intel Xeon E5-4620 processor. Each socket has eight cores running at 2.6 GHz. It dedicates 128 GB of DRAM for the emulated NVM and its L3 cache size is 20 MB. We use the Intel memory latency checker <ref type="bibr" target="#b60">[63]</ref> to validate the emulator's latency and bandwidth settings. The engines access NVM storage using the allocator and filesystem interfaces of the emulator as described in Section 2.2. We set up the DBMS to use eight partitions in all of the experiments. We configure the node size of the STX B+tree and the CoW B+tree implementations to be 512 B and 4 KB respectively. All transactions execute with the same serializable isolation level and durability guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmarks</head><p>We first describe the benchmarks we use in our evaluation. The tables in each database are partitioned in such way that there are only single-partition transactions <ref type="bibr" target="#b49">[52]</ref>.</p><p>YCSB: This is a widely-used key-value store workload from Yahoo! <ref type="bibr" target="#b17">[20]</ref>. It is representative of the transactions handled by webbased companies. It contains a single table comprised of tuples with a primary key and 10 columns of random string data, each 100 bytes in size. Each tuple's size is approximately 1 KB. We use a database with 2 million tuples (∼2 GB).</p><p>The workload consists of two transaction types: (1) a read transaction that retrieves a single tuple based on its primary key, and (2) an update transaction that modifies a single tuple based on its primary key. We use four types of workload mixtures that allow us to vary the I/O operations that the DBMS executes. These mixtures represent different ratios of read and update transactions:</p><p>• Read-Only: 100% reads</p><p>• Read-Heavy: 90% reads, 10% updates • Balanced: 50% reads, 50% updates</p><p>• Write-Heavy: 10% reads, 90% updates</p><p>We modified the YCSB workload generator to support two different levels of skew in the tuple access patterns that allows us to create a localized hotspot within each partition:</p><p>• Low Skew: 50% of the transactions access 20% of the tuples.</p><p>• High Skew: 90% of the transactions access 10% of the tuples.</p><p>For each workload mixture and skew setting pair, we pre-generate a fixed workload of 8 million transactions that is divided evenly among the DBMS's partitions. Using a fixed workload that is the same across all the engines allows us to compare their storage footprints and read/write amplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TPC-C:</head><p>This benchmark is the current industry standard for evaluating the performance of OLTP systems <ref type="bibr" target="#b58">[61]</ref>. It simulates an order-entry environment of a wholesale supplier. The workload consists of five transaction types, which keep track of customer orders, payments, and other aspects of a warehouse. Transactions involving database modifications comprise around 88% of the workload. We configure the workload to contain eight warehouses and 100,000 items. We map each warehouse to a single partition. The initial storage footprint of the database is approximately 1 GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Runtime Performance</head><p>We begin with an analysis of the impact of NVM's latency on the performance of the storage engines. To obtain insights that are applicable for various NVM technologies, we run the YCSB and TPC-C benchmarks under three latency configurations on the emulator: (1) default DRAM latency configuration (160 ns), (2) a low NVM latency configuration that is 2× higher than DRAM latency (320 ns), and (3) a high NVM latency configuration that is 8× higher than DRAM latency (1280 ns). Prior work <ref type="bibr" target="#b24">[27]</ref> has shown that the sustained bandwidth of NVM is likely to be lower than that of DRAM. We therefore leverage the bandwidth throttling mechanism in the hardware emulator <ref type="bibr" target="#b24">[27]</ref> to throttle the NVM bandwidth to 9.5 GB/s, which is 8× lower than the available DRAM bandwidth on the platform. We execute all workloads three times on each engine and report the average throughput. YCSB: Figs. 5 to 7 present the throughput observed with the YCSB benchmark while varying the workload mixture and skew settings under different latency configurations. We first consider the read-only workload results shown in Figs. 5a, 6a and 7a. These results provide an upper bound on performance since transactions do not modify the database and the engines therefore do not need to flush changes from CPU caches to NVM during execution.</p><p>The most notable observation is that the NVM-InP engine is not faster than the InP engine for both skew settings. This is because both engines perform reads using the allocator interface. The CoW engine's throughput is lower than the in-place updates engine because for every transaction, it fetches the master record and then looks-up the tuple in the current directory. As the NVM-CoW engine accesses the master record and the non-volatile copy-on-write B+tree efficiently using the allocator interface, it is 1.9-2.1× faster than the CoW engine. The Log engine is the slowest among all the engines because it coalesces entries spread across different LSM tree components to reconstruct tuples. The NVM-Log engine accesses the immutable MemTables using the allocator interface and delivers 2.8× higher throughput compared to its traditional counterpart. We see that increasing the workload skew improves the performance of all the engines due to caching benefits. The benefits are most evident for the InP and NVM-InP engines; they achieve 1.3× higher throughput compared to the low skew setting. The performance gains due to skew are minimal in case of the Log and NVM-Log engines due to tuple coalescing costs.</p><p>We also observe that the performance gap between the two types of engines decreases in the read-only workload when we increase the NVM latency. In the high latency configuration, the NVM-CoW and the NVM-Log engines are 1.4× and 2.5× faster than their traditional counterparts. This is because the benefits of accessing data structures using the allocator interface are masked by slower    NVM loads. The engines' throughput decreases sub-linearly with respect to the increased NVM latency. For example, with 8× higher latency, the throughput of the engines only drop by 2-3.4×. The NVM-aware engines are more sensitive to the increase in latency as they do not incur tuple transformation and copying costs that dampen the effect of slower NVM accesses in the traditional engines.</p><p>For the read-heavy workload, the results shown in Figs. 5b, 6b and 7b indicate that the throughput decreases for all the engines compared to the read-only workload because they must flush transactions' changes to NVM. Unlike before where the two engines had the same performance, in this workload we observe that the NVM-InP engine is 1.3× faster than the InP engine due to lower logging overhead. The performance of the CoW engine drops compared to its performance on the read-only workload because of the overhead of persisting the current directory. The drop is less prominent in the high skew workload because the updates are now concentrated over a few hot tuples and therefore the number of copy-on-write B+tree nodes that are copied when creating the dirty directory is smaller.</p><p>The benefits of our optimizations are more prominent for the balanced and write-heavy workloads. For the NVM-InP and the NVM-Log engines, we attribute this to lower logging overhead. In case of the NVM-CoW engine, this is because it does not have to copy and transform tuples from the filesystem whenever it modifies them. This allows this engine to achieve 4.3-5.5× higher throughput than the CoW engine. The performance gap between the Log and the CoW engines decreases because the former incurs lower tuple coalescing costs in these workloads. The Log engine is therefore 1.6-4.1× faster than the CoW engine. It still lags behind the InP engine, however, because batching updates in the MemTable are not as beneficial in the NVM-only storage hierarchy. With increased latency, the throughput of all the engines decreases less on these write-intensive workloads compared to the workloads that contain more reads. The throughput does not drop linearly with increasing NVM latency. With an 8× increase in latency, the throughput of the engines only drops by 1.8-2.9×. We attribute this to the effects of caching and memory-level parallelism in the emulator.  <ref type="figure" target="#fig_10">Fig. 8</ref> shows the engines' throughput while executing TPC-C under different latency configurations. Among all the engines, the NVM-InP engine performs the best. The NVM-aware engines are 1.8-2.1× faster than the traditional engines. The NVM-CoW engine exhibits the highest speedup of 2.3× over the CoW engine. We attribute this to the write-intensive nature of the TPC-C benchmark. Under the high NVM latency configuration, the NVM-aware engines deliver 1.7-1.9× higher throughput than their traditional counterparts. These trends closely follow the results for the write-intensive workload mixture in the YCSB benchmark. The benefits of our optimizations, however, are not as significant as previously observed with the YCSB benchmark. This is because the TPC-C transactions' contain more complex program logic and execute more queries per transaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TPC-C:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Reads &amp; Writes</head><p>We next measure the number of times that the storage engines access the NVM device while executing the benchmarks. This is important because the number of write cycles per bit is limited in different NVM technologies as shown in <ref type="table" target="#tab_1">Table 1</ref>. We compute these results using hardware performance counters on the emulator with the perf framework [3]. These counters track the number of loads (i.e. reads) from and stores (i.e. writes) to the NVM device during execution. In each trial, the engines' access measurements are collected after loading the initial database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YCSB:</head><p>The results for NVM reads and writes while executing the YCSB benchmark are shown in Figs. 9 and 10, respectively. In    the read-only workload, we observe that the Log engine performs the most load operations due to tuple coalescing. The NVM-aware engines perform up to 53% fewer loads due to better cache locality as they do not perform any tuple deserialization operations. When we increase the workload skew, there is a significant drop in the NVM loads performed by all the engines. We attribute this to caching of hot tuples in the CPU caches.</p><p>In the write-intensive workloads, we observe that the CoW engine now performs the most NVM stores. This is because it needs to copy several pages while creating the dirty directory. This engine also performs the largest number of load operations. The copying mechanism itself requires reading data off NVM. Further, the I/O overhead of maintaining this directory reduces the number of hot tuples that can reside in the CPU caches.</p><p>On the write-heavy workload, the NVM-aware engines perform 17-48% fewer stores compared to their traditional counterparts. We attribute this to their lightweight durability mechanisms and smaller storage footprints that enable them to make better use of hardware caches. Even with increased workload skew, the NVM-aware engines perform 9-41% fewer NVM writes. We note that the NVM accesses performed by the storage engines correlate inversely with the throughput delivered by these engines as shown in Section 5.2. <ref type="figure" target="#fig_1">Fig. 11</ref> presents the NVM accesses performed while executing the TPC-C benchmark. NVM-aware engines perform 31-42% fewer writes compared to the traditional engines. We see that the access patterns are similar to that observed with the writeintensive workload mixture in the YCSB benchmark. The Log engine performs more writes in this benchmark compared to the YCSB benchmark because it has more indexes. This means that updating a tuple requires updating several indexes as well.  <ref type="figure" target="#fig_1">Figure 12</ref>: Recovery Latency -The amount of time that the engines take to restore the database to a consistent state after a restart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TPC-C:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Recovery</head><p>In this experiment, we evaluate the recovery latency of the storage engines. For each benchmark, we first execute a fixed number of transactions and then force a hard shutdown of the DBMS (SIGKILL). We then measure the amount of time for the system to restore the database to a consistent state. That is, a state where the effects of all committed transactions are durable, and the effects of uncommitted transactions are removed. The number of transactions that need to be recovered by the DBMS depends on the frequency of checkpointing for the InP engine and on the frequency of flushing the MemTable for the Log engine. The CoW and NVM-CoW engines do not perform any recovery mechanism after the OS or DBMS restarts because they never overwrite committed data. They have to perform garbage collection to clean up the previous dirty directory. This is done asynchronously and does not have a significant impact on the throughput of the DBMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YCSB:</head><p>The results in <ref type="figure" target="#fig_1">Fig. 12a</ref> show the recovery measurements for the YCSB benchmark. We do not show the CoW and NVM-CoW engines as they never need to recover. We observe that the latency of the InP and Log engines grows linearly with the number of transactions that need to be recovered. This is because these engines first redo the effects of committed transactions before undoing the effects of uncommitted transactions. In contrast, the NVM-InP and NVM-Log engines' recovery time is independent of the number of transactions executed. These engines only need to undo the effects of transactions that are active at the time of failure and not the ones since the last checkpoint or flush. The NVM-aware engines therefore have a short recovery that is always less than a second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TPC-C:</head><p>The results for the TPC-C benchmark are shown in <ref type="figure" target="#fig_1">Fig. 12b</ref>. The recovery latency of the NVM-InP and NVM-Log engines is slightly higher than that in the YCSB benchmark because the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Storage</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recovery</head><p>Index Other</p><formula xml:id="formula_0">In P C o W L o g N V M -In P N V M -C o W N V M -L o g 0 60 100</formula><p>Time (%) Time (%)  <ref type="figure" target="#fig_1">Figure 13</ref>: Execution Time Breakdown -The time that the engines spend in their internal components when running the YCSB benchmark.</p><formula xml:id="formula_1">(a) Read-only Workload In P C o W L o g N V M -In P N V M -C o W N V M -L o g 0 Time (%) (b) Read-heavy Workload In P C o W L o g N V M -In P N V M -C o W N V</formula><formula xml:id="formula_2">(c) Balanced Workload In P C o W L o g N V M -In P N V M -C o W N V</formula><p>TPC-C transactions perform more operations. However, the latency is still independent of the number of transactions executed unlike the traditional engines because the NVM-aware engines ensure that the effects of committed transactions are persisted immediately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Execution Time Breakdown</head><p>In this experiment, we analyze the time that the engines spend in their internal components during execution. We only examine YCSB with low skew and low NVM latency configuration, which allows us to better understand the benefits and limitations of our implementations. We use event-based sampling with the perf framework [3] to track the cycles executed within the engine's components. We start this profiling after loading the initial database.</p><p>The engine's cycles are classified into four categories: (1) storage management operations with the allocator and filesystem interfaces, (2) recovery mechanisms like logging, (3) index accesses and maintenance, and (4) other miscellaneous components. This last category is different for each engine; it includes the time spent in synchronizing the engine's components and performing engine-specific tasks, such as compaction in case of the Log and NVM-Log engines. As our testbed uses a lightweight concurrency control mechanism, these results do not contain any overhead from locking or latching <ref type="bibr" target="#b56">[59]</ref>.</p><p>The most notable result for this experiment, as shown in <ref type="figure" target="#fig_1">Fig. 13</ref>, is that on the write-heavy workload, the NVM-aware engines only spend 13-18% of their time on recovery-related tasks compared to the traditional engines that spend as much as 33% of their time on them. We attribute this to the lower logging overhead in the case of the NVM-InP and NVM-Log engines, and the reduced cost of committing the dirty directory in the NVM-CoW engine. We observe that the proportion of the time that the engines spend on recovery mechanisms increases as the workload becomes writeintensive. This explains why the benefits of our optimizations are more prominent for the balanced and write-heavy workloads.</p><p>These results highlight the benefits of optimizing the memory allocator to leverage NVM's characteristics. This is because the NVM-aware engines spend most of their time performing storage management operations since their recovery mechanisms are so efficient. Interestingly, the engines performing copy-on-write updates spend a higher proportion of time on recovery-related tasks compared to other engines, particularly on the read-heavy workload. This highlights the cost of creating and maintaining the dirty directory for large databases, even using an efficient CoW B+tree. Another observation from <ref type="figure" target="#fig_1">Fig. 13</ref> is that the Log and NVM-Log engines spend a higher fraction of their time accessing and maintaining indexes. This is because they perform multiple index look-ups on the LSM tree to reconstruct tuples. We observe that the NVM-Log engine spends less time performing the compaction process compared to the Log engine. This is due to the reduced overhead of maintaining the MemTables using the allocator interface.   </p><formula xml:id="formula_3">In P C o W L o g N V M -In P N V M -C o W N V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Storage Footprint</head><p>Lastly, we compare the engines' usage of NVM storage at runtime. The storage footprint of an engine is the amount of space that it uses for storing tables, logs, indexes, and other internal data structures. This metric is important because we expect that the first NVM products will initially have a higher cost than current storage technologies <ref type="bibr" target="#b36">[39]</ref>. For this experiment, we periodically collect statistics maintained by our allocator and the filesystem meta-data during the workload execution. This is done after loading the initial database for each benchmark. We then report the peak storage footprint of each engine. For all of the engines, we allow their background processes (e.g., group commit, checkpointing, garbage collection, compaction) to execute while we collect these measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YCSB:</head><p>We use the balanced workload mixture and low skew setting for this experiment. The initial size of the database is 2 GB. The results shown in <ref type="figure" target="#fig_1">Fig. 14a</ref> indicate that the CoW engine has the largest storage footprint. Since this workload contains transactions that modify the database and tuples are accessed more uniformly, this engine incurs high overhead from continually creating new dirty directories and copying tuples. The InP and Log engines rely on logging to improve their recovery latency at the expense of a larger storage footprint. The InP engine checkpoints have a high compression ratio and therefore consume less space.</p><p>The NVM-aware engines have smaller storage footprints compared to the traditional engines. This is because the NVM-InP and NVM-Log engines only record non-volatile pointers to tuples and non-inlined fields in the WAL. As such, they consume 17-21% less storage space than their traditional counterparts. For the CoW engine, its large storage footprint is due to duplicated data in its internal cache. In contrast, the NVM-CoW engine accesses the non-volatile copy-on-write B+tree directly using the allocator interface, and only records non-volatile tuple pointers in this tree and not entire tuples. This allows it to use 25% less storage space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TPC-C:</head><p>The graph in <ref type="figure" target="#fig_1">Fig. 14b</ref> shows the storage footprint of the engines while executing TPC-C. For this benchmark, the initial size of the database is 1 GB and it grows to 2.4 GB. Transactions inserting new tuples increase the size of the internal data structures in the CoW and Log engines (i.e., the copy-on-write B+trees and the SSTables stored in the filesystem). By avoiding unnecessary data duplication using NVM's persistence property, the NVM-aware engines have 31-38% smaller storage footprints. The space savings are more significant in this benchmark because the workload is write-intensive with longer running transactions. Thus, the logs in the InP and the Log engines grow more quickly compared to the small undo logs in their NVM-aware counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Discussion</head><p>Our analysis shows that the NVM access latency has the most impact on the runtime performance of the engines, more so than the amount of skew or the number of modifications to the database in the workload. This difference due to latency is more pronounced with the NVM-aware variants; their absolute throughput is better than the traditional engines, but longer latencies cause their performance to drop more significantly. This behavior is because they are no longer bottlenecked by heavyweight durability mechanisms <ref type="bibr" target="#b20">[23]</ref>.</p><p>The NVM-aware engines also perform fewer store operations, which will help extend NVM device lifetimes. We attribute this to the reduction in redundant data that the engines store when a transaction modifies the database. Using the allocator interface with non-volatile pointers for internal data structures also allows them to have a smaller storage footprint. This in turn avoids polluting the CPU's caches with unnecessary copying and transformation operations. It also improves the recovery times of the engines that use a WAL since they no longer record redo information.</p><p>Overall, we find that the NVM-InP engine performs the best across a wide set of workload mixtures and skew settings for all NVM latency configurations. The NVM-CoW engine did not perform as well for write-intensive workloads, but may be a better fit for DBMSs that support non-blocking read-only transactions. For the NVM-Log engine, many of its design assumptions are not copacetic for a single-tier storage hierarchy. The engine is essentially performing in-place updates like the NVM-InP engine but with additional overhead of maintaining its legacy components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>We now discuss previous research on using NVM. SafeRAM <ref type="bibr" target="#b18">[21]</ref> is one of the first projects that explored the use of NVM in software applications. Using simulations, they evaluated the improvement in throughput and latency of OLTP workloads when disk is replaced by battery-backed DRAM. Later work demonstrated the utility of NVM in a client-side cache for distributed filesystems <ref type="bibr" target="#b5">[8]</ref>.</p><p>Recently, several application-level APIs for programming with persistent memory have been proposed. Mnemosyne <ref type="bibr" target="#b61">[64]</ref> and NVheaps <ref type="bibr" target="#b15">[18]</ref> use software transactional memory to support transactional updates to data stored on NVM. NVMalloc <ref type="bibr" target="#b46">[49]</ref> is a memory allocator that considers wear leveling of NVM. The primitives provided by these systems allow programmers to use NVM in their applications but do not provide the transactional semantics required by a DBMS.</p><p>In the context of DBMSs, recent work demonstrated that memoryoriented DBMSs perform only marginally better than disk-oriented DBMSs when using NVM because both systems still assume that memory is volatile <ref type="bibr" target="#b21">[24]</ref>. Others have developed new recovery mechanisms for DBMSs that are using a NVM + DRAM storage hierarchy <ref type="bibr" target="#b50">[53,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr" target="#b62">65]</ref>. Pelley et al. introduce a group commit mechanism to persist transactions' updates in batches to reduce the number of write barriers required for ensuring correct ordering <ref type="bibr" target="#b50">[53]</ref>. This work is based on the Shore-MT engine <ref type="bibr" target="#b34">[37]</ref>, which means that the DBMS records page-level before-images in the undo logs before performing in-place updates. This results in high data duplication.</p><p>Wang et al. present a passive group commit method for a distributed logging protocol extension to Shore-MT <ref type="bibr" target="#b62">[65]</ref>. Instead of issuing a barrier for every processor at commit time, the DBMS tracks when all of the records required to ensure the durability of a transaction are flushed to NVM. This approach is similar to other work on a log manager that directly writes log records to NVM, and addresses the problems of detecting partial writes and recoverability <ref type="bibr" target="#b25">[28]</ref>. Both of these projects rely on software-based simulation of a NVM + DRAM system.</p><p>MARS <ref type="bibr" target="#b14">[17]</ref> is an in-place updates engine optimized for NVM that relies on a hardware-assisted primitive that allows multiple writes to arbitrary locations to happen atomically. MARS does away with undo log records by keeping the changes isolated using this primitive. Similarly, it relies on this primitive to apply the redo log records at the time of commit. In comparison, our NVM-InP engine is based on non-volatile pointers, a software-based primitive. It removes the need to maintain redo information in the WAL, but still needs to maintain undo log records until the transaction commits.</p><p>SOFORT <ref type="bibr" target="#b48">[51]</ref> is a hybrid storage engine designed for a storage hierarchy with both NVM and DRAM. The engine is designed to not perform any logging and uses MVCC. Similar to SOFORT, we make use of non-volatile pointers <ref type="bibr" target="#b55">[58]</ref>. Our usage of persistent pointers is different. Their persistent pointer primitive is a combination of page ID and offset. We eschew the page abstraction in our engines, as NVM is byte-addressable. Hence, we use raw NVM pointers to build non-volatile data structures. For example, the engines performing in-place and log-structured updates need to perform undo logging, which is where we use non-volatile pointers to reduce data duplication.</p><p>Beyond DBMSs, others have looked into using NVM in filesystems. BPFS is a filesystem designed for persistent memory <ref type="bibr" target="#b16">[19]</ref>. It uses a variant of shadow paging that supports atomic updates by relying on a special hardware instruction that ensures ordering between writes in different epochs. PMFS is another filesystem from Intel Labs that is designed for byte-addressable NVM <ref type="bibr" target="#b24">[27]</ref>. It relies on write-ahead logging to preserve meta-data consistency and uses shadow paging only for data. It assumes a simpler hardware barrier primitive than epochs. Further, it optimizes memory-mapped I/O by directly mapping the persistent memory to the application's address space. The filesystem interface used by the traditional storage engines in our DBMS testbed is backed by PMFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>This paper explored the fundamentals of storage and recovery methods in OLTP DBMSs running on an NVM-only storage hierarchy. We implemented three storage engines in a modular DBMS testbed with different architectures: (1) in-place updates, (2) copyon-write updates, and (3) log-structured updates. We then developed optimized variants of each of these engines that better make use of NVM's characteristics. Our experimental analysis with two different OLTP workloads showed that our NVM-aware engines outperform the traditional engines by up to 5.5× while reducing the number of writes to the storage device by more than half on write-intensive workloads. We found that the NVM access latency has the most impact on the runtime performance of the engines, more so than the workload skew or the number of modifications to the database in the workload. Our evaluation showed that the NVM-aware in-place updates engine achieved the best throughput among all the engines with the least amount of wear on the NVM device.</p><p>In this section, we present a cost model to estimate the amount of data written to NVM per operation, by the traditional and NVMoptimized storage engines. This model highlights the strengths and weaknesses of these engines.</p><p>We begin the analysis by stating the assumptions we use to simplify the model. First, the database operations are presumed to be always successful. The amount of data written to NVM while performing an aborted operation will depend on the stage at which the operation fails. We therefore restrict our analysis to only successful operations. Second, the engines handle fixed-length and variablelength tuple fields differently. The fixed-length fields are stored in-line, while the variable-length fields are stored separately. To illustrate this difference, we assume that the update operation alters one fixed-length field and one variable-length field. Note that the tuple itself can contain any number of fixed-length and variable-length fields depending on the database schema.</p><p>Let us now describe some notation. We denote the size of the tuple by T . This depends on the specific table on which the engine performs the database operation. Let the size of the fixed-length field and the variable-length field altered by the update operation be F and V , respectively. These parameters depend on the table columns that are modified by the engine. The size of a pointer is represented by p. The NVM-optimized engines use non-volatile pointers to tuples and variable-length tuple fields to reduce data duplication. We use θ to denote the write-amplification factor of the engines performing log-structured updates. θ could be attributed to the periodic compaction mechanism that these engines perform to bound read-amplification and depends on the type of LSM tree. Let B represent the size of a node in the CoW B+tree used by the CoW and NVM-CoW engines. We indicate small fixed-length writes to NVM, such as those used to maintain the status of tuple slots, by ε.</p><p>Given this notation, we present the cost model in <ref type="table" target="#tab_7">Table 3</ref>. The data written to NVM is classified into three categories: (1) memory, (2) log, and (3) table storage. We now describe some notable entries in the table. While performing an insert operation, the InP engine writes three physical copies of a tuple. In contrast, the NVM-InP engine only records the tuple pointer in the log and table data structures on NVM. In the case of the CoW and NVM-CoW engines, there are two possibilities depending on whether a copy of the relevant B+tree node is absent or present in the dirty directory. For the latter, the engines do not need to make a copy of the node before applying  the desired transformation. We distinguish these two cases in the relevant table entries using vertical bars. Note that these engines have no logging overhead as they always apply modifications in the dirty directory. The performance gap between the traditional and the NVM-optimized engines, particularly for write-intensive workloads, directly follows from the cost model presented in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. IMPACT OF B+TREE NODE SIZE</head><p>We examine the sensitivity of our experimental results to size of the B+tree nodes in this section. The engines performing in-place and log-structured updates use the STX B+tree <ref type="bibr" target="#b7">[10]</ref> for maintaining indexes, while the engines performing copy-on-write updates use the append-only B+tree <ref type="bibr" target="#b53">[56,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b33">36]</ref> for storing the directories. In all our experiments, we use the default node size for both the STX B+tree (512 B) and copy-on-write B+tree (4 KB) implementations. For this analysis, we vary the B+tree node size and examine the impact on the engine's throughput, while executing different YCSB workloads under low NVM latency (2×) and low workload skew settings. We restrict our analysis to the NVM-aware engines as they are representative of other engines.</p><p>The graphs, shown in <ref type="figure" target="#fig_1">Fig. 15</ref>, indicate that the impact of B+tree node size is more significant for the CoW B+tree than the STX B+tree. In case of the CoW B+tree, we observe that increasing the node size improves the engine's performance on read-heavy workloads. This can be attributed to smaller tree depth, which in turn reduces the amount of indirection in the data structure. It also reduces the amount of metadata that needs to be flushed to NVM to ensure recoverability. However, the engine's performance on write-heavy workloads drops as the B+tree nodes get larger. This is because of the copying overhead when performing updates in the dirty directory of the CoW B+tree. We found that the engines performing copy-on-write updates perform well on both types of workloads when the node size is 4 KB. With the STX B+tree, our experiments suggest that the optimal node size is 512 B. This setting provides a nice balance between cache misses, instructions executed, TLB misses, and space utilization <ref type="bibr" target="#b31">[34]</ref>. Hence, in all of our experiments in Section 5, we configured the B+trees used by all the engines to their optimal performance settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. NVM INSTRUCTION SET EXTENSIONS</head><p>In this section, we explore the impact of newly proposed NVMrelated instruction set extensions <ref type="bibr" target="#b1">[2]</ref> on the performance of the engines. These extensions have been added by Intel in late 2014 and are not currently commercially available. As we describe in Section 2.3, we currently implement the sync primitive using the SFENCE and CLFLUSH instructions. We believe that these new extensions, such as the PCOMMIT and CLWB instructions <ref type="bibr" target="#b1">[2]</ref>, can be used to ensure the correctness and improve the performance of this primitive in future processors because they are more efficient and provide better control for how a DBMS interacts with NVM.</p><p>The PCOMMIT instruction guarantees the durability of stores to persistent memory. When the data is written back from the CPU caches, it can still reside in the volatile buffers on the memory controller. After the PCOMMIT instruction is executed, the store must become persistent. The CLWB instruction writes back a target cache line to NVM similar to the CLFLUSH instruction. It is, however, different in two ways: (1) it is a weakly-ordered instruction and can thus perform better than the strongly-ordered CLFLUSH instruction, and (2) it can retain a copy of the line in the cache hierarchy in exclusive state, thereby reducing the possibility of cache misses during subsequent accesses. In contrast, the CLFLUSH instruction always invalidates the cacheline, which means that data has to be retrieved again from NVM.</p><p>To understand the performance impact of the sync primitive comprising of PCOMMIT and CLWB instructions, we emulate its latency using RDTSC and PAUSE instructions. We note that our software-based latency emulation does not capture all the complex interactions in real processors. However, it still allows us to perform a useful whatif analysis before these instruction set extensions are available. We vary the latency of the sync primitive from 10-10000 ns and compare it with the currently used sync primitive. Since the traditional engines use PMFS <ref type="bibr" target="#b24">[27]</ref>, which is loaded in as a kernel module, they require more changes for this experiment. We therefore restrict our analysis to the NVM-aware engines. We execute different YCSB workloads under low NVM latency (2×) and low workload skew settings.</p><p>The results in <ref type="figure" target="#fig_1">Fig. 16</ref> show that the engines are sensitive to the performance of the sync primitive. Performance measurements of the engines while using the current sync primitive are shown on the left side of each graph to serve as a baseline. We observe that the throughput of all the engines drops significantly with the increasing sync primitive latency. This is expected as these engines make extensive use of this primitive in their non-volatile data structures. The impact is therefore more pronounced on write-intensive workloads.  <ref type="figure" target="#fig_1">Figure 16</ref>: NVM Instruction Set Extensions -The impact of sync primitive latency on the performance of the NVM-aware engines. The engines run the YCSB workloads under low NVM latency (2×) and low skew settings. Performance obtained using the current primitive, built using the SFENCE and CLFLUSH instructions, is shown on the left side of each graph to serve as a baseline.</p><p>We note that the NVM-CoW engine is slightly less sensitive to latency of the sync primitive than the NVM-InP and NVM-Log engines. We attribute this to the fact that this engine primarily uses data duplication to ensure recoverability and only uses the sync primitive to ensure the consistency of the CoW B+tree. In case of the NVM-Log engine, its performance while executing the writeheavy workload is interesting. Its throughput becomes less than the throughput on the balanced workload only when the latency of the sync primitive is above 1000 ns. This is because the engine needs to reconstruct tuples from entries spread across different LSM tree components.</p><p>We conclude that the trade-offs that we identified in these NVMaware engines in the main body of the paper still hold at higher sync primitive latencies. Overall, we believe that these new instructions will be required to ensure recoverability and improve the performance of future NVM-aware DBMSs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. FUTURE WORK</head><p>A hybrid DRAM and NVM storage hierarchy is a viable alternative, particularly in case of high NVM latency technologies and analytical workloads. We plan to expand our NVM-aware engines to run on that storage hierarchy. There are several other aspects of DBMS architectures for NVM that we would like study further. We plan to investigate concurrency control algorithms that are more sophisticated than the scheme that we used in our testbed to see whether they can be made to scale on a NVM-based system. In particular, we are interested in exploring methods for supporting hybrid workloads (i.e., OLTP + OLAP) on NVM.</p><p>We will evaluate state-of-the-art index data structures to understand whether their trade-offs are appropriate for NVM (e.g., wearleveling) and adapt them to exploit its persistence properties. We also anticipate the need for techniques to protect the contents of the database from errant code running and prevent the DBMS from corrupting the database by modifying a location in NVM that it should not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. ACKNOWLEDGEMENTS</head><p>This work was supported (in part) by the Intel Science and Technology Center for Big Data and the U.S. National Science Foundation (CCF-1438955). All of you need to recognize how Jignesh Patel be tearing through bits with QuickStep like he's slinging rocks at less than retail. Respect in the 608. Word is bond.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of the durable write bandwidth of Intel Lab's NVM emulator using the allocator and filesystem interfaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the architecture of the DBMS testbed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>In-place Updates (NVM-InP) !"#$%"&amp;'(&amp;)*+",-$"#$./01)*234/))* !"#$%&amp;% '()$% *-(%*+,$3-",(%&lt;=&gt;% ?@,,$8-%*+,$3-",Copy-on-Write Updates (NVM-CoW) Log-structured Updates (NVM-Log) NVM-Aware Engines -Architectural layout of the NVM-optimized storage engines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>YCSB Performance (DRAM Latency) -The throughput of the engines for the YCSB benchmark without any latency slowdown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>YCSB Performance (Low Latency) -The throughput of the engines for the YCSB benchmark under the low NVM latency configuration (2×).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>YCSB Performance (High Latency) -The throughput of the engines for the YCSB benchmark under the high NVM latency configuration (8×).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>TPC-C Throughput -The performance of the engines for TPC-C benchmark for all three NVM latency settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>YCSB Reads -The number of load operations executed by the engines while running the YCSB workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>YCSB Writes -The number of store operations executed by the engines while running the YCSB workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>TPC-C Reads &amp; Writes -The number of load and store operations executed by the engines while running the TPC-C benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 14 :</head><label>14</label><figDesc>Storage Footprint -The amount of storage occupied in NVM by the internal components of the engines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 15 :</head><label>15</label><figDesc>NVM-Log engine (STX B+tree) B+Tree Node Size -The impact of B+tree node size on the performance of the NVM-aware engines. The engines run the YCSB workloads under low NVM latency (2×) and low skew settings. NVM-Log engine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>DRAM PCM RRAM MRAM SSD HDD</figDesc><table><row><cell>Read latency</cell><cell>60 ns</cell><cell cols="3">50 ns 100 ns 20 ns</cell><cell cols="2">25 µs 10 ms</cell></row><row><cell>Write latency</cell><cell>60 ns</cell><cell cols="3">150 ns 100 ns 20 ns</cell><cell cols="2">300 µs 10 ms</cell></row><row><cell>Addressability</cell><cell>Byte</cell><cell cols="2">Byte Byte</cell><cell>Byte</cell><cell cols="2">Block Block</cell></row><row><cell>Volatile</cell><cell>Yes</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell>No</cell></row><row><cell cols="2">Energy/ bit access 2 pJ</cell><cell>2 pJ</cell><cell cols="4">100 pJ 0.02 pJ nJ 0.1 J</cell></row><row><cell>Endurance</cell><cell cols="2">&gt;10 16 10</cell><cell>10 8</cell><cell>10 15</cell><cell>5</cell><cell>&gt;10 16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of emerging NVM technologies with other storage technologies</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>NVM-InP Engine NVM-CoW Engine NVM-Log Engine</head><label></label><figDesc>INSERT• Sync tuple with NVM.• Record tuple pointer in WAL.• Sync log entry with NVM.• Mark tuple state as persisted.• Add tuple entry in indexes.• Sync tuple with NVM.• Store tuple pointer in dirty dir.• Update tuple state as persisted.• Add tuple entry in secondary indexes.• Sync tuple with NVM.• Record tuple pointer in WAL.• Sync log entry with NVM.• Mark tuple state as persisted.• Add tuple entry in MemTable.UPDATE• Record tuple changes in WAL.• Sync log entry with NVM.• Perform modifications on the tuple.• Sync tuple changes with NVM.• Make a copy of the tuple.• Apply changes on the copy.• Sync tuple with NVM.• Store tuple pointer in dirty dir.• Update tuple state as persisted.• Add tuple entry in secondary indexes.• Record tuple changes in WAL.• Sync log entry with NVM.• Perform modifications on the tuple.• Sync tuple changes with NVM.</figDesc><table><row><cell cols="2">DELETE • Record tuple pointer in WAL.</cell><cell>• Remove tuple pointer from dirty dir.</cell><cell>• Record tuple pointer in WAL.</cell></row><row><cell></cell><cell>• Sync log entry with NVM.</cell><cell>• Discard entry from secondary indexes.</cell><cell>• Sync log entry with NVM.</cell></row><row><cell></cell><cell>• Discard entry from table and indexes.</cell><cell>• Recover tuple space immediately.</cell><cell>• Mark tuple tombstone in MemTable.</cell></row><row><cell></cell><cell>• Reclaim space at the end of transaction.</cell><cell></cell><cell>• Reclaim space during compaction.</cell></row><row><cell>SELECT</cell><cell>• Find tuple pointer using index/table.</cell><cell>• Locate tuple pointer in appropriate dir.</cell></row><row><cell></cell><cell>• Retrieve tuple contents.</cell><cell>• Fetch tuple contents from dir.</cell></row></table><note>• Find tuple entries in relevant LSM runs.• Rebuild tuple by coalescing entries.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>An overview of the steps performed by the NVM-aware storage engines, while executing primitive database operations. The syncing mechanism is implemented using CLFLUSH and SFENCE instructions on the hardware emulator. We describe the sync primitive in Section 2.3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Analytical cost model for estimating the amount of data written to NVM, while performing insert, update, and delete operations, by each engine.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ANALYTICAL COST MODEL</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Cassandra</surname></persName>
		</author>
		<ptr target="http://datastax.com/documentation/cassandra/2.0/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="https://software.intel.com/sites/default/files/managed/0d/53/319433-022.pdf" />
	</analytic>
	<monogr>
		<title level="j">Intel Architecture Instruction Set Extensions Programming Reference</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://linux.die.net/man/3/numa" />
		<title level="m">NUMA policy library</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agigaram</forename><surname>Nvdimm</surname></persName>
		</author>
		<ptr target="http://www.agigatech.com/ddr3.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">System R: relational approach to database management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Astrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Blasgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Chamberlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Eswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Lorie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Mcjones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Mehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Putzolu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Traiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="137" />
			<date type="published" when="1976-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-volatile memory for fast, reliable file systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Asami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Deprit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="10" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Concurrency Control and Recovery in Database Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hadzilacos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bingmann</surname></persName>
		</author>
		<ptr target="http://panthema.net/2007/stx-btree/" />
		<title level="m">STX B+ tree C++ template classes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The necessary death of the block device interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bjørling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bouganim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Overview of candidate device technologies for storage-class memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Kurdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="464" />
			<date type="published" when="2008-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A limits study of benefits from nanostore-based future data-centric system architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A protected block device for persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mesnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th Symposium on Mass Storage Systems and Technologies (MSST)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MDB: A Memory-Mapped Database and Backend for OpenLDAP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenLDAP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From ARIES to MARS: Transaction support for next-generation, solid-state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Coburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bunker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP &apos;13</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="197" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nv-heaps: making persistent objects fast and safe with next-generation, non-volatile memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Coburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Grupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jhala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="105" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Better I/O through byte-addressable, persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Condit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coetzee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="133" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with YCSB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoCC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Copeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">The case for safe ram. VLDB</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="327" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leveldb</surname></persName>
		</author>
		<ptr target="http://leveldb.googlecode.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A prolegomenon on OLTP database systems for non-volatile memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Debrabant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arulraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dulloor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADMS@VLDB</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Anti-caching: A new approach to database management system architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Debrabant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1942" to="1953" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Implementation techniques for main memory database systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Olken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Rec</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Latest advances and future prospects of STT-RAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Driskill-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Non-Volatile Memories Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">System software for persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Dulloor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Keshavamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Subbareddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High performance database logging using storage class memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-I</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDE</title>
		<imprint>
			<biblScope unit="page" from="1221" to="1231" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Concurrency control and recovery. The Computer Science and Engineering Handbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="1058" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PCMLogging: Reducing transaction logging overhead with pcm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CIKM</title>
		<imprint>
			<biblScope unit="page" from="2401" to="2404" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Main memory database systems: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Salem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="page" from="509" to="516" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Varieties of concurrency control in IMS/VS Fast Path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gawlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinkade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<pubPlace>Tandem</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The recovery manager of the system R database manager</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcjones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blasgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lorie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Putzolu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Traiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="242" />
			<date type="published" when="1981-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Effect of node size on the performance of cache-conscious b+-trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Hankins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS &apos;03</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="283" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">OLTP through the looking glass, and what we found there</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harizopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="981" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Copy-on-write B+ Tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hedenfalk</surname></persName>
		</author>
		<ptr target="http://www.bzero.se/ldapd/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shore-MT: a scalable storage manager for the multicore era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pandis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="24" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cache write policies and performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual International Symposium on Computer Architecture, ISCA</title>
		<meeting>the 20th Annual International Symposium on Computer Architecture, ISCA</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evaluating phase change memory for enterprise storage systems: A study of caching and tiering approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Dickey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 12th USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>FAST 14</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A Comparison of Fractal Trees to Log-Structured Merge (LSM) Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Tokutek</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">3-D data management: Controlling data volume, velocity and variety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Energy management for commercial servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lefurgy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rawson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Felter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kistler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Implementation details of LevelDB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leveldb</surname></persName>
		</author>
		<ptr target="https://leveldb.googlecode.com/svn/trunk/doc/impl.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A simple PCM block device simulator for Linux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Macko</surname></persName>
		</author>
		<ptr target="https://code.google.com/p/pcmsim/people/list" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking main memory OLTP recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Malviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weisberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Challenges and future directions for the scaling of dynamic random-access memory (DRAM)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Mandelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Dennard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Bronner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Debrosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Divakaruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Radens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ARIES: a transaction recovery method supporting fine-granularity locking and partial rollbacks using write-ahead logging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haderle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lindsay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirahesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="162" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hackenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Muller</surname></persName>
		</author>
		<title level="m">Memory performance and cache coherency effects on an intel nehalem multiprocessor system. PACT &apos;09</title>
		<imprint>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Consistent, durable, and safe memory management for byte-addressable non volatile main memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Moraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tolia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRIOS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The log-structured merge-tree (lsm-tree)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gawlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>O'neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Inf</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="385" />
			<date type="published" when="1996-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">SOFORT: A hybrid SCM-DRAM storage engine for fast data recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oukid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Booss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bumbulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Willhalm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<pubPlace>DaMoN</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Skew-aware automatic database partitioning in shared-nothing, parallel OLTP systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Storage management in the NVRAM era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Non-volatile memory: Emerging technologies and their impact on memory systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PURCS Technical Report</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Phase-change random access memory: A scalable technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Breitwisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rettner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shelby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-L</forename><surname>Lung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>4.5</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">B-trees, shadowing, and clones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rodeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The design and implementation of a log-structured file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="52" />
			<date type="published" when="1992-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Persistent memory library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudoff</surname></persName>
		</author>
		<ptr target="https://github.com/pmem/linux-examples/tree/master/libpmem" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The end of an architectural era: (it&apos;s time for a complete rewrite)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harizopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Helland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The missing memristor found</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Strukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">7191</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<ptr target="http://www.tpc.org/tpcc/" />
		<title level="m">The Transaction Processing Council. TPC-C Benchmark</title>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
	<note>Revision 5.9.0</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Consistent and durable data structures for non-volatile byte-addressable memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tolia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Intel Memory Latency Checker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Willhalm</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/intelr-memory-latency-checker" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Mnemosyne: lightweight persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<editor>R. Gupta and T. C. Mowry</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Scalable logging through emerging non-volatile memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="865" to="876" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Flash &amp; dram si scaling challenges, emerging non-volatile memory technology enablement -implications to enterprise storage and server compute systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Tressler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Flash Memory Summit</title>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
