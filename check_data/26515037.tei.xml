<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-05-01">1 May 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst Amherst</orgName>
								<address>
									<postCode>01003</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<email>mccallum@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Amherst Amherst</orgName>
								<address>
									<postCode>01003</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-05-01">1 May 2015</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1412.6623v4[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Current work in lexical distributed representations maps each word to a point vector in low-dimensional space. Mapping instead to a density provides many interesting advantages, including better capturing uncertainty about a representation and its relationships, expressing asymmetries more naturally than dot product or cosine similarity, and enabling more expressive parameterization of decision boundaries. This paper advocates for density-based distributed embeddings and presents a method for learning representations in the space of Gaussian distributions. We compare performance on various word embedding benchmarks, investigate the ability of these embeddings to model entailment and other asymmetric relationships, and explore novel properties of the representation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>In recent years there has been a surge of interest in learning compact distributed representations or embeddings for many machine learning tasks, including collaborative filtering <ref type="bibr" target="#b21">(Koren et al., 2009)</ref>, image retrieval <ref type="bibr" target="#b35">(Weston et al., 2011)</ref>, relation extraction <ref type="bibr" target="#b30">(Riedel et al., 2013)</ref>, word semantics and language modeling <ref type="bibr" target="#b4">(Bengio et al., 2006;</ref><ref type="bibr" target="#b27">Mnih &amp; Hinton, 2008;</ref><ref type="bibr" target="#b25">Mikolov et al., 2013)</ref>, and many others. In these approaches input objects (such as images, relations or words) are mapped to dense vectors having lower-dimensionality than the cardinality of the inputs, with the goal that the geometry of his low-dimensional latent embedded space be smooth with respect to some measure of similarity in the target domain. That is, objects associated with similar targets should be mapped to nearby points in the embedded space.</p><p>While this approach has proven powerful, representing an object as a single point in space carries some important limitations. An embedded vector representing a point estimate does not naturally express uncertainty about the target concepts with which the input may be associated. Point vectors are typically compared by dot products, cosine-distance or Euclean distance, none of which provide for asymmetric comparisons between objects (as is necessary to represent inclusion or entailment). Relationships between points are normally measured by distances required to obey the triangle inequality. This paper advocates moving beyond vector point representations to potential functions <ref type="bibr" target="#b0">(Aizerman et al., 1964)</ref>, or continuous densities in latent space. In particular we explore Gaussian function embeddings (currently with diagonal covariance), in which both means and variances are learned from data. Gaussians innately represent uncertainty, and provide a distance function per object. KLdivergence between Gaussian distributions is straightforward to calculate, naturally asymmetric, and has a geometric interpretation as an inclusion between families of ellipses.</p><p>There is a long line of previous work in mapping data cases to probability distributions, perhaps the most famous being radial basis functions (RBFs), used both in the kernel and neural network literature. We draw inspiration from this work to propose novel word embedding algorithms that embed words directly as Gaussian distributional potential functions in an infinite dimensional function space. This allows us to map word types not only to vectors but to soft regions in space, modeling uncertainty, inclusion, and entailment, as well as providing a rich geometry of the latent space. <ref type="figure">Figure 1</ref>: Learned diagonal variances, as used in evaluation (Section 6), for each word, with the first letter of each word indicating the position of its mean. We project onto generalized eigenvectors between the mixture means and variance of query word Bach. Nearby words to Bach are other composers e.g. Mozart, which lead to similar pictures.</p><p>After discussing related work and presenting our algorithms below we explore properties of our algorithms with multiple qualitative and quantitative evaluation on several real and synthetic datasets. We show that concept containment and specificity matches common intuition on examples concerning people, genres, foods, and others. We compare our embeddings to Skip-Gram on seven standard word similarity tasks, and evaluate the ability of our method to learn unsupervised lexical entailment. We also demonstrate that our training method also supports new styles of supervised training that explicitly incorporate asymmetry into the objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This paper builds on a long line of work on both distributed and distributional semantic word vectors, including distributional semantics, neural language models, count-based language models, and, more broadly, the field of representation learning.</p><p>Related work in probabilistic matrix factorization <ref type="bibr" target="#b28">(Mnih &amp; Salakhutdinov, 2007)</ref> embeds rows and columns as Gaussians, and some forms of this do provide each row and column with its own variance <ref type="bibr" target="#b33">(Salakhutdinov &amp; Mnih, 2008)</ref>. Given the parallels between embedding models and matrix factorization <ref type="bibr" target="#b10">(Deerwester et al., 1990;</ref><ref type="bibr" target="#b30">Riedel et al., 2013;</ref><ref type="bibr" target="#b24">Levy &amp; Goldberg, 2014)</ref>, this is relevant to our approach. However, these Bayesian methods apply Bayes' rule to observed data to infer the latent distributions, whereas our model works directly in the space of probability distributions and discriminatively trains them. This allows us to go beyond the Bayesian approach and use arbitrary (and even asymmetric) training criteria, and is more similar to methods that learn kernels <ref type="bibr" target="#b22">(Lanckriet et al., 2004)</ref> or function-valued neural networks such as mixture density networks <ref type="bibr" target="#b5">(Bishop, 1994)</ref>.</p><p>Other work in multiplicative tensor factorization for word embeddings <ref type="bibr" target="#b19">(Kiros et al., 2014)</ref> and metric learning <ref type="bibr" target="#b36">(Xing et al., 2002)</ref> learns some combinations of representations, clusters, and a distance metric jointly; however, it does not effectively learn a distance function per item. Fitting Gaussian mixture models on embeddings has been done in order to apply Fisher kernels to entire documents <ref type="bibr" target="#b9">(Clinchant &amp; Perronnin, 2013b;</ref><ref type="bibr">a)</ref>. Preliminary concurrent work from <ref type="bibr" target="#b20">Kiyoshiyo et al. (2014)</ref> describes a significantly different model similar to Bayesian matrix factorization, using a probabilistic Gaussian graphical model to define a distribution over pairs of words, and they lack quantitative experiments or evaluation.</p><p>In linguistic semantics, work on the distributional inclusion hypothesis <ref type="bibr" target="#b15">(Geffet &amp; Dagan, 2005)</ref>, uses traditional count-based vectors to define regions in vector space <ref type="bibr" target="#b12">(Erk, 2009)</ref> such that subordinate concepts are included in these regions. In fact, one strength of our proposed work is that we extend these intuitively appealing ideas (as well as the ability to use a variety of asymmetric distances between vectors) to the dense, low-dimensional distributed vectors that are now gaining popularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>Our goal is to map every word type w in some dictionary D and context word type c in a dictionary C to a Gaussian distribution over a latent embedding space, such that linguistic properties of the words are captured by properties of and relationships between the distributions. For precision, we call an element of the dictionary a word type, and a particular observed token in some context a word token. This is analogous to the class vs. instance distinction in object-oriented programming.</p><p>In unsupervised learning of word vectors, we observe a sequence of word tokens {t(w) i } for each type w, and their contexts (sets of nearby word tokens), {c(w) i }. The goal is to map each word type w and context word type c to a vector, such that types that appear in similar contexts have similar vectors. When it is unambiguous, we also use the variables w and c to denote the vectors associated to that given word type or context word type.</p><p>An energy function <ref type="bibr" target="#b23">(LeCun et al., 2006</ref>) is a function E θ (x, y) that scores pairs of inputs x and outputs y, parametrized by θ. The goal of energy-based learning is to train the parameters of the energy function to score observed positive input-output pairs higher (or lower, depending on sign conventions) than negative pairs. This is accomplished by means of a loss function L which defines which pairs are positive and negative according to some supervision, and provides gradients on the parameters given the predictions of the energy function.</p><p>In prediction-based (energy-based) word embedding models, the parameters θ correspond to our learned word representations, and the x and y input-output pairs correspond to word tokens and their contexts. These contexts can be either positive (observed) or negative (often randomly sampled). In the word2vec Skip-Gram <ref type="bibr" target="#b25">(Mikolov et al., 2013)</ref> word embedding model, the energy function takes the form of a dot product between the vectors of an observed word and an observed context w c. The loss function is a binary logistic regression classifier that treats the score of a word and its observed context as the score of a positive example, and the score of a word and a randomly sampled context as the score of a negative example.</p><p>Backpropagating <ref type="bibr" target="#b32">(Rumelhart et al., 1986)</ref> this loss to the word vectors trains them to be predictive of their contexts, achieving the desired effect (words in similar contexts have similar vectors). In recent work, word2vec has been shown to be equivalent to factoring certain types of weighted pointwise mutual information matrices <ref type="bibr" target="#b24">(Levy &amp; Goldberg, 2014)</ref>.</p><p>In our work, we use a slightly different loss function than Skip-Gram word2vec embeddings. Our energy functions take on a more limited range of values than do vector dot products, and their dynamic ranges depend in complex ways on the parameters. Therefore, we had difficulty using the word2vec loss that treats scores of positive and negative pairs as positive and negative examples to a binary classifier, since this relies on the ability to push up on the energy surface in an absolute, rather than relative, manner. To avoid the problem of absolute energies, we train with a ranking-based loss. We chose a max-margin ranking objective, similar to that used in Rank-SVM <ref type="bibr" target="#b18">(Joachims, 2002)</ref> or Wsabie <ref type="bibr" target="#b35">(Weston et al., 2011)</ref>, which pushes scores of positive pairs above negatives by a margin:</p><formula xml:id="formula_0">L m (w, c p , c n ) = max(0, m − E(w, c p ) + E(w, c n ))</formula><p>In this terminology, the contribution of our work is a pair of energy functions for training Gaussian distributions to represent word types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WARMUP: EMPIRICAL COVARIANCES</head><p>Given a pre-trained set of word embeddings trained from contexts, there is a simple way to construct variances using the empirical variance of a word type's set of context vectors.</p><p>For a word w with N word vector sets {c(w) i } representing the words found in its contexts, and window size W , the empirical variance is</p><formula xml:id="formula_1">Σ w = 1 N W N i W j (c(w) ij − w)(c(w) ij − w)</formula><p>This is an estimator for the covariance of a distribution assuming that the mean is fixed at w. In practice, it is also necessary to add a small ridge term δ &gt; 0 to the diagonal of the matrix to regularize and avoid numerical problems when inverting.</p><p>However, in Section 6.2 we note that the distributions learned by this empirical estimator do not possess properties that we would want from Gaussian distributional embeddings, such as unsupervised entailment represented as inclusion between ellipsoids. By discriminatively embedding our predictive vectors in the space of Gaussian distributions, we can improve this performance. Our models can learn certain forms of entailment during unsupervised training, as discussed in Section 6.2 and exemplified in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ENERGY-BASED LEARNING OF GAUSSIANS</head><p>As discussed in Section 3, our architecture learns Gaussian distributional embeddings to predict words in context given the current word, and ranks these over negatively sampled words. We present two energy functions to train these embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SYMMETRIC SIMILARITY: EXPECTED LIKELIHOOD OR PROBABILITY PRODUCT KERNEL</head><p>While the dot product between two means of independent Gaussians is a perfectly valid measure of similarity (it is the expected dot product), it does not incorporate the covariances and would not enable us to gain any benefit from our probabilistic model.</p><p>The most logical next choice for a symmetric similarity function would be to take the inner product between the distributions themselves. Recall that for two (well-behaved) functions f, g ∈ R n → R, a standard choice of inner product is</p><formula xml:id="formula_2">x∈R n f (x)g(x)dx</formula><p>i.e. the continuous version of i f i g i = f, g for discrete vectors f and g.</p><p>This idea seems very natural, and indeed has appeared before -the idea of mapping data cases w into probability distributions (often over their contexts), and comparing them via integrals has a history under the name of the expected likelihood or probability product kernel <ref type="bibr" target="#b17">(Jebara et al., 2004)</ref>.</p><p>For Gaussians, the inner product is defined as</p><formula xml:id="formula_3">E(P i , P j ) = x∈R n N (x; µ i , Σ i )N (x; µ j , Σ j )dx = N (0; µ i − µ j , Σ i + Σ j )</formula><p>The proof of this identity follows from simple calculus. This is a consequence of the broader fact that the Gaussian is a stable distribution, i.e. the convolution of two Gaussian random variables is another Gaussian.</p><p>Since we aim to discriminatively train the weights of the energy function, and it is always positive, we work not with this quantity directly, but with its logarithm. This has two motivations: firstly, we plan to use ranking loss, and ratios of densities and likelihoods are much more commonly worked with than differences -differences in probabilities are less interpretable than an odds ratio. Secondly, it is easier numerically, as otherwise the quantities can get exponentially small and harder to deal with.</p><p>The logarithm of the energy</p><formula xml:id="formula_4">(in d dimensions) is log N (0; µ i −µ j , Σ i +Σ j ) = − 1 log det(Σ i +Σ j )− 1 (µ i −µ j ) (Σ i +Σ j ) −1 (µ i −µ j )− d log(2π).</formula><p>Recalling that the gradient of the log determinant is ∂ ∂A log det A = A −1 , and the gradient <ref type="bibr" target="#b29">Petersen, 2006)</ref> we can take the gradient of this energy function with respect to the means µ and covariances Σ:</p><formula xml:id="formula_5">∂ ∂A x A −1 y = −A − xy A − (</formula><formula xml:id="formula_6">∂ log E(P i , P j ) ∂µ i = − ∂ log E(P i , P j ) ∂µ j = −∆ ij ∂ log E(P i , P j ) ∂Σ i = ∂ log E(P i , P j ) ∂Σ j = 1 (∆ ij ∆ ij − (Σ i + Σ j ) −1 )</formula><p>where</p><formula xml:id="formula_7">∆ ij = (Σ i + Σ j ) −1 (µ i − µ j )</formula><p>For diagonal and spherical covariances, these matrix inverses are trivial to compute, and even in the full-matrix case can be solved very efficiently for the small dimensionality common in embedding models. If the matrices have a low-rank plus diagonal structure, they can be computed and stored even more efficiently using the matrix inversion lemma.</p><p>This log-energy has an intuitive geometric interpretation as a similarity measure. Gaussians are measured as close to one another based on the distance between their means, as measured through the Mahalanobis distance defined by their joint inverse covariance. Recalling that log det A + const. is equivalent to the log-volume of the ellipse spanned by the principle components of A, we can interpret this other term of the energy as a regularizer that prevents us from decreasing the distance by only increasing joint variance. This combination pushes the means together while encouraging them to have more concentrated, sharply peaked distributions in order to have high energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">ASYMMETRIC SIMILARITY: KL DIVERGENCE</head><p>Training vectors through KL-divergence to encode their context distributions, or even to incorporate more explicit directional supervision re: entailment from a knowledge base or WordNet, is also a sensible objective choice. We optimize the following energy function (which has a similarly tractable closed form solution for Gaussians):</p><formula xml:id="formula_8">−E(P i , P j ) = D KL (N j ||N i ) = x∈R n N (x; µ i , Σ i ) log N (x; µ j , Σ j ) N (x; µ i , Σ i ) dx = 1 (tr(Σ −1 i Σ j ) + (µ i − µ j ) Σ −1 i (µ i − µ j ) − d − log det(Σ j ) det(Σ i ) )</formula><p>Note the leading negative sign (we define the negative energy), since KL is a distance function and not a similarity. KL divergence is a natural energy function for representing entailment between concepts -a low KL divergence from x to y indicates that we can encode y easily as x, implying that y entails x. This can be more intuitively visualized and interpreted as a soft form of inclusion between the level sets of ellipsoids generated by the two Gaussians -if there is a relatively high expected log-likelihood ratio (negative KL), then most of the mass of y lies inside x.</p><p>Just as in the previous case, we can compute the gradients for this energy function in closed form:</p><formula xml:id="formula_9">∂E(P i , P j ) ∂µ i = − ∂E(P i , P j ) ∂µ j = −∆ ij ∂E(P i , P j ) ∂Σ i = 1 (Σ −1 i Σ j Σ −1 i + ∆ ij ∆ ij − Σ −1 i ) ∂E(P i , P j ) ∂Σ j = 1 (Σ −1 j − Σ −1 i )</formula><p>where</p><formula xml:id="formula_10">∆ ij = Σ −1 i (µ i − µ j )</formula><p>using the fact that ∂ ∂A tr(X A −1 Y ) = −(A −1 Y X A −1 ) and ∂ ∂A tr(XA) = X <ref type="bibr" target="#b29">(Petersen, 2006)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">UNCERTAINTY OF INNER PRODUCTS</head><p>Another benefit of embedding objects as probability distributions is that we can look at the distribution of dot products between vectors drawn from two Gaussian representations. This distribution is not itself a one-dimensional Gaussian, but it has a finite mean and variance with a simple structure in the case where the two Gaussians are assumed independent <ref type="bibr" target="#b6">(Brown &amp; Rutemiller, 1977)</ref>. For the distribution P (z = x y), we have</p><formula xml:id="formula_11">µ z = µ x µ y Σ z = µ x Σ x µ x + µ y Σ y µ y + tr(Σ x Σ y )</formula><p>this means we can find e.g. a lower or upper bound on the dot products of random samples from these distributions, that should hold some given percent of the time. Parametrizing this energy by some number of standard deviations c, we can also get a range for the dot product as:</p><formula xml:id="formula_12">µ x µ y ± c µ x Σ x µ x + µ y Σ y µ y + tr(Σ x Σ y )</formula><p>We can choose c in a principled using an (incorrect) Gaussian approximation, or more general concentration bounds such as Chebyshev's inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">LEARNING</head><p>To learn our model, we need to pick an energy function (EL or KL), a loss function (max-margin), and a set of positive and negative training pairs. As the landscape is highly nonconvex, it is also helpful to add some regularization.</p><p>We regularize the means and covariances differently, since they are different types of geometric objects. The means should not be allowed to grow too large, so we can add a simple hard constraint to the 2 norm:</p><formula xml:id="formula_13">µ i 2 ≤ C, ∀i</formula><p>However, the covariance matrices need to be kept positive definite as well as reasonably sized. This is achieved by adding a hard constraint that the eigenvalues λ i lie within the hypercube [m, M ] d for constants m and M . mI</p><formula xml:id="formula_14">≺ Σ i ≺ M I, ∀i</formula><p>For diagonal covariances, this simply involves either applying the min or max function to each element of the diagonal to keep it within the hypercube, Σ ii ← max(m, min(M, Σ ii )).</p><p>Controlling the bottom eigenvalues of the covariance is especially important when training with expected likelihood, since the energy function includes a log det term that can give very high scores to small covariances, dominating the rest of the energy.</p><p>We optimize the parameters using AdaGrad <ref type="bibr" target="#b11">(Duchi et al., 2011)</ref> and stochastic gradients in small minibatches containing 20 sentences worth of tokens and contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>We evaluate the representation learning algorithms on several qualitative and quantitative tasks, including modeling asymmetric and linguistic relationships, uncertainty, and word similarity. All Gaussian experiments are conducted with 50-dimensional vectors, with diagonal variances except where noted otherwise. Unsupervised embeddings are learned on the concatenated ukWaC and WaCkypedia corpora <ref type="bibr" target="#b1">(Baroni et al., 2009)</ref>, consisting of about 3 billion tokens. This matches the experimental setup used by <ref type="bibr" target="#b2">Baroni et al. (2012)</ref>, aside from leaving out the small British National Corpus, which is not publicly available and contains only 100 million tokens. All word types that appear less than 100 times in the training set are dropped, leaving a vocabulary of approximately 280 thousand word types.</p><p>When training word2vec Skip-Gram embeddings for baselines, we follow the above training setup (50 dimensional embeddings), using our own implementation of word2vec to change as little as possible between the two models, only the loss function. We train both models with one pass over the data, using separate embeddings for the input and output contexts, 1 negative sample per positive example, and the same subsampling procedure as in the word2vec paper <ref type="bibr" target="#b25">(Mikolov et al., 2013)</ref>. The only other difference between the two training regimes is that we use a smaller 2 regularization constraint when using the word2vec loss function, which improves performance vs. the diagonal Gaussian model which does better with "spikier" mean embeddings with larger norms (see the comment in Section 6.4). The original word2vec implementation uses no constraint, but we saw better performance when including it in our training setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">SPECIFICITY AND UNCERTAINTY OF EMBEDDINGS</head><p>In <ref type="figure">Figure 2</ref>, we examine some of the 100 nearest neighbors of several query words as we sort from largest to smallest variance, as measured by determinant of the covariance matrix, using diagonal Gaussian embeddings. Note that more specific words, such as joviality and electroclash have smaller variance, while polysemous words or those denoting broader concepts have larger variances, such as mix, mind, and graph. This is not merely an artifact of higher frequency words getting more variance -when sorting by those words whose rank by frequency and rank by variance are most dissimilar, we see that genres with names like chillout, avant, and shoegaze overindex their variance compared Figure 3: Entailment: We compare empirical and learned variances, both diagonal (D) and spherical (S). E is the dataset of <ref type="bibr" target="#b2">Baroni et al. (2012)</ref>. Measures of similarity are symmetric (cosine between means) and asymmetric (KL) divergence for Gaussians. balAPinc is an asymmetric similarity measure specific to sparse, distributional count-based representations.</p><p>to how frequent they are, since they appear in different contexts. Similarly, common emotion words like sadness and sincerity have less variance than their frequency would predict, since they have fairly fixed meanings. Another emotion word, coldness, is an uncommon word with a large variance due to its polysemy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">ENTAILMENT</head><p>As can be seen qualitatively in <ref type="figure">Figure 1</ref>, our embeddings can learn some forms of unsupervised entailment directly from the source data. We evaluate quantitatively on the Entailment dataset of <ref type="bibr" target="#b2">Baroni et al. (2012)</ref>. Our setup is essentially the same as theirs but uses slightly less data, as mentioned in the beginning of this section. We evaluate with Average Precision and best F1 score. We include the best F1 score (by picking the optimal threshold at test) because this is used by <ref type="bibr" target="#b2">Baroni et al. (2012)</ref>, but we believe AP is better to demonstrate the correlation of various asymmetric and symmetric measures with the entailment data.</p><p>In <ref type="figure">Figure 3</ref>, we compare variances learned jointly during embedding training by using the expected likelihood objective, with empirical variances gathered from contexts on pre-trained word2vec-style embeddings. We compare both diagonal (D) and spherical (S) variances, using both cosine similarity between means, and KL divergence. Baseline asymmetric measurements, such as the difference between the sizes of the two embeddings, did worse than the cosine. We see that KL divergence between the entailed and entailing word does not give good performance for the empirical variances, but beats the count-based balAPinc measure when used with learned variances.</p><p>For the baseline empirical model to achieve reasonable performance when using KL divergence, we regularized the covariance matrices, as the unregularized matrices had very small entries. We regularized the empirical covariance by adding a small ridge δ to the diagonal, which was tuned to maximize performance, to give the largest possible advantage to the baseline model. Interestingly, the empirical variances do worse with KL than the symmetric cosine similarity when predicting entailment. This appears to be because the empirically learned variances are so small that the choice is between either leaving them small, making it very difficult to have one Gaussian located "inside" another Gaussian, or regularizing so much that their discriminative power is washed out. Additionally, when examining the empirical variances, we noted that common words like "such," which receive very large variances in our learned model, have much smaller empirical variances relative to rarer words. A possible explanation is that the contrastive objective forces variances of commonly sampled words to spread out to avoid loss, while the empirical variance sees only "positive examples" and has no penalty for being close to many contexts at once.</p><p>While these results indicate that we can do as well or better at unsupervised entailment than previous distributional semantic measures, we would like to move beyond purely unsupervised learning. Although certain forms of entailment can be learned in an unsupervised manner from distributional data, many entailing relationships are not present in the training text in the form of lexical substitutions that reflect the is-a relationship. For example, one might see phrases such as "look at that bird," "look at that eagle," "look at that dog," but rarely "look at that mammal." One appealing aspect of our models versus count-based ones is that they can be directly discriminatively trained to embed hierarchies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">DIRECTLY LEARNING ASYMMETRIC RELATIONSHIPS</head><p>In <ref type="figure" target="#fig_0">Figure 4</ref>, we see the results of directly embedding simple tree hierarchies as Gaussians. We embed nodes as Gaussians with diagonal variances in two-dimensional space using gradient descent on the KL divergence between parents and children. We create a Gaussian for each node in the tree, and randomly initialize means. Negative contexts come from randomly sampled nodes that are neither ancestors nor descendents, while positive contexts come from ancestors or descendents using the appropriate directional KL divergence. Unlike our experiments with symmetric energy, we must use the same set of embeddings for nodes and contexts, or else the objective function will push the variances to be unboundedly large. Our training process captures the hierarchical relationships, although leaf-level siblings are not differentiated from each other by this objective function. This is because out of all the negative examples that a leaf node can receive, only one will push it away from its sibling node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">WORD SIMILARITY BENCHMARKS</head><p>We evaluate the embeddings on seven different standard word similarity benchmarks <ref type="bibr" target="#b31">(Rubenstein &amp; Goodenough, 1965;</ref><ref type="bibr" target="#b34">Szumlanski et al., 2013;</ref><ref type="bibr" target="#b16">Hill et al., 2014;</ref><ref type="bibr" target="#b26">Miller &amp; Charles, 1991;</ref><ref type="bibr" target="#b7">Bruni et al., 2014;</ref><ref type="bibr" target="#b37">Yang &amp; Powers, 2006;</ref><ref type="bibr" target="#b14">Finkelstein et al., 2001)</ref>. A comparison to all of the state of the art word-embedding numbers for different dimensionalities as in <ref type="bibr" target="#b3">(Baroni et al., 2014)</ref> is out of the scope of this evaluation. However, we note that the overall performance of our 50-dimensional embeddings matches or beats reported numbers on these datasets for the 80-dimensional Skip-Gram vectors at wordvectors.org <ref type="bibr" target="#b13">(Faruqui &amp; Dyer, 2014)</ref>, as well as our own Skip-Gram implementation. Figure 5: Similarity: We evaluate our learned Gaussian embeddings (LG) with spherical (S) and diagonal (D) variances, on several word similarity benchmarks, compared against standard Skip-Gram (SG) embeddings on the trained on the same dataset. We evaluate Gaussian embeddings with both cosine between means (m), and cosine between the distributions themselves (d) as defined by the expected likelihood inner product.</p><p>While it is good to sanity-check that our embedding algorithms can achieve standard measures of distributional quality, these experiments also let us compare the different types of variances (spherical and diagonal). We also compare against Skip-Gram embeddings with 100 latent dimensions, since our diagonal variances have 50 extra parameters.</p><p>We see that the embeddings with spherical covariances have an overall slight edge over the embeddings with diagonal covariances in this case, in a reversal from the entailment experiments. This could be due to the diagonal variance matrices making the embeddings more axis-aligned, making it harder to learn all the similarities and reducing model capacity. To test this theory, we plotted the absolute values of components of spherical and diagonal variance mean vectors on a q-q plot and noted a significant off-diagonal shift, indicating that diagonal variance embedding mean vectors have "spikier" distributions of components, indicating more axis-alignment.</p><p>We also see that the distributions with diagonal variances benefit more from including the variance in the comparison (d) than the spherical variances. Generally, the data sets in which the cosine between distributions (d) outperforms cosine between means (m) are similar for both spherical and diagonal covariances. Using the cosine between distributions never helped when using empirical variances, so we do not include those numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this work we introduced a method to embed word types into the space of Gaussian distributions, and learn the embeddings directly in that space. This allows us to represent words not as low-dimensional vectors, but as densities over a latent space, directly representing notions of uncertainty and enabling a richer geometry in the embedded space. We demonstrated the effectiveness of these embeddings on a linguistic task requiring asymmetric comparisons, as well as standard word similarity benchmarks, learning of synthetic hierarchies, and several qualitative examinations.</p><p>In future work, we hope to move beyond spherical or diagonal covariances and into combinations of low rank and diagonal matrices. Efficient updates and scalable learning is still possible due to the Sherman-Woodbury-Morrison formula. Additionally, going beyond diagonal covariances will enable us to keep our semantics from being axis-aligned, which will increase model capacity and expressivity. We also hope to move past stochastic gradient descent and warm starting and be able to learn the Gaussian representations robustly in one pass from scratch by using e.g. proximal or block coordinate descent methods. Improved optimization strategies will also be helpful on the highly nonconvex problem of training supervised hierarchies with KL divergence.</p><p>Representing words and concepts as different types of distributions (including other elliptic distributions such as the Student's t) is an exciting direction -Gaussians concentrate their density on a thin spherical ellipsoidal shell, which can lead to counterintuitive behavior in high dimensions.</p><p>Multimodal distributions represent another clear avenue for future work. Combining ideas from kernel methods and manifold learning with deep learning and linguistic representation learning is an exciting frontier.</p><p>In other domains, we want to extend the use of potential function representations to other tasks requiring embeddings, such as relational learning with the universal schema <ref type="bibr" target="#b30">(Riedel et al., 2013)</ref>. We hope to leverage the asymmetric measures, probabilistic interpretation, and flexible training criteria of our model to tackle tasks involving similarity-in-context, comparison of sentences and paragraphs, and more general common sense reasoning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Synthetic experiments on embedding two simple hierarchies in two dimensions directly using KL divergence. The embedding model captures all of the hierarchical relationships present in the tree. Sibling leaves are pushed into overlapping areas by the objective function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Elements of the top 100 nearest neighbor sets for chosen query words, sorted by descending variance (as measured by determinant of covariance matrix). Note that less specific and more ambiguous words have greater variance.</figDesc><table><row><cell cols="4">Query Word Nearby Words, Descending Variance</cell><cell></cell></row><row><cell>rock</cell><cell cols="5">mix sound blue folk jazz rap avant hardcore chillout shoegaze powerpop</cell></row><row><cell></cell><cell>electroclash</cell><cell></cell><cell></cell><cell></cell></row><row><cell>food</cell><cell cols="5">drink meal meat diet spice juice bacon soya gluten stevia</cell></row><row><cell>feeling</cell><cell cols="5">sense mind mood perception compassion sadness coldness sincerity</cell></row><row><cell></cell><cell cols="3">perplexity diffidence joviality</cell><cell></cell></row><row><cell>algebra</cell><cell cols="5">theory graph equivalence finite predicate congruence topology</cell></row><row><cell></cell><cell cols="3">quaternion symplectic homomorphism</cell><cell></cell></row><row><cell cols="2">Figure 2: Model</cell><cell cols="4">Test Similarity Best F1 AP</cell></row><row><cell></cell><cell cols="2">Baroni et al. (2012) E</cell><cell>balAPinc</cell><cell>75.1</cell><cell>-</cell></row><row><cell></cell><cell>Empirical (D)</cell><cell>E</cell><cell>KL</cell><cell>70.05</cell><cell>.68</cell></row><row><cell></cell><cell>Empirical (D)</cell><cell>E</cell><cell>Cos</cell><cell>76.24</cell><cell>.71</cell></row><row><cell></cell><cell>Empirical (S)</cell><cell>E</cell><cell>KL</cell><cell>71.18</cell><cell>.69</cell></row><row><cell></cell><cell>Empirical (S)</cell><cell>E</cell><cell>Cos</cell><cell>76.24</cell><cell>.71</cell></row><row><cell></cell><cell>Learned (D)</cell><cell>E</cell><cell>KL</cell><cell>79.01</cell><cell>.80</cell></row><row><cell></cell><cell>Learned (D)</cell><cell>E</cell><cell>Cos</cell><cell>76.99</cell><cell>.73</cell></row><row><cell></cell><cell>Learned (S)</cell><cell>E</cell><cell>KL</cell><cell>79.34</cell><cell>.78</cell></row><row><cell></cell><cell>Learned (S)</cell><cell>E</cell><cell>Cos</cell><cell>77.36</cell><cell>.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Note that the numbers are not directly comparable since we use a much older version of Wikipedia (circa 2009) in our WaCkypedia dataset, but this should not give us an edge.</figDesc><table><row><cell>Dataset</cell><cell cols="6">SG (50d) SG (100d) LG/50/m/S LG/50/d/S LG/50/m/D LG/50/d/D</cell></row><row><cell>SimLex</cell><cell>29.39</cell><cell>31.13</cell><cell>32.23</cell><cell>29.84</cell><cell>31.25</cell><cell>30.50</cell></row><row><cell>WordSim</cell><cell>59.89</cell><cell>59.33</cell><cell>65.49</cell><cell>62.03</cell><cell>62.12</cell><cell>61.00</cell></row><row><cell cols="2">WordSim-S 69.86</cell><cell>70.19</cell><cell>76.15</cell><cell>73.92</cell><cell>74.64</cell><cell>72.79</cell></row><row><cell cols="2">WordSim-R 53.03</cell><cell>54.64</cell><cell>58.96</cell><cell>54.37</cell><cell>54.44</cell><cell>53.36</cell></row><row><cell>MEN</cell><cell>70.27</cell><cell>70.70</cell><cell>71.31</cell><cell>69.65</cell><cell>71.30</cell><cell>70.18</cell></row><row><cell>MC</cell><cell>63.96</cell><cell>66.76</cell><cell>70.41</cell><cell>69.17</cell><cell>67.01</cell><cell>68.50</cell></row><row><cell>RG</cell><cell>70.01</cell><cell>69.38</cell><cell>71.00</cell><cell>74.76</cell><cell>70.41</cell><cell>77.00</cell></row><row><cell>YP</cell><cell>39.34</cell><cell>35.76</cell><cell>41.50</cell><cell>42.55</cell><cell>36.05</cell><cell>39.30</cell></row><row><cell>Rel-122</cell><cell>49.14</cell><cell>51.26</cell><cell>53.74</cell><cell>51.09</cell><cell>52.28</cell><cell>53.54</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGEMENTS</head><p>This work was supported in part by the Center for Intelligent Information Retrieval, in part by IARPA via DoI/NBC contract #D11PC20152, and in part by NSF grant #CNS-0958392 The U.S. Government is authorized to reproduce and distribute reprint for Governmental purposes notwithstanding any copyright annotation thereon. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Theoretical foundations of the potential function method in pattern recognition learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Aizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rozonoer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automation and Remote Control, number 25 in Automation and Remote Control</title>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="page" from="821" to="837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silvia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="209" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffaella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngoc-Quynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chung-Chieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;12</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean-Sébastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean-Luc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Means and variances of stochastic vector products with applications to random linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><forename type="middle">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rutemiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Herbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="216" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nam-Khanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Textual similarity with a bag-of-embedded-words model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on the Theory of Information Retrieval, ICTIR &apos;13</title>
		<meeting>the 2013 Conference on the Theory of Information Retrieval, ICTIR &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="117" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Aggregating continuous word embeddings for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representing words as regions in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="57" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Community evaluation and exchange of word vectors at wordvectors.org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evgeniy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ehud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The distributional inclusion hypotheses and lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Geffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probability product kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="819" to="844" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A multiplicative model for learning distributed text-based attribute representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distribution representation of the meaning of words and phrases by a gaussian distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiyoshiyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Masayasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Muraoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Futo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yotaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Processing Society 20th Annual Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In Japanese</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
		<idno>0018-9162</idno>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning the kernel matrix with semidefinite programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><forename type="middle">Rg</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="27" to="72" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">G</forename><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and cognitive processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The matrix cookbook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaare</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Limin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlin</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
		<idno>0001-0782</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hintont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian probabilistic matrix factorization using markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="880" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new set of norms for semantic relatedness measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Szumlanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerie</forename><forename type="middle">K</forename><surname>Sims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Verb similarity on the taxonomy of wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M W</forename><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 3rd International WordNet Conference (GWC-06)</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
