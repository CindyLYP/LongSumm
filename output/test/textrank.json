{"1000": "unfortunately, the best completely unsupervised english pos tagger (that does not make use of a tagging dictionary) reaches only accuracy (christodoulopoulos et al., 2010), making its practical usability questionable at best.\nto this end, we construct a bilingual graph over word types to establish a connection between the two languages (3), and then use graph label propagation to project syntactic information from english to the foreign language (4).\nthese universal pos categories not only facilitate the transfer of pos information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed english labels.\nour final average pos tagging accuracy of compares very favorably to the average accuracy of berg-kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0), and considerably bridges the gap to fully supervised pos tagging performance (96.6).\nas discussed in more detail in 3, we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the english side are individual word types.\nthe edge weights between the foreign language trigrams are computed using a co-occurence based similarity function, designed to indicate how syntactically 2see christodoulopoulos et al.\nalgorithm bilingual pos induction require: parallel english and foreign language data de and df , unlabeled foreign training data f ; english tagger.\n1: def word-align-bitext(de,df ) 2: de pos-tag-supervised(de) 3: a extract-alignments(def , de) 4: g construct-graph(f ,df ,a) 5: g graph-propagate(g) 6: extract-word-constraints(g) 7: f pos-induce-constrained(f ,) 8: return f similar the middle words of the connected trigrams are (3.2).\nto establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (3.3).3 since we have no labeled foreign data, our goal is to project syntactic information from the english side to the foreign side.\nthey considered a semi-supervised pos tagging scenario and showed that one can use a graph over trigram types, and edge weights based on distributional similarity, to improve a supervised conditional random field tagger.\nbecause the information flow in our graph is asymmetric (from english to the foreign language), we use different types of vertices for each language.\nthe foreign language vertices (denoted by vf ) correspond to foreign trigram types, exactly as in subramanya et al.\nfurthermore, we do not connect the english vertices to each other, but only to foreign language vertices.4 the graph vertices are extracted from the different sides of a parallel corpus (de, df ) and an additional unlabeled monolingual foreign corpus f , which will be used later for training.\nwe use two different similarity functions to define the edge weights among the foreign vertices and between vertices from different languages.\nnote, however, that it would be possible to use our graph-based framework also for completely unsupervised pos induction in both languages, similar to snyder et al.\nsince our graph is built from a parallel corpus, we can use standard word alignment techniques to align the english sentences de 5note that many combinations are impossible giving a pmi value of 0; e.g., when the trigram type and the feature instantiation don\u2019t have words in common.\nand their foreign language translations df label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence ( 0.9) alignments def based on these high-confidence alignments we can extract tuples of the form u v, where u is a foreign trigram type, whose middle word aligns to an english word type v.\nto initialize the graph for label propagation we use a supervised english tagger to label the english side of the bitext.7 we then simply count the individual labels of the english tokens and normalize the counts to produce tag distributions over english word types.\nthese tag distributions are used to initialize the label distributions over the english vertices in the graph.\nin the label propagation stage, we propagate the automatic english tags to the aligned italian trigram types, followed by further propagation solely among the italian vertices.\ngiven the bilingual graph described in the previous section, we can use label propagation to project the english pos labels to the foreign language.\nin the first stage, we run a single step of label propagation, which transfers the label distributions from the english vertices to the connected foreign language vertices (say, v lf ) at the periphery of the graph.\nthis stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui vf aligns to english words vy tagged with label y: ri(y) vy ui vy y vy ui vy (1) the second stage consists of running traditional label propagation to propagate labels from these peripheral vertices v lf to all foreign language vertices in the graph, optimizing the following objective: c(q) uivf\\v lf ,ujn (ui) wijqi qj2 uivf\\v lf qi u2 s.t. y qi(y) ui qi(y) ui, y qi ri ui v lf (2) where the qi (i 1, , vf ) are the label distributions over the foreign language vertices and and are hyperparameters that we discuss in we use a squared loss to penalize neighboring vertices that have different label distributions: qi qj2 y(qi(y)qj(y))2, and additionally regularize the label distributions towards the uniform distribution u over all possible labels y it can be shown that this objective is convex in q.\nafter running label propagation (lp), we compute tag probabilities for foreign word types x by marginalizing the pos tag distributions of foreign trigrams ui x x x over the left and right context words: p(yx) x,x qi(y) x,x,y qi(y ) (6) we then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value : tx(y) if p(yx) otherwise (7) we describe how we choose in this vector tx is constructed for every word in the foreign vocabulary and will be used to provide features for the unsupervised foreign language pos tagger.\nwe develop our pos induction model based on the feature-based hmm of berg-kirkpatrick et al.\n(2010) found that this direct gradient method performed better (7 absolute accuracy) than using a feature-enhanced modification of the expectation-maximization (em) algorithm (dempster et al., 1977).8 moreover, this route of optimization outperformed a vanilla hmm trained with em by we adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model.\nthe function : f c maps from the language specific fine-grained tagset f to the coarser universal tagset c and is described in detail in 6.2: ft(x, z) log(tx(y)), if (z) y (11) note that when tx(y) the feature value is and has no effect on the model, while its value is when tx(y) and constrains the hmm\u2019s state space.\nthis formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold on the posterior distribution of tags for a given word type (eq.\ntherefore, the number of fine tags varied across languages for our experiments; however, one could as well have fixed the set of hmm states to be a constant across languages, and created one mapping to the universal pos tagset.\nwe tried two versions of our graph-based approach: no lp: our first version takes advantage of our bilingual graph, but extracts the constraint feature after the first stage of label propagation (eq.\nbecause many foreign word types are not aligned to an english word (see table 3), and we do not run label propagation on the foreign side, we expect the projected information to have less coverage.\nas a result, we are able to extract the constraint feature for all foreign word types and furthermore expect the projected tag distributions to be smoother and more stable.\nthe feature-hmm model works better for all languages, generalizing the results achieved for english by berg-kirkpatrick et al.\nfor comparison, the completely unsupervised feature-hmm baseline accuracy on the universal pos tags for english is 79.4, and goes up to with a treebank dictionary.\nas indicated by bolding, for seven out of eight languages the improvements of the with lp setting are statistically significant with respect to the other models, including the no lp setting.11 overall, it performs better than the hitherto state-of-the-art feature-hmm baseline, and better than direct projection, when we macro-average the accuracy over all languages.", "1001": "fisher vectors have been shown to provide a significant performance gain on many different applications in the domain of computer vision 39, 33, 2, in the domain of video action recognition, fisher vectors and stacked fisher vectors have recently outperformed state-of-theart methods on multiple datasets 33, fisher vectors (fv) have also recently been applied to word embedding (e.g. word2vec 30) and have been shown to provide state of the art results on a variety of nlp tasks 24, as well as on image annotation and image search tasks in all of these contributions, the fv of a set of local descriptors is obtained as a sum of gradients of the loglikelihood of the descriptors in the set with respect to the parameters of a probabilistic mixture model that was fitted on a training set in an unsupervised manner.\nthis work presents a novel approach for fv representation of sequences using a recurrent neural network (rnn).\nthe rnn is trained to predict the next element of a sequence given the previous elements.\nit is applied to two different and challenging tasks: video action recognition and image annotation by sentences.\nseveral recent works have proposed to use an rnn for sentence representation 44, 1, 31, the recurrent neural network fisher vector (rnn-fv) method differs from these works in that a sequence is represented by using derived gradient from the rnn as features, instead of using a hidden or an output layer of the rnn.\nthe paper explores two different approaches for training the rnn for the image annotation and image search tasks.\nin the classification approach, the rnn is trained to predict the following word in the sentence.\nthe vgg convolutional neural network (cnn) is used to extract features from the frames of the video and the rnn is trained to predict the embedding of the next frame given the previous ones.\nspecifically, the vgg image embedding of a frame is projected using a linear transformation which was learned on matching images and sentences by the canonical correlation analysis (cca) algorithm the proposed rnn-fv method achieves state-of-theart results in action recognition on the hmdb51 and ucf101 datasets.\nin image annotation and image search tasks, the rnn-fv method is used for the representation of sentences and achieves state-of-the-art results on the flickr8k dataset and competitive results on other benchmarks.\nused word2vec as the word embedding and then applied fisher vector based on a hybrid gaussian-laplacian mixture model (hglmm) in order to pool the word2vec embeddings of the words in a given sentence into a single representation.\nsince a sentence can be seen as a sequence of words, many works have used a recurrent neural network (rnn) in order to represent sentences 14, 48, 27, 16, to address the need for capturing long term semantics in the sentence, these works mainly use long short-term memory (lstm) or gated recurrent unit (gru) cells.\ngenerally, the rnn treats a sentence as an ordered sequence of words, and incrementally encodes a semantic vector of the sentence, word-by-word.\nour work also uses an rnn in order to represent sentences but takes the derived gradient from the rnn as features, instead of using a hidden or an output layer of the rnn.\ngiven a multiset of vectors, x x1, x2, , xn rd, the standard fv is defined as the gradient of the log-likelihood of x with respect to the parameters of a pre-trained diagonal-covariance gaussian mixture model (gmm).\nin this section, we show that such a model can be obtained by training an rnn to predict the next element in a sequence, given the previous elements.\nhaving this, we propose, for the first time, the rnn-fv: a fisher vector that is based on such an rnn sequence model.\nthe rnn is trained to predict, at each time step i, the next element xi1 of the sequence, given the previous elements x0, ..., xi.\ngiven a sequence of input vectors x , the regression rnn is trained to predict the next vector in the sequence s, i.e., the sequence y the output layer of the network is a fully-connected layer, the size of which would be d, i.e., the dimension of the input vector space.\nthe target at time step i is xi1 (the next element in the sequence), and the loss is: loss(xi1, vi) xi1 vi (2) the rnn can be seen as a generative model, and the likelihood of any vector x being the next element of the sequence, given x0, ..., xi, can be defined as: p (xx0, ..., xi) (2) d/2 exp ( x vi ) (3) we are generally interested in the likelihood of the correct prediction, i.e., in the likelihood of the vector xi1 given x0, ..., xi: p (xi1x0, ..., xi).\nthe rnn-based likelihood of the entire sequence x is: p(x) n1 i0 p (xi1x0, ..., xi) (4) the negative log likelihood of x is: l(x) log (p(x)) n1 i0 log (p (xi1x0, ..., xi)) nd log(2) n1 i0 xi1 vi (5) in order to represent x using the fisher vector scheme, we have to compute the gradient of l(x) with respect to our model\u2019s parameters.\nthe obtained gradient wl(x) would be the (unnormalized) rnn-fv representation of x notice that this gradient is not used to update the network\u2019s weights as done in training - here we perform backpropagation at inference time.\nthe classification application is applicable for predicting a sequence of symbols w1,w2,...,wn that have matching vector representations r(w1) x1, r(w2) x2, ..., r(wn ) xn the rnn predicts the sequence u (w1, w2, , wn ) from the sequence x (x0, x1, denote by m the size of our symbol alphabet, i.e., the number of unique symbols in the input sequences.\nafter the rnn is trained, it is ready to be used as a feature vector extractor for new sequences.\nthe cross-entropy loss at time step i is derived from the probability given to the correct next symbol: lossi log ( piwi1 ) log (pr (wi1w0, ..., wi)) (6) the rnn can be seen as a generative model which gives likelihood to the sequence u : pr(u) n1 i0 pr (wi1w0, ..., wi) n1 i0 piwi1 (7) the negative log likelihood of u is: l(u) log (pr(u)) n1 i0 log ( piwi1 ) (8) by (6) and (8), we get that l(u) equals the loss that would be obtained when x is fed as input, and u as output to the rnn.\ntherefore, we approximated it directly from the gradients of the training sequences, by computing the mean of l(xw ) for each w the normalized partial derivatives of the fv are then: f 1/2 w l(xw ) the action recognition pipeline contains the underlying appearance features used to encode the video, the sequence encoding using the rnn-fv, and an svm classifier on top.\nthe rnn-fv is capable of encoding the sequence properties, and as underlying features, we rely on video encodings that are based on single frames or on fixed length blocks of frames.\ncca using the same vgg representation of video frames as mentioned above and the code of 181, we represented each frame by a vector as follows: we considered the common image-sentence vector space obtained by the cca algorithm, using the best model (gmmhglmm) of trained on the coco dataset we mapped each frame to that vector space, getting a 4096-dimensional image representation.\nthe cca was trained for an unrelated task of image to sentence matching, and its success, therefore, suggests a new application of transfer learning: from image annotation to action recognition.\nwe train the rnn to predict the next element in our video representation sequence, given the previous elements, as described in sec.\nafter each rnn epoch, we extract the rnn-fv representation as described above, train a linear svm classifier on the training set and evaluate the performance on the validation set.\na sentence, being an ordered sequence of words, can be represented as a vector using the rnn-fv scheme.\nan rnn is trained to predict, at each time step i, the next word wi1 of the sentence, given the previous words w0, ..., wi.\nwhen training the rnn for regression, the same 300d input is used, followed by an lstm layer of size the output layer, in this case, is fully-connected, where the (300 dimensional) word embedding of next word is predicted.\nthe regularized cca algorithm 47, where the regularization parameter is selected based on the validation set, is used to match the the vgg representation with the sentence rnn-fv representation.\nrnn training data we employed either the training data of each split in the respective benchmark, or the 2010-englishwikipedia-1m dataset made available by the leipzig corpora collection this dataset contains million sentences randomly sampled from english wikipedia.\nword embedding a word was represented either by word2vec, or by the gmmhglmm representation of 18, projected to a 300d sentence to vgg-encoded-image cca space.\nthe validated values of k were in the set 1, 2, 4, 8, 16, the parameter for rnn-fv was the stopping point of the rnn training, as described in sec.\nnotice that the representation dimension of mean pooling is (like the features we used), the gmm-fv dimension is k 500, where k is the number of clusters and the rnn-fv dimension is table compares our proposed rnn-fv method, combining multiple features together, with recently published methods on both datasets.\nthe combinations were performed using early fusion, i.e, we concatenated the normalized low-dimensional gradients of the models and train multi-class linear svm on the combined representation.\ndataset hmdb51 ucf101 method mp gmm-fv rnn-fv mp gmmfv rnn-fv vgg pca vgg cca c3d table comparing pooling techniques (mean pooling, gmm-fv and rnn-fv) on hmdb51 and ucf101.\nthe effectiveness of rnn-fv as sentence representation is evaluated on the bidirectional image and sentence retrieval task.\nthe first rnn is a generic one: it was trained with the wikipedia sentences as training data and word2vec as word embedding.\nin addition, for each of the three datasets, we trained three rnns with the dataset\u2019s training sentences as training data: one with word2vec as word embedding; one with the cca word embedding derived from the semantic vector space of 18, as explained in sec.\nfor flickr8k, we also trained an rnn for classification (with flickr8k training sentences, and word2vec embedding).\ntraining a classification model on the larger datasets is virtually impractical, since the number of unique words in these datasets is much higher, resulting in a very large softmax layer and a huge number of weights.\nin addition to its lower dimension and natural handling of unseen words, the results obtained by regression rnn-fv are better.\nsec- ond, we notice the competitive performance of the model trained on wikipedia sentences, which demonstrates the generalization power of the rnn-fv, being able to perform well on data different than the one which the rnn was trained on.\nthe rnn-fv representation surpasses the state-of-theart results for video action recognition on two challenging datasets.\nwhen used for representing sentences, the rnnfv representation achieves state-of-the-art or competitive results on image annotation and image search tasks.\nsince the length of the sentences in these tasks is usually short and, therefore, the ordering is less crucial, we believe that using the rnn-fv representation for tasks that use longer text will provide an even larger gap between the conventional fv and the rnn-fv.", "1002": "automatic summarization is studied exhaustively for the news domain (cheng and lapata, 2016; see et al., 2017), while summarization of scientific papers is less studied, mainly due to the lack of large-scale training data.\nin such talks, the presenter (usually a co-author) must describe their paper coherently and concisely (since there is a time limit), provid- ing a good basis for generating summaries.\nbased on this idea, in this paper, we propose a new method, named talksumm (acronym for talk- based summarization), to automatically generate extractive content-based summaries for scientific papers based on video talks.\nthen, using unsupervised alignment algo- rithms, we map the transcripts to the correspond- ing papers\u2019 text, and create extractive summaries.\nsummaries generated with our approach can then be used to train more complex and data- demanding summarization models.\nour main contributions are as follows: (1) we propose a new approach to automatically gener- ate summaries for scientific papers based on video talks; (2) we create a new dataset, that contains summaries for papers from several computer science conferences, that can be used as training data; (3) we show both automatic and human eval- uations for our approach.\nto our knowl- edge, this is the first approach to automatically cre- ate extractive summaries for scientific papers by utilizing the videos of conference talks.\nseveral works focused on generating train- ing data for scientific paper summariza- tion (yasunaga et al., 2019; jaidka et al., 2018; collins et al., 2017; cohan and goharian, 2018).\nmost prominently, the cl-scisumm shared tasks (jaidka et al., 2016, 2018) provide a total of human generated summaries; there, a citations- based approach is used, where experts first read citation sentences (citances) that reference the paper being summarized, and then read the whole paper.\nthus, to create an extractive paper summary based on the transcript, we model the alignment between spo- ken words and sentences in the paper, assuming the following generative process: during the talk, the speaker generates words for describing ver- bally sentences from the paper, one word at each time step.\nas for the transition probabilities, we must model the speaker\u2019s behavior and the transitions between any two sentences in the paper.\n(2015), using to fit it to our case where transitions be- tween any two sentences are allowed, and to handle rare cases where k is close to, or even larger than t then, for each sentence index k 1, ...,k, we define: t (k, k) t (k, k j) k j1, j t (k, k j) k j1, j where , , k (0, 1), and are factors reflecting assumptions (2) and (3) respectively, and for all k, k is normalized s.t. the values of , , and were fixed through- out our experiments at 0.75, 0.5, and the average value of , across all papers, was around the values of these parameters were determined based on eval- uation over manually-labeled alignments between the transcripts and the sentences of a small set of papers.\ndata for evaluation we evaluate the quality of our dataset generation method by training an extractive summarization model, and evaluating this model on a human-generated dataset of sci- entific paper summaries.\ntraining data using the hmm importance scores, we create four training sets, two with fixed-length summaries (150 and words), and two with fixed ratio between summary and paper lengths (0.3 and 0.4).\nfirst, for talksumm-only, we generate a 150- words summary out of the top-ranked sentences extracted by our trained model (sentences from the acknowledgments section are omitted, in case the model extracts any).\nin the second approach, a 150-words summary is created by augmenting the abstract with non-redundant sentences extracted by our model, similarly to the hybrid ap- proach of yasunaga et al.\nbaselines we compare our results to scisumm- net (yasunaga et al., 2019) trained on sci- entific papers summarized by human annotators.\nautomatic evaluation table summarizes the results: both gcn cited text spans and talksumm-only models, are not able to obtain better performance than abstract8 however, for the hybrid approach, where the abstract is aug- mented with sentences from the summaries emit- ted by the models, our talksumm-hybrid out- performs both gcn hybrid and abstract.\nas our goal is to test more comprehensive summaries, we generated summaries composed of sentences (approximately of a long paper).\nwe propose a novel automatic method to gener- ate training data for scientific papers summariza- tion, based on conference talks given by authors.\nthis section elaborates on the example presented in table table extends table by showing the manually-labeled alignment between the com- plete text of the paper\u2019s introduction section, and the corresponding transcript.\neach row in this table corresponds to an interval of consecutive time-steps (i.e., a sub-sequence of the transcript) in which the same paper sentence was selected by the viterbi algorithm.\nthe first column (pa- per sentence) shows the selected sentences; the second column (asr transcript) shows the tran- script obtained by the asr system; the third col- umn (human transcript) shows the manually cor- rected transcript, which is provided for readability title: split and rephrase: better evaluation and stronger baselines (aharoni and goldberg, 2018) paper: processing long, complex sentences is challenging.\ndigging further, we find that of the simple sentences (more than of the unique ones) in the validation and test sets also appear in the training set, which coupled with the good memorization capabilities of seq2seq models and the relatively small number of distinct simple sentences helps to explain the high bleu score to aid further research on the task, we propose a more challenging split of the data we also establish a stronger baseline by extending the seq2seq approach with a copy mechanism, which was shown to be helpful in similar tasks (gu et al., 2016; merity et al., 2017; see et al., 2017).\ntalk transcript: let\u2019s begin with the motivation so processing long complex sentences is a hard task this is true for arguments like children people with reading disabilities second language learners but this is also true for sentence level and nlp systems for example previous work show that dependency parsers degrade performance when they\u2019re introduced with longer and longer sentences in a similar result was shown for neural machine translation where neural machine translation systems introduced with longer sentences starting degrading performance the question rising here is can we automatically break a complex sentence into several simple ones while preserving the meaning or the semantics and this can be a useful component in nlp pipelines for example the split and rephrase task was introduced in the last emnlp by narayan gardent and shimarina where they introduced a dataset an evaluation method and baseline models for this task.\nthis is the full-text version of the example shown in table (our model predicted the alignment based on the raw asr output); finally, the forth column shows whether our model has correctly aligned a paper sentence with a sub-sequence of the transcript.\nrows with no values in this column correspond to transcript sub-sequences which were not asso- ciated with any paper sentence in the manually- labeled alignment.", "1003": "emotions are an important element of human nature and detecting them in the textual messages written by users has many applications in information retrieval and human-computer interaction a common approach to emotion analysis and modeling is categorization, e.g., according to ekman\u2019s basic emotions; namely, anger, disgust, fear, happiness, sadness, and surprise approaches to categorical emotion classi cation based on text often employ supervised machine learning classi ers, which require labeled training data.\nto overcome these limitations, pseudo-labeled datasets are gathered from social media platforms where social media posts are explicitly tagged by the author by using the hashtag symbol () or by adding emoticons this tagged data can be used to create large-scale training data labeled with emotions in a non-speci c domain as in given such a dataset (manually or pseudo labeled), it is then common to train a linear classi er based on bag-of-words (bow) 1url/ permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page.\ndoi: 10.1145/3121050.3121093 representation: representing text samples as sparse vectors, where each vector entry corresponds to the presence of a speci c feature (such as n-grams, punctuation and other) as reported recently by 11, deep learning is a promising approach for solving nlp tasks including text classi cation.\nwhile the aforementioned approach utilizes bow representation and linear classi ers, neural network methods are based on dense vector representations of text samples (word embedding) and are nonlinear.\nin order to utilize the high quality but small size datasets, we propose an ensemble approach that combines both a linear model based on bow, and a non-linear model based on the pre-trained word vectors.\nto our knowledge, this is the rst research that shows how to utilize pre-trained word vectors to improve emotion detection from text in domain-speci c datasets.\npre-trained word vectors were used as an input to neural networks to improve sentiment analysis classi cation 14, this approach also requires large-scale data for the neural network training.\nwe have experimented with several document representations, combining the word vectors, following the notation: dwe (t1, ..., tk ) 1k i1 ai k i1 ai v (ti ) , (1) where dwe is the word embedding based vector representation for document d with k terms, v is some pre-trained word-to-vector mapping (described in section 5), and ai is some weight indicating the relative importance of term ti the document representations we experimented with include: cbow (continuous bag of words) 18: in this case, i,ai 1, which means uniform weights for all terms.\nclassi er weights (class): in this approach we calculated a weight function, w (t , e ) for each term t in the training data which indicates its importance in classifying a document as expressing emotion e we did this by rst representing the documents in a bow binary vector representation where we only extracted unigram features.\nensembles tend to achieve better results when there is a signi cant diversity among the classi ers thus, we utilized the above classi ers that are based on di erent document representations, to form an ensemble model.\nthe ensemble methods we experimented with, follow the notation: men (d ) (mbow (d )) (1 ) (mwe (d )) , (4) wheremen (d ) is the output probability vector for the ensemble classi er given a test document d ,mbow (d ) andmwe (d ) are the output probability vectors of the bow and the word embedding based classi ers respectively, and is a parameter which corresponds to the speci c ensemble method used.\nwe have experimented with the following weighed average probabilities methods: equal weights ( 0.5), stacking ( is learned by an additional classi er) and precision-based weighting ( re ects the ratio between the macro precision scores for the two classi ers over the training data).\npenalty parameter for bow classi er: c ; penalty and kernel parameters for the word embedding based classi er: c 4, we evaluated our methods for each dataset by using 10-fold crossvalidation.\nwe compared the quality of two publicly available pre-trained word vector sources, based on glove4 and word2vec (googlenews)5, in terms of emotion detection performance.\ntable depicts the macro f1-scores for each dataset and each document representation method that is based on word vectors, as detailed in section results show that for all datasets and representation methods, word vectors trained by glove achieved higher performance than using word2vec based vectors.\ntable depicts the macro f1-scores for each dataset, and for the di erent models: bow is our baseline, presented in section en-cbow, en-tfidf and en-class are the ensemble models of bow and the corresponding embedded representations presented in section our ensemble methods outperformed the baseline for each document representation method.\nwe presented class, a novel method for representing a document as a dense vector based on the importance of the document\u2019s terms in respect to emotion classi cation.\nour results show that an ensemble that combines bow and embedded representations using our class method, outperforms previous approaches for domain-speci c datasets.", "1004": "typical emotions expressed by customers in the context of social media service dialogues include anger and frustration, as well as gratitude and more (gelbrich, 2010).\na possible application here is recommending to customer service agents what should be their emotional response (for example, in each situation, should they apologize, should they thank the customer, etc.) another interesting trend in customer service, in addition to the use of social media described above, is the automation of various functions of customer interaction.\ngiven the importance of emotions in service dialogues, such systems will benefit from the ability to detect (customer) emotions and will need to guide employees (and machines) regarding the right emotional technique in various situations (e.g., apologizing at the right point).\n(2) this is the first research using unique dialogue features (e.g., emotions expressed in previous dialogue turns by the agent and customer, time between dialogue turns) to improve emotion detection.\n(3) this is the first research studying the prediction of the agent emotional techniques to be used in the response to customer turns.\ntheir approach for emotion classification is different from ours, for example they only considered the last turn as informative (we consider the full context of the dialogue), and focused on eliciting emotions, while we focus on predicting the agent emotional technique.\nin the present work, we define a dialogue to be a sequence of turns between a specific customer and an agent, where the customer initiates the first turn.\ngiven the nature of customer support services, we assume the last turn in the dialogue is an agent turn (e.g., you\u2019re very welcome.\nthe first objective of our work is to detect emotions expressed in customer turns and the second is to predict the emotional technique in agent turns.\nwhen detecting emotions in a customer turn, the turn\u2019s content is available at classification time (as well as the history of the dialogue) - meaning, the customer has already provided her input and the system must now understand what is the emotion being expressed.\nwhereas, when predicting the emotional technique for an agent turn, the turn\u2019s content is not available during classification time, but only the agent action and the history of the dialogue since the agent did not respond yet.\nthis difference stems from the fact that in order to train an automated service agent to respond based on customer input, the agent\u2019s emotional technique needs to be computed before the agent generates its response sentence.\nwe defined a different set of relevant emotion classes for each party in the dialogue (customer or agent), based on our above survey of research on customer service (e.g., (gelbrich, 2010)).\nusing these features for emotion classification in written dialogues is novel, and as our experimental results show, it improves performance compared to a model based only on features extracted from the turn\u2019s text.\nexamples of topics include ac- count issues, payments, technical problem and more this feature set captures the notion that customer emotions are influenced by the event that led the customer to contact the customer service (steunebrink et al., 2009).\nagent essence: a set of local binary features that represent the action used by the agent to address the last customer turn, independently of any emotional technique expressed.\nfor instance, asking for more information and offering a solution are possible essences this feature set captures the notion that customer emotions are influenced by actions of agents (little et al., 2013).\nthe emotional family of features includes agent emotion and customer emotion: these two sets of local binary features represent emotions predicted for previous turns.\nour model generates predictions of emotions for each customer and agent turn, and uses these predictions as features to classify a later customer or agent turn with emotion expression.\nthe temporal family of features includes the following features extracted from the timeline of the dialogue: customer/agent response time: two local features that indicate the time elapsed between the timestamp of the last customer/agent turn and the timestamp of the subsequent turn.\nthese features, that are historical, include the emotional features family and local integral features (namely agent emotions, customer emotions and agent essence).\nfigure shows an example of the historical features in relation to the classification of customer turn ti, for history size between and these features are extracted from the text of a customer turn, without considering the context of the dialogue.\nfor both of the agent and customer turn classification tasks, we implemented two different models which incorporate all of the feature sets we have detailed above.\ntextual features are extracted for ti if it is a customer turn, or for ti1 if it is an agent turn (recall that the system does not have the content of agent turn ti at classification time).\nas discussed above, agent essence is assumed to be an input to our module, while agent emotion and customer emotion features are propagated from classification results of previous turns during testing (or from ground truth labels during training), where the number of previous turns is determined according to the value of history.\nsince emotions expressed in customer and agent turns are different, we treated them as different classification tasks (like in our previous approach) and trained a separate classifier for each emotion.\nwe made the following changes when using svm-hmm: (1) we treated the emotion classification problem of turn ti as a sequence classification problem of the sequence t1, t3, ..., ti (i.e., only customer turns) if ti is a customer turn and t2, t4, ..., ti (i.e., only agent turns) if it is an agent turn.\nagent emotion) feature sets when representing a feature vector for a customer (resp.\neach 4url/ judge performed the following tagging tasks given the full dialogue: emotion tagging: indicate the intensity of emotion expressed in each turn (customer or agent) for each emotion, on a scale of (0...5), such that defines no emotion, a low emotion intensity and a high emotion intensity.\neach test dialogue was classified by its order of turns, where each turn type (customer or agent) is classified by its corresponding classifier.\nthus, the baseline representation of an agent turn consisted of textual features extracted from its preceding customer turn.\nthis means that information about the behavior of the customer and agent in past turns is beneficial for detecting customer emotions in a current turn.\nfor assessing the performance of our predictions of agent turns emotion techniques, we first note that we tested with history range, since we assume that the minimal information needed for agent turn classification is the information extracted from the last customer turn.\nthis indicates that for agent emotion technique prediction the last customer turn is the most informative one.\ntable depicts the detailed classification results for optimal history values that obtained maximal macro f1-score, namely for customer emotion detection history and for agent emotion technique prediction history the table presents performance for each emotion, for macro and micro average results over all dialogues, and for each data source (gen or tech) separately.\nfor customer turn emotion detection, the svmhmm dialogue model performed better than the svm dialogue model, and reached a macro and micro average f1-score improvements over all dialogues of and 11.7, respectively.\nfor predicting the agent emotional technique, the svm dialogue model obtained slightly better results than svm-hmm dialogue model, and reached a macro and micro average f1-score improvements over all dialogues of and 43.5, respectively.\nspecifically, when history size is large, as in customer emotion prediction, svm-hmm dialogue model, which internally captures dependencies in past classifications, outperforms the simplistic svm dialogue model.\nin this work we studied emotions being expressed in customer service dialogues in the social media.\nspecifically, we described two classification tasks, one for detecting customer emotions and the other for predicting the emotional technique used by support service agent.", "1005": "in meta-learning, the inputs to both train and test phases are not images, but instead a set of few-shot tasks, ti, each k-shot / n -way task containing a small amount k (usually 1-5) of labeled support images and some amount of unlabeled query images for each of the n categories of the task.\nin light of the above, in this paper we set to explore methods for architecture search, their meta-adaptation and optimization for fsc.\nin this work, we build our few-shot task-adaptive architecture search upon a technique from d-nas (darts 34).\nour goal is to learn a neural network where connections are controllable and adapt to the few-shot task with novel categories.\nthe metadapt controllers adjust the weights of the different operations, such that if some operations are better for the current task they will get higher weights, thus, effectively modifying the architecture and adapting it to the task.\nto summarize, our contributions in this work are as follows: (1) we show that darts-like bi-level iterative optimization of layer weights and network connections performs well for few-shot classification without suffering from overfitting due to over-parameterization; (2) we show that adding small neural networks, metadapt controllers, that adapt the connections in the main network according to the given task further (and significantly) improves performance; (3) using the proposed method, we obtain improvements over fsc state-of-the-art on two popular fsc benchmarks: miniimagenet and fc100 the major approaches to few-shot learning include: metric learning, generative (or augmentation) based methods, and meta learning (or learning-to-learn).\nthis type of methods 64,55,49 learn a non-linear embedding into a metric space where l2 nearest neighbor (or similar) approach is used to classify instances of new categories according to their proximity to the few labeled training examples embedded in the same space.\nthis family of approaches refers to methods that (learn to) generate more samples from the one or a few examples available for training in a given few-shot learning task.\nthese methods include synthesizing new data from few examples using a generative model, or using external data for obtaining additional examples that facilitate learning on a given few shot task.\nthese approaches include: (i) semi-supervised approaches using additional unlabeled data 9,14; (ii) fine tuning from pre-trained models 31,62,63; (iii) applying domain transfer by borrowing examples from relevant categories or using semantic vocabularies 3,15; (iv) rendering synthetic examples 42,10,56; (v) augmenting the training examples using geometric and photometric transformations or learning adaptive augmentation strategies 21; (vi) example synthesis using generative adversarial networks (gans) 69,25,20,48,45,35,11,23,2.\nin 61, a generator sub-net is added to a classifier network and is trained to synthesize new examples on the fly in order to improve the classifier performance when being fine-tuned on a novel (few-shot) task.\nin models are trained to perform set-operations (e.g. union) and then can be used to synthesise samples for few-shot multi-label classifications.\nthese methods are trained on a set of few-shot tasks (also known as episodes\u2019) instead of a set of object instances, with the motivation to learn a learning strategy that will allow effective adaptation to new such (few-shot) tasks using one or few examples.\nan important sub-category of meta learning methods is metric-meta-learning, combining metric learning as explained above with task-based (episodic) training of meta-learning.\nin matching networks 60, a non-parametric k-nn classifier is meta-learned such that for each few-shot task the learned model generates an adaptive embedding space for which the task can be better solved.\nanother family of meta-learning approaches is the so-called gradient based approaches\u2019, that try to maximize the adaptability\u2019, or speed of convergence, of the networks they train to new (few-shot) tasks (usually assuming an sgd optimizer).\nin other words, the meta-learned classifiers are optimized to be easily fine-tuned on new few-shot tasks using small training data.\nnotably, in all previous meta-learning methods, only the parameters of a (fixed) neural network are optimized through meta-learning in order to become adaptable (or partially adaptable) to novel few-shot tasks.\nin this work, we both learn a specialized backbone architecture that would facilitate this adaptability, as well as meta-learn to become capable of adapting that architecture itself to the task, thus going beyond the parameter only adaptation of all previous metalearning approaches.\nalthough achieving state of the art performance at the time, these methods required enormous amount of gpu-hours efficient nas (enas) 43, a reinforcement learning based method, used weight sharing across its child models, which are sub graphs of a larger one.\nauto-meta used pnas based search for few-shot classification, but with a focus on searching for a small architecture (resulting in a relatively low performance w.r.t. to current state-of-the-art).\nin particular, the possibility of adapting the architecture at test-time to a specific novel task, as proposed in this work, has not been explored before.\nwe introduce the task-adaptable block, it has a graph structure with adaptable connections that can modulate the architecture, adapting it to the few-shot task at hand.\nwe use resnet9 followed by a single task-adaptive block with nodes (v 4) in our experiments, resulting in about times more parameters compared with the original resnet12 (due to large set of operations on all connections combined).\nthey are responsible for predicting, given a few-shot task, the best way of adapting the mixing coefficients ( (i,j) o ) for the corresponding edge operations.\nfinally, (i,j) (i,j) (i,j) (3) are the final task-adapted coefficients used for the mixed operation calculation as in equation the architecture for each metadapt controller, predicting (i,j), is as follows.\nresearchers have faced the same problem with differentiable architecture search, where the objective is to train a large neural network with weighted connections that are then pruned to form the final chosen architecture.\nthe \u2019s are optimized using sgd with a secondorder approximation of the model after convergence of w, by applying: losstrain(w losstrainw(w,), ) (5) where is the learning rate for the metadapt controllers\u2019 parameters are trained as a final step, with all other parameters freezed, using sgd on the entire training set for a single epoch.\nwe then replace the last residual block of the resnet12 backbone with our dag task-adaptive block, keeping the first resnet blocks (resnet9) fixed and perform the architecture search for epochs.\nfor the initial training we use the sgd optimizer with intial learning rate 0.1, momentum and weight decay decreasing the learning rate to at epoch 20, at epoch and at epoch for weights optimization during the search and meta adaptation phases we use the sgd optimizer with learning rate 0.001, momentum and weight decay for the architecture optimization we use adam optimizer with learning rate 104, 0.5, 0.99, weight decay and the cosine annealing learning rate scheduler with min following previous works, e.g. 13,5, we perform test time augmentations and fine-tuning.\ntables and compare the metadapt performance with the state-of-the-art few-shot classification methods that use plain resnet backbones.\nwe follow this route, showing in our experiment that we can learn the values on fc100 and transfer them to miniimagenet (and then train the weights w of the transferred architecture on this dataset).\nwe find that the architecture transferred from fc100 to miniimagenet is performing well, with results comparable to other state-of-the-art methods, 62.82/79.35 for 1/5-shot.\nto confirm our hypothesis, we used sgd to train our suggested dag architecture, using fixed uniform instead of the learned (making it even less likely to over-fit compared to the ablation in darts).\nthen, we add the metadapt controllers, so the architecture is adapted to the current task according to the support samples.\ndarts, at search time the training is done on the full model at each iteration where each edge is a weighted-sum of its operations according to i,j contrarily, in snas i,j are treated as probabilities of a multinomial distribution and at each iteration a single operation is sampled accordingly.\nin this work we have proposed metadapt, a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks.\nthe proposed approach effectively applies tools from the neural architecture search (nas) literature, extended with the concept of metadapt controllers\u2019, in order to learn adaptive architectures.\nthese tools help mitigate over-fitting to the extremely small data of the few-shot tasks and domain shift between the training set and the test set.\nsome interesting future work directions include extending the proposed approach to progressively searching the full network architecture (instead of just the last block), applying the approach to other few-shot tasks such as detection and segmentation, and researching into different variants of task-adaptivity including global connections modifiers and inter block adaptive wiring.", "1006": "automated conversational agents (chatbots) are becoming widely used for various tasks such as personal assistants or as customer service agents.\nas an aid to chatbot improvement, analysis of egregious conversations can often point to problems in training data or system logic that can be repaired.\nspecifically, we consider customer inputs throughout a whole conversation, and detect cues such as rephrasing, the presence of heightened emotions, and queries about whether the chatbot is a human or requests to speak to an actual human.\nfinally, we analyze the larger conversational context exploring, for example, where the presence of a not trained response might be especially problematic (e.g., in the presence of strong customer emotion).\nthe main contributions of this paper are twofold: (1) this is the first research focusing on detecting egregious conversations in conversational agent (chatbot) setting and (2) this is the first research using unique agent, customer, and customer-agent interaction features to detect egregiousness.\ndetecting egregious conversations is a new task, however, there is related work that aim at measuring the general quality of the interactions in conversational systems.\nspecifically, these works evaluated chat functionality by asking users to make conversations with an intelligent agent and measured the user satisfaction along with other features such as the automatic speech recognition (asr) quality and intent classification quality.\nin our work, we likewise use emotion analysis as predictive features for egregious conversation.\nour work also explores user reformulation (or rephrasing) as one of the features to predict egregious conversations.\nthe main difference with our work is that we focus on chat logs for domains for which the expected user utterances are a bit more diverse, using interaction features as well as features that are not sensitive to any architectural aspects of the conversational system (e.g., asr component).\nthe objective of this work is to reliably detect egregious conversations between a human and a virtual agent.\nwhile we are currently applying this to complete conversations (i.e., the classification is done on the whole conversation), some of the features examined here could likely be used to detect egregious conversations as they were unfolding in real time.\nto perform egregious conversation detection, features from both customer inputs and agent responses are extracted, together with features related to the combination of specific inputs and responses.\nusing this set of features for detecting egre- gious conversations is novel, and as our experimental results show, improves performance compared to a model based solely on features extracted from the conversation\u2019s text.\nwhen the agent starts losing the context of a conversation, fails in understanding the customer intention, or keeps repeating the same responses, the illusion of conversing with a human is lost and the conversation may become extremely annoying.\nin other cases, unsupported intents can lead to customer dissatisfaction (sarikaya, 2017), and cascade to an egregious conversation (as discussed below in section 3.3).\ncustomer features are summarized in the middle part of table when a customer repeats or rephrases an utterance, it usually indicates a problem with the agent\u2019s understanding of the customer\u2019s intent.\nusually, high positive emotions capture different styles of thanking the agent, or indicate that the customer is somewhat satisfied (rychalski and hudson, 2017), thus, the conversation is less likely to become egregious.\nin examining the conversation logs, we noticed that it is not unusual to find a customer asking to be transferred to a human agent.\nthe assumption is that single word (unigram) sentences are probably short customer responses (e.g., no, yes, thanks, okay), which in most cases do not contribute to the egregiousness of the conversation.\nwe also looked at features across conversation utterance-response pairs in order to capture a more complete picture of the interac- tion between the customer and the virtual agent.\nfor example, a pair may contain a turn in which the customer expressed negative emotions and received a response of not trained by the agent.\nthese features are summarized in the last part of table we also calculated the similarity between the customer\u2019s turn and the virtual agent\u2019s response in cases of customer rephrasing.\nwhen a similarity score between the customer\u2019s turn and the agent\u2019s response is low, this may indicate a misclassified intent, as the agent\u2019s responses are likely to share some textual similarity to the customer\u2019s utterance.\nanother similarity feature is between two customer\u2019s subsequent turns when the agent\u2019s response was not trained.\nwe extracted data from two commercial systems that provide customer support via conversational bots (hereafter denoted as company a and company b).\neach system logs conversations, and each conversation is a sequence of tuples, where each tuple consists of conversation id, turn id, customer input, agent response.\na model that was trained to predict egregiousness given the conversation\u2019s text (all customer and agent\u2019s text dur- 8judges that are hci experts and have experience in designing conversational agents systems.\nin (herzig et al., 2017) emotions are detected from text, which can be thought of as similar to our task of predicting egregious conversations.\nbased on the groups of feature sets that we defined in section 3, we tested the performance of different group combinations, added in the following order: agent, customer and customer-agent interactions.\nspecifically, in our setting, the relevant motivations are12: (1) natural language understanding (nlu) error - the agent\u2019s intent detection is wrong, and thus the agent\u2019s response is semantically far from the customer\u2019s turn; (2) language generation (lg) limitation - the intent is detected correctly, but the customer is not satisfied by the response (for example, the response was too generic); (3) unsupported intent error - the customer\u2019s intent is not supported by the agent.\nin order to detect nlu errors, we measured the similarity between the first customer turn (before the rephrasing) and the agent response.\nto detect unsupported intent error we used the approach described in section as reported in table 4, rephrasing due to an unsupported intent is more common in egregious conversations (18 vs.\nwe did not encounter any unsupported intent errors leading to customer rephrasing, which affected the ability of the rule-based model to classify those conversations as egregious.\nin addition, the customer intents that appeared in those conversations were very diverse.\nwhile customer rephrasing was captured by the egr model, for the text-based model some of the intents were new (did not appear in the training data) and thus were difficult for the model to capture.\nin this paper, we have shown how it is possible to detect egregious conversations using a combination of customer utterances, agent responses, and customer-agent interactional features.\nwe also plan to extend the work to detect egregious conversations in real time (e.g., for escalating to a human operators), and create log analysis tools to analyze the root causes of egregious conversations and suggest possible remedies.", "1007": "filters also detect negative items in ngrams they not only select for a family of ngrams but often actively suppress a related family of negated ngrams (section 5.4).\nwe also show that the filters are trained to work with naturally-occurring ngrams, and can be easily misled (made to produce values substantially larger than their expected range) by selected nonnatural ngrams.\nwe consider the common architecture in which each word in a document is represented as an embedding vector, a single convolutional layer with m filters is applied, producing an m-dimensional vector for each document ngram.\ncurrent common wisdom posits that filters serve as ngram detectors: each filter searches for a specific class of ngrams, which it marks by assigning them high scores.\nthe final decision is then based on the set of ngrams in the max-pooled vector (represented by the set of corresponding filters).\nintuitively, ngrams which any filter scores highly (relative to how it scores other ngrams) are ngrams which are highly relevant for the classification of the text.\nin this section we refine this view by attempting to answer the questions: what information about ngrams is captured in the max-pooled vector, and how is it used for the final classification?1 consider the pooled vector p rm on which the classification is based.\nnote that the final classification does not observe the ngram identities directly, but only through the scores assigned to them by the filters.\ndeliberate ngrams end up in sp because they were scored high by their filter, likely because they are informative regarding the final decision.\nwe instead work with the purity of a filter-threshold combination, defined as the percentage of informative (correlative) ngrams which were scored above the threshold3.\nfor the rest of this work we will assume a known threshold value for every filter in the model which we can use to identify important ngrams.\nprevious work looked at the top-k scoring ngrams for each filter.\nwe insead look at the set of deliberate ngrams: those that pass the filter\u2019s threshold value.\nthe ngram score can be decomposed as a sum of individual word scores by considering the inner products between every word embedding wi in u and every parallel slice in f : u, f i0 wi, fid:i(d1) we refer to slice fid:i(d1) as slot i of the filter weights, denoted as f(i).\nwe can now move from examining the activation of an ngram-filter pair u : w1; ...;w, f to examining its slot activation vector: (w1, f(1), ..., w, f()).\nin our elec model, when averaging over all filters, the top naturally-occurring ngrams score less than the top possible ngrams.\nthis bias is a single number (per filter) which is added to the sum of slot activations to arrive at the ngram activation which is passed to the max-pooling layer.\ntop-scoring natural ngrams almost never fully activate all slots in a filter.\ntable shows the top-scoring naturally occurring and possible ngrams for nine filters in the elec model.\ntable zooms in on one of the filters and shows its top7 naturally occurring ngrams and top-7 most activated words in each slot.\non naturally occurring ngrams, the filters do not achieve maximum values in all slots but only on some of them.\nwe consider two hypotheses to explain this behavior: (i) each filter captures multiple semantic classes of ngrams, and each class has some dominating slots and some non-dominating slots (which we define as a slot activation pattern).\nwe explore hypothesis (i) by clustering thresholdpassing (naturally occurring) ngrams in each filter according to their activation vectors.\nto summarize, by discarding noisy ngrams which do not pass the filter\u2019s threshold and then clustering those that remain according to their slot activation patterns, we arrived at a clearer image 6intuitively, we can think of the sampling noise as the ngram embeddings, and the probability distribution as defined by a function of the filter weights.\nour second theory to explain the discrepancy between the activations of naturally occurring and possible ngrams is that certain filter slots are not used to detect a class of highly activating words, but rather to rule out a class of highly negative words.\nindeed, ngrams containing not or n\u2019t in slot do not pass the threshold for this filter.\nidentifying negative slot activations would be very useful for understanding the semantics captured by a filter and the reasoning behind the dismissal of an ngram, as we discuss in sections and respectively.\nin order to identify case negative ngrams, we heuristically test whether the changed words\u2019 scores directly influence the status of the activation relative to the threshold: given an already identified negative ngram, if the ngram scoresans the bottom-k negative slot activations (considering a hamming distance of k and given that there are k negative slot activations)passes the threshold, yet it does not pass the threshold by including the negative slot activations, then the ngram is considered a case negative ngram.\nwe propose to associate each filter with the following items: 1) the class which this filter\u2019s strong signals contribute to (in the sentiment task: positive or negative); 2) the threshold value for the filter, together with its purity and coverages percentages (which essentially capture how informative this filter is); 3) a list of semantic patterns identified by this filter.\nin particular, by clustering the activated ngrams according to their slot activation patterns and showing the top-k in each clusters, we get a much more refined coverage of the linguistic patterns that are captured by the filter.\nhere we improve these previous attempts by considering only ngrams that pass the threshold for their filter.\nnote that in example 1, many negative-class filters were forced to choose an ngram in max-pooling despite there not being strongly negative phrasesbut those ngrams do not pass the threshold and are thus cleaned from the explanation.\nfinally, we can also mark cases of negative-ngrams (section 5.4), where an ngram has high slot activations for some words, but these are negated by a highly-negative slot and as a consequence are not selected by max-pooling, or are selected but do not pass the filter\u2019s threshold.\nwe decompose the ngram score into word-level scores by treating the convolution of a filter as a sum of word-level convolutions, allowing us to examine the word-level composition of the activation.\nspecifically, by maximizing the word-level activations by iterating over the vocabulary, we observed that filters do not maximize activations at the word-level, but instead form slot activation patterns that give different types of ngrams similar activation strengths.\nby clustering high-scoring ngrams according to their slotactivation patterns we can identify the groups of linguistic patterns captured by a filter.\nwe also show that filters sometimes opt to assign negative values to certain word activations in order to cause the ngrams which contain them to receive a low score despite having otherwise highly activating words.", "1008": "on the other hand, hard attention methods perform abstraction only on text regions that were initially selected by some extraction process (chen and bansal, 2018; nallapati et al., 2017; liu et al., 2018).\ncompared to previous works, whose final summary is either entirely extracted or generated using an abstractive process, in this work, we suggest a new idea of editorial network (editnet) a mixed extractive-abstractive summarization approach.\na summary generated by editnet may include sentences that were either extracted, abstracted or of both types.\neditnet is applied as a post-processing step over a given input summary whose sentences were initially selected by some extractor.\nthe first decision is to keep the extracted sentence untouched (represented by label e in figure 1).\nfor a given sentence s d, we now denote by se and sa its original (extracted) and paraphrased (abstracted) versions.\nrecall that, for each sentence si s (in order) the editor makes one of the three possible decisions: extract, abstract or reject si.\ntherefore, the editor may modify summary s by paraphrasing or rejecting some of its sentences, resulting in a mixed extractive-abstractive summary s.\nin each step i 1, 2, , l, in order to make an educated decision, the editor considers both sentence representations esi and asi as its input, together with two additional auxiliary representations.\nthe second auxiliary representation is that of the summary that was generated by the editor so far, denoted at step i as gi1 rn, with g0 such a representation provides a local context for decision making.\ngiven the four representations as an input, the editor\u2019s decision for sentence si s is implemented using two fully-connected layers, as follows: softmax (v tanh (wcesi , asi , gi1, d bc) b) , (1) where denotes the vectors concatenation, v r3m, wc rm4n, bc rm and b r3 are learnable parameters.\ngiven text s (with l extracted sentences), let (1, , l) denote its editing decisions 1such a representation is basically a combination of a temporal convolutional model followed by a bilstm encoder.\nwe define the following soft crossentropy loss: l(s) l sis ie,a,r y(i) log p(i), (3) where, for a given sentence si s, y(i) denotes its soft-label for decision.\n, i1, i) denote the average r() value obtained by decision sequences that start with the prefix (1, based on , the soft label y(i) is then calculated3 as follows: y(i) r(1, , i1, j) (4) we trained, validated and tested our approach using the non-annonymized version of the cnn/dailymail dataset (hermann et al., 2015).\nwe further applied the teacher-forcing approach (lamb et al., 2016) during training, where we considered the true-label instead of the 3for i we have: r(1 , editor\u2019s decision (including when updating gi at each step i according to eq.\nthis includes the extractor (rnn-ext-rl) and abstractor (rnn-ext-absrl) components of (chen and bansal, 2018) that we utilized for implementing editnet we further report the quality of editnet when it was being enforced to take an extract-only or abstract-only decision, denoted hereinafter as editnete and editneta, respectively.\nyet, while neusum applies an extraction-only approach, summaries generated by editnet include a mixture of sentences that have been either extracted or abstracted.\nmoreover, on average, per summary, editnet keeps only of the original (extracted) sentences, while the rest (67) are abstracted ones.\nthis demonstrates that, editnet has a high capability of utilizing abstraction, while being also able to maintain or reject the original extracted text whenever it is estimated to provide the best benefit for the summary\u2019s quality.\nmoreover, editnet implements a novel sentence rejection decision, allowing to correct initial sentence selection decisions which are predicted to negatively effect summarization quality.", "1009": "a reli- able approach for generating phonetically similar words is equally crucial for chinese text (xia et al., 2006).\nnear-homonyms of from table are shown in table since both dm and soundex ignore vowels and tones, words with dissimilar pronunciations are incorrectly assigned to the same encoding (e.g. and ), while true nearhomonyms are encoded much further apart (e.g. on the other hand, additional candidates with similar phonetic distances such as xin1fan2xi1fang1 for should be generated, for consumption by downstream applications such as text normalization.\nthe example highlights the importance of considering all pinyin components and their characteristics when calculating chinese phonetic similarity (xia et al., 2006).\ninitial groups z, c, zh, ch, z, zh and zh, ch are all similar, which cannot be captured using a one dimensional representation (e.g., an encoding of zh0,z1,c2,ch3 fails to identify the zh, ch pair as similar.) aline (kondrak, 2003) is another illustration of the challenge of manually assigning numerical values in order to accurately represent the complex relative phonetic similarity relationships across various languages.\nthis paper presents dimsim, a learned ndimensional phonetic encoding for chinese along with a phonetic similarity algorithm, which uses the encoding to generate and rank phonetically similar words.\nto address the complexity of relative phonetic similarities in pinyin components, we propose a supervised learning approach to learn n dimensional encodings for finals and initials where n can be easily extended from one to two or higher dimensions.\na simple and effective phonetic similarity algorithm to generate and rank phonetically similar chinese words.\na package release of the implemented algorithm and a constructed dataset of chinese words with phonetic corrections.2 dimsim generates ranked candidate words with similar pronunciation to a seed word.\nan important characteristic of pinyin is that the three components, initial, final and tone, can be independently phonetically compared.\nfor example, the phonetic similarity of the finals ie and ue is identical in the pinyin pairs xie2,xue2 and lie2,lue2, in spite of the varying initials.\nformally, the phonetic similarity s between the pronunciation of ci and ci is computed using manhattan distance as the sum of the distances between the three pairs of components, as follows: 1ik s(ci, c i) 1ik sp(pici , pici) sp(p f ci , p f ci ) st (p t ci , p t ci ) (1) manhattan distance is an appropriate metric since the three components are independent.\nit follows that the similarity between two words is computed as the sum of the phonetic distances of characters.\nthe latter consists of word pairs, with specific pairs of initials or finals manually annotated for phonetic similarity.\nthe set of annotated pairs between initials and finals are then used to learn the n-dimensional encodings of initials and finals, which will in turn be used for generating phonetically similar candidates.\nmeasuring phonetic similarity dimsim represents a given chinese word w as a list of chinese characters ci1 i k where k is the umber of characters in w and pci denotes the pinyin of ith character.\nformally, the phonetic similarity s between the pronunciation of two characters, ci and c0i is computed using manhattan distance as the sum of the distances between the three pairs of components, as follows: x 1ik s(ci, c i) x 1ik sp(pici , p i c0i ) sp(p f ci , p f c0i ) st (p t ci , p t c0i ) (1) manhattan distance is an appropriate metric since the three components are independent.\nfollowing the same logic, the phonetic similarity between two words w and w0 is computed as the sum of the distances between the pinyins.\nlearning pinyin encodings therefore, the next task is to generate an accurate representation of phonetic similarity for every pair of initials, finals, and tones.\nwe use a supervised machine learning approach that uses pinyin linguistic characteristics combined with manually labeled data sets of phonetic similarity.\nthe training data sets consist of word pairs that highlight a pair of initials (or finals), and are used as the context for an annotator-provided phonetic similarity score.\ngenerating similar word pairs phonetically similar word pairs are used to create annotations representing the phonetic similarity of a pair of initials, or finals.\nwe observe that the phonetic similarity of chinese pinyin is greatly impacted by the pronunciation methods and the place of articulation.\nspecifically, this is done by grouping the pinyin components into initial clusters according to the pinyin pronunciation tables (iso, 2015) and only annotating the pairs within each cluster along with a single pairwise distance between clusters.\nhowever, we observe that the phonetic similarity of pinyin is greatly impacted by the pronunciation methods and the place of articulation - this allows us to improve the accuracy and simplify the annotation task.\nthe number of pairs of initials decreases from to we use a similar method for finals, partitioning them into six groups by the six basic vowels ( ,o,e,i,u,u) (e.g., i,in,ing are clustered together.) we t n use ed t distance and common sequence length con traints to g ide the pair generation; specifically, we compare a pair of finals if the edit distance between them is or since the length of finals on average is two, an edit distance of three means a complete change to the final, resulting in pairs with the lowest similarity.\nfor each created word w, we change the initial (or final) from p1 to p2, retrieve the corresponding words from the dictionary and generate the word pairs to compare.\nequation inverts the labels so that the output can be used as a distance metric (phonetically similar initials or finals are closer together), and scales the result to more accurately measure phonetic similarities.\n() 1/a b (2) once the average distances between pairs are computed from the annotated data sets, we define a constrained optimization to compute encodings of the initials and finals.\nthe distance sp of a pair p of points (x1, x2, ..., xn), (y1, y2, ..., yn) is calculated using euclidean distance as shown in equation sp 1in (xi yi)2 (3) the model aims to minimize the sum of the absolute differences between the euclidean distances of component pairs and the average distances obtained from the annotated training data across all pairs for initials (or finals) c.\nwe also incorporate a penalty function, p, for pairs deviating from the manually annotated distance so that more phonetically similar pairs are penalized more highly (we discuss further in section 3.2).\ninterestingly, the learning algorithms organically discovers new clusters that are not reflected in table 2; namely that r,n and r,l are pairs of phonetically similar initials.\nthere is an additional constraint: any pairwise difference in initials or finals must have input : word w, threshold th,dict dict; output: words outws; begin pys getpinyins(w,dict); headpys getsimpinyins(pys(0), th); headwords getwordswithheadpy(headpys, dict); for cw headwords do if cw.size w.size then continue; end sim getsimilarity(cw,w); if sim th then outws.add(cw); end end sortbyascsim(outws); return outws; end algorithm 1: generating phonetic candidates.\nhaving determined the phonetic encodings and the mechanism to compute the phonetic similarity using learned phonetic encodings, we now describe how to generate and rank similar candidates in algorithm given a word w, a similarity threshold th, and a chinese pinyin dictionary dict, we retrieve the pinyin py of w from dict.\n3we study the impact of varying th in section we collect words from social media (wu, 2016), and annotate each with 1-3 phonetically similar words.\nwe therefore use our method (equation 1) to rank the dm-generated candidates, to create a second baseline, dm-rank.4 the third baseline, aline, measures phonetic similarity based on manually coded multi-valued articulatory features weighted by their relative importance with respect to feature salience (again, manually determined).\nranking dm candidates using the dimsim phonetic distance defined in equation improves its average mrr by a factor of however, even dm-rank is outperformed by the simple med 4we do not compare with soundex as dm is accepted to be an improved phonetic similarity algorithm over soundex.\nchoosing words randomly from the test set, we use dmrank, med, aline and dimsim2 to generate top-k candidates for each seed word (k 5).5 the annotators mark each candidate as phonetically similar to the seed word (1) or not (0), also marking the one candidate they believe to be the most similar-sounding (2), which may be any of 5we do not evaluate dm and dimsim1 as they perform worse than dm-rank and dimsim2, respectively.\nwe also see that moving from n1 to n2 increases the average mrr by 1.14x however, further increasing the number of dimensions to n2 no longer improves average mrr, indicating that learning a two-dimensional encoding is enough to capture the phonetic relationships between pinyin components.\nthese algorithms fail to capture chinese phonetic similarity since the conversion rules do not consider pronunciation properties of pinyin.\nhowever, the phonetic similarity used in these systems cannot be applied to chinese words since pinyin has its own specific characteristics, which do not easily map to english, for determining phonetic similarity.\ndimsim learns separate high dimensional encodings for initials and finals, and uses them to calculate and rank the distances between pinyin representations of chinese word pairs.\ndimsim generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components.", "1010": "figure illustrates the process used to parse two sample questions in a kbqa system: (a) a single-relation question, which can be answered with a single head-entity, relation, tail-entity kb tuple (fader et al., 2013; yih et al., 2014; bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled for multiple entities in the question.\nthe kbqa system in the figure performs two key tasks: (1) entity linking, which links n-grams in questions to kb entities, and (2) relation detection, which identifies the kb relation(s) a question refers to.\nfirst, in most general relation detection tasks, the number of target relations is limited, normally smaller than in contrast, in kbqa even a small kb, like freebase2m (bordes et al., 2015), contains more than 6,000 relation types.\nsecond, relation detection for kbqa often becomes a zero-shot learning task, since some test instances may have unseen relations in the training data.\nfirst, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching.\nthat original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level relation representations.\nthird, we use deep bidirectional lstms (bilstms) to learn different levels of question representations in order to match the different levels of relation information.\nfinally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question representations, thus improves hierarchical matching.\ngiven an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the kbqa process: (1) re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model.\nthe above steps are followed by an optional constraint detection step, when the question cannot be answered by single relations (e.g., multiple entities in the question).\nour main contributions include: (i) an improved relation detection model by hierarchical matching between questions and relations with residual learning; (ii) we demonstrate that the improved relation detector enables our simple kbqa system to achieve state-of-the-art results on both single-relation and multi-relation kbqa tasks.\nrelation detection in kbqa systems relation detection for kbqa also starts with featurerich approaches (yao and van durme, 2014; bast and haussmann, 2015) towards usages of deep networks (yih et al., 2015; xu et al., 2016; dai et al., 2016) and attention models (yin et al., 2016; golub and he, 2016).\nmany of the above relation detection research could naturally support large relation vocabulary and open relation sets (especially for qa with openie kb like paralex (fader et al., 2013)), in order to fit the goal of open-domain question answering.\nto the end, there are two main solutions: (1) use pre-trained relation embeddings (e.g. from transe (bordes et al., 2013)), like (dai et al., 2016); (2) factorize the relation names to sequences and formulate relation detection as a sequence matching and ranking task.\nfor relation detection in kbqa, such information is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one kb entity could have multiple types (type vocabulary size larger than 1,500).\nthis makes kb entity typing itself a difficult problem so no previous used entity information in the relation detection model.3 kb relations previous research (yih et al., 2015; yin et al., 2016) formulates kb relation detection as a sequence matching problem.\nfor example, in figure 1, when treating relation names as single tokens, it will be difficult to match the questions to relation names episodes written and starring roles if these names do not appear in training data their relation embeddings hrs will be random vectors thus are not comparable to question embeddings hqs.\nthis is because the incorrect relation contains word plays, which is more similar to the question 3such entity information has been used in kbqa systems as features for the final answer re-rankers.\nsince both these levels of granularity have their own pros and cons, we propose a hierarchical matching approach for kb relation detection: for a candidate relation, our approach matches the input question to both word-level and relation-level representations to get the final ranking score.\nthis section describes our hierarchical sequence matching with residual learning approach for relation detection.\nin order to match the question to different aspects of a relation (with different abstraction levels), we deal with three problems as follows on learning question/relation representations.\nusually relation names could match longer phrases in the question and relation words could match short phrases.\nas a result, we hope the question representations could also comprise vectors that summarize various lengths of phrase information (different levels of abstraction), in order to match relation representations of different granularity.\nnote that the first(second)-layer of question representations does not necessarily correspond to the word(relation)-level relation representations, instead either layer of question representations could potentially match to either level of relation representations.\nthis raises the difficulty of matching between different levels of relation/question representations; the following section gives our proposal to deal with such problem.\nnow we have question contexts of different lengths encoded in (1)1:n and (2) 1:n unlike the standard usage of deep bilstms that employs the representations in the final layer for prediction, here we expect that two layers of question representations can be complementary to each other and both should be compared to the relation representation space (hierarchical matching).\nfor example in table 1, the relation word written could be matched to either the same single word in the question or a much longer phrase be the writer of.\nthis is mainly because (1) deep bilstms do not guarantee that the two-levels of question hidden representations are comparable, the training usually falls to local optima where one layer has good matching scores and the other always has weight close to (2) the training of deeper architectures itself is more difficult.\nthen we generate the kb queries for q following the four steps illustrated in algorithm algorithm 1: kbqa with two-step relation detection input : question q, knowledge base kb, the initial top-k entity candidates elk(q) output: top query tuple (e, r, (c, rc)) entity re-ranking (first-step relation detection): use the raw question text as input for a relation detector to score all relations in the kb that are associated to the entities in elk(q); use the relation scores to re-rank elk(q) and generate a shorter list el0k0(q) containing the top-k0 entity candidates (section 5.1) relation detection: detect relation(s) using the reformatted question text in which the topic entity is replaced by a special token e (section 5.2) query generation: combine the scores from step and 2, and select the top pair (e, r) (section 5.3) constraint detection (optional): compute similarity between q and any neighbor entity c of the entities along r (connecting by a relation rc) , add the high scoring c and rc to the query (section 5.4).\nhaving observed that different entity candidates usually connect to different relations, here we propose to help entity disambiguation in the initial entity linking with relations detected in questions.\nin this step, we use the raw question text as input for a relation detector to score all relations in the kb with connections to at least one of the entity candidates in elk(q).\nwe call this step relation detection on entity set since it does not work on a single topic entity as the usual settings.\nfor each question q, after generating a score srel(r; q) for each relation using hr-bilstm, we use the top l best scoring relations (rlq) to re-rank the original entity candidates.\nin this step, for each candidate entity e el0k(q), we use the question text as the input to a relation detector to score all the relations r re that are associated to the entity e in the kb.4 because we have a single topic entity input in this step, we do the following question reformatting: we replace the the candidate e\u2019s entity mention in 4note that the number of entities and the number of relation candidates will be much smaller than those in the previous step.\nthis helps the model better distinguish the relative position of each word compared to the entity.\nwe use the hr-bilstm model to predict the score of each relation r re: srel(r; e, q).\n(2016) also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set.\n(2016), we use s-mart (yang and chang, 2015) entity-linking outputs.7 in order to evaluate the relation detection models, we create a new relation detection task from the webqsp data set.8 for each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length 2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples.\nfor both the relation detection experiments and the second-step relation detection in kbqa, we have entity replacement first (see section and figure 1).\nwe re-implemented the bicnn model from (yih et al., 2015), where both questions and relations are represented with the word hash trick on character tri-grams.\nthe baseline bilstm with relation word sequence appears to be the best baseline on webqsp and is close to the previous best result of ampcnn on simplequestions.\nnote that using only relation names instead of words results in a weaker baseline bilstm model.\nfirst, hierarchical matching between questions and both relation names and relation words yields improvement on both datasets, especially for simplequestions (93.3 vs.\nsecond, compared to our deep bilstm with shortcut connections, we have the hypothesis that for kb relation detection, training deep bilstms is more difficult without shortcut connections.\nin order to highlight the effect of different relation detection models on the kbqa end-task, we also implemented another baseline that uses our kbqa system but replaces hr-bilstm with our implementation of ampcnn (for simplequestions) or the char-3-gram bicnn (for webqsp) relation detectors (second block in table 3).\ncompared to the baseline relation detector (3rd row of results), our method, which includes an improved relation detector (hr-bilstm), improves the kbqa end task by 2-3 (4th row).\nsince the reranking step relies on the relation detection models, this shows that our hr-bilstm model contributes to the good performance in multiple ways.\n(2015) for the three models used), we also try to use the top-3 relation detectors from section as shown on the last row of table 3, this gives a significant performance boost, resulting in a new state-of-the-art result on simplequestions and a result comparable to the state-of-the-art on webqsp.\nkb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks.\nwe propose a novel kb relation detection model, hr-bilstm, that performs hierarchical matching between questions and kb relations.\nour model outperforms the previous methods on kb relation detection tasks and allows our kbqa system to achieve state-of-the-arts.\nassociation for computational linguistics, berlin, germany, pages classifying relations by ranking with convolutional neural networks.\nassociation for computational linguistics, austin, texas, pages semi-supervised relation extraction with large-scale word clustering.\nassociation for computational linguistics, san diego, california, pages relation classification via multi-level attention cnns.\nassociation for computational linguistics, san diego, california, pages relation classification via convolutional deep neural network.", "1011": "in this work we propose a feature agnostic approach for dictionary expansion based on lightweight neural language models, such as word2vec to prevent semantic drift during the dictionary expansion, we effectively include humanin-the-loop (huml).\ngiven an input text corpus and a set of seed examples, the proposed approach runs in two phases, explore and exploit, to identify new potential dictionary entries.\nthe explore phase tries to identify similar instances to the dictionary entries that are present in the input text corpus, using term vectors from the neural language model to calculate a similarity score.\nthe exploit phase tries to construct more complex multi-term phrases based on the instances already in the input dictionary.\nin the exploit phase, the approach generates new phrases by analyzing the single terms of the instances in the input dictionary.\nwe use two phrase generation algorithms: (i) modify the phrases by replacing single terms with similar terms from the text corpus, e.g., abnormal behavior can be modified to strange behavior; (ii) extend the instances with terms from the text corpus that are related to the terms in the instance, e.g., abnormal blood clotting problems is a an adverse drug reaction, which doesn\u2019t appear as such in a large text corpus, however the instances abnormal blood count, blood clotting and clotting problems appear several times in the corpus, which can be used to build the more complex instance.\nthe approach allows us to construct new multi-term instances that don\u2019t appear as such in the text corpus, but there is enough statistical evidence in the corpus that such instances might be of interest for the user.\nhence, a lot of work in the literature focuses on identifying new approaches for more efficient and more effective dictionary extraction from unstructured text.\nwhile both approaches have been proven to achieve high effectiveness for dictionary extension, both of the approaches can only identify new dictionary entries that are only present in the input text corpus.\nour approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus and generate new unseen instances based on user feedback (exploit).\nthe explore phase uses the instances available in the input dictionary to identify similar candidates that are already present in the corpus vocabulary vtc , which are then accepted or rejected by the huml.\nduring the exploit phase, we use the instances in the input dictionary to construct more complex phrases that might be of interest for the user.\nformally, the similarity between two terms w1 and w2, with vectors v1 and v2, is calculated as the cosine similarity between the vectors v1 and v2: sim(w1, w2) v1 v2 v1 v2 (1) we calculate the similarity between the instances in the input dictionary and all the words in the corpus vocabulary vtc we sort the vocabulary in descending order using the cumulative similarity score, and choose the top-n candidates to present to the huml.\nin the exploit phase we try to identify more complex phrases that don\u2019t exist in the corpus vocabulary by analyzing the structure of the instances in the input dictionary.\nin the first approach, we first break each instance in to a set of single terms t t1, t2, ..., tn, then for each term ti in t we identify a set of similar terms tsti ts1, ts2, ..., tss in the vocabulary vtc using equation in the next step, we build new phrases by replacing ti with a term tsi from tsti the new phrases are sorted based on the similarity score and the top-n are selected as candidates.\nin the second approach, we generate new phrases by extending the instances with terms from the text corpus that are related to the terms in the instance.\nas before, we first break each instance in to a set of single terms t t1, t2, ..., tn, then for each term ti in t we identify a set of similar terms trti tr1, tr2, ..., trr in the vocabulary vtc using equation in the next step, we build new phrases by appending a term tri from trti to each term ti from t the new phrases are sorted based on the relatedness score and the top-n are selected as candidates.\nfor example, given the instance clotting problems in the input dictionary the approach first tries to identify related terms in the text corpus for clotting.\nas an input set of seed examples we use a set of instances referring to adverse drug events, which were labeled by a medical doctor in this experiment we compare the performance of the explore, exploit and the explore/exploit approaches for discovering new dictionary instances.\nwhen using the exploit approach the number of newly discovered instances sharply decreases as no new base terms are introduced, thus the exploit cannot generate new instances that can be added in the dictionary.", "1012": "grounding of textual phrases, i.e., finding bounding boxes in images which relate to textual phrases, is an important problem for human-computer interaction, robotics and mining of knowledge bases, three applications that are of increasing importance when considering autonomous systems, augmented and virtual reality environments.\nmore specifically, deep net models are designed to extract features from given bounding boxes and textual data, which are then compared to measure their fitness.\nto obtain suitable bounding boxes, many of the textual grounding frameworks, such as 38, 15, make use of region proposals.\nbased on those image concepts\u2019 which are represented as score maps, we formulate textual grounding as a search over all possible bounding boxes.\nadditional modeling of object context relationship were explored in 32, video 9/5/2017 bbestredraw 1/1 datasets, although not directly related to our work in this paper, were used for spatiotemporal language grounding in 27, common datasets for visual grounding are the referitgame dataset and a newly introduced flickr 30k entities dataset 35, which provides bounding box annotations for noun phrases of the original flickr 30k dataset in contrast to all of the aforementioned methods, which are largely based on region proposals, we suggest usage of efficient subwindow search as a suitable inference engine.\nin contrast to the aforementioned techniques for textual grounding, which typically use a small set of bounding box proposals, we formulate our language grounding approach as an energy minimization over a large number of bounding boxes.\nthe search over a large number of bounding boxes allows us to retrieve an accurate bounding-box prediction for a given phrase and an image.\nall those concepts come in the form of score maps which we combine linearly before searching for the bounding box containing the highest accumulated score over the combined score map.\nwe obtain a bounding box prediction y given our data x, by solving the energy minimization y arg min yy e(x, y, w), (1) to global optimality.\ndespite the fact that we are only\u2019 interested in a single bounding box, the product space y is generally too large for exhaustive minimization of the energy specified in eq.\nto apply branch and bound, we assume that the energy function e(x, y, w) depends on two sets of parameters w wtt , w t r t , i.e., the top layer parameters wt of a neural net, and the remaining parameters wr.\nin light of this decomposition, our approach requires the energy function to be of the following form: e(x, y, w) wtt (x, y, wr).\nenergy function details: our energy function e(x, y, w) is based on a set of image concepts,\u2019 such as semantic segmentation of object categories, detections, or word priors, all of which we subsume in the set c.\ngiven a bounding box y, we use the scalar c(x, y, wr) r to refer to the score accumulated within the bounding box y of score map c(x,wr).\nbased on this definition, we formulate the energy function as follows: e(x, y, w) ss:s1 cc ws,cc(x, y, wr), (2) where ws,c is a parameter connecting a word s s to an image concept c c.\nin our case, we use c k1 k2 k3 of those maps, which capture three kinds of information: (i) k1 word-priors; (ii) k2 geometric information cues; and (iii) k3 image based segmentations and detections.\nwith the corresponding subset of image-text pairs and respective bounding box annotations at hand, we compute the average number of times a pixel is covered by a bounding box.\nthe k3 image based features are computed using deep neural nets as proposed by 4, 37, we obtain probability maps for a set of class categories, i.e., a subset of the nouns of interest.\nthe feature accumulates the scores within the hypothesized bounding box y.\ninference: the algorithm to find the bounding box y with lowest energy as specified in eq.\ngiven such a repetitive decomposition strategy for the output space, and since the energy e(x, y, w) for a bounding box y is obtained using a linear combination of word priors and accumulated segmentation masks, we can design an efficient branch and bound based search algorithm to exactly solve the inference problem specified in eq.\nfor each subspace, the algorithm computes a lower bound e(x,yj , w) for the energy of all possible bounding boxes within the respective subspace.\nto this end, it remains to show how to compute a lower bound e(x,yj , w) on the energy for an output space, and to illustrate the conditions which guarantee convergence to the global minimum of the energy function.\nfor the latter, we note that two conditions are required to ensure convergence to the optimum: (i) the bound of the considered product space has to lower-bound the true energy for each of its bounding box hypothesis y y , i.e., y y , e(x, y, w) e(x, y, w); (ii) the bound has to be exact for all possible bounding boxes y y , i.e., e(x, y, w) e(x, y, w).\nfor the former, we note that bounds on score maps for bounding box intervals can be computed by considering either the largest or the smallest possible bounding box in the bounding box hypothesis, y , depending on whether the corresponding weight in wt is positive or negative and whether the feature maps contain only positive or negative values.\nintuitively, if the weight is positive and the feature mask contains only positive values, we obtain the smallest lower bound e(x, y, w) by considering the content within the smallest possible bounding box.\nit is important to note that computation of the bound e(x, y, w) has to be extremely effective for the algorithm to run at a reasonable speed.\nhowever, computing the feature mask content for a bounding box is trivially possible using integral images.\nformally, we are given a training set d (x, y) containing pairs of input data x and groundtruth bounding boxes y.\nsince maximization is identical to negated minimization, the computation of the bounds for the energy e(x, y, w) remains identical.\nobserve that, e(x, y, w), is a weighted sum of the feature maps over the region specified by a hypothesized bounding box.\ngiven an integral image, the computation for each of the bounding box is simply a look-up operation.\nwe use the same bounding boxes as and the same training test set split, i.e., 10,000 images for testing, 9,000 images for training and 1,000 images for validation.\nthe flickr 30k entities dataset consists of more than 275k bounding boxes from 31k image, where each bounding box is annotated with the corresponding natural language phrase.\nwe observe that for a few cases word prior may hurt the prediction (e.g., shoes are typically on the bottom half of the image.) also our system may fail when the energy is not a linear combination of the feature scores.\nas can be seen, ws,c is large, when the phrase word and the concept are related, (e.g. this demonstrates that our model successfully learns the relationship between phrase words and image concepts.\nin contrast to existing approaches which are generally based on a small set of bounding box proposals, we efficiently search over all possible bounding boxes.", "1013": "ods of this family are based on maximum mean discrepancy (mmd) 11, 12, and the method of moments (mom) while mom based methods embed a probability distribution into a finite-dimensional vector (i.e., matching of a finite number of moments), mmd based methods embed a distribution into an infinite-dimensional vector a challenge for mmd methods is to define a kernel function that is statistically efficient and can be used with small minibatch sizes a solution comes by using adversarial learning for the online training of kernel functions 21, however, this solution inherits the problematic min/max game of adversarial learning.\naddressed these two issues by defining the moments as features and derivatives from a moment network that is trained online (together with the generator) by using a specially designed objective function.\nour proposed approach, named generative feature matching networks (gfmn), learns implicit generative models by performing mean and covariance matching of features extracted from all convolutional layers of pretrained deep convnets.\nsome interesting properties of gfmns include: (a) the loss function is directly correlated to the generated image quality; (b) mode collapsing is not an issue; and (c) the same pretrained feature extractor can be used across different datasets.\nthe main contributions of this work can be summarized as follows: (1) we propose a new effective moment matchingbased approach to train implicit generative models that does not use adversarial or online learning of kernel functions, provides stable training, and achieves state-of-the-art results; (2) we show theoretical results that demonstrate gfmn convergence under the assumption of the universality of perceptual features; (3) we propose an adam-based moving average method that allows effective training with small minibatches; (4) our extensive quantitative and qualitative experimental results demonstrate that pretrained autoencoders and dcnn classifiers can be effectively used as (cross-domain) feature extractors for gfmn training.\nour proposed approach consists in training g by minimizing the following loss function: min m j1 jpdata jpg()2 jpdata jpg()2 (1) where: jpdata expdataej(x) rdj jpg() ezn (0,inz )ej(g(z; )) r dj jpdata, expdataej,(x) j,pdata 2, dj and is the l2 loss; x is a real data point sampled from the data generating distribution pdata; z rnz is a noise vector sampled from the normal distribution n (0, inz ); ej(x), denotes the output vector/feature map of the hidden layer j from e; m l is the number of hidden layers used to perform feature matching.\nthe decoder part of an ae consists exactly of an image generator that uses features extracted by the encoder.\nour hypothesis is that imagenet-based pfs are informative enough to allow the training of (crossdomain) generators by feature matching.\n1, we keep moving averages vj of the difference of feature means (covariances) at layer j between real and generated data.\nsince we can now rely on vj to get better estimates of the population feature means of real and generated data while using a small minibatch of sizen for a similar result using the feature matching loss given in eq.\nthe main advantage of ama over simple moving average (ma) is in its adaptive first and second order moments that ensure stable estimation of the moving averages vj in fact, this is a non-stationary estimation since the mean of the generated data changes in the training, and it is well known that adam works well for such online non-stationary losses in section we provide experimental results supporting: (1) the memory advantage that the ama formulation of feature matching offers over the naive implementation; (2) the stability advantage and improved generation results that ama allows compared to the naive implementation.\nwe discuss in appendix the advantage of ama on ma from a regret bounds point of view our proposed approach is related to the recent body of work on mmd or mm based generative models 22, 8, 21, 3, we highlight the main differences between mmd-gans and gfmn in terms of requirements on the kernel for mmd-gan and on the feature map (extractor) for gfmn, that ensure convergence of the generator to the data distribution.\nthis elegant setup for mmd matching with universal kernels, while avoiding the difficult min/max game in gan, does not translate into good results in image generation.\nnevertheless, learning generative models remains challenging with it as it boils down to a min/max game as in original gan gfmn convergence: mmd matching with universal features.\nthis is of interest since gfmn corresponds to mmd matching with a kernel k defined on a fixed feature map (x)j(x)ji , where i is finite.\ngfmn converges to the real distribution by matching in a feature space s j , j i, where i is a countable set, if the features set s is universal (informally means that any continuous functions can be written as linear combination in the span of s) s is universal k is universal hence mmd(k, pdata, q) iff q pdata.\nwe see that for gfmn to be convergent with pretrained feature extractors ej that are perceptual features (such as features from vgg or resnet pretrained on imagenet), we need to assume universality of those features in the image domain.\ngfmn is related to the recent body of work on mmd and moment matching based generative models 22, 8, 21, 3, the closest to our method is the generative moment matching network autoencoder (gmmnae) proposed in in gmmnae, the objective is to train a generator g that maps from a prior uniform distribution to the latent code learned by a pretrained ae, and then uses the frozen pretrained decoder to map back to image space.\nas discussed in section one key difference in our approach is that, while gmmnae uses a gaussian kernel to perform moment matching using the ae low dimensional latent code, gfmn performs mean and covariance matching in a pf space induced by a non-linear kernel function (a dcnn) that is orders of magnitude larger than the ae latent code, and that we argued is universal in the image domain.\nmmd-gans, discussed in section 3, demonstrated competitive results with the use of adversarial learning by learning a feature map in conjuction with a gaussian kernel 21, recently proposed a method to perform online learning of the moments while training the generator.\nour work is also related to ae-based generative models variational ae (vae) 19, adversarial ae (aae) and wasserstein ae (wae) however, gfmn is quite distinct from these methods because it uses pretrained aes to play the role of feature extractors only, while these methods aim to impose a prior distrib.\ngfmn generator: in most of our experiments the generator g uses a dcgan-like architecture for cifar10, stl10, lsun and celeba6464, we use two extra layers as commonly used in previous works 28, for celeba128128 and some experiments with cifar10 and stl10, we use a resnet-based generator such as the one in architecture details are in the supplementary material.\nthis section presents a comparative study on the use of pretrained autoencoders and cross-domain classifiers as feature extractors in gfmn.\nshows the inception score (is) and frechet inception distance (fid) for gfmn trained on cifar10 using different feature extractors e.\nwhile the best is with encoders is 4.95, the lowest is with imagenet classifier is additionally, when using simultaneously vgg19 and resnet18 as feature extractors (two last rows), which increases the number of features to 832k, we get even better performance.\npresents samples from gfmnvgg19 trained with celeba dataset with resolution 128128, which shows that gfmn can achieve good performance with image resolutions larger than these results also demonstrate that: (1) the same classifier (vgg19 trained on imagenet) can be successfully applied to train gfmn models across different domains; (2) perceptual features from dcnns encapsulate enough statistics to allow the learning of good generative models through moment matching.\nin other words, features from classifiers are significantly more informative than aes features for the purpose of training generators by feature matching.\nour pytorch implementation of gfmn can only handle minibatches of size up to when using vgg19 as a feature extractor and image size on a tesla k40 gpu w/ 12gb of memory.\nall experiments in this section use celeba training set, and a feature extractor using the encoder from an ae following a dcgan-like architecture.\nthis feature extractor is smaller than vgg19/resnet18 allowing for minibatches of size up to for image size shows generated images from gfmn trained with either ma or our proposed ama.\nfor ma, generated images from gfmn trained with and minibatch size are presented in figs.\nfor ama, increasing the minibatch size from to does not improve the quality of generated images for the given dataset and feature extractor.\nin the supplementary material, we show a comparison between ma and ama with vgg19 imagenet classifier as feature extractor for a minibatch size of ama also displays a very positive effect on the quality of generated images when a stronger feature extractor is used.\nnote that ganbased methods that perform conditional generation use direct feedback from the labels in the form of log likelihoods from the discriminator (e.g. in contrast, our generator is trained with a loss function that only performs feature matching.\nwe achieve successful non-adversarial training of implicit generative models by introducing different key ingredients: (1) moment matching on perceptual features from all layers of pretrained neural networks; (2) a more robust way to compute the moving average of the mean features by using adam optimizer, which allows us to use small minibatches; and (3) the use of perceptual features from multiple neural networks at the same time (vgg19 resnet18).\nthis is an impressive result for a nonadversarial feature matching-based approach that uses pretrained cross-domain feature extractors and has stable train- ing.\ngfmn achieves better results than the method of learned moments (molm) 33, while using a much smaller number of features to perform matching.\nthe best performing model from 33, molm1536, uses around million moments to train the cifar10 generator, while our best gfmn model uses around 850k moments/features only, almost 50x less.\none may argue that the best gfmn results are obtained with feature extractors trained with classifiers.\nhowever, there are two important points to note: (1) we use a cross domain feature extractor and do not use labels from the target datasets (cifar10, stl10, lsun, celeba); (2) classifier accuracy does not seem to be the most important factor for generating good features: vgg19 classifier produces features as good as the ones from resnet18, although the former is less accurate (more details in supplementary material).\nwe are confident that gfmn can achieve state-ofthe-art results with features from classifiers trained with unsupervised methods such as in conclusion, this work presents important theoretical and practical contributions that shed light on the effectiveness of perceptual features for training implicit generative models through moment matching.\nas we already discussed the moving average of v of the difference of features means t n n i1 e(xi) n n i1 e(g(zi, t)) between real and generated data at each time step t in the gradient descent up to time t , can be seen as a gradient descent in an online setting on the following cost : f min v t t1 ft(v) t t1 v t22 note that we are in the online setting since t is only known when t of the generator is updated.\nin this appendix, we present comparative results between gfmn with mean feature matching vs.\nin table 5, we can see that for different feature extractors, performing mean covariance feature matching produces significantly better results in terms of both is and fid.\nfor these two last datasets, the resnet generator has resblocks only, and the output size of the dense layer is both vgg19 and resnet18 networks are trained with sgd with fixed learning rate, momentum term, and weight decay set to we pick models with best top-1 accuracy on the validation set over epochs of training; for vgg19 (image size 3232), and for resnet18 (image size 3232).\nextraction figure shows generated images from generators that were trained with a different number of layers employed to feature matching.\nin this appendix, we present a comparison between the simple moving average (ma) and adam moving average (ama) for the case where vgg19 imagenet classifier is used as a feature extractor.\nwe show results for both simple moving average (ma) and adam moving average (ama), for both cases we use a minibatch size of 11, we show generated images from gfmn trained with either vgg19 features (top row) or autoencoder (ae) features (bottom row).\nwe show images generated by gfmn models trained with simple moving average (ma) and adam moving average (ama).\nwe can note in the images that, although vgg19 features are from a cross-domain classifier, they lead to much better generation quality than ae features, specially for the ma case.", "1014": "a key assumption made by matrix factorization- and completion-based collaborative filtering algorithms is that the underlying rating matrix is of low-rank since only a few factors typically contribute to an individual\u2019s form utility however, a user\u2019s demand is not only driven by form utility, but is the combined effect of both form utility and time utility.\ngiven purchase triplets (user, item, time) and item categories, the objective is to make recommendations based on users\u2019 overall predicted combination of form utility and time utility.\nspecifically, we model a user\u2019s time utility for an item by comparing the time t since her most recent purchase within the item\u2019s category and the item category\u2019s underlying inter-purchase duration d; the larger the value of d t, the less likely she needs this item.\nthen the purchase intention for a (user, item, time) triplet is given by x h, where x denotes the user\u2019s form utility.\nthis observation allows us to cast demand-aware recommendation as the problem of learning users\u2019 form utility tensor x and items\u2019 inter-purchase durations vector d given the binary tensor p although the learning problem can be naturally formulated as a tensor nuclear norm minimization problem, the high computational cost significantly limits its application to large-scale recommendation problems.\nmore severely, the optimization problem involves mnl entries, where m, n, and l are the number of users, items, and time slots, respectively.\nto overcome this limitation, we develop an efficient alternating minimization algorithm and show that its time complexity is only approximately proportional to the number of nonzero elements in the purchase records tensor p since p is usually very sparse, our algorithm is extremely efficient and can solve problems with millions of users and items.\ncompared to existing recommender systems, our work has the following contributions and advantages: (i) to the best of our knowledge, this is the first work that makes demand-aware recommendation by considering inter-purchase durations for durable and nondurable goods; (ii) the proposed algorithm is able to simultaneously infer items\u2019 inter-purchase durations and users\u2019 real-time purchase intentions, which can help e-retailers make more informed decisions on inventory planning and marketing strategy; (iii) by effectively exploiting sparsity, the proposed algorithm is extremely efficient and able to handle large-scale recommendation problems.\na variety of time-aware recommender systems have been proposed to exploit time information, but none of them explicitly consider the notion of time utility derived from inter-purchase durations in item categories.\ngiven a set of m users, n items, and l time slots, we construct a third-order binary tensor p 0, 1mnl to represent the purchase history.\nspecifically, entry pijk indicates that user i has purchased item j in time slot k.\ngiven p and c, we further generate a tensor t rmrl where ticjk denotes the number of time slots between user i\u2019s most recent purchase within item category cj until time k.\nif user i has not purchased within item category cj until time k, ticjk is set to in this work, we formulate users\u2019 utility as a combined effect of form utility and time utility.\nin addition, we employ a non-negative vector d rr to measure the underlying inter-purchase duration times of the r item categories.\nlet dcj be the inter-purchase duration time of item j\u2019s category cj , and let ticjk be the time gap of user i\u2019s most recent purchase within item category cj until time k.\nthen if dcj ticjk, a previously purchased item in category cj continues to be useful, and thus user i\u2019s utility from item j is weak.\non the other hand, dcj ticjk indicates that the item is nearing the end of its lifetime and the user may be open to recommendations in category cj we use a hinge loss max(0, dcj ticjk) to model such time utility.\nin addition, the form utility tensor x should be of low-rank to capture temporal dynamics of users\u2019 interests, which are generally believed to be dictated by a small number of latent factors by combining asymmetric sampling and the low-rank property together, we jointly recover the tensor x and the inter-purchase duration vector d by solving the following tensor nuclear norm minimization (tnnm) problem: min xrmnl, drr ijk: pijk1 max1 (xijk max(0, dcj ticjk)), (1 ) ijk: pijk0 x2ijk x, (2) where x denotes the tensor nuclear norm, a convex combination of nuclear norms of x \u2019s unfolded matrices given the learned x and d, the underlying binary tensor y can be recovered by (1).\nto this end, we first construct a low-rank matrix x wht , where w rm10 and h rn10 are random gaussian matrices with entries drawn from n (1, 0.5), and then normalize x to the range of 0, we randomly assign all the n items to r categories, with their inter-purchase durations d equaling 10, 20, we then construct the high purchase intension set (i, j, k) ticjk dcj and xij 0.5, and sample a subset of its entries as the observed purchase records.\naccuracy figure 1(a) and 1(b) clearly show that the proposed algorithm can perfectly recover the underlying inter-purchase durations with varied numbers of users, items, and categories.\nto further evaluate the robustness of the proposed algorithm, we randomly flip some entries in tensor p from to to simulate the rare cases of purchasing two items in the same category in close temporal succession.\nscalability to verify the scalability of the proposed algorithm, we fix the numbers of users and items to be million, the number of time slots to be 1, 000, and vary the number of purchase records (i.e., p0).\nwe observe that the proposed algorithm is extremely efficient, e.g., even with million users, million items, and more than million purchase records, the running time of the proposed algorithm is less than hours.\nin order to generate the subsets, we randomly sample item categories for tmall dataset and select the users who have purchased at least items within these categories, leading to the purchase records of users and items.\nsince a purchase record (u, i, t) may suggest that in time slot t, user u needed an item that share similar functionalities with item i, category prediction essentially checks whether the recommendation algorithms recognize this need.\nin the second task, we record the number of slots between the true purchase time t and its nearest predicted purchase time within item i\u2019s category.\nthis is verified by the performance of wr-mf: it significantly outperforms m3f and pmf by considering the pu issue and obtains the second-best item prediction accuracy on both datasets (while being unable to provide a purchase time prediction).\nin this study, we replace category prediction with a more strict evaluation metric item prediction 8, which indicates the predicted ranking of item i among all items at time t for each purchase record (u, i, t) in the test set.\nas a final note, we want to point out that tmall and amazon review may not take full advantage of the proposed algorithm, since (i) their categories are relatively coarse and may contain multiple sub-categories with different durations, and (ii) the time stamps of amazon review reflect the review time instead of purchase time, and inter-review durations could be different from inter-purchase durations.\nin this paper, we examine the problem of demand-aware recommendation in settings when interpurchase duration within item categories affects users\u2019 purchase intention in combination with intrinsic properties of the items themselves.\nwe formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter-purchase durations, and propose a scalable optimization algorithm with a tractable time complexity.\non two real-world datasets, tmall and amazon review, we show that our algorithm outperforms six state-of-the-art recommendation algorithms on the tasks of category, item, and purchase time predictions.", "1015": "thus, it is desirable for automated agents to be capable of generating responses that express a target personality.\nin this paper we study how to encode personality traits as part of neural response generation for conversational agents.\nour approach builds upon a sequence-to-sequence (seq2seq) architecture (sutskever et al., 2014) by adding an additional layer that represents the target set of personality traits, and a hidden layer that learns high-level personality based features.\nthe first response (in each example), is generated by a standard seq2seq response generation system that ignores personality modeling and in effect generates the consensus response of the humans represented in the training data.\nthe second response is generated by our system, and is aimed to generate data for an agent that expresses a high level of a specific trait.\nto our knowledge, this work is the first to train a neural response generation model that encodes target personality traits.\nneural response generation models (vinyals and le, 2015; shang et al., 2015) are based on a seq2seq architecture (sutskever et al., 2014) and employ an encoder to represent the user utterance and an attention-based decoder that generates the agent response one token at a time.\nin this section we present our personality-based model (figure 2) which generates responses conditioned on a target set of personality traits values which the responses should express.\nthus, our model\u2019s response is conditioned on generation parameters which are based on personality traits.\nour model is designed to generate text conditioned on a target set of personality traits.\nwe then extracted the agents\u2019 personality traits using an external service (described in appendix b), from the training data for each agent.\nnote that, we extracted target personality traits for agents in the training set using their training data, or, for agents in the test set, using validation data.\nresults from both experiments demonstrate that we can better model the linguistic variation in agent responses by conditioning on target personality traits.\nthis evaluation measures whether the responses generated by our model are correlated with the target personality traits.\nwe focused on two personality traits from the big five model that are important to customer service: agreeableness and conscientiousness (blignaut et al., 2014; sackett, 2014).\nwe generated a high-trait target personality distribution (trait was either agreeableness or conscientiousness), where trait was set to a value of 0.9, and all other traits to similarly, we created a low-trait version where trait was set to for each trait and customer utterance we generated a response for the high-trait and low-trait versions.\nafter discarding ties, we found that the high-trait responses generated by our personality-based model were judged either more expressive or somewhat more expressive than the low-trait corresponding responses in of cases.\nwe have presented a personality-based response generation model and tested it in customer care tasks, outperforming baseline seq2seq model.\nin future work, we would like to generate responses adapted to the personality traits of the customer as well, and to apply our model to other tasks such as education systems.", "1016": "they allow to solve complex tasks using a tiny fraction of the energy consumed by stored-program computers while the successful artificial neural networks may not operate the same way as the brain, both of them utilize highly parallel and hierarchical architectures that gradually abstract input data to more meaningful concepts 8, 51, however, event-based computation has not been equally adopted work done as an intern at ibm research - almaden.\nto achieve the low energy and high temporal resolution benefits of event-based inputs, computations must be performed asynchronously.\nto benefit from sparse and asynchronous computation, neuromorphic processors have been developed 44, 24, 30, 9, these processors represent input events as spikes and process them in parallel using a large neuron population.\nevent-based stereo provides additional advantages over other depth estimation methods that increase accuracy and save energy, such as high temporal resolution, high dynamic range, and robustness to interference with other agents.\nmost global methods 40, 17, 49, are derived from the marr and poggio cooperative stereo algorithm the algorithm assumes depth continuity and often event-based implementations are not tested with objects tilted in depth.\nwe propose a fully neuromorphic event-based stereo disparity algorithm.\na live-feed version of the system running on nine truenorth chips is shown to calculate disparity maps per second, and the ability to increase this up to 2,000 disparities per second (subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as dvs.\ncompared to frame-based computation, in the asynchronous, event-based computation supported by truenorth, at each time cycle, in general only neurons that have input spikes are computed, and only spike events are communicated.\nwhen the data in a cycle is sparse, as is the case with a dvs sensor, most neurons would not compute for most of the time, resulting in low active power this processing differs from traditional architectures that use frame-buffers and other conventional data structures; where same memory fetching and computation is repeated for each pixel every frame, independent of scene activity.\nthe proposed event-based disparity method is implemented using a stereo pair of davis sensors (a version of dvs) and nine truenorth ns1e boards however, the method is applicable to other spiking neuromorphic architectures, and it is also tested offline on larger models using a truenorth simulator.\ncnns have been used to learn stereo matching cost 66, ground truth disparity maps from benchmark frame-based datasets 27, 54, 26, are used to train these models, followed by sparse-to-dense conversions 18, feature based matching techniques, such as color, edge, histogram, and sift based matching, produce sparse disparity maps 28, 38, 21, in contrast, event-based stereo correspondence literature is relatively new.\nhowever, most of the recent event-based implementations of the cooperative algorithm do not consider depth gradients 47, 48, 23, inject neighborhood similarity of candidate matches into the cooperative network.\n60, propose dsp implementation of a spatiotemporal similarity method using two live event sensors use a rotating pair of event-based line (vertical) sensors in static scenes and render events from each rotation to an edge map 33, which is subsequently processed using a frame-based panoramic stereo algorithm time-based methods utilize event timestamps for matching.\nalthough spike dynamics vary among pixels and sensors and events cannot be matched based on exact timestamps.\neach neuron can generate an output event deterministically, if the membrane potential v (t) exceeds a threshold; or stochastically, with a probability that is a function of the difference between the membrane potential and its threshold 2, the membrane potential is updated at each tick t to v (t) v (t 1) v (t) t , followed by the application of an activation function an(v (t)) where an(v (t)) 1, if v (t) n 0, otherwise (1) each neuron is assigned an initial membrane potential v (0).\ntruenorth programs are written in the corelet programming language a hierarchical, compositional, object-oriented language the proposed local event-based stereo correspondence algorithm is implemented end-to-end as a neuromorphic event-based algorithm.\ndepicts the sequence of operations performed by the corelets using inputs from stereo event sensors.\nthe events at each rectified pixel p h w l,r,, , are generated through splitter neurons which replicate corresponding sensor pixels.\ntheir membrane potential v splp (t) is defined by v splp (t) t i(t 1; p) where i(t; p) 0, denotes whether a sensor event is produced at time t and the sensor pixel p corresponding to the rectified pixel p.\nthese temporal scales are implemented through the use of splitter neurons which cause each event to appear at its corresponding pixel multiple times, depending on the desired temporal scale, or through the use of temporal ring buffer mechanisms, which lead to lower event rates.\nthe ring buffer is implemented by storing events in membrane potentials of memory cell neurons in a circular buffer, and through the use of control neurons which spike periodically to polarize appropriate memory cell neurons.\nthe ith of these coordinates is represented by neuron activations a1(v l, x (i) l,p (t)) and a1(v r, x (i) r,p (t)) in 1disjunction is implemented by sending input events to the same neuron input axon, effectively merging any input events to a single input event.\nthe left and right sensor\u2019s positive () or negative () polarity channel.2 given a pair of spatiotemporal coordinate tensors xl,p, xr,q centered at coordinates p, q in the left and right rectified image respectively and representing k coordinates each, we calculate the binary hadamard product fl(p, t) fr(q, t) associated with the corresponding patches at time t, where fl(p, t) ia1(v l x (i) l,p (t)) 0, 1k and fr(q, t) ia1(v r x (i) r,q (t)) 0, 1k the product is calculated in parallel across multiple neurons, as k pairwise logical and operations of corresponding feature vector entries, resulting in (a1(v dot p,q,1), ...,a1(v dot p,q,k)) where v dotp,q,i(t) t a1(v l x (i) l,p (t 1)) a1(v r x (i) r,q (t 1)) the population code representation of the hadamard product output is converted to a thermometer code3, which is passed to the winner-take-all circuit described below.\nthen for any 0, 1, 2, 0, 1, ..., b, we define the conversion of candidate disparity level d 0, ..., d to a qt-coded membrane potential v cnv,,d (t) as 2for notational simplicity we henceforth drop the , superscripts: the left and right sensors could produce distinct event streams based on event polarity, or could merge events in a single polarity-agnostic stream.\nthe outputs of each stream are represented by d retinotopic maps expressed in a fixed resolution (dvi,j,d(t), d 0, ..., d 1, v l,r), where events represent the retinotopic winner disparities for that stream.\nthe streams are then merged to produce the disparity map d l,r i,j,d(t) a1(v l,r i,j,d (t)) where v l,ri,j,d (t) t dli,j,d(t 1) d r i,jd,d(t 1) a1(v spl (i,j,l,)(t t)) (7) where t is the propagation delay of the first layer splitter output events until the left-right consistency constraint merging takes place.\npower is measured using the same process described in we calculate the power consumed by an n-chip system by measuring power on a single truenorth chip model running on an ns1t board with a high event rate input generated by the fan sequence.\ntotal chip power is the sum of passive power, computed by multiplying the idle power by the fraction of the chip\u2019s cores under use, and active power computed by subtracting idle power from the total power measured when the system is accepting input events the rds is tested on a model using spatial windows, left-right consistency constraints, no morphological erosion/dilation after rectification, and disparity levels (0-30) plus a no-disparity\u2019 indicator (often occurring due to self-occlusions).\nthe models that run on live davis input are operated at spike injection rate of up to 2,000hz (a new input every 1/2,000 seconds) and disparity map throughput of 400hz at a 0.5ms tick period (400 distinct disparity maps produced every second) across a cluster of truenorth chips.\nby adding a multiplexing spiking network to the network, we are able to reuse each feature-extraction/wta circuit to process the disparities for different pixels, effectively decreasing the maximum disparity map throughput from 2,000hz to 400hz, requiring fewer chips to process the full image (9 truenorth chips).\nwe have introduced an advanced neuromorphic 3d vision system uniting a pair of davis cameras with multiple truenorth processors, to create an end-to-end, scalable, event-based stereo system.\nby using a spiking neural network, with low-precision weights, we have shown that the system is capable of injecting event streams and ejecting disparity maps at high throughputs, low latencies, and low power.\nthe system is highly parameterized and can operate with other event based sensors such as atis or dvs table compares our approach with the literature on event based disparity.\nthe implemented neuromorphic stereo disparity system achieves these advantages, while consuming less power per pixel per disparity map compared to the stateof-the-art the homogeneous computational substrate provides the first example of a fully end-to-end low-power, high throughput fully event-based neuromorphic stereo system capable of running on live input event streams, using a fully graph-based computation model, where no frames, arrays or other such data-structures are used.", "1017": "one of the most popular languages used to encode the invariances needed to reason about causal relations, for both learning and inference, is based on graphical models, and appears under the rubric of causal graphs 16, 21, a causal graph is a directed acyclic graph (dag) with latent variables, where each edge encodes a causal relationship between its endpoints: x is a direct cause of y , i.e., x y , if, when the remaining factors are held constant, forcing x to take a specific value affects the realization of y , where x,y are random variables representing some relevant features of the system.\nfor example, if px,z(y) , px(y), for some y, we can read that fz y fx , fx,z accordingly, given a set of interventional distributions, we construct an augmented graph by introducing an f-node for every unique set difference between pairs of controlled intervention sets (more on that later on).\nto encapsulate the distributional invariants directly induced by the causal calculus rules3, we call a set of interventional distributions i-markov to a graph, if these distributions respect the causal calculus rules relative to that graph.\nwe call two causal graphs d1,d2 i-markov equivalent if the set of distributions that are i-markov tod1 andd2 are the same.\nour contributions can be summarized as follows: we propose a characterization of i-markov equivalence between two causal graphs with latent variables for a given intervention set i that is based on a generalization of do-calculus rules to arbitrary subsets of interventions.\ncausal bayesian network (cbn): let p(v) be a probability distribution over a set of variables v, and let px(v) denote the distribution resulting from the hard intervention do(x x), which sets x v to constants x.\na directed acyclic graph (dag) over v is said to be a causal bayesian network compatible with p if and only if, for all x v, px(v) ivix p(vipai), for all v consistent with x, and where pai is the set of parents of vi 16, 1, pp.\ngiven that a subset of the variables are unmeasured or latent,d(vl,e) represents the causal graph where v and l denote the measured and latent variables, respectively, and e denotes the edges.\nalso, two causal graphs are called markov equivalent whenever they share the same set of conditional independences over v.\naccordingly, the interventional distribution px(v) becomes as follows, where p(xipai) , p(xipai) is the new conditional distribution set by the intervention: px(v) l ixix p(xipai) jt jx p(t jpa j) in this work, we assume that all the soft interventions are controlled.\na maximal ancestral graph (mag) is a graph that is both ancestral and maximal given a causal graphd(v,l), a magmd over v can be constructed such that both the independence and the ancestral relations among variables in v are retained, see, for example, 27, p.\n,w,z,y, is a discriminating path for z if (1) p includes at least three edges; (2) z is a non-endpoint node on p, and is adjacent to y on p; and (3) x is not adjacent to y , and every node between x and z is a collider on p and is a parent of y two mags are markov equivalent if and only if (1) they have the same adjacencies; (2) they have the same unshielded colliders; and (3) if a path p is a discriminating path for a vertex z in both graphs, then z is a collider on the path in one graph if and only if it is a collider on the path in the other.\na pag, which represents a markov equivalence class of a mag, is learnable from the independence model over the observed variables, and the fci algorithm is a standard sound and complete method to learn such an object related work: learning causal graphs from a combination of observational and interventional data has been studied in the literature 3, 11, 7, 20, 8, 12, for causally sufficient systems, the notion and characterization of interventional markov equivalence has been introduced in 9, more recently, showed that the same characterization can be used for both hard and soft interventions.\none of the most celebrated results in causal inference comes under the rubric of do-calculus (or causal calculus) 18, the calculus consists of a set of inference rules that allows one to create a map between distributions generated by a causal graph when certain graphical conditions hold in the graph.\nthe first rule of the calculus is a d-separation type of statement relative to a specific interventional distribution px, which says that y z w ind implies the corresponding conditional independence px(yw, z) px(yw).\nto witness, consider the causal graph d x y, x cd y, and suppose we have the observational and interventional distributions p(y, x) and px(y, x), respectively.\nusing the ci tests p(y, x) , p(y)p(x) and px(y, x) , px(y) px(x), we infer that the two variables are dependent (or not independent) and consequently d-connected in the graph, while no claim can be made about the causal relation between them.\nalternatively, ifd x y, then px(y x) p(y x), under faithfulness, implies the absence of a latent variable by the converse of rule broadly speaking, rule allows one to infer causal relations between variables, and consequently directed edges in the causal graph.\nfor instance, consider the causal graph d c cd a b,c cd b and suppose we have the interventional distributions pa,b and pc,b.\nthe two conditions used in the definition correspond to rule of theorem and that of corollary notice that the traditional markov definition only considers the first condition over the observational distribution p(v); a case included in the i-markov whenever i accordingly, two causal graphs are said to be i-markov equivalent if they license the same set of distribution tuples.\ngiven two causal graphsd1 (vl1,e1) andd2 (vl2,e2), and an intervention set i 2v,d1 andd2 are called i-markov equivalent if pi (d1,v) pi (d2,v).\nnote that this type of construction has been used in the literature to model interventions 17, for example, for i , x, figure 2a presents the augmented graph corresponding to the causal graph, which is the induced subgraph over x,w,z,y.\nthe augmented graph of d with respect to i , denoted as augi (d), is the graph constructed as follows: augi (d) (v f ,e e) where f b fiik and e (fi, j)ik, jsi the significance of the augmented graph construction is illustrated by proposition 1, which provides criteria to test the d-separation statements in definition equivalently from the corresponding augmented graph of a causal graph.\nback to the example in figure 2a, the statement y x z in dx can be equivalently tested by the statement y fx z in the corresponding augmented graph.\nconsider a causal graphd (v l,e) and the corresponding augmented graph augi (d) (v l f ,e e) with respect to an intervention set i , where f fiik.\nfor disjoint y,z,w v: (y z w )d (y z w, fk )aug(d) (1) for disjoint y,w v, where wi bw si,r b si \\wi: (y si w \\wi )dwi ,r(w) (y fi w, fk\\i )aug(d) (2) in order to characterize causal graphs that are i-markov equivalent, we draw some insight from the markov equivalence of causal graphs with latents.\nancestral graphs, and more specifically mags, were proposed as a representation to encode the d-separation statements of a causal graph among the measured variables while not explicitly encoding the latent nodes.\nsince all the constraints in the i-markov definition can be tested by d-separation statements in the augmented graph, then an augmented mag preserves all those constraints.\ngiven a causal graphd (v l,e) and an intervention set i , the augmented mag is the mag constructed over v from augi (d), i.e., mag(augi (d)).\nbelow, we derive a characterization for two causal graphs to be i-markov equivalent two causal graphs are i-markov equivalent if their corresponding augmented mags satisfy the three conditions given in theorem for example, the two augmented mags in figures 2c and 2d satisfy the three conditions, hence the original causal graphs are in the same i-markov equivalence class.\ntwo causal graphs d1 (v l1,e1) and d2 (v l2,e2) are i-markov equivalent for a set of controlled experiments i if and only if for m1 mag(augi(d1)) and m2 mag(augi (d2)): m1 andm2 have the same skeleton; m1 andm2 have the same unshielded colliders; if a path p is a discriminating path for a node y in bothm1 andm2, then y is a collider on the path in one graph if and only if it is a collider on the path in the other.\nin this section, we develop an algorithm to learn the augmented graph from a combination of observational and interventional data, which consequently recovers the causal graph.\ngiven a causal graph d and an intervention set i , letm mag(augi(d)) and let m be the set of augmented mags corresponding to all the causal graphs that are i-markov equivalent tod.\nalgorithm algorithm for learning augmented pag 1: function learnaugpag(i , (pi)ii ,v) 2: (f ,s, ) createaugmentednodes(i ,v) 3: v v f 4: phase i: learn adjacencies and seperating sets 5: form the complete graph g on v where between every pair of nodes there is an edge 6: for every pair x,y v do 7: if x f y f then 8: s eps et(x,y) , s epflag(x,y) true 9: else 10: (s eps et(x,y), s epflag) do-constraints((pi)ii , x,y,v,f , ) 11: if s epflag true then 12: remove the edge between x,y in g.\nto explain the algorithm, we first describe fci which, given an independence model over the measured variables, proceeds in three phases 25: in phase i, the algorithm initializes a complete graph with circle edges (), then it removes the edge between any pair of nodes if a separating set between the pair exists and records the set.\nwe also observe that if two nodes x,y are separated given z in augi (d), they are also separated given z f since f are root nodes by construction, i.e., all the edges incident on f-nodes are out of them.\nthe algorithm starts by creating a complete graph of circle edges between v f then, it removes the edge between any two nodes x and y if a separating set exists.\nthe function routine works as follows: if the two nodes are random variables (and not f-nodes), then an arbitrary distribution is chosen and we find a subset w that establishes conditional independence between x and y (rule of thm.\nrule (inducing paths): if fk f is adjacent to a node y sk and sk 1, e.g., sk x, then orient x y out of x, i.e., x y the intuition for this rule is as follows: if fk is adjacent to a node y sk in g, then there is an inducing path p between fk and y in augi(d), where d is any causal graph in the equivalence class.\nnodes fx and z are separable in augi (d) given the empty set and this can be tested by the do-constraint p(z) px(z).\nfigure 3c shows the graph obtained after applying the seven rules of the fci together with rule finally, by applying rule 9, we infer that the edge between x and y has a tail at x and we obtain the graph in figure 3d.\nconsider a set of interventional distributions (pi)ii c-faithful to a causal graph d (v l), where i is a set of controlled experiments.", "1018": "more complex queries need to be evaluated as an acyclic expression graph over nodes representing kb access, set, logical, and arithmetic operators (andreas et al., 2016a).\neach step of the program selects an atomic operator and a set of previously defined variables as arguments and writes the result to scratch memory, which can then be used in subsequent steps.\nwith this approach it is now possible to first train separate modules for each of the atomic operations involved and then train a program induction model that learns to use these separately trained models and invoke the sub-modules in the correct fashion to solve the task.\nthese sub-modules can even be task-agnostic generic models that can be pretrained with much more extensive training data, while the program induction model learns from examples pertaining to the specific task.\nprogram induction has also seen initial promise in translating simple natural language queries into programs executable in one or two hops over a kb to obtain answers (liang et al., 2017).\nmain contributions we present complex imperative program induction from terminal rewards\u2019\u2019 (cipitr),2 an advanced neural program induction (npi) system that is able to answer complex logical, quantitative, and comparative queries by inducing programs of length up to 7, using atomic operators and variable types.\ncipitr reduces the combinatorial program space to only semantically correct programs by (i) incorporating symbolic constraints guided by kb schema and inferred answer type, and (ii) adopting pragmatic programming techniques by decomposing the final goal into a hierarchy of sub-goals, thereby mitigating the sparse reward problem by considering additional auxiliary rewards in a generic, task-independent way.\nwe evaluate cipitr on the following two challenging tasks: (i) complex kbqa posed by the recently-published csqa data set (saha et al., 2018) and (ii) multi-hop kbqa in one of the more 2the code and reinforcement learning environment of cipitr is made public inurl/ cipitr.\non a data set such as csqa, contemporary models like neural symbolic machines (nsm) fail to handle exponential growth of the program search space caused by a large number of operator choices at every step of a lengthy program.\non one of the hardest class of programs of around steps (i.e., comparative reasoning), cipitr outperformed nsm by a factor of and kvmnet by a factor of further, we empirically observe that among all the competing models, cipitr shows the best generalization across diverse program classes.\none of the state-of-the-art neural models for kbqa, the keyvalue memory network kvmnet (miller et al., 2016) learns to answer questions by attending on the relevant kb subgraph stored in its memory.\ncsqa is particularly suited to study the complex program induction (cpi) challenge over other kbqa data sets because: it contains large-scale training data of question-answer pairs across diverse classes of complex queries, each requiring different inference tools over large kb sub-graphs.\nrelevant statistics of the resulting data set are presented in table use of gold entity, type, and relation annotations to standardize comparisons: our focus being on the reasoning aspect of the kbqa problem, we use the gold annotations of canonical kb entities, types, and relations available in the data set along with the the queries, in order to remove a prominent source of confusion in comparing kbqa systems (i.e., all systems take as inputs the natural language query, with spans identified with kb ids of entities, types, relations, and integers).\nalthough annotation accuracy affects a complete kbqa system, our focus here is on complex, multi-step program generation with only final answer as the distant supervision, and not entity/type/relation linking.\nas in this work, we are focusing on inducing programs where the gold entity relation annotations are known; for this data set as well, we use the human-annotations to collect all the entities and relations in the oracle subgraph associated with the query.\nthe npi model has to understand the role of these gold program inputs in question-answering and learn to induce a program to reflect the same inferencing.\nnine variable-types: (distinct from kb types) kb artifacts: ent(entity), rel(relation), type base data types: int, bool, none (empty argument type used for padding) composite data types: set (i.e., set of kb entities) or mapset and mapint (i.e., a mapping function from an entity to a set of kb entities or an integer) twenty operators: genset(ent, rel, type) set verify(ent, rel, ent) bool genmap set(type, rel, type) mapset mapcount(mapset) mapint set union/ints/diff(set, set) set mapunion/ints/diff(mapset, mapset) mapset setcount(set) int selectatleast/atmost/more/less/ equal/approx(map int, int) set selectmax/min(map int) ent noop() (i.e., no action taken) symbols and hyperparameters: (typical values) numop: number of operators (20) numvartypes: number of variable types (9) maxvar: maximum number of variables accommodated in memory for each type (3) m: maximum number of arguments for an operator (none padding for fewer arguments) (3) dkey dval: dimension of the key and value embeddings (dkey dval) (100, 300) np nv: number of operators and argument variables sampled per operator each time (4, 10) f with subscript: some feed-forward network embedding matrices: the model is trained with a vocabulary of operators and variable-types.\noperator prototype matrices: these matrices store the argument variable type information for the m arguments of every operator in mop arg 0, 1, , numvartypesnum opm and the output variable type created by it in mop out 0, 1, memory matrices: this is the query-specific scratch memory for storing new program variables as they get created by cipitr.\ncipitr consists of three components: the preprocessor takes the input query and the kb and performs the task of entity, relation, and type linking which acts as input to the program induction.\nthe programmer model takes as input the natural language question, the kb, and the pre-populated variable memory tables to generate a program (i.e., a sequence of operators invoked with past instantiated variables as their arguments and generating new variables in memory).\nxkey f(x), xdist softmax(mx keyxkey) feasibility sampling: to restrict the search space to meaningful programs, cipitr incorporates both high-level generic or task-specific constraints when sampling any action.\nthe task specific constraints ensure that the generated program is consistent as per the kb schema or on execution gives an answer of the desired variable type.\nalgorithm feasibility sampling input: xdist rn (where n is the size of the population set over which lookup needs to be done) xfeas 0, 1n (boolean feasibility vector) k (top-k sampled) procedure: feassampling (xdist, xfeas, k) xdist xdist xfeas (elementwise multiply) xdist l1-normalized(xdist) xsampled k-argmax(xdist) output: xdist, xsampled writing a new variable to memory: this operation takes a newly generated variable, say x, of type xtype and adds its key and value embedding to the row corresponding to xtype in the memory matrices.\nargument variable sampler: for each sampled operator p, it takes: (i) program state ht, (ii) the list of variable types v typep of the m arguments obtained by looking up the operator prototype matrix mop arg, and (iii) a boolean vector v feasp that indicates the valid variable configurations for the m-tuple arguments of the operator p.\nprocedure: argvarsampler(ht, v typep , v feasp , nv) forj 1, 2, ,m do vattp,j softmax(m var attv typep,j ) fvtype(ht) vdistp,j lookup(vattp,j , fvar,mvar keyv type p,j ) v distp v dist p,0 vdistp,1 vdistp,m , joint distribution v distp , vp feassampling(v distp , v feasp , nv) output: vp end-to-end cipitr training: cipitr takes a natural language query and generates an output program in a number of steps.\nto reduce exposure bias (ranzato et al., 2015), cipitr uses a beam search to obtain multiple candidate programs to provide feedback to the model from a single training instance.\nthen, for each of the np operators, it samples nv feasible algorithm cipitr pseudo-code (beam size1) query encoding: q gru(query) initialization: e1, h1 f(q), a for t 1, , t do pfeast feasibleop() pt operatorsampler(ht, pfeast , np) c for p pt do v typep v type p,1 , , vtypep,m mop argp v feasp feasiblevar(p) vp argvarsampler(ht, v typep , v feasp , nv) for v vp do c c (p, v, v typep ) (p, v, v typep ) argmax(c) ukeyp , u val p , u type p outvargen(p, v typep , v ) writevartomem(ukeyp , uvalp , utypep ) et1, ht1 npicore(et, ht) a.append((p, v )) output: a variable instantiations, resulting in a total of np nv candidates out of which b most-likely actions are sampled for the b beams and the corresponding newly generated variables written into memory.\nhandling complex queries by expanding the operator set and generating longer programs blows up the program space to a huge size of (numop (maxvar) m)t this, in absence of gold programs, poses serious training challenges for the programmer.\nthe predicted answer type helps in directing the program search toward the correct answer type by biasing the sampling towards feasible operators that can produce the desired answer type.\nan invalid program gets a reward of further, to mitigate the sparsity of the extrinsic rewards, an additional auxiliary feedback is designed to reward the model on generating an answer of the predicted answer-type.\nsampling only feasible actions: sampling a feasible action requires first sampling a feasible operator and then its feasible variable arguments: the operator must be allowed in the current phase of the model\u2019s program induction.\nfor this work, we limit our effort on kbqa to the setting where the query is annotated with the gold kb-artifacts, which standardizes the input to the program induction for the competing models.\nadditionally, the entity/relation linker outputs used by these models are also not made public, making it difficult to set up a fair ground for evaluating the program induction model, especially because we are interested in the program induction given the program inputs and handling the entity/relation linking is beyond the scope of this work.\nto avoid these issues, we use the human-annotated entity/relation linking data available along with the questions as input to the program induction model.\nfurther, to gauge the proficiency of the proposed program induction model, we construct a rule-based model which is aware of the human annotated semantic parsed form of the querythat is, the inference chain of relations and the exact constraints that need to be additionally applied to reach the answer.\non the other hand, the task of cipitr is to actually learn the program by looking at training examples of the query and corresponding answer.\na comparative performance analysis of the proposed cipitr model, the rule-based model and the sparql executor is tabulated in table the main take-away from these results is that cipitr is indeed able to learn the rules behind the multi-step inference process simply from the distance supervision provided by the questionanswer pairs and even perform slightly better in some of the query classes.\nkvmnet with decoder (2016), which performed best on csqa data set (saha et al., 2018) (as discussed in section 2), learns to attend on a kb subgraph in memory and decode the attention over memory-entries as their likelihood of being in the answer.\nas the nsm code was not available, we implemented it and further incorporated most of the six techniques presented in table however, constraints like action repetition, biasing last operator selection, and phase change cannot be incorporated in nsm while keeping the model generic, as it decodes the program token by token.\nkvmnet does not have any beam search, the nsm model uses a beam size of 50, and cipitr uses only beams for exploring the program space.\nto summarize, cipitr has the following advantages, inducing programs more efficiently and pragmatically, as illustrated by the sample outputs in table 5: generating syntactically correct programs: because of the token-by-token decoding of the program, nsm cannot restrict its search to only syntactically correct programs, but rather only resorts to a post-filtering step during training.\nhowever, at test time, it could still generate programs with wrong syntax, as shown in table for example, for the logical question, it invokes a genset with a wrong argument type none and for the quantitative count question, it invokes the setunion operator on a non-set argument.\nas shown in table 5, cipitr is able to generate at least meaningful programs having the desired answer-type or without repeating lines of code.\nefficient search-space exploration: owing to the different strategies used to explore the program space more intelligently, cipitr scales better to a wide variety of complex queries by using less than half of nsm\u2019s beam size.", "1019": "contact author: haggaiil.ibm.com to illustrate the effect of summary length on this tradeoff, using the duc dataset, figure reports the summarization quality which was obtained by the cross entropy summarizer (ces) a state of the art unsupervised query-focused multidocument extractive summarizer saliency was measured according to cosine similarity between the summary\u2019s bigram representation and that of the input documents.\naiming at better handling the saliency versus focus tradeoff, in this work, we propose dual-ces an extended ces summarizer similar to ces, dual-ces is an unsupervised query-focused, multi-document, extractive summarizer.\nto this end, like ces, dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents, whose combination is predicted to produce a good summary.\nyet, differently from ces, dual-ces does not attempt to address both saliency and focus goals in a single optimization step.\nusing such an approach, dual-ces tries to handle the tradeoff by gradually shifting from generating a long summary that is more salient in the first step to generating a short summary that is more focused in the second step.\ndual-ces provides a fully unsupervised end-to-end query-focused multi-document extractive summarization solution.\nusing an evaluation with the duc 2005, and benchmarks, we show that, dual-ces generates a focused (and shorter) summary which has much higher saliency (and hence a better tradeoff handling).\noverall, dual-ces provides a significantly better summarization quality compared to other alternative unsupervised summarizers; and in many cases, it even outperforms that of state-of-the art supervised summarizers.\nalternatively, we distill informative hints from summarized documents, aiming to improve the saliency of produced focused summaries.\nlet d denote a set of one or more matching documents to be summarized and lmax be the maximum allowed summary length (in words).\nto this end, we produce summary s (with maximum length lmax) by choosing a subset of sentences s d which maximizes a given quality target q(sq,d).\ninstead, following 6, q(sq,d) is surrogated by several summary quality prediction measures qi(sq,d) (i 1, 2, each predictor qi(sq,d) is designed to estimate the level of saliency or focus of a given candidate summary s and is presumed to correlate (up to some extent) with actual summarization quality, e.g., rouge for simplicity, similar to ces, various predictions are assumed to be independent and are combined into a single optimization objective by taking their product, i.e. the ce-method provides a generic monte-carlo optimization framework for solving hard combinatorial problems previously, it was utilized for solving the sentence subset selection problem to this end, the ce-method gets as an input q(q,d), a constraint on maximum summary length l and an optional pseudo-reference summary sl, whose usage will be explained later on.\ndifferently from ces, dual-ces does not attempt to maximize both saliency and focus goals in a single optimization step.\nyet, each such invocation utilizes a bit different set of summary quality predictors qi(sq,d), depending on whether the summarizer\u2019s goal should lay towards higher summary saliency or focus.\nin the first step, dual-ces relaxes the summary length constraint, aiming at producing a longer and more salient summary.\nyet, at the second step, similar to ces, the primary goal is actually to produce a focused summary (with maximum length limit lmax).\noverall, dual-ces is simply implemented as follows: cem(qfoc(q,d), lmax,cem(qsal(q,d), l, )).\nhere, qsal(q,d) and qfoc(q,d) denote the saliency and focus summary quality objectives which are optimized during the cascade, respectively.\nthe purpose of the first step is to produce a single longer summary (with length l lmax) which will be used as a pseudo-reference for saliency-based feedback distillation.\nsimilar to ces, qsal(q,d) is calculated as the product of several summary quality predictors.\nto target even higher saliency, we suggest a fourth predictor, inspired by the risk minimization framework to this end, we measure the kullback-leibler (kl) similarity between the two (unsmoothed) unigram language models induced from the centroid representation1 of s (s) and d (d), formally: qkl(sq,d) def exp ( w p(ws) log p(ws) p(wd) ) while producing a longer summary may result in higher saliency, as was further illustrated in figure 1, such a summary may be less focused.\nto this end, we add a predictor: qqf (sq,d) def wq p(ws), which acts as a query-anchor and measures to what extent summary s\u2019s unigram model is devoted to the information need q.\nhere, the target measure qfoc(q,d) guides the optimization towards the production of a focused summary, while still keeping high saliency as much as possible.\nto this end, on the first saliency-driven step, for dual-ces, we fixed the (strict) upper bound limit on summary length to l dual-ces-a, on the other hand, adaptively adjusts such length limit and was initialized with lt0 both variants were further set with a summary limit lmax for their second focus-driven step.\nwe compare the summary quality of dual-ces to the results that were previously reported for several competitive summarization baselines.\nby distilling saliency-based pseudo-feedback between step transitions, dual-ces manages to better utilize the ce-method for selecting a more promising subset of sentences.\na simple combination of all predictors (except predictor which is unique to dual-ces since it requires a pseudo-reference summary) does not directly translates to a better tradeoff handling.\nthis, therefore, serves as a strong empirical evidence of the importance of the dualcascade optimization approach implemented by dual-ces, which allows to produce focused summarizes with better saliency.\nthe pseudo-feedback distillation approach employed between the two steps of dualces has some resemblance to attention models that are used by state-of-the-art deep learning summarization methods 1, 12, first we note that, dual-ces significantly improves over these attentive baselines on rouge-1.\nusing dual-ces as a reference method for comparison, apparently, crsumsf attendance on salient words first and then on salient sentences based on such words seems as the better strategy.\nin a sense, similar to crsumsf, dual-ces also first attends on salient words which are distilled from the pseudo-feedback reference summary.\ndual-ces then utilizes such salient words for better selection of salient sentences within its second step of focused summary production.\ntable reports the sensitivity of dual-ces (measured by rouge-x recall) to the value of hyperparameter l, using the duc benchmark.\nwe proposed dual-ces, an unsupervised, query-focused, extractive multi-document summarizer.\ndual-ces was shown to better handle the tradeoff between saliency and focus, providing the best summarization quality compared to other alternative stateof-the-art unsupervised summarizers.", "1020": "in recent years we are experiencing a dramatic improvement of the synthesized speech quality in tts systems, with the introduction of systems that are based on neural networks (nn).\na major improvement in quality was achieved by using attention based models such as tacotron and by replacing vocoders with a nn based waveform generators such as wavenet a useful feature of systems with trainable models is the ability to adapt the tts to an unseen voice using a small amount of training data (from a few seconds to an hour of speech).\nusing this approach allows later retraining of only a subsets of the model parameters or prediction of the speaker embedding vector 3, 4, the drawback of this approach is that the resulting systems use large nn models.\nthis was carried out by retraining nn models that had already been trained using a large highquality voice, on a small amount of data from the new voice.\nrecently, an efficient neural vocoder called lpcnet was introduced the lpcnet inference runs faster than realtime on a single cpu while producing a high quality speech output.\nin this paper we show that we can get a considerable quality improvement by modifying a tts system that produced the world vocoder parameters to predict parameters for lpcnet as in the previous work 6, we conduct multiple adaptation experiments, applied on multiple vctk voices and show that the new system has much better quality and similarity to the target voices but can still run much faster than real-time in a single-cpu mode.\nan overview of our new tts system is presented in figure the system is a cascade of a rule-based front-end, a nn based prosody generator, a nn synthesizer and an lpcnet decoder.\nfinally, an lpcnet block (section 2.3) is used to convert the stream of the acoustic feature vectors to a speech signal.\nthe prosody generator, synthesizer and lpcnet blocks use neural-net models for generating their output.\nthe input features, derived from the tts front end, are comprised of 1-hot coded categorical features and standard positional features in this architecture the prosody adaptation to unseen speaker is based on a variational auto encoder (vae) utterance prosody embedding, averaged over all the speaker utterances 6, as presented on figure in the current work we used multi-speaker baseline models for prosody adaptation to unseen voices, as it resulted in better quality than the single speaker models.\nfrom the top layer we generate by linear transformations the speech parameters that the lpcnet requires as input: cepstral vector, pitch and a pitch correlation parameters with first and second derivatives for all (total of parameters).\nthe final parameters which we use as input for the lpcnet are found by solving the maximum likelihood parameter generation (mlpg) equations we also apply a formant enhancement filter on the cepstral coefficients ck, k1n to compensate for the nn averaging and to improve the speech quality similar to the enhancement starts by multiplication of the high-order coefficients: c k c k kk ck kk (1) we choose and k2.\nfinally, to compensate for the energy change, we apply: c 0c0n log10( e c ec ) (2) the architecture of the synthesizer is shown in figure the size of the layers is: phonetic embedding: 32, phonetic convolution: 128, pitch convolution: 32, lstm: and full: the network is trained using an aligned corpus where the inputs are the frame based phonetic labels and pitch values, while the outputs are the corresponding lpcnet parameters.\nthe lpcnet decoder is a wavernn variant that uses a nn model to generate speech samples from equidistant-intime input of cepstrum, pitch and pitch correlation parameters.\nunlike other waveform generative models, such as wavenet and wavernn, the lpcnet uses its nn to predict the lpc residual (the vocal source signal) and then apply to it an lpc filter calculated from the cepstrum.\nthe lpcnet model was reported to perform well in speaker independent setting, when trained on multi-speaker datasets 8, however, we experimentally found that its performance further improves when retraining the initial multi-speaker same-gender model with the target voice specific data.\nfor each of those voices we built the following single speaker tts systems: a world based system at 22khz as described in an lpcnet based system at 16khz as described in the previous section.\ntacotron2 based tts with wavenet decoder at 22khz we used each one of these systems to synthesize a set of held-out sentences and compared them by a mos test to the original recordings.\nfor the female voice the statistical significance of the difference between the lpcnet and the tacotron systems is small (i.e. we can see from these results that the lpcnet model has a huge impact on the quality compared to the world system.\none should note relatively low mos scores for the original natural samples, which can be explained by the assumption that the listeners subjectively judged speaker pleasantness together with the speech quality and naturalness.\nthe networks for the acoustic features and the lpcnet where adapted from the corresponding, same gender networks that where trained in section the prosody network was adapted from a multi-speaker baseline model (that was originally trained on high-quality voices and vctk voices).\nin addition, we also built a world based tts for each voice by adapting the world based acoustic feature networks from the corresponding same gender networks of section using the full voice data.\nwe evaluated each systems quality with mos tests as in section for reference, the tests also included samples from the original vctk datasets.\nwe can compare these results to those of the vcc hub task although the task setup and the listening tests conditions are a bit different we can see that our system mos and similarity score are comparable to those of the best vcc system (n10 with quality of and similarity of where the corresponding scores for the original speech are and 95).\nthe similarity scores for each voice, were normalized to the range between the scores of natural samples from different and same speakers.\nwe have presented in this article a new tts system that addresses the challenging goals of producing high quality speech while operating at faster than real-time rate without an expensive gpu support.\nthe system is built around three nn models for generating the prosody, acoustic features and the final speech signal.\nwe tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems.\nj. shen et al., natural tts synthesis by conditioning wavenet on mel spectrogram predictions, ieee international conference on acoustics, speech and signal processing (icassp), calgary, ab, 2018, pp.\nzhouh, neural voice cloning with a few samples, arxiv preprint arxiv:1802.06006, ye jia, et al., transfer learning from speaker verification to multispeaker text-to-speech synthesis, arxiv preprint arxiv:1806.04558, sample efficient adaptive text-to-speech.\nozawa, world: a vocoderbased high-quality speech synthesis system for real-time applications, ieice transactions on information and systems, 99(7):18771884, j.", "1021": "for instance, the hilbert-schmidt independence criterion (hsic) uses the maximum mean discrepancy (mmd) to assess the dependency between two variables, i.e. hsic(x,y ) mmd(pxy, pxpy), which can be easily estimated from samples via kernel mean embeddings in a reproducing kernel hilbert space (rkhs) in this paper we introduce the sobolev independence criterion (sic), a form of gradient regularized integral probability metric (ipm) between the joint distribution and the product of marginals.\nin section we study the case where the sic witness function f is restricted to an rkhs and show that it leads to an optimization problem that is jointly convex in f and the importance scores we show that in this case sic decomposes into the sum of feature scores, which is ideal for feature selection.\nin section we show how sic and conditional generative models can be used to control the false discovery rate using the recently introduced holdout randomization test and knockoffs we validate sic and its fdr control on synthetic and real datasets in section motivation: feature selection.\nwe start by motivating gradient-sparsity regularization in sic as a mean of selecting the features that maintain maximum dependency between two randoms variable x (the input) and y (the response) defined on two spaces x rdx and y rdy (in the simplest case dy 1).\ninstead of the usual kl divergence, the metric d with its witness function, or critic, f(x, y) measures the distance between the joint pxy and the product of marginals pxpy with this generalized definition of mutual information, the feature selection problem can be formalized as finding a sparse selector or gate w rdx such thatd(pw x,y, pw xpy) is maximal , i.e. supw,w0sd(pw x,y, pw xpy), where is a pointwise multiplication and w0 jwj this problem can be written in the following penalized form: (p) : sup w sup ff epxyf(w x, y) epxpyf(w x, y) w0 we can relabel f(x, y) f(w x, y) and write (p) as: supff epxy f(x, y)epxpy f(x, y), where f f f(x, y) f(w x, y)f f , w0 s.\nhence, we can reformulate the problem (p) as follows: (sic): sup ff epxyf(x, y) epxpyf(x, y) ps(f), where ps(f) is a penalty that controls the sparsity of the gradient of the witness function f on the support of the measures.\nsuch smoothness penalties became popular in deep learning partly following the introduction of wgan-gp 17, and were used as regularizer for distance measures between distributions in connection to optimal transport theory let be a dominant measure of pxy and pxpy the most commonly used gradient penalties is l2(f) e(x,y) xf(x, y) while this penalty promotes smoothness, it does not control the desired sparsity as discussed in the previous section.\nwe therefore elect to instead use the nonlinear sparsity penalty introduced in : 0(f) je(x,y) f(x,y)xj 0, and its relaxation : s(f) dx j1 e(x,y) f(x, y)xj as discussed in 14, e(x,y) f(x,y)xj implies that f is constant with respect to variable xj , if the function f is continuously differentiable and the support of is connected.\nthese considerations motivate the following definition of the sobolev independence criterion (sic): sic(l1)2(pxy, pxpy) sup ff epxyf(x, y) epxpyf(x, y) (s(f)) ef2(x, y).\nthen, given samples (xi, yi), i 1, , n from the joint probability distribution pxy and iid samples (xi, yi), i 1, , n from pxpy , sic can be estimated as follows: sic(l1)2(pxy, pxpy) sup ff n n i1 f(xi, yi) n n i1 f(xi, yi) ( s(f) )2 n n i1 f2(xi, yi), where s(f) dx j1 n n i1 f(xi,yi)xj throughout this paper we consider feature selection only on x since y is thought of as the response.\nd, aj we have: (d j1 aj )2 inf d j1 aj j : , j d j1 j 1, optimum achieved at j aj/ j aj we alleviate first the issue of non smoothness of the square root by adding an (0, 1), and we define: s, dx j1 e(x,y) f(x,y)xj using lemma the nonlinear sparsity inducing gradient penalty can be written as : (s,(f)) inf dx j1 epxpy f(x,y)xj j : , j 0, dx j1 j 1, where the optimum is achieved for : j, jdx k1 k , where 2j epxpy f(x,y)xj we refer to j, as the normalized importance score of feature j.\nhence, substituting (s)(f) with s,(f) in its equivalent form we obtain the perturbed sic: sic(l1)2,(pxy, pxpy) infl(f, ) : f f , j , j 0, dx j1 j where l(f, ) (f, pxy, pxpy) dx j1 epxpy f(x,y)xj j 2epxpyf 2(x, y), and (f, pxy, pxpy) epxyf(x, y) epxpyf(x, y).\nwe will now specify the function space f in sic and consider in this section critics of the form: f f f(x, y) u,(x, y) , u2 , where : x y rm is a fixed finite dimensional feature map.\nwe can write the constraint u2 as the penalty term u define l(u, ) u, (pxpy) (pxy) u, ( dx j1 dj(pxpy) j c(pxpy) im ) u observe that : sic(l1)2,(pxy, pxpy) infl(u, ) : u rm, j , j 0, dx j1 j we start by remarking that sic is a form of gradient regularized maximum mean discrepancy previous mmd work comparing joint and product of marginals did not use the concept of nonlinear sparsity.\nlet (u, ) be the limit defined in theorem we have that sic(l1)2(pxy, pxpy) ( epxyf(x, y) epxpyf(x, y) ) dx j1 epxpy f(x, y) xj epxpyf,2(x, y) f2f moreover, epxpy f(x,y) xj js,l1(f) and dx j1 j the terms j can be seen as quantifying how much dependency as measured by sic can be explained by a coordinate j.\nas we show below, using our sparsity inducing gradient penalties with such networks, results in input sparsity at the level of the witness function f of sic.\nthe theorem states that f is homogeneous of degree k if and only if kf(x) xf(x), x dx j1 f(x) xj xj now consider deep relu networks with biases removed for any number of layers l: frelu f f(x, y) u,(x) , where (x, y) (wl (w2(w1x, y))), u rm, : rdxdy rm, where (t) max(t, 0),wj are linear weights.\nwe define the empirical non convex sic(l1)2 using this function space frelu as follows: sic(l1)2(pxy, pxpy) infl(f, ) : f frelu , j , j 0, dx j1 j 1, where (vec(w1) vec(wl), u) are the network parameters.\ninspired by importance scores in random forest, we define boosted sic as the arithmetic mean or the geometric mean of controlling the false discovery rate (fdr) in feature selection is an important problem for reproducible discoveries.\ntpr and fdr are defined as follows: tpr : e i : i s s i : i s fdr : e i : i s\\s i : i s (1) we explore in this paper two methods that provably control the fdr: 1) the holdout randomization test (hrt) introduced in 8, that we specialize for sic in algorithm 4; 2) knockoffs introduced in that can be used with any basic feature selection method such as neural sic, and guarantees provable fdr control.\nwe are interested in measuring the conditional dependency between a feature xj and the response variable y conditionally on the other features noted xj hence we have the following null hypothesis: h0 : xj y xj pxy pxj xjpyxjpxj in order to simulate the null hypothesis, we propose to use generative models for sampling from xj xj (see appendix d).\nwe apply hrt-sic on a shortlist of pre-selected features per their ranking of j knockoffs work by finding control variables called knockoffs x that mimic the behavior of the real features x and provably control the fdr we use here gaussian knockoffs and train sic on the concatenation of x, x, i.e we train sic(x; x, y ) and obtain that has now twice the dimension dx, i.e for each real feature j, there is the real importance score j and the knockoff importance score jdx knockoffs-sic consists in using the statistics wj j jdx and the knockoff filter to select features based on the sign of wj (see alg.\nwe experiment with two datasets: a) complex multivariate synthetic data (sinexp), which is generated from a complex multivariate model proposed in sec 5.3, where ground truth features xi out of generate the output y through a non-linearity involving the product and composition of the cos, sin and exp functions (see appendix f.1).\ntpr top fdr top tpr hrt fdr hrt tpr top fdr top tpr hrt fdr hrt po we r a nd f dr elastic net random forest tpr top fdr top tpr hrt fdr hrt tpr top fdr top tpr hrt fdr hrt mse sobolev penalty sic dataset sinexp, n125 samples dataset sinexp, n500 samples feature selection on drug response dataset.\nthe sic critic and regressor nn were respectively the bigcritic and regressornn described with training details in appendix f.3, while the random forest is trained with default hyper parameters from scikit-learn we can see that, with just j , informative features are selected for the downstream regression task, with performance comparable to those selected by elasticnet, which was trained explicitly for this task.\nthe features selected with high j values and their overlap with the features selected by elasticnet are listed in appendix f.2 table hiv-1 drug resistance with knockoffs-sic.\nresults are summarized in table we introduced in this paper the sobolev independence criterion (sic), a dependency measure that gives rise to feature importance which can be used for feature selection and interpretable decision making.\ngiven samples from the joint and the marginals, it is easy to see that the empirical loss l can be written in the same way with empirical feature mean embeddings (pxy) 1n n i1 (xi, yi) and (pxpy) n n i1 (xi, yi), covariances c(pxpy) 1n n i1 (xi, yi) (xi, yi) and derivatives grammians dj(pxpy) n n i1 (xi,yi) xj (x,y)xj given the strict convexity of l jointly in u and , alternating optimization as given in algorithm in appendix is known to be convergent to a global optima (theorem in 40).\nsimilarly block coordinate descent (bcd) using first order methods as given in algorithms and (in appendix): gradient descent on u and mirror descent on (in order to satisfy the simplex constraint 22) are also known to be globally convergent (theo in 41.) algorithm alternating optimization inputs: ,, , , initialize j 1dx ,j, (pxy) (pxpy) for i .maxiter do u( dx j1 dj(pxpy) j c(pxpy) im )1 j u,dj(pxpy)udx k1 u,dk(pxpy)u end for output: u, algorithm block coordinate descent inputs: ,, , , , (learning rates), initialize j 1dx ,j , softmax(z) ez/ dx j1 e zj for i .maxiter do gradient step u: u u l(u,)u mirror descent : logit log() l(u,) softmax(logit) stable implementation of softmax end for output: u, algorithm (non convex) neural sic(x,y ) (stochastic bcd ) inputs: x,y dataset x rndx , y rndy , such that (xi xi,., yi yi,.) pxy hyperparameters: ,, , , , (learning rates) initialize j 1dx ,j , softmax(z) ez/ dx j1 e zj for iter .maxiter do fetch a minibatch of size n (xi, yi) pxy fetch a minibatch of size n (xi, yi) pxpy yi obtained by permuting rows of y stochastic gradient step on : l(f,) we use adam mirror descent : logit log() l(f,) softmax(logit) stable implementation of softmax end for output: f, algorithm hrt with sic (x,y ) inputs: dtrain (xtr, ytr) , a heldout set dholdout (x,y ), features cutoff k sic: (f , ) sic(dtrain) alg.\nscore of witness on hold out : s mean(f(x,y )) conditional generators pre-trained conditional generator : g(xj , j) predicts xj xj shortlist : i indextopk() p-values for j i; randomizations tests for j i do for r r do construct x , x.,k x.,kk j and x.,j g(xj , j) simulate null hyp.\nsj,r mean(f(x, y )) score of witness function on the null end for pj r1 ( r r1 1srjs ) end for discoveries bh(p,targetfdr) benjaminihochberg procedure output: discoveries algorithm model-x knockoffs fdr control with sic inputs: dtrain (xtr, ytr) , model-x knockoff features x modelx(xtr), target fdr q train sic: (f , ) sic(xtr, x, y ), alg.\nwhere xtr, x is the concatenation of xtr and knockoffs x for j 1, , dx do compute importance score of j feature: wj j jdx , where jdx is the of feature knockoff xj end for compute threshold by setting min t : j:wjt j:wjt q output: discoveries j : wj proof of theorem we have l(u, ) u, u, (c(pxpy) im)u j u,dj(pxpy)u j , u rm and dx where dx is the probability simplex.\nit follows that j epxpy f (x,y)xj k epxpy f (x,y)xk note that we have epxyf (x, y) epxpyf (x, y) , u u, dx j1 dj(pxpy) j, c(pxpy) im u dx j1 epxpy f (x, y) xj epxpyf,2 (x, y) f 2f sic(l1)2, epxyf (x, y) epxpyf (x, y) ( dx j1 epxpy f (x, y) xj epxpyf,2 (x, y) f 2f ) dx j1 epxpy f (x, y) xj epxpyf,2 (x, y) f 2f ( epxyf (x, y) epxpyf (x, y) ) we conclude by taking the holdout randomization test (hrt) is a principled method to produce valid p-values for each feature, that enables the control over the false discovery of a predictive model the p-value associated to each feature xj essentially quantifies the result of a conditional independence test with the null hypothesis stating that xj is independent of the output y, conditioned on all the remaining features xj (x1, this in practice requires the availability of an estimate of the complete conditional of each feature xj , i.e. hrt then samples the values of xj from this conditional distribution to obtain the p-value associated to it.\nf.1 synthetic datasets f.1.1 complex multivariate synthetic dataset (sinexp) the sinexp dataset is generated from a complex multivariate model proposed in sec 5.3, where features xi out of generate the output y through a non-linearity involving the product and composition of the cos, sin and exp functions, as follows: y sin(x1(x1 x2)) cos(x3 x4x5) sin(e x5 ex6 x2).\nf.1.2 liang dataset liang dataset is a variant of the synthetic dataset proposed by the dataset prescribes a regression model with 500-dimensional correlated input features x, where the 1-d regression target y depends on the first features only (the last correlated features are ignored).\nf.3 sic neural network descriptions and training details the first critic network used in the experiments (with sinexp and hiv-1 datasets) is a standard three-layer relu dropout network with no biases, i.e. when using this network, the inputs x and y are first concatenated then given as input to the network.\nthe two first layers have size 100, while the last layer has size we train the network using adam optimizer with 0.5, 0.999, weightdecay1e-4 learning rate 1e-3 and 0.1, and perform training iterations/updates, computed with batches of size all nns used in our experiments were implemented using pytorch smallcritic( (branchxy): sequential( (0): linear(infeatures51, outfeatures100, biasfalse) (1): relu() (2): dropout(p0.3) (3): linear(infeatures100, outfeatures100, biasfalse) (4): relu() (5): dropout(p0.3) (6): linear(infeatures100, outfeatures1, biasfalse) ) ) the critic network used in the experiments with liang and ccle datasets contains two different branches that separately process the inputs x (branchx) and y (branchy), then the output of these two branches are concatenated and processed by a final branch that contains three-layer leakyrelu network (branchxy)."}