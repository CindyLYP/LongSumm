<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Scene Understanding for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-08-08">8 Aug 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>De Brabandere</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ESAT-PSI</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country>KU</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Scene Understanding for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-08-08">8 Aug 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1708.02550v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Most approaches for instance-aware semantic labeling traditionally focus on accuracy. Other aspects like runtime and memory footprint are arguably as important for realtime applications such as autonomous driving. Motivated by this observation and inspired by recent works that tackle multiple tasks with a single integrated architecture [13], [20], [22], in this paper we present a real-time efficient implementation based on ENet [18] that solves three autonomous driving related tasks at once: semantic scene segmentation, instance segmentation and monocular depth estimation. Our approach builds upon a branched ENet architecture with a shared encoder but different decoder branches for each of the three tasks. The presented method can run at 21 fps at a resolution of 1024x512 on the Cityscapes dataset without sacrificing accuracy compared to running each task separately.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The last years the re-appearance of Convolutional Neural Networks (CNNs), whose origin traces back to the 1970s and 1980s, has led to significant advances in many computer vision tasks, such as image classification <ref type="bibr" target="#b13">[14]</ref>, object detection <ref type="bibr" target="#b7">[8]</ref>, semantic scene segmentation <ref type="bibr" target="#b15">[16]</ref>, instance segmentation <ref type="bibr" target="#b8">[9]</ref>, and monocular depth estimation <ref type="bibr" target="#b5">[6]</ref> to name a few. The majority of these works rely on finetuning or slightly altering a CNN architecture, typically the VGG network <ref type="bibr" target="#b18">[19]</ref>, resulting in task-specific CNNs with long inference times that each require a single GPU to run. Admittedly, this is not enough for autonomous driving applications where many of the aforementioned tasks should run in parallel, in real-time, and on a limited number of GPU devices. Furthermore, as shown in recent works <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref> there is merit in combining multiple tasks in a single integrated architecture, as one task might benefit from another leaving smaller space for 'blindspots', which is crucial for self-driving vehicles.</p><p>Motivated by these observations, in this paper we focus on street scene understanding and present an efficient implementation that combines the tasks of semantic scene segmentation, instance segmentation, and monocular depth estimation. Unlike state-of-the-art methods, that use networks with huge number of parameters and long inference times (e.g. VGG <ref type="bibr" target="#b18">[19]</ref>, SegNet <ref type="bibr" target="#b1">[2]</ref>, FCN <ref type="bibr" target="#b15">[16]</ref>), we build upon a real-time architecture, in particular ENet <ref type="bibr" target="#b17">[18]</ref> that has proven to offer image processing rates higher than 10 fps on a single GPU device. Specifically, we use a common ENet encoding step for all tasks, but introduce a branched ENet architecture for the decoding step (i.e. one branch for each of the three different tasks). <ref type="figure" target="#fig_0">Fig. 1</ref> gives an overview of our approach. . Overview of our method. From left to right: the input image is passed though the encoding step of an ENet-inspired architecture to create feature maps, which in turn are forwarded to different branches that perform decoding to arrive at the three outputs, i.e. semantic labels, instance labels and depth. A video is available at https://youtu.be/55ElRh-g_7o.</p><p>Although we do not introduce a new architecture, in this paper we show how to efficiently combine existing components to build a solid architecture for real-time scene understanding. In Sec. II we describe related work on integrated architectures that tackle multiple tasks. Next, we present the implementation details of our method in Sec. III. Finally, in Sec. IV and V we report results for each of the tasks and provide some insights into the strengths and limitations of the presented approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The amount of research performed in literature on the three main tasks studied in this paper, i.e. semantic scene segmentation, instance segmentation, and monocular depth estimation, is vast. In what follows, we solely focus on related works that have combined one or more of these tasks in a single integrated architecture.</p><p>Eigen and Fergus <ref type="bibr" target="#b4">[5]</ref> addressed the tasks of depth prediction, surface normal estimation, and semantic labeling using a multiscale convolutional network architecture that progressively refines predictions from a sequence of scales. Uhrig et al. <ref type="bibr" target="#b21">[22]</ref> presented a method that leverages a FCN network to predict semantic labels, depth, and an instancebased encoding using each pixel's direction towards its corresponding instance center and consequently applying lowlevel computer vision techniques. Kokkinos <ref type="bibr" target="#b12">[13]</ref> went one step further from the previous approaches, and introduced a CNN, namely UberNet, that jointly handles low-, mid-, and high-level vision tasks in a unified architecture. His universal network tackles boundary detection, normal estimation, saliency estimation, semantic segmentation, human part segmentation, semantic boundary detection, region proposal generation, and object detection. Despite obtaining competitive performance while jointly addressing many different tasks, all these approaches suffer from poor inference times making them unsuitable for real-time autonomous driving applications with high frame-rate demands.</p><p>Recently, Teichmann et al. <ref type="bibr" target="#b19">[20]</ref> argued that improving the computational times is more important than improving performance, especially for the case of self-driving vehicles. They presented an approach to joint classification, detection, and semantic segmentation via a unified architecture where the encoder is shared amongst the three tasks, marginally reaching a computational time of 10 fps on the KITTI dataset. Our approach also focuses on further improving the computational times but addresses different tasks, in particular semantic scene segmentation, instance segmentation, and monocular depth estimation, and achieves a computational time of 21 fps on the Cityscapes dataset. To our knowledge this is the first system to estimate depth, semantic and instance segmentation at these frame-rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In order to predict depth, semantic and instance segmentation in real-time, we modify the ENet architecture into a multi-branched network, having three output branches, one for each task (see <ref type="figure" target="#fig_0">Fig. 1</ref>). The original network, as described in <ref type="bibr" target="#b17">[18]</ref>, consists of an encoding step that has three stages (stage 1, 2, 3) and a decoding step that has two stages (stage 4, 5). Since the ENet decoding step is merely for upscaling and finetuning the output of the encoding step, sharing the full encoder (stages 1, 2, 3) between all branches would lead to poor results. Instead, our multi-branch network is constructed as follows: our shared "encoder" consists of stages 1 and 2 of the original Enet network, before continuing to each branch that combines stage 3 of the original ENet encoder with stages 4 and 5 of the original ENet decoder. In what follows, we dive into the details of the individual branches that each performs one task.</p><p>Semantic segmentation The semantic segmentation branch is trained using the standard pixel-wise cross-entropy loss. The classes are weighted using the method described in <ref type="bibr" target="#b17">[18]</ref> and trained until convergence. The semantic segmentation is used for free space detection as well for classifying the objects found by the instance segmentation branch.</p><p>Instance segmentation In order to perform instance segmentation using a typical feed-forward network without having to resort to slower detect-and-segment approaches, we use a recently introduced discriminative loss function <ref type="bibr" target="#b3">[4]</ref> suited for real-time instance segmentation that can be plugged into an off-the-shelf network. The intuition behind the proposed loss function is that pixel embeddings (i.e. the network's output for each pixel) with the same label (i.e. same instance) should end up close together, while embeddings with a different label (i.e. different instance) should end up far apart.</p><p>Inspired by Weinberger et al. <ref type="bibr" target="#b22">[23]</ref> and other distance metric learning approaches, the authors propose a loss function with two competing terms to achieve this objective: a variance term pulling pixel embeddings towards the mean embedding of their cluster, and a distance term pushing the clusters away from each other. To relax the constraints on the network, the variance and distance terms are hinged: embeddings within a distance of δ v from their cluster centers are no longer attracted to it and cluster centers further apart than 2δ v are no longer repulsed. A small regularization pullforce that draws all clusters towards the origin keeps the activations bounded. These three terms can be written as follows, with C the number of clusters in ground truth, N c the number of elements in cluster c, x i an embedding, µ c the mean embedding of cluster c, • the L2 distance, and [x] + = max(0, x) denotes the hinge:</p><formula xml:id="formula_0">               L var = 1 C C c=1 1 Nc Nc i=1 [ µ c − x i − δ v ] 2 + L dist = 1 C(C−1) C c A =1 C c B =1, c A =c B [2δ d − µ c A − µ c B ] 2 + L reg = 1 C C c=1 µ c</formula><p>(1) The final loss can then be written as the sum of the above terms:</p><formula xml:id="formula_1">L inst = L var + L dist + L reg .</formula><p>When the loss has converged, all pixel embeddings are within a distance of δ v from their cluster center and all cluster centers are at least 2δ d apart. By setting δ d &gt; 2δ v , each embedding is closer to all embeddings of its own cluster than to any embedding of a different cluster. During inference we can then threshold with bandwidth b = δ v around any embedding to select all embeddings belonging to the same cluster. Since the loss on the test set will not be zero, we apply a GPU accelerated variant of the mean-shift algorithm <ref type="bibr" target="#b6">[7]</ref> to shift to a center pixel around which we threshold, avoiding outliers.</p><p>Depth estimation from a single image The standard loss used in most regression problems, like monocular depth estimation, is the L2 loss. It minimizes the difference between predicted D and ground truth D * depth:</p><formula xml:id="formula_2">L2(D, D * ) = 1 n i d 2 i , with d = D − D * .</formula><p>Recently, Eigen and Fergus <ref type="bibr" target="#b4">[5]</ref> added two more terms to the typical L2 loss for the depth estimation task; one for scale invariance (− 1 2n</p><formula xml:id="formula_3">2 ( i d i ) 2 )</formula><p>, and another for similarity in local structure</p><formula xml:id="formula_4">( 1 n i [(∇ x d i ) 2 + (∇ y d i ) 2 ]</formula><p>, with ∇ x and ∇ y denoting the horizontal and vertical image gradients). Instead, the depth estimation branch uses the reverse Huber loss (berHu) <ref type="bibr" target="#b16">[17]</ref>,</p><formula xml:id="formula_5">L dep = |d| |d| ≤ c d 2 +c 2 2c |d| &gt; c,<label>(2)</label></formula><p>that shows a good balance between penalizing high residuals that usually account for the mean depth and low residuals that explain the smaller depth details. We have experimentally found that this choice yields a better final error than using the L2 loss, even with the added terms. Notice that, the reverse Huber loss formulation above is continuous and first order differentiable at point c, which is set to c = 1 5 max i (d i ) as in <ref type="bibr" target="#b14">[15]</ref>. We use the SGM-calculated disparity depth maps of the Cityscapes dataset as ground truth for this task.</p><p>Training To train our multi-task network, the three losses described above are summed and equally weighted. Although different weights can also be used for each task we found that using equal weights already leads to good performance. We start from a pretrained encoder, trained for Cityscapes segmentation, and continue training the three tasks together. We train with a batch size of 10 at a resolution of 1024x512 and use Adam with a learning rate of 5e-4. Note that, we keep the parameters of the batch norm layers fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic and instance segmentation</head><p>We report Cityscapes semantic segmentation results in Tab. I and instance segmentation results on the car class in Tab. II. We notice that by jointly training our network for 3 different tasks, we match and even slightly outperform standard ENet for semantic scene segmentation. This justifies our hypothesis that training with multiple tasks at once can increase the performance of each individual task.</p><p>As expected, our result for instance segmentation lacks behind the other methods on the Cityscapes benchmark, since they are all optimized for accuracy and are far from real-time. They either rely on a big network or use highly accurate pre-generated semantic segmentation labels, which explains their significantly higher performance, compared to our result. Nevertheless, this work can serve as a baseline for methods that also focus on speed.</p><p>Depth In <ref type="figure" target="#fig_1">Fig. 2</ref> we plot for each car in the dataset its ground truth depth versus its predicted depth, which is calculated as an average over the predicted depth map masked out with the ground truth instance mask. The expected trend of nearby cars being predicted more accurately than faraway cars is clearly visible. Some of the extreme outliers  are caused by cars that are mostly occluded and thus only consist of a few pixels. These extreme cases can in principle be detected and filtered out using the instance mask. We encourage others to include similar plots in their work on car depth estimation, as it is more informative than a single summary number. Nevertheless, we follow <ref type="bibr" target="#b21">[22]</ref> and report three metrics in Tab. III: mean absolute error (MAE), root mean squared error (RMSE) and absolute relative error (ARD). Note that we calculate the depth of each car by average pooling the predicted depth map with the ground truth instance masks. This is unlike <ref type="bibr" target="#b21">[22]</ref>, who calculate the depth with the predicted instance masks, and report the metrics only over predicted cars that match with ground truth cars. This means that the metric they report does not take the depth estimation of undetected smaller or badly visible cars into account, leading to a number that is dependent on the instance segmentation performance. By reporting the numbers over the ground truth car masks we avoid this entanglement, but some caution is necessary when comparing the numbers. We provide the numbers at different maximum depths of 100m, 50m and 25m.</p><p>Multi-task network and speed In Tab. IV we provide a comparison between training the tasks separately (each running on an ENet of their own), versus training them together with a shared encoder as explained in the previous section. The benefits of training the three tasks together in a single multi-task network are clear: the speed almost doubles and the memory usage decreases drastically. This makes our approach suitable for real-time autonomous driving applications that require a low memory footprint. Important to note is that the accuracy of the individual tasks does not  decrease when training together: in fact we even notice a slight performance increase. This suggests that the shared encoder can effectively learn to exploit the common structure of the three related semantic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Overall, our system is fast but lags behind the state-of-art in terms of segmentation accuracy. Nevertheless, we believe that it can serve as a low-complexity baseline for other multitask approaches that focus on speed, and as a starting point for further exploration of the speed-accuracy trade-off in scene understanding. Furthermore, we observe that jointly training tasks can potentially lead to increased performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1. Overview of our method. From left to right: the input image is passed though the encoding step of an ENet-inspired architecture to create feature maps, which in turn are forwarded to different branches that perform decoding to arrive at the three outputs, i.e. semantic labels, instance labels and depth. A video is available at https://youtu.be/55ElRh-g_7o.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Left: ground truth and predicted depth for each car in the dataset. The expected trend of nearby cars being predicted more accurately than far-away cars is clearly visible. Right: a qualitative result (top: prediction, bottom: ground truth).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Another qualitative result on the validation set. Top: predicted and ground truth summary picture. Bottom: predicted semantic, instance and depth maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SEMANTIC</head><label>I</label><figDesc>SEGMENTATION RESULTS ON THE CITYSCAPES BENCHMARK.</figDesc><table><row><cell></cell><cell cols="3">IoU class IoU category</cell><cell></cell></row><row><cell>Segnet [2]</cell><cell cols="2">56.1</cell><cell>79.8</cell><cell></cell></row><row><cell>ENet [18]</cell><cell cols="2">58.3</cell><cell>80.4</cell><cell></cell></row><row><cell>SQ [21]</cell><cell cols="2">59.8</cell><cell>84.3</cell><cell></cell></row><row><cell>Ours</cell><cell cols="2">59.3</cell><cell>80.4</cell><cell></cell></row><row><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell></row><row><cell cols="5">INSTANCE SEGMENTATION RESULTS ON THE CITYSCAPES BENCHMARK.</cell></row><row><cell></cell><cell>AP</cell><cell>AP0.5</cell><cell cols="2">AP100m AP50m</cell></row><row><cell>InstanceCut [12]</cell><cell>23.7</cell><cell>44.8</cell><cell>38.9</cell><cell>42.5</cell></row><row><cell>PPLoss</cell><cell>24.4</cell><cell>43.2</cell><cell>40.0</cell><cell></cell></row><row><cell>Pixelwise DIN [1]</cell><cell>25.7</cell><cell>45.7</cell><cell>41.1</cell><cell>44.2</cell></row><row><cell>DWT [3]</cell><cell>31.5</cell><cell>48.5</cell><cell>50.8</cell><cell>53.5</cell></row><row><cell>Shape-aware [10]</cell><cell>35.7</cell><cell>54.7</cell><cell>58.2</cell><cell>63.1</cell></row><row><cell>SGN</cell><cell>39.4</cell><cell>59.7</cell><cell>60.1</cell><cell>63.2</cell></row><row><cell>Mask R-CNN [11]</cell><cell>46.9</cell><cell>68.3</cell><cell>65.5</cell><cell>67.4</cell></row><row><cell>Ours</cell><cell>21.0</cell><cell>38.6</cell><cell>34.8</cell><cell>38.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III SINGLE</head><label>III</label><figDesc>IMAGE DEPTH RESULTS ON THE CITYSCAPES VALIDATION SET. SOME CAUTION NECESSARY WHEN COMPARING WITH<ref type="bibr" target="#b21">[22]</ref> (SECT. IV).</figDesc><table><row><cell></cell><cell cols="2">MAE RMSE</cell><cell>ARD</cell></row><row><cell>Uhrig et al. [22] (val)</cell><cell>7.7m</cell><cell>24.8m</cell><cell>11.3%</cell></row><row><cell>Ours (val, &lt; 100m)</cell><cell>7.5m</cell><cell>11.9m</cell><cell>13.8%</cell></row><row><cell>Ours (val, &lt; 50m)</cell><cell>3.5m</cell><cell>5.6m</cell><cell>11.0%</cell></row><row><cell>Ours (val, &lt; 25m)</cell><cell>1.5m</cell><cell>2.5m</cell><cell>8.9%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV SEMANTIC</head><label>IV</label><figDesc>SEGMENTATION (IOUC ), INSTANCE SEGMENTATION (AP), DEPTH (MAE 100M ON THE VAL SET), MEMORY, AND SPEED (FORWARD PASS ON A PASCAL TITANX) AT TEST TIME WHEN TRAINED SEPARATELY VERSUS TOGETHER.</figDesc><table><row><cell></cell><cell cols="3">semantic instance depth</cell><cell>mem</cell><cell>speed</cell></row><row><cell>Trained separately</cell><cell>58.3%</cell><cell>0.20%</cell><cell>9.2m</cell><cell cols="2">2.6 GB 12 fps</cell></row><row><cell>Trained together</cell><cell>59.3%</cell><cell>0.21%</cell><cell>7.5m</cell><cell cols="2">1.2 GB 21 fps</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: The work was supported by Toyota, and was carried out at the TRACE Lab at KU Leuven (Toyota Research on Automated Cars in Europe -Leuven).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pixelwise instance segmentation with a dynamically instantiated network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep watershed transform for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08303</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic instance segmentation with a discriminative loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Bert De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno>arXiv:XXXX.XXXXX</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The estimation of the gradient of a density function, with applications in pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keinosuke</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Shape-aware instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Hayder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03129</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08272</idno>
		<title level="m">Instancecut: from edges to instances with multicut</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ubernet: Training auniversal&apos;convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02132</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A robust hybrid of lasso and ridge regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Art</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contemporary Mathematics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Enet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multinet: Real-time joint semantic reasoning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Zoellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07695</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Speeding up semantic segmentation for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Treml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Arjona-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Durgesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Schuberth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>GCPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
