[{"id": 1000, "pred": "the focus of this work is on building pos taggers for foreign languages, assuming that we have an english pos tagger and some parallel text between the two languages. the focus of this work is on building pos taggers for foreign languages, assuming that we have an english pos tagger and some parallel text between the two languages. algorithm bilingual pos induction require : parallel english and foreign language data de and df, unlabeled foreign training data f; english tagger. ensure: f, a set of parameters learned using a constrained unsupervised model. this paper proposes a novel approach to the problem of unsupervised learning of the foreign language. in this paper, we use a supervised english tagger to label the english side of the bitext.7 this paper proposes a novel approach to the problem of unsupervised learning of the foreign language. in this paper, we use a supervised english tagger to label the english side of the bitext.7 this paper proposes a novel approach to the problem of unsupervised learning of the foreign language. in this paper, we use a supervised english tagger to label the english side of the bitext.7 this paper presents a novel approach to unsupervised learning of the pos tagset in languages that have no labeled resources. the model is a supervised hmm trained on the universal pos tagset of petrov et al. ( 2011 ) . \n the model uses both stages of label propagation before extracting the constraint features. as a result, we are able to extract the constraint feature for all foreign word types and furthermore expect the projected tag distributions to be smoother and more stable. our projection baseline is able to benefit from the bilingual information and greatly improves upon the no lp model by on an average. this paper presents a graph - based approach to pos taggers for languages that do not have any labeled resources, but have translations into a resource-rich language. we show the efficacy of graph-based label propagation for projecting part of speech information across languages. our results suggest that it is possible to learn accurate pos taggers for languages which do not have any annotated data, but have translations into a resource-rich language. our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised pos tagging models."}, {"id": 1001, "pred": "a recurrent neural network ( rnn) is trained to predict the next element of a sequence given the previous elements. the rnn is trained to predict the next element of a sequence given the previous elements. the new representation is sensitive to ordering and mitigates the disadvantage of using the standard fisher vector representation. it is applied to two different tasks: video action recognition and image annotation by sentences. in image annotation and image search tasks, the rnn-fv method is used for the representation of sentences and achieves state-of-the-art results on the flickr8k dataset and competitive results on other benchmarks. the rnn can be seen as a generative model which gives likelihood to the sequence u : pr ( u) n1 i0 pr ( wi1w0, ... , wi) n1 i0 piwi1 ( wi1w0, ... , wi) n1 i0 piwi1 ( wi1w0, ... , wi) in this paper , we present a new approach to action recognition. the method is based on a convolutional neural network ( c3d) that is trained to predict the next element in a video representation sequence, given the previous elements. the network is trained to predict the next element in a video representation sequence, given the previous elements. the network is trained to predict the next element in a video representation sequence, given the previous elements. the network is trained to predict the next element in a video representation sequence, given the previous elements. this paper introduces a novel fv representation for sentences that is derived from rnns. this paper introduces a novel fv representation for sentences that is derived from rnns. this paper introduces a novel fv representation for sentences that is derived from rnns. this paper introduces a novel fv representation for sentences that is derived from rnns. this paper introduces a novel fv representation for sentences that is derived from rnns. this paper introduces a novel fv representation for sentences that is derived from rnns. this paper introduces a novel fv representation for sentences that is derived from rnns."}, {"id": 1002, "pred": "the rate of generaating publications of scientific papers is increasing and it is almost impossible for re- searchers to keep up with relevant research. in general, there are two com- mon approaches to summarizing scientific papers : citations- based, and content- based, based on the paper itself. several methods were suggested to reduce these efforts, still they are not scalable as they require human annotations. the authors contributed equally. in this paper , we propose a novel method to automatically generate a summary of a paper. the model is trained on a subset of top- ranked sentences from the paper. the model is trained on a subset of top- ranked sentences from the paper. the model is trained on a subset of top- ranked sentences from the paper. the model is trained on a subset of top- ranked sentences from the paper. the model is trained on a subset of top- ranked sentences from the paper. the model is trained on a subset of top- ranked sentences from the paper. the task is to automatically break a complex sentence into several simple ones while preserving the meaning or the semantics and this can be a useful component in nlp pipelines for example the split and rephrase task was introduced in the last emnlp by narayan gardent and shimarina where they introduced a dataset an evaluation method and baseline models for this task."}, {"id": 1003, "pred": "this paper proposes an ensemble approach that combines both a linear model based on the pre-trained word vectors and a neural network based system. in order to utilize the high quality but small size datasets, we propose an ensemble approach that combines both a linear model based on the pre-trained word vectors, and a neural network based system. in addition, we propose a new method for realizing a sentence level representation from the single words vectors. this work studied the use of pretrained word vectors for emotion detection. we presented class, a novel method for representing a document as a dense vector based on the importance of the document s terms in respect to emotion classi cation. also, the class method we 2url/ 3url/ 4url 5url/ proposed outperformed the other embedded document representation methods. also, the class method we 2url/ 3url/ 4url 5url/ proposed outperformed the other embedded document representation methods."}, {"id": 1004, "pred": "the purpose of this paper is to automatically detect emotions expressed in customer support dialogues using the twitter platform. the paper shows that it is possible to automatically detect emotions expressed in customer support dialogues and to predict the emotional technique that is likely to be used by a human agent in a given situation. this analysis reflects our ultimate goal: to enable a computer system to discern the emotions expressed by human customers , and to develop computerized tools that mimic the emotional technique used by a human customer service agent in a particular situation. a dialogue is a sequence of turns between a specific customer and an agent, where the customer initiates the first turn. consecutive posts of the same party ( customer or agent) uninterrupted by the other party, are considered as a single turn ( even if there are several tweets). given the nature of customer support services, we expect an even number of turns in the dialogue. we extracted this data from december until june specifically, for each customer that posted a tweet to the customer support accounts, we searched for the previous, if any , turn to which it replied. given this method we traced back previous turns and reconstructed entire dialogues. the goal of this paper is to use a kth-order hidden markov model ( svm) to classify dialogue acts. the svm classifier generates models that are isomorphic to a kth-order hidden markov model. this process generates models that are isomorphic to a kth-order hidden markov model. thus, dependency in past classification results is captured internally by modeling transition probabilities between emotion states. the temporal response time values were translated to low/medium/high categorical values according to their relation to the 33-th and 66th percentiles. in this work we studied emotions being expressed in customer service dialogues in the social media. specifically, we described two classification tasks, one for detecting customer emotions and the other for predicting the emotional technique used by support service agent. we proposed two different models ( svm dialogue and svm-hmm dialogue models) for these tasks. we studied the impact of dialogue features and dialogue history on the quality of the classification and showed improvement in performance for both models and both classification tasks. we also showed the robustness of our models across different data sources."}, {"id": 1005, "pred": "few-shot meta-learning ( learning-to-learn) is a method for approaching few-shot classification ( fsc) where the inputs to both training and test phases are not images, but instead a set of few-shot tasks, ti, each k-shot / n-way task containing a small amount k ( usually) of labeled support images and some amount of unlabeled query images for each of the n categories of the task. the goal of meta-learning is to find a base model that is easily adapted to the specific task at hand, so that it will generalize well to tasks built from novel unseen categories and fulfill differentiable architecture search (nas) is a method for progressively searching for a larger architecture. these methods are mostly focused, and perform well, on datasets such as cifarnet and imagenet. so far, little attention has been given to their adaptation to few-shot tasks. auto-meta used pnas based search for few-shot classification, but with a focus on searching for a small architecture ( resulting in a relatively low performance w.r.t. to current state-of-the-art). asap achieves higher accuracy with a shorter training time. in this paper, we propose a new method to train a large neural network to perform architecture search. the method is based on iteratively optimizing the convolutional layers of the neural network with a second approximation of the model after convergence of the model. in order to see the effect of optimizing using iterative intermittent optimization for w and using different folds of the training set, we perform training with and without it. in order to see the effect of optimizing using iterative intermittent optimization for w and using different folds of the training set, we perform training with and without it. we propose metadapt, a few-shot learning approach that enables meta-learned network architecture that is adaptive to novel few-shot tasks. the proposed approach effectively applies tools from the neural architecture search ( nas) literature, extended with the concept of metadapt controllers , in order to learn adaptive architectures. we demonstrate that the proposed approach successfully improves state-of-the-art results on two popular few-shot benchmarks, miniimagenet and fc100, and carefully ablate the different optimization steps and design choices of the proposed approach."}, {"id": 1006, "pred": "in this paper we study the problem of detecting egregious conversations in conversational agent ( agent) setting. the goal is to automatically detect the worst conversations (in our experience, typically under of the total) using different features. specifically, we consider customer inputs throughout a whole conversation, and detect cues such as rephrasing, the presence of heightened emotions, and queries about whether the utterance is a human or requests to speak to an actual human. in addition, we analyze the responses, looking for repetitions ( e.g. from loops that might be due to flow problems), and the presence of not trained responses. the goal of this work is to reliably detect egregious between a human and a virtual agent. we treat this as a binary classification task, where the target classes are egregious and nonegregious. some of the features examined here could likely be used to detect egregious conversations as they were unfolding in real time. to perform egregious conversation detection, features from both customer inputs and agent responses are extracted, together with features related to the combination of specific inputs and responses. in addition, some of these features are contextual, meaning that they are dependent on where they appear. the goal of this paper is to show how it is possible to detect egregious conversations using a combination of customer utterances. in this context, future work includes collecting more data and using neural approaches (e.g., rnn, cnn) for analysis, validating our models on a range of domains beyond the two explored here. in this context, future work includes collecting more data and using neural approaches (e.g., rnn, cnn) for analysis, validating our models on a range of domains beyond the two explored here."}, {"id": 1007, "pred": "convolutional neural networks ( cnns) have been shown to achieve strong performance on text classification tasks. these techniques unfortunately do not trivially apply to discrete sequences, as they assume a continuous input space used to represent images. intuitions about how cnns work on an abstract level also may not carry over from image inputs to text. in this work, we examine and attempt to understand how cnns process text, and then use this information for the more practical goals of improving model-level and prediction-level interpretability. this paper examines the question of how to use a max-pooling layer to separate informative from uninformative ngrams. the paper proposes to use a threshold for each filter in the max-pooling layer to determine whether a filter is informative or uninformative. the paper proposes to use a threshold for each filter in the max-pooling layer to determine whether a filter is informative or uninformative. the paper proposes to use a threshold for each filter in the max-pooling layer to determine whether a filter is informative or uninformative. this paper investigates the relationship between the slot activations of a filter and the activations of the ngrams it classifies. the paper also investigates the relationship between the slot activations of a filter and the activations of the ngrams it classifies. this paper also investigates the relationship between the slot activations of a filter and the activations of the ngrams it classifies. this paper also investigates the relationship between the slot activations of a filter and the activations of the ngrams it classifies. this paper also investigates the relationship between the slot activations of a filter and the activations of the ngrams it classifies."}, {"id": 1008, "pred": "a summary generated by editnet may include sentences that were either extracted, abstracted or of both types. moreover, per considered sentence, editnet may decide not to take either of these decisions and completely reject the sentence. as another example, based on the interaction between both sentence versions with either of the local or global contexts ( and possibly among the last two), the network may learn that both sentence versions may only add superfluous or redundant information to the summary, and therefore, decide to reject both. editnet is a novel alternative summarization approach that instead of solely applying extraction or abstraction, implements both together."}, {"id": 1009, "pred": "this paper presents dimsim, a phonetic similarity algorithm to generate and rank phonetically similar chinese words. incorrect homophone and synophones, whether used in error or in text messages, pose challenges for a wide range of nlp tasks, such as named entity identification, text normalization and spelling correction. this paper presents dimsim, a phonetic similarity algorithm to generate and rank phonetically similar chinese words. incorrect homophone and synophones, whether used in error or in text messages, pose challenges for a wide range of nlp tasks, such as named entity identification, text normalization and spelling correction. in this paper, we propose a new method for the annotation of the phonetic similarity of chinese pinyins (i.e., the phonetic similarity of the pronunciation of two characters, ci and c0i) in the context of a word pair (i.e., the phonetic similarity of two characters, ci and c0i) in the context of a word pair (i.e., the phonetic similarity of two characters, ci and c0i) in the context of a word pair (i.e., the phonetic similarity of two characters, ci and c0i) in the context of a word pair the aim of this paper is to improve the accuracy of the phonetic encodings of the chinese pinyins by using a learning model that is based on the euclidean distance of the chinese pinyins. the learning model is based on the euclidean distance of the chinese pinyins and the difference between the tones of two pinyins as the raw measure of distance, ranging in value from to (e.g., st ( xue2, xue4 ) 2. dimsim is a learning model that is based on the euclidean distance of the chinese pinyins. dimsim is a phonetic similarity algorithm that generates phonetically similar candidate words based on learned encodings that capture the pronunciation characteristics of pinyin initial, final, and tone components. dimsim learns separate high dimensional encodings for initials and finals, and uses them to calculate and rank the distances between pinyin representations of chinese word pairs. the original motivation for this work was to improve the quality of downstream nlp tasks, such as named entity identification, text normalization and spelling correction. these tasks all share a dependency on reliable phonetic similarity as an intermediate step, especially for languages such as chinese where incorrect homophones and synophones abound."}, {"id": 1010, "pred": "given an input question and a set of candidate entities retrieved by an entity linker based on the question, our proposed relation detection model plays a key role in the kbqa process: ( 1 ) re-ranking the entity candidates according to whether they connect to high confident relations detected from the raw question text by the relation detection model. this step is important to deal with the ambiguities normally present in entity linking results. ( 2) finding the core relation ( chains) for each topic entity2 selection from a much smaller candidate entity set after re-ranking. ( 2) finding the core relation ( chains) for each topic entity2 selection this paper proposes a hierarchical matching approach for kb relation detection : for a candidate relation, our approach matches the input question to both word-level and relation-level representations to get the final ranking score. this paper proposes a hierarchical matching approach for kb relation detection : for a candidate relation, our approach matches the input question to both word-level and relation-level representations to get the final ranking score. this paper proposes a hierarchical matching approach for kb relation detection : for a candidate relation, our approach matches the input question to both word-level and relation-level representations to get the final ranking score. for the kbqa end task, we use the question text as input for a relation detector to score all relations in the kb with connections to at least one of the entity candidates in elk ( q). we call this step relation detection on entity set since it does not work on a single topic entity as the usual settings. for each question q, after generating a score srel ( r; q) for each relation using hr-bilstm, we use the top l best scoring relations ( rlq) to re-rank the original entity candidates. kb relation detection is a key step in kbqa and is significantly different from general relation extraction tasks. we propose a novel kb relation detection model, hr-bilstm, that performs hierarchical matching between questions and kb relations. our model outperforms the previous methods on kb relation detection tasks and allows our kbqa system to achieve state-of-the-art results. association for computational linguistics , beijing , china, pages freebase qa : information extraction or semantic parsing? xuchen yao and benjamin van durme. association for computational linguistics , beijing , china, pages freebase qa: information extraction or semantic parsing? xuchen yao and benjamin van durme."}, {"id": 1011, "pred": "in this work we propose a feature agnostic approach for dictionary extraction based on neural language models, such as word2 to prevent semantic drift during the dictionary expansion, we effectively include humanin-the-loop ( huml) for a surveillance application ( e.g., drug side effects mentioned on twitter) it reduces how frequently a human needs to tune up the lexicon to make sure it is catching all relevant entity instances. it reduces how frequently a human needs to tune up the lexicon to make sure it is catching all relevant entity instances."}, {"id": 1012, "pred": "this paper presents an interpretable mechanism which additionally alleviates any issues arising due to a limited number of region proposals. our approach is based on a number of image concepts, such as semantic segmentations and priors for any number of objects of interest. our approach is based on a number of image concepts, such as semantic segmentations and priors for any number of objects of interest. we formulate our language grounding approach as an energy minimization over a large number of bounding boxes. importantly, by leveraging an efficient branch- and bound techniques, we are able to find the global minimizer for a given energy function very effectively. in this paper, we present an efficient branch and bound based inference algorithm to find the bounding box with lowest energy. the algorithm proceeds by iteratively decomposing a product space y into two subspaces y1 and y2. for each subspace, the algorithm computes a lower bound e ( x, yj, w) for the energy of all possible bounding boxes within the respective subspace. the algorithm proceeds by choosing the subspace with lowest lower-bound until this subspace consists of a single element, i.e., until y in this paper, we present a novel approach for learning the relationship between word vectors and image concepts. in order to learn the relationship between word vectors and image concepts, we use a structured support vector machine ( svm) based surrogate loss minimization (svm) approach. in order to learn the relationship between word vectors and image concepts, we use a structured support vector machine (svm) based surrogate loss minimization (svm) approach. in order to learn the relationship between word vectors and image concepts, we use a structured support vector machine (svm) based surrogate loss minimization (svm) approach."}, {"id": 1013, "pred": "the use of features from deep convolutional neural networks ( dcnns) pretrained on imagenet has led to important advances in computer vision. in this work we demonstrate that, by using pfs to perform moment matching, one can overcome some of the difficulties found in current moment matching approaches. more specifically, we propose a simple but effective moment matching method that : ( 1) breaks away from the problematic min/max game completely; ( 2) does not use online learning of kernel functions; ( 3) is very efficient with regard to both number of used moments and required minibatch size. gfmn is related to the recent body of work on mmd and moment matching based generative models 22, 8, 21, 3, we highlight the main differences between mmd-gans and gfmn in terms of requirements on the kernel for mmd-gans and on the feature map ( extractor) for gfmn, that ensure convergence of the generator to the data distribution. we know from transfer learning that features from imagenet pretrained vgg/resnet can express any functions for a downstream task by finding a linear weight in their span. gfmn is an unsupervised learning approach that uses a dcganlike architecture in the generator. the generator is trained with a loss function that only performs feature matching. the generator is agnostic to the labels and there is no feedback in the form of a log likelihood from the labeled data. the main benefit of adam moving average ( ama) is the promotion of stable training when using small minibatches. the ability to train with small minibatches is essential due to gfmn s need for large number of features from dcnns, which becomes a challenge in terms of gpu memory usage. this work presents important theoretical and practical contributions that shed light on the effectiveness of perceptual features for training implicit generative models through moment matching. the objective of the experiments presented in this section is to evaluate if wgan-gp can benefit from dcnn classifiers pretrained on imagenet. the objective of the experiments presented in this section is to evaluate if wgan-gp can benefit from dcnn classifiers pretrained on imagenet. the objective of the experiments presented in this section is to evaluate if wgan-gp can benefit from dcnn classifiers pretrained on imagenet. the objective of the experiments presented in this section is to evaluate if wgan-gp can benefit from dcnn classifiers pretrained on imagenet."}, {"id": 1014, "pred": "e- recommender systems aim to present items with high utility to the consumers: the item is desired as it is, and time utility: the item is desired at the given point in time. a key assumption made by matrix factorization- and completion-based collaborative filtering algorithms is that the underlying rating matrix is of low-rank since only a few factors typically contribute to an individual s form utility however, a user s form utility is not only driven by form utility, but is the combined effect of both form utility and time utility. in this work, we use a hinge loss model to capture temporal dynamics of users interests. we use a hinge loss model to capture temporal dynamics of users interests, which are generally believed to be dictated by a small number of latent factors by combining a small number of latent factors by combining a small number of latent factors by combining a low-rank property together. in this way, we can recover a rank tensor of size takes the state-of-the-art tensor factorization algorithm. in this way, we can recover a rank tensor of size takes the state-of-the-art tensor factorization algorithm. this paper proposes a demand-aware recommendation algorithm for one -sided sampling (onesided sampling, or daross for short) , which is based on a proximal gradient descent algorithm for nuclear norm minimization. the proposed algorithm can be applied to solve the problem within o ( nk2t mk2t p0kt) time, where t is the number of iterations. since k and t are usually very small , the time complexity to solve problem ( 4) is dominated by the term p0, which is a significant improvement over the naive approach with at least o ( mnl) complexity. in this paper, we examine the problem of demand-aware recommendation in settings when inter-purchase duration within item categories affects users s purchase intention in combination with intrinsic properties of the items themselves. we formulate it as a tensor nuclear norm minimization problem that seeks to jointly learn the form utility tensor and a vector of inter-purchase durations, and propose a scalable optimization algorithm with a tractable time complexity. on two real-world datasets, tmall and amazon review, we show that our algorithm outperforms six state-of-the-art recommendation algorithms on the tasks of category, item, and purchase time predictions."}, {"id": 1015, "pred": "automated agents benefit from adapting their personality according to the task at hand ( reeves and nass, 1996; tapus and mataric, 2008; tapus and mataric, 2008) or to the customer ( herzig et al., 2016) this paper presents a personality-based response generation model that is able to generate responses that are adapted to the personality traits of the customer. the model learns a personality vector for each agent in the training data. the model is able to generate responses that are adapted to the personality traits of the customer. the model is able to generate responses that are adapted to the personality traits of the big five model that are important to customer service: agreeableness and conscientiousness ( blignaut et al., 2014; sackett et al., 2014 )"}, {"id": 1016, "pred": "the event-based disparity method is implemented using a stereo pair of davis sensors ( a version of dvs) and nine truenorth ns1e boards however, the method is applicable to other spiking neuromorphic architectures, and it is also tested offline on larger models using a truenorth simulator. a live-feed version of the system running on nine truenorth chips is shown to calculate disparity maps per second, and the ability to increase this up to 2,000 disparities per second ( subject to certain trade-offs) is demonstrated, for use with high speed event cameras, such as dvs. truenorth is a hierarchical, compositional , object - oriented language the proposed local event-based stereo correspondence algorithm is implemented end-to-end as a neuromorphic event-based algorithm. this consists of systems of equations defining the behavior of truenorth neurons, encased in modules called corelets , and the subsequent composition of the inputs and outputs of these modules. for a set of qt-coded inputs, the wta system is realized by a cascade of feed-forward pruning networks where each of the pruning networks process only 3-bits of the qt codes and prune the inputs not equal to the bitwise maximum of corresonding 3-bits thermometer codes from all inputs. now starting from the most significant bits, all the inputs smaller than the maximum will be pruned at different stages and only the winner (s) will survive at the output of the last cascade network. we introduce an advanced neuromorphic 3d vision system uniting a pair of davis cameras with multiple truenorth processors, to create an end-to-end, scalable , event-based stereo system. by using a spiking neural network, with low precision weights, we have shown that the system is capable of injecting event streams and ejecting disparity maps at high throughputs, low latencies, and low power. the system is highly parameterized and can operate with other event based sensors such as atis or dvs table compares our approach with the literature on event based disparity."}, {"id": 1017, "pred": "in this paper, we propose a characterization of i-markov equivalence between two causal graphs with latent variables for a given intervention set i that is based on a generalization of do-calculus rules to arbitrary subsets of interventions. a soft intervention affects the mechanism that generates the variable, while keeping the causal connections intact. we call a set of interventional distributions i-markov to a graph, if these distributions respect the causal calculus rules relative to that graph. the calculus consists of a set of inference rules that allows one to create a map between distributions generated by a causal graph when certain graphical conditions hold in the graph. a pag, which represents a markov equivalence class of a mag, is learnable from the independence model over the observed variables, and the fci algorithm is a standard sound and complete method to learn such an object related work: learning causal graphs from a combination of observational and interventional data has been studied in the literature 3, 11, 7, 20, 8, 12, for causally sufficient systems, the notion and characterization of interventional markov equivalence has been introduced in two causal graphs are said to be i-markov equivalent if they license the same set of distribution tuples. this notion is formalized in the following definition. given two causal graphsd1 ( v l , e1) andd2 ( vl2, e2) , and an intervention set i 2v, d1 andd2 are called i-markov equivalent if pi ( d1 , v) pi ( d2, v) in order to characterize causal graphs that are i-markov equivalent, we draw some insight from the markov equivalence of causal graphs with latents. in this paper, we develop an algorithm to learn the augmented graph from a combination of observational and interventional data, which consequently recovers the causal graph. however, similar to the observational case, it is typically impossible to completely determine the causal graph from the available measured data, especially when latents are present. then, the objective is to learn a class of augmented mags consistent with data. in this paper , we develop a new algorithm to learn an interventional equivalence class from data , which includes new orientation rules . in this paper, we develop a new algorithm to learn an interventional equivalence class from data . in this paper, we develop a new algorithm to learn an interventional equivalence class from data . in this paper, we develop a new algorithm to learn an interventional equivalence class from data . in this paper, we develop a new algorithm to learn an interventional equivalence class from data . in this paper, we develop a new algorithm to learn an interventional equivalence class from data ."}, {"id": 1018, "pred": "program induction has been used for decades, with rule induction and probabilistic program induction techniques and by constructing algorithms utilizing formal theoremproving. on even complex programs of length 25, cipitr outperformed nsm by a factor of at least higher f1 than both. on one of the hardest class of programs of around steps ( i.e., correct counting set sizes), cipitr outperformed nsm by a factor of at least higher f1 than both. cipitr consists of three components: the preprocessor takes the input query and the kb and performs the task of entity, relation, and type linking which acts as input to the program induction. it also prepopulates the variable memory matrices with any entity, relation, type, or integer variable directly extracted from the query. cipitr uses a beam search to obtain multiple candidate programs to provide feedback to the model from a single training instance. in this paper, we compare the performance of cipitr against other models on kbsqa. in this paper, we compare the performance of cipitr against other models on kbsqa. in this paper, we compare the performance of cipitr against other models on kbsqa. in this paper, we compare the performance of cipitr against other models on kbsqa. in this paper, we compare the performance of cipitr against other models on kbsqa. in this paper, we compare the performance of cipitr against other models on kbsqa. this paper proposes a new model called cipitr for program induction and program decomposition. the model is based on the nsm (numerical search model) and the abstract high level program decomposition (arpde) approach. the model is able to generate at least meaningful programs having or without repeating lines of code. moreover, the model is able to generate at least meaningful programs having the desired answer or constrain the search. moreover, the model is able to generate at least meaningful programs having the desired answer or constrain the search. on the other hand, the model is able to generate semantically correct programs."}, {"id": 1019, "pred": "in this work, we propose dual-ces an extended ces summarizer similar to ces , dual-ces is an unsupervised query-focused multi-document extractive summarizer. to this end, like ces, dual-ces utilizes the cross entropy method for selecting a subset of sentences extracted from input documents, whose combination is predicted to produce a good summary. yet, differently from ces, dual-ces does not attempt to address both saliency and focus goals in a single optimization step. instead, dual-ces implements a novel two-step dual-cascade optimization approach, which utilizes two sequential ces- this paper proposes a novel twostep dual-ces optimization approach, which utilizes two ces-like invocations. both invocations consider the same sentence powerset solution space. yet, each invocation utilizes a bit different set of summary predictors depending on whether the summarizer s goal should lay towards higher summary saliency or focus. this summary is then treated as a pseudoeffective reference summary from which saliency-based pseudo-feedback is distilled. such pseudo-feedback is then utilized in the second step of the cascade for setting an additional auxiliary saliency-driven goal. in this paper, we evaluate the dualcascade learning approach for summarization of a topic and a set of english documents. given a topic statement, which is expressed by one or more questions, and a set of english documents, the main task is to produce a 250-word ( i.e., lmax 250) topic summary. the number of documents to be summarized per topic is 32, and in the duc 2005, and benchmarks, respectively. each document was pre-segmented ( by nist) into sentences."}, {"id": 1020, "pred": "in this paper we show that we can get a considerable quality improvement by modifying a tts system that produced the world vocoder parameters to predict parameters for lpcnet as in the previous work 6, we conduct multiple adaptation experiments, applied on multiple vctk voices and show that the new system has much better quality and similarity to the target voices but can still run much faster than real-time in a single-cpu mode. in this paper we present a new tts system that addresses the challenging goals of producing high quality speech while operating at faster than real-time rate without an expensive support. the system is built around three nn models for generating the prosody , acoustic features and the final speech signal. we tested this system using two proprietary tts voice datasets and demonstrated that our system produces high quality speech that is comparable to larger and much slower tacotron2 wavenet systems."}, {"id": 1021, "pred": "in this paper we introduce the sobolev independence criterion ( sic) , a form of gradient regularized integral probability metric ( ipm) between the joint distribution and the product of marginalspy. sic relies on the statistics of the gradient of a witness function, or critic, for both ( 1) defining the ipm constraint and ( 2) finding the features that discriminate between the joint and the marginalspy. intuitively, the magnitude of the average gradient with respect to a feature gives an importance score for each feature. hence, promoting its sparsity is a natural constraint for feature selection. sic is a form of gradient regularized maximum mean discrepancy previous mmd work comparing joint and product of marginals did not use the concept of nonlinear sparsity. sic is a form of gradient regularized maximum mean discrepancy previous mmd work comparing joint and product of marginals did not use the concept of nonlinear sparsity. for a feature selection problem given the ground- truth set of features s, and a feature selection method such as neural sic, our goal is to maximize the tpr ( true positive rate) or the power, and to keep the false discovery rate ( fdr) under control. in this paper we introduce the sobolev independence criterion ( sic) that gives rise to feature importance which can be used for feature selection and interpretable decision making. we laid down the theoretical foundations of sic and showed how it can be used in conjunction with the holdout randomization test and the holdout dependency randomization test to control the fdr, enabling reliable discoveries. we demonstrated the merits of sic for feature selection in extensive synthetic and real-world experiments with controlled fdr. algorithms and empirical convex sic from samples. in this paper, we show how to use a neural network to generate a multivariate gaussian regression model. the neural network is trained to generate a multivariate gaussian regression model. the output of the neural network is a nbins-dimensional softmax over bins tessellating the range of the distribution of the input feature, such that the bins are uniform quantiles of the inverse cdf of the distribution of the input feature. the neural network is trained using adam optimizer with 0.5, 0.999, weightdecay1e-4 learning rate 1e-3 and 0.1, and perform training iterations/updates. in this paper we propose a new regression model based on a three - layer relu dropout network ( relu dropout network) and a linear regression network ( relu dropout network) . \n the proposed model is a generalization of the well - known leakyrelu model . \n the model is trained with the same adam settings as above for updates with a batchsize of we did not perform any hyperparameter tuning or model selection on heldout mse performance. in this paper \n we propose a new regression model based on a three - layer relu dropout network ( relu dropout network) and a linear regression network ( relu"}]