<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Software Engineering for Machine Learning: A Case Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
							<email>samershi@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Begel</surname></persName>
							<email>andrew.begel@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bird</surname></persName>
							<email>cbird@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Deline</surname></persName>
							<email>rdeline@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Gall</surname></persName>
							<email>gall@ifi.uzh.ch</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Zurich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ece</forename><surname>Kamar</surname></persName>
							<email>eckamar@microsoft.com</email>
							<affiliation key="aff5">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachiappan</forename><surname>Nagappan</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Besmira</forename><surname>Nushi</surname></persName>
							<email>besmira.nushi@microsoft.com</email>
							<affiliation key="aff7">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
							<affiliation key="aff8">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Software Engineering for Machine Learning: A Case Study</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>AI</term>
					<term>Software engineering</term>
					<term>process</term>
					<term>data</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components-models may be &quot;entangled&quot; in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Personal computing. The Internet. The Web. Mobile computing. Cloud computing. Nary a decade goes by without a disruptive shift in the dominant application domain of the software industry. Each shift brings with it new software engineering goals that spur software organizations to evolve their development practices in order to address the novel aspects of the domain.</p><p>The latest trend to hit the software industry is around integrating artificial intelligence (AI) capabilities based on advances in machine learning. AI broadly includes technologies for reasoning, problem solving, planning, and learning, among others. Machine learning refers to statistical modeling techniques that have powered recent excitement in the software and services marketplace. Microsoft product teams have used machine learning to create application suites such as Bing Search or the Cortana virtual assistant, as well as platforms such as Microsoft Translator for real-time translation of text, voice, and video, Cognitive Services for vision, speech, and language understanding for building interactive, conversational agents, and the Azure AI platform to enable customers to build their own machine learning applications <ref type="bibr" target="#b0">[1]</ref>. To create these software products, Microsoft has leveraged its preexisting capabilities in AI and developed new areas of expertise across the company.</p><p>In this paper, we describe a study in which we learned how various Microsoft software teams build software applications with customer-focused AI features. For that, Microsoft has integrated existing Agile software engineering processes with AI-specific workflows informed by prior experiences in developing early AI and data science applications. In our study, we asked Microsoft employees about how they worked through the growing challenges of daily software development specific to AI, as well as the larger, more essential issues inherent in the development of large-scale AI infrastructure and applications. With teams across the company having differing amounts of work experience in AI, we observed that many issues reported by newer teams dramatically drop in importance as the teams mature, while some remain as essential to the practice of largescale AI. We have made a first attempt to create a process maturity metric to help teams identify how far they have come on their journeys to building AI applications.</p><p>As a key finding of our analyses, we discovered three fundamental differences to building applications and platforms for training and fielding machine-learning models than we have seen in prior application domains. First, machine learning is all about data. The amount of effort and rigor it takes to discover, source, manage, and version data is inherently more complex and different than doing the same with software code. Second, building for customizability and extensibility of models require teams to not only have software engineering skills but almost <ref type="figure">Fig. 1</ref>. The nine stages of the machine learning workflow. Some stages are data-oriented (e.g., collection, cleaning, and labeling) and others are model-oriented (e.g., model requirements, feature engineering, training, evaluation, deployment, and monitoring). There are many feedback loops in the workflow. The larger feedback arrows denote that model evaluation and monitoring may loop back to any of the previous stages. The smaller feedback arrow illustrates that model training may loop back to feature engineering (e.g., in representation learning).</p><p>always require deep enough knowledge of machine learning to build, evaluate, and tune models from scratch. Third, it can be more difficult to maintain strict module boundaries between machine learning components than for software engineering modules. Machine learning models can be "entangled" in complex ways that cause them to affect one another during training and tuning, even if the software teams building them intended for them to remain isolated from one another.</p><p>The lessons we identified via studies of a variety of teams at Microsoft who have adapted their software engineering processes and practices to integrate machine learning can help other software organizations embarking on their own paths towards building AI applications and platforms.</p><p>In this paper, we offer the following contributions. 1) A description of how several Microsoft software engineering teams work cast into a nine-stage workflow for integrating machine learning into application and platform development. 2) A set of best practices for building applications and platforms relying on machine learning. 3) A custom machine-learning process maturity model for assessing the progress of software teams towards excellence in building AI applications. 4) A discussion of three fundamental differences in how software engineering applies to machine-learning-centric components vs. previous application domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Software Engineering Processes</head><p>The changing application domain trends in the software industry have influenced the evolution of the software processes practiced by teams at Microsoft. For at least a decade and a half, many teams have used feedback-intense Agile methods to develop their software <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> because they needed to be responsive at addressing changing customer needs through faster development cycles. Agile methods have been helpful at supporting further adaptation, for example, the most recent shift to re-organize numerous team's practices around DevOps <ref type="bibr" target="#b4">[5]</ref>, which better matched the needs of building and supporting cloud computing applications and platforms. <ref type="bibr" target="#b0">1</ref> The change to DevOps occurred fairly quickly because these teams were able to leverage prior capabilities <ref type="bibr" target="#b0">1</ref> https://docs.microsoft.com/en-us/azure/devops/learn/devops-at-microsoft/ in continuous integration and diagnostic-gathering, making it simpler to implement continuous delivery.</p><p>Process changes not only alter the day-to-day development practices of a team, but also influence the roles that people play. 15 years ago, many teams at Microsoft relied heavily on development triads consisting of a program manager (requirements gathering and scheduling), a developer (programming), and a tester (testing) <ref type="bibr" target="#b5">[6]</ref>. These teams' adoption of DevOps combined the roles of developer and tester and integrated the roles of IT, operations, and diagnostics into the mainline software team.</p><p>In recent years, teams have increased their abilities to analyze diagnostics-based customer application behavior, prioritize bugs, estimate failure rates, and understand performance regressions through the addition of data scientists <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, who helped pioneer the integration of statistical and machine learning workflows into software development processes. Some software teams employ polymath data scientists, who "do it all," but as data science needs to scale up, their roles specialize into domain experts who deeply understand the business problems, modelers who develop predictive models, and platform builders who create the cloud-based infrastructure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. ML Workflow</head><p>One commonly used machine learning workflow at Microsoft has been depicted in various forms across industry and research <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. It has commonalities with prior workflows defined in the context of data science and data mining, such as TDSP <ref type="bibr" target="#b11">[12]</ref>, KDD <ref type="bibr" target="#b12">[13]</ref>, and CRISP-DM <ref type="bibr" target="#b13">[14]</ref>. Despite the minor differences, these representations have in common the data-centered essence of the process and the multiple feedback loops among the different stages. <ref type="figure">Figure 1</ref> shows a simplified view of the workflow consisting of nine stages.</p><p>In the model requirements stage, designers decide which features are feasible to implement with machine learning and which can be useful for a given existing product or for a new one. Most importantly, in this stage, they also decide what types of models are most appropriate for the given problem. During data collection, teams look for and integrate available datasets (e.g., internal or open source) or collect their own. Often, they might train a partial model using available generic datasets (e.g., ImageNet for object detection), and then use transfer learning together with more specialized data to train a more specific model (e.g., pedestrian detection). Data cleaning involves removing inaccurate or noisy records from the dataset, a common activity to all forms of data science.</p><p>Data labeling assigns ground truth labels to each record. For example, an engineer might have a set of images on hand which have not yet been labeled with the objects present in the image. Most of the supervised learning techniques require labels to be able to induce a model. Other techniques (e.g., reinforcement learning) use demonstration data or environment rewards to adjust their policies. Labels can be provided either by engineers themselves, domain experts, or by crowd workers in online crowd-sourcing platforms.</p><p>Feature engineering refers to all activities that are performed to extract and select informative features for machine learning models. For some models (e.g. convolutional neural networks), this stage is less explicit and often blended with the next stage, model training. During model training, the chosen models (using the selected features) are trained and tuned on the clean, collected data and their respective labels. Then in model evaluation, the engineers evaluate the output model on tested or safeguard datasets using pre-defined metrics. For critical domains, this stage might also involve extensive human evaluation. The inference code of the model is then deployed on the targeted device(s) and continuously monitored for possible errors during real-world execution.</p><p>For simplicity the view in <ref type="figure">Figure 1</ref> is linear, however, machine learning workflows are highly non-linear and contain several feedback loops. For example, if engineers notice that there is a large distribution shift between the training data and the data in the real world, they might want to go back and collect more representative data and rerun the workflow. Similarly, they may revisit their modeling choices made in the first stage, if the problem evolves or if better algorithms are invented. While feedback loops are typical in Agile software processes, the peculiarity of the machine learning workflow is related to the amount of experimentation needed to converge to a good model for the problem. Indeed, the dayto-day work of an engineer doing machine learning involves frequent iterations over the selected model, hyper-parameters, and dataset refinement. Similar experimental properties have been observed in the past in scientific software <ref type="bibr" target="#b14">[15]</ref> and hardware/software co-design <ref type="bibr" target="#b15">[16]</ref>. This workflow can become even more complex if the system is integrative, containing multiple ML components which interact together in complex and unexpected ways <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Software Engineering for Machine Learning</head><p>The need for adjusting software engineering practices in the recent era has been discussed in the context of hidden technical debt <ref type="bibr" target="#b17">[18]</ref> and troubleshooting integrative AI <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. This work identifies various aspects of ML system architecture and requirements which need to be considered during system design. Some of these aspects include hidden feedback loops, component entanglement and eroded boundaries, nonmonotonic error propagation, continuous quality states, and mismatches between the real world and evaluation sets. On a related line of thought, recent work also discusses the impact that the use of ML-based software has on risk and safety concerns of ISO standards <ref type="bibr" target="#b20">[21]</ref>. In the last five years, there have been multiple efforts in industry to automate this process by building frameworks and environments to support the ML workflow and its experimental nature <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. However, ongoing research and surveys show that engineers still struggle to operationalize and standardize working processes <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b22">[23]</ref>. The goal of this work is to uncover detailed insights on ML-specific best practices used by developers at Microsoft. We share these insights with the broader community aspiring that such take-away lessons can be valuable to other companies and engineers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Process Maturity</head><p>Software engineers face a constantly changing set of platforms and technologies that they must learn to build the newest applications for the software marketplace. Some engineers learn new methods and techniques in school, and bring them to the organizations they work for. Other learn new skills on the job or on the side, as they anticipate their organization's need for latent talent <ref type="bibr" target="#b24">[25]</ref>. Software teams, composed of individual engineers with varying amounts of experience in the skills necessary to professionally build ML components and their support infrastructure, themselves exhibit varying levels of proficiency in their abilities depending on their aggregate experience in the domain.</p><p>The software engineering discipline has long considered software process improvement as one of its vital functions. Researchers and practitioners in the field have developed several well-known metrics to assess it, including the Capability Maturity Model (CMM) <ref type="bibr" target="#b25">[26]</ref> and Six Sigma <ref type="bibr" target="#b26">[27]</ref>. CMM rates the software processes of organizations on five levels, from initial (ad hoc processes), repeatable, defined, capable (i.e., quantitatively measured), and efficient (i.e., deliberate process improvement). Inspired by CMM, we build a first maturity model for teams building systems and platforms that integrate machine learning components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. STUDY</head><p>We collected data in two phases: an initial set of interviews to gather the major topics relevant to our research questions and a wide-scale survey about the identified topics. Our study design was approved by Microsoft's Ethics Advisory Board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Interviews</head><p>Because the work practice around building and integrating machine learning into software and services is still emerging and is not uniform across all product teams, there is no systematic way to identify the key stakeholders on the topic of adoption. We therefore used a snowball sampling strategy, starting with (1) leaders of teams with mature use of machine learning (ML) (e.g., Bing), (2) leaders of teams where AI is a major aspect of the user experience (e.g., Cortana), and (3) people conducting company-wide internal training in AI and ML. As we chose informants, we picked a variety of teams to get different levels of experience and different parts of the ecosystem (products with AI components, AI frameworks and platforms, AI created for external companies). In all, we interviewed 14 software engineers, largely in senior leadership roles. These are shown in <ref type="table" target="#tab_0">Table I</ref>. The interviews were semi-structured and specialized to each informant's role. For example, when interviewing Informant I3, we asked questions related to his work overseeing teams building the product's architectural components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Survey</head><p>Based on the results of the interviews, we designed an open-ended questionnaire whose focus was on existing work practice, challenges in that work practice, and best practices ( <ref type="figure">Figure 2</ref>). We asked about challenges both directly and indirectly by asking informants to imagine "dream tools" and improvements that would make their work practice better. We sent the questionnaire to 4195 members of internal mailing lists on the topics of AI and ML. 551 software engineers responded, giving us a 13.6% response rate. For each openresponse item, between two and four researchers analyzed the responses through a card sort. Then, the entire team reviewed the card sort results for clarity and consistency.</p><p>Respondents were fairly well spread across all divisions of the company and came from a variety of job roles: Data and applied science (42%), Software engineering (32%), Program management (17%), Research (7%), and other (1%). 21% of respondents were managers and 79% were individual contributors, helping us balance out the majority manager perspective in our interviews.</p><p>In the next sections, we discuss our interview and survey results, starting with the range of AI applications developed by Microsoft, diving into best practices that Microsoft engineers have developed to address some of the essential challenges in building large-scale AI applications and platforms, showing how the perception of the importance of the challenges changes as teams gain experience building AI applications, and finally, describing our proposed AI process maturity model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPLICATIONS OF AI</head><p>Many teams across Microsoft have augmented their applications with machine learning and inference, some in some surprising domains. We asked survey respondents for the ways that they used AI on their teams. We card sorted this data twice, once to capture the application domain in which AI was being applied, and a second time to look at the (mainly) ML algorithms used to build that application.</p><p>We found AI is used in traditional areas such as search, advertising, machine translation, predicting customer purchases, voice recognition, and image recognition, but also saw it being used in novel areas, such as identifying customer leads, providing design advice for presentations and word processing documents, providing unique drawing features, healthcare, and improving gameplay. In addition, machine learning is being used heavily in infrastructure projects to manage incident reporting, identify the most likely causes for bugs, monitor fraudulent fiscal activity, and to monitor network streams for security breaches.</p><p>Respondents used a broad spectrum of ML approaches to build their applications, from classification, clustering, dynamic programming, and statistics, to user behavior modeling, social networking analysis, and collaborative filtering. Some areas of the company specialized further, for instance, Search worked heavily with ranking and relevance algorithms along with query understanding. Many divisions in the company work on natural language processing, developing tools for entity recognition, sentiment analysis, intent prediction, summarization, machine translation, ontology construction, text similarity, and connecting answers to questions. Finance and Sales have been keen to build risk prediction models and do forecasting. Internal resourcing organizations make use of decision optimization algorithms such as resource optimization, planning, pricing, bidding, and process optimization.</p><p>The takeaway for us was that integration of machine learning components is happening all over the company, not just on teams historically known for it. Thus, we could tell that we were not just hearing from one niche corner of the company, but in fact, we received responses from a broad range of perspectives spread throughout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. BEST PRACTICES WITH MACHINE LEARNING IN SOFTWARE ENGINEERING</head><p>In this section, we present our respondents' viewpoints on some of the essential challenges associated with building largescale ML applications and platforms and how they address them in their products. We categorized the challenges by card sorting interview and survey free response questions, and then used our own judgment as software engineering and AI researchers to highlight those that are essential to the practice of AI on software teams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. End-to-end pipeline support</head><p>As machine learning components have become more mature and integrated into larger software systems, our participants recognized the importance of integrating ML development support into the traditional software development infrastructure. They noted that having a seamless development experience covering (possibly) all the different stages described in <ref type="figure">Figure 1</ref> was important to automation. However, achieving this level of integration can be challenging because of the different characteristics of ML modules compared with traditional software components. For example, previous work in this field <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> found that variation in the inherent uncertainty (and error) of data-driven learning algorithms and complex component entanglement caused by hidden feedback loops could impose substantial changes (even in specific stages) which were previously well understood in software engineering (e.g., specification, testing, debugging, to name a few). Nevertheless, due to the experimental and even more iterative nature of ML development, unifying and automating the dayto-day workflow of software engineers reduces overhead and facilitate progress in the field.</p><p>Respondents report to leverage internal infrastructure in the company (e.g. AEther 2 ) or they have built pipelines specialized to their own use cases. It is important to develop a "rock solid, data pipeline, capable of continuously loading and massaging data, enabling engineers to try out many permutations of AI algorithms with different hyper-parameters without hassle." The pipelines created by these teams are automated, supporting training, deployment, and integration of models with the product they are a part of. In addition, some pipeline engineers indicated that "rich dashboards" showing the value provided to users are useful.</p><p>Several respondents develop openly available IDEs to enable Microsoft's customers to build and deploy their models (e.g. Azure ML for Visual Studio Code <ref type="bibr" target="#b2">3</ref> and Azure ML Studio <ref type="bibr" target="#b3">4</ref> ). According to two of our interviewees, the goal https://www.slideshare.net/MSTechCommunity/ ai-microsoft-how-we-do-it-and-how-you-can-too https://marketplace.visualstudio.com/items?itemName=ms-toolsai.vscode-ai 4 https://azure.microsoft.com/en-us/services/machine-learning-studio/ of these environments is to help engineers discover, gather, ingest, understand, and transform data, and then train, deploy, and maintain models. In addition, these teams customize the environments to make them easier to use by engineers with varying levels of experience. "Visual tools help beginning data scientists when getting started, but once they know the ropes and branch out, such tools may get in their way and they may need something else." B. Data availability, collection, cleaning, and management Since many machine learning techniques are centered around learning from large datasets, the success of ML-centric projects often heavily depends on data availability, quality and management <ref type="bibr" target="#b27">[28]</ref>. Labeling datasets is costly and timeconsuming, so it is important to make them available for use within the company (subject to compliance constraints). Our respondents confirm that it is important to "reuse the data as much as possible to reduce duplicated effort." In addition to availability, our respondents focus most heavily on supporting the following data attributes: "accessibility, accuracy, authoritativeness, freshness, latency, structuredness, ontological typing, connectedness, and semantic joinability." Automation is a vital cross-cutting concern, enabling teams to more efficiently aggregate data, extract features, synthesize labelled examples. The increased efficiency enables teams to "speed up experimentation and work with live data while they experiment with new models."</p><p>We found that Microsoft teams have found it necessary to blend data management tools with their ML frameworks to avoid the fragmentation of data and model management activities. A fundamental aspect of data management for machine learning is the rapid evolution of data sources. Continuous changes in data may originate either from (i) operations initiated by engineers themselves, or from (ii) incoming fresh data (e.g., sensor data, user interactions). Either case requires rigorous data versioning and sharing techniques, for example, "Each model is tagged with a provenance tag that explains with which data it has been trained on and which version of the model. Each dataset is tagged with information about where it originated from and which version of the code was used to extract it (and any related features)." This practice is used for mapping datasets to deployed models or for facilitating data sharing and reusability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Education and Training</head><p>The integration of machine learning continues to become more ubiquitous in customer-facing products, for example, machine learning components are now widely used in productivity software (e.g., email, word processing) and embedded devices (i.e., edge computing). Thus, engineers with traditional software engineering backgrounds need to learn how to work alongside of the ML specialists. A variety of players within Microsoft have found it incredibly valuable to scaffold their engineers' education in a number of ways. First, the company hosts a twice-yearly internal conference on machine learning and data science, with at least one day devoted to introductions to the basics of technologies, algorithms, and best practices. In addition, employees give talks about internal tools and the engineering details behind novel projects and product features, and researchers present cutting-edge advances they have seen and contributed to academic conferences. Second, a number of Microsoft teams host weekly open forums on machine learning and deep learning, enabling practitioners to get together and learn more about AI. Finally, mailing lists and online forums with thousands of participants enable anyone to ask and answer technical and pragmatic questions about AI and machine learning, as well as frequently share recent results from academic conferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Debugging and Interpretability</head><p>Debugging activities for components that learn from data not only focus on programming bugs, but also focus on inherent issues that arise from model errors and uncertainty. Understanding when and how models fail to make accurate predictions is an active research area <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, which is attracting more attention as ML algorithms and optimization techniques become more complex. Several survey respondents and the larger Explainable AI community <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> propose to use more interpretable models, or to develop visualization techniques that make black-box models more interpretable. For larger, multi-model systems, respondents apply modularization in a conventional, layered, and tiered software architecture to simplify error analysis and debuggability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Model Evolution, Evaluation, and Deployment</head><p>ML-centric software goes through frequent revisions initiated by model changes, parameter tuning, and data updates, the combination of which has a significant impact on system performance. A number of teams have found it important to employ rigorous and agile techniques to evaluate their experiments. They developed systematic processes by adopting combo-flighting techniques (i.e., flighting a combination of changes and updates), including multiple metrics in their experiment score cards, and performing human-driven evaluation for more sensitive data categories. One respondent's team uses "score cards for the evaluation of flights and storing flight information: How long has it been flighted, metrics for the flight, etc." Automating tests is as important in machine learning as it is in software engineering; teams create carefully put-together test sets that capture what their models should do. However, it is important that a human remains in the loop. One respondent said, "we spot check and have a human look at the errors to see why this particular category is not doing well, and then hypothesize to figure out problem source." Fast-paced model iterations require more frequent deployment. To ensure that system deployment goes smoothly, several engineers recommend not only to automate the training and deployment pipeline, but also to integrate model building with the rest of the software, use common versioning repositories for both ML and non-ML codebases, and tightly couple the ML and non-ML development sprints and standups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Compliance</head><p>Microsoft issued a set of principles around uses of AI in the open world. These include fairness, accountability, transparency, and ethics. All teams at Microsoft have been asked to align their engineering practices and the behaviors of fielded software and services in accordance with these principles. Respect for them is a high priority in software engineering and AI and ML processes and practices. A discussion of these concerns is beyond the scope of this paper. To learn more about Microsoft's commitments to this important topic, please read about its approach to AI. <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Varied Perceptions</head><p>We found that as a number of product teams at Microsoft integrated machine learning components into their applications, their ability to do so effectively was mediated by the amount of prior experience with machine learning and data science. Some teams fielded data scientists and researchers with decades of experience, while others had to grow quickly, picking up their own experience and more-experienced team members on the way. Due to this heterogeneity, we expected that our survey respondents' perceptions of the challenges their teams' faced in practicing machine learning would vary accordingly.</p><p>We grouped the respondents into three buckets (low, medium, and high), evenly divided by the number of years of experience respondents personally had with AI. First, we ranked each of the card sorted categories of respondents' challenges divided by the AI experience buckets. This list is presented in <ref type="table" target="#tab_0">Table II</ref>, initially sorted by the respondents with low experience with AI.</p><p>Two things are worth noticing. First, across the board, Data Availability, Collection, Cleaning, and Management, is ranked as the top challenge by many respondents, no matter their experience level. We find similarly consistent ranking for issues around the categories of end-to-end pipeline support and collaboration and working culture. Second, some of the challenges rise or fall in importance as the respondents' experience with AI differs. For example, education and training is far more important to those with low experience levels in AI than those with more experience. In addition, respondents with low experience rank challenges with integrating AI into larger systems higher than those with medium or high experience. This means that as individuals (and their teams) gain experience building applications and platforms that integrate ML, their increasing skills help shrink the importance of some of the challenges they perceive. Note, the converse also occurs. Challenges around tooling, scale, and model evolution, evaluation, and deployment are more important for engineers with a lot of experience with AI. This is very likely because these more experienced individuals are tasked with the more essentially difficult engineering tasks on their team; those with low experience are probably tasked to easier problems until they build up their experience. We also compared the overall frequency of each kind of challenge using the same three buckets of AI experience. Looking again at the top ranked challenge, Data Availability, Collection, Cleaning, and Management, we notice that it was reported by low and medium experienced respondents at similar rates, but represented a lot more of the responses (60%) given by those with high experience. This also happened for challenges related to Specifications. However, when looking at Education and Training, Integrating AI into larger systems, and Education: Guidance and Mentoring, their frequency drops significantly from the rate reported by the low experience bucket than reported by the medium and high buckets. We interpret this to mean that these challenges were less important to the medium and high experience respondents than to those with low experience levels. Thus, this table gives a big picture of both which problems are perceived as most important within each experience bucket, and which problems are perceived as most important across the buckets.</p><p>Finally, we conducted a logistic regression analysis to build a model that could explain the differences in frequency when controlling for personal AI experience, team AI experience, overall work experience, the number of concurrent AI projects, and whether or not a respondent had formal education in machine learning or data science techniques. We found five significant coefficients:</p><p>• Education and Training was negatively correlated with personal AI experience with a coefficient of -0.18 (p &lt; 0.02), meaning that people with less AI experience found this to be a more important issue. • Educating Others was positively correlated with personal AI experience with a coefficient of 0.26 (p &lt; 0.01), meaning that people with greater AI experience found this to be a more important issue. • Tool issues are positively correlated with team AI experience with a coefficient of 0.13 (p &lt; 0.001), meaning that as the team gains experience working on AI projects, the degree to which they rely on others' and their own tools goes up, making them think about their impact more often.</p><p>• End-to-end pipeline support was positively correlated with formal education (p &lt; 0.01), implying that only those with formal education were working on building such a pipeline. • Specifications were also positively correlated with formal education (p &lt; 0.03), implying that those with formal education are the ones who write down the specifications for their models and engineering systems.</p><p>The lesson we learn from these analyses is that the kinds of issues that engineers perceive as important change as they grow in their experience with AI. Some concerns are transitory, related to one's position within the team and the accidental complexity of working together. Several others are more fundamental to the practice of integrating machine learning into software applications, affecting many engineers, no matter their experience levels. Since machine learning-based applications are expected to continue to grow in popularity, we call for further research to address these important issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. TOWARDS A MODEL OF ML PROCESS MATURITY</head><p>As we saw in Section V-G, we see some variance in the experience levels of AI in software teams. That variation affects their perception of the engineering challenges to be addressed in their day-to-day practices. As software teams mature and gel, they can become more effective and efficient in delivering machine learning-based products and platforms.</p><p>To capture the maturity of ML more precisely than using a simple years-of-experience number, we created a maturity model with six dimensions evaluating whether each workflow stage: (1) has defined goals, (2) is consistently implemented, (3) documented, (4) automated, (5) measured and tracked, and (6) continuously improved. The factors are loosely based on the concepts behind the Capability Maturity Model (CMM) <ref type="bibr" target="#b25">[26]</ref> and Six Sigma <ref type="bibr" target="#b26">[27]</ref>, which are widely used in software development to assess and improve maturity of software projects.</p><p>In the survey, we asked respondents to report the maturity for the two workflow stages that each participant spent the most time on (measured by number of hours they reported spending on each activity). Specifically, we asked participants to rate their agreement with the following statements S 1 ..S 6 (bold text was in the original survey) using a Likert response format from Strongly Disagree (1) to Strongly Agree (5): S1: My team has goals defined for what to accomplish with this activity. S2: My team does this activity in a consistent manner. S3: My team has largely documented the practices related to this activity. S4: My team does this activity mostly in an automated way. S5: My team measures and tracks how effective we are at completing this activity. S6: My team continuously improves our practices related to this activity.</p><p>We gathered this data for the stages that respondents were most familiar with because we found that they often specialize in various stages of the workflow. This question was intended to be lightweight so that respondents could answer easily, while at the same time accounting for the wide variety of ML techniques applied. Rather than being prescriptive (i.e., do this to get to the next maturity level), our intention was to be descriptive (e.g., how much automation is there in a particular workflow stage? how well is a workflow stage documented?). More work is needed to define maturity levels similar to CMM.</p><p>To analyze the responses, we defined an Activity Maturity Index (AMI) to combine the individual scores into a single measure. This index is the average of the agreement with the six maturity statements S 1 ..S 6 . As a means of validating the Maturity Index, we asked participants to rate the Activity Effectiveness (AE) by answering "How effective do you think your team's practices around this activity are on a scale from 1 (poor) to 5 (excellent)?". The Spearman correlation between the Maturity Index and the Effectiveness was between 0.4982 and 0.7627 (all statistically significant at p &lt; 0.001) for all AI activities. This suggests that the Maturity Index is a valid composite measure that can capture the maturity and effectiveness of AI activities.</p><p>In addition to the Activity Maturity Index and Activity Effectiveness, we collected an Overall Effectiveness (OE) score by asking respondents the question "How effectively does your team work with AI on a scale from 1 (poor) to 5 (excellent)" Having the AMI, AE, and OE measures allowed us to compare the maturity and effectiveness of different organizations, disciplines, and application domains within Microsoft, and identify areas for improvement. We plot one of these comparisons in <ref type="figure" target="#fig_0">Figure 3</ref> and show the average overall effectiveness scores divided by nine of the most represented AI application domains in our survey. There are two things to notice. First, the spread of the y-values indicates that the OE metric can numerically distinguish between teams, meaning that some respondents feel their teams are at different levels of maturity than others. Second, an ANOVA and Scott Knott test show significant differences in the reported values, demonstrating the potential value of this metric to identify the various ML process maturity levels.</p><p>We recognize that these metrics represent a first attempt at quantifying a process metric to enable teams to assess how well they practice ML. In future work, we will refine our instrument and further validate its utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head><p>In this section, we synthesize our findings into three observations of some fundamental differences in the way that software engineering has been adapted to support past popular application domains and how it can be adapted to support artificial intelligence applications and platforms. There may be more differences, but from our data and discussions with ML experts around Microsoft, these three rose to prominence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data discovery and management</head><p>Just as software engineering is primarily about the code that forms shipping software, ML is all about the data that powers learning models. Software engineers prefer to design and build systems which are elegant, abstract, modular, and simple. By contrast, the data used in machine learning are voluminous, context-specific, heterogeneous, and often complex to describe. These differences result in difficult problems when ML models are integrated into software systems at scale.</p><p>Engineers have to find, collect, curate, clean, and process data for use in model training and tuning. All the data has to be stored, tracked, and versioned. While software APIs are described by specifications, datasets rarely have explicit schema definitions to describe the columns and characterize their statistical distributions. However, due to the rapid iteration involved in ML, the data schema (and the data) change frequently, even many times per day. When data is ingested from large-scale diagnostic data feeds, if ML engineers want to change which data values are collected, they must wait for the engineering systems to be updated, deployed, and propagated before new data can arrive. Even "simple" changes can have significant impacts on the volume of data collected, potentially impacting applications through altered performance characteristics or increased network bandwidth usage.</p><p>While there are very well-designed technologies to version code, the same is not true for data. A given data set may contain data from several different schema regimes. When a single engineer gathers and processes this data, they can keep track of these unwritten details, but when project sizes scale, maintaining this tribal knowledge can become a burden. To help codify this information into a machine-readable form, Gebru et al. propose to use data sheets inspired by electronics to more transparently and reliably track the metadata characteristics of these datasets <ref type="bibr" target="#b33">[34]</ref>. To compare datasets against each other, the Datadiff <ref type="bibr" target="#b34">[35]</ref> tool enables developers to formulate viable transformation functions over data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Customization and Reuse</head><p>While it is well-understood how much work it takes to customize and reuse code components, customizing ML models can require much more. In software, the primary units of reuse are functions, algorithms, libraries, and modules. A software engineer can find the source code for a library (e.g. on Github), fork it, and easily make changes to the code, using the same skills they use to develop their own software.</p><p>Although fully-trained ML models appear to be functions that one can call for a given input, the reality is far more complex. One part of a model is the algorithm that powers the particular machine learning technique being used (e.g., SVM or neural nets). Another is the set of parameters that controls the function (e.g., the SVM support vectors or neural net weights) and are learned during training. If an engineer wants to apply the model on a similar domain as the data it was originally trained on, reusing it is straightforward. However, more signficant changes are needed when one needs to run the model on a different domain or use a slightly different input format. One cannot simply change the parameters with a text editor. In fact, the model may require retraining, or worse, may need to be replaced with another model. Both require the software developer to have machine learning skills, which they may never have learned. Beyond that, retraining or rebuilding the model requires additional training data to be discovered, collected, and cleaned, which can take as much work and expertise as the original model's authors put in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ML Modularity</head><p>Another key attribute of engineering large-scale software systems is modularity. Modules are separated and isolated to ensure that developing one component does not interfere with the behavior of others under development. In addition, software modularity is strengthened by Conway's Law, which makes the observation that the teams that build each component of the software organize themselves similarly to its architecture. Thus, separate modules are often assigned to separate teams. Module interactions are controlled by APIs which do dual duty to enable software modules to remain apart, but also describe the interfaces to minimize the amount of communication needed between separate teams to make their modules work together <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</p><p>Maintaining strict module boundaries between machine learned models is difficult for two reasons. First, models are not easily extensible. For example, one cannot (yet) take an NLP model of English and add a separate NLP model for ordering pizza and expect them to work properly together. Similarly, one cannot take that same model for pizza and pair it with an equivalent NLP model for French and have it work. The models would have to be developed and trained together.</p><p>Second, models interact in non-obvious ways. In largescale systems with more than a single model, each model's results will affect one another's training and tuning processes. In fact, one model's effectiveness will change as a result of the other model, even if their code is kept separated. Thus, even if separate teams built each model, they would have to collaborate closely in order to properly train or maintain the full system. This phenomenon (also referred to as component entanglement) can lead to non-monotonic error propagation, meaning that improvements in one part of the system might decrease the overall system quality because the rest of the system is not tuned to the latest improvements. This issue is even more evident in cases when machine learning models are not updated in a compatible way and introduce new, previously unseen mistakes that break the interaction with other parts of the system which rely on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. LIMITATIONS</head><p>Our case study was conducted with teams at Microsoft, a large, world-wide software company with a diverse portfolio of software products. It is also one of the largest purveyors of machine learning-based products and platforms. Some findings are likely to be specific to the Microsoft teams and team members who participated in our interviews and surveys. Nevertheless, given the high variety of projects represented by our informants, we expect that many of the lessons we present in this paper will apply to other companies. Some of our findings depend on the particular ML workflow used by some software teams at Microsoft. The reader should be able to identify how our model abstractions fit into the particulars of the models they use. Finally, interviews and surveys rely on self-selected informants and self-reported data. Wherever appropriate, we stated that findings were our informants' perceptions and opinions. This is especially true with this implementation of our ML process maturity model, which triangulated its measures against other equally subjective measures with no objective baseline. Future implementations of the maturity model should endeavor to gather objective measures of team process performance and evolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>Many teams at Microsoft have put significant effort into developing an extensive portfolio of AI applications and platforms by integrating machine learning into existing software engineering processes and cultivating and growing ML talent. In this paper, we described the results of a study to learn more about the process and practice changes undertaken by a number of Microsoft teams in recent years. From these findings, we synthesized a set of best practices to address issues fundamental to the large-scale development and deployment of ML-based applications. Some reported issues were correlated with the respondents' experience with AI, while others were applicable to most respondents building AI applications. We presented a ML process maturity metric to help teams selfassess how well they work with machine learning and offer guidance towards improvements. Finally, we identified three aspects of the AI domain that make it fundamentally different than prior application domains. Their impact will require significant research efforts to address in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>The average overall effectiveness (OE) of a team's ML practices divided by application domain (anonymized). The y-axis labels have been elided for confidentiality. An ANOVA and Scott Knott test identified two distinct groups to the OE metric, labeled in black (A-F) and red (G-I).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>STAKEHOLDERS WE INTERVIEWED FOR THE STUDY.</figDesc><table><row><cell>Id</cell><cell>Role</cell><cell>Product Area</cell><cell>Manager?</cell></row><row><cell>I1</cell><cell>Applied Scientist</cell><cell>Search</cell><cell>Yes</cell></row><row><cell>I2</cell><cell>Applied Scientist</cell><cell>Search</cell><cell>Yes</cell></row><row><cell>I3</cell><cell>Architect</cell><cell>Conversation</cell><cell>Yes</cell></row><row><cell>I4</cell><cell cols="2">Engineering Manager Vision</cell><cell>Yes</cell></row><row><cell>I5</cell><cell>General Manager</cell><cell>ML Tools</cell><cell>Yes</cell></row><row><cell>I6</cell><cell>Program Manager</cell><cell>ML Tools</cell><cell>Yes</cell></row><row><cell>I7</cell><cell>Program Manager</cell><cell cols="2">Productivity Tools Yes</cell></row><row><cell>I8</cell><cell>Researcher</cell><cell>ML Tools</cell><cell>Yes</cell></row><row><cell>I9</cell><cell>Software Engineer</cell><cell>Speech</cell><cell>Yes</cell></row><row><cell>I10</cell><cell>Program Manager</cell><cell>AI Platform</cell><cell>No</cell></row><row><cell>I11</cell><cell>Program Manager</cell><cell>Community</cell><cell>No</cell></row><row><cell>I12</cell><cell>Scientist</cell><cell>Ads</cell><cell>No</cell></row><row><cell>I13</cell><cell>Software Engineer</cell><cell>Vision</cell><cell>No</cell></row><row><cell>I14</cell><cell>Software Engineer</cell><cell>Vision</cell><cell>No</cell></row><row><cell>1. Part 1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">1.1. Background and demographics:</cell><cell></cell></row><row><cell></cell><cell>*</cell><cell></cell><cell></cell></row><row><cell cols="2">2.2. Effectiveness rating</cell><cell></cell><cell></cell></row><row><cell cols="2">2.3. Maturity ratings</cell><cell></cell><cell></cell></row><row><cell>3. Part 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3.1. Dream tools*</cell><cell></cell><cell></cell></row><row><cell cols="2">3.2. Best practices*</cell><cell></cell><cell></cell></row><row><cell cols="2">3.3. General comments*</cell><cell></cell><cell></cell></row></table><note>1.1.1. years of AI experience 1.1.2. primary AI use case* 1.1.3. team effectiveness rating 1.1.4. source of AI components 1.2. Challenges* 1.3. Time spent on each of the nine workflow activities 1.4. Time spent on cross-cutting activities 2. Part 2 (repeated for two activities where most time spent) 2.1. Tools usedFig. 2. The structure of the study's questionnaire. An asterisk indicates an open-response item.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II THE</head><label>II</label><figDesc>TOP-RANKED CHALLENGES AND PERSONAL EXPERIENCE WITH AI. RESPONDENTS WERE GROUPED INTO THREE BUCKETS (LOW, MEDIUM, HIGH) BASED ON THE 33RD AND 67TH PERCENTILE OF THE NUMBER OF YEARS OF AI EXPERIENCE THEY PERSONALLY HAD (N=308). THE COLUMN Frequency SHOWS THE INCREASE/DECREASE OF THE FREQUENCY IN THE MEDIUM AND HIGH BUCKETS COMPARED TO THE LOW BUCKETS. THE COLUMN Rank SHOWS THE RANKING OF THE CHALLENGES WITHIN EACH EXPERIENCE BUCKET, WITH 1 BEING THE MOST FREQUENT CHALLENGE.</figDesc><table><row><cell></cell><cell></cell><cell>Frequency</cell><cell></cell><cell></cell><cell>Rank</cell></row><row><cell></cell><cell>Medium</cell><cell>High</cell><cell></cell><cell></cell><cell>Experience</cell></row><row><cell>Challenge</cell><cell cols="2">vs. Low vs. Low</cell><cell>Trend</cell><cell cols="3">Low Medium High</cell></row><row><cell>Data Availability, Collection, Cleaning, and Management</cell><cell>-2%</cell><cell>60%</cell><cell></cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Education and Training</cell><cell>-69%</cell><cell>-78%</cell><cell></cell><cell>1</cell><cell>5</cell><cell>9</cell></row><row><cell>Hardware Resources</cell><cell>-32%</cell><cell>13%</cell><cell></cell><cell>3</cell><cell>8</cell><cell>6</cell></row><row><cell>End-to-end pipeline support</cell><cell>65%</cell><cell>41%</cell><cell></cell><cell>4</cell><cell>2</cell><cell>4</cell></row><row><cell>Collaboration and working culture</cell><cell>19%</cell><cell>69%</cell><cell></cell><cell></cell><cell>6</cell><cell>6</cell></row><row><cell>Specification</cell><cell>2%</cell><cell>50%</cell><cell></cell><cell>5</cell><cell>8</cell></row><row><cell>Integrating AI into larger systems</cell><cell>-49%</cell><cell>-62%</cell><cell></cell><cell>5</cell><cell>16</cell><cell>13</cell></row><row><cell>Education: Guidance and Mentoring</cell><cell>-83%</cell><cell>-81%</cell><cell></cell><cell></cell><cell>21</cell><cell>18</cell></row><row><cell>AI Tools</cell><cell>144%</cell><cell>193%</cell><cell></cell><cell></cell><cell>3</cell></row><row><cell>Scale</cell><cell>154%</cell><cell>210%</cell><cell></cell><cell></cell><cell>4</cell><cell>3</cell></row><row><cell>Model Evolution, Evaluation, and Deployment</cell><cell>137%</cell><cell>276%</cell><cell></cell><cell></cell><cell>6</cell><cell>4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.microsoft.com/en-us/ai/our-approach-to-ai</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Microsoft AI Platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salvaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Tok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning with Azure</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Usage and perceptions of agile software development in an industrial context: An exploratory study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Begel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)</title>
		<imprint>
			<date type="published" when="2007-09" />
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pair programming: What&apos;s in it for me</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Begel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement</title>
		<meeting>of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Have agile techniques been the silver bullet for software development at microsoft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Begel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM/IEEE Intl. Symp. on Empirical Software Engineering and Measurement</title>
		<imprint>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DevOps capabilities, practices, and challenges: Insights from a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Senapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buchan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Osman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd International Conference on Evaluation and Assessment in Software</title>
		<meeting>of the 22nd International Conference on Evaluation and Assessment in Software</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="57" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maintaining mental models: A study of developer work habits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Latoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venolia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th International Conference on Software Engineering</title>
		<meeting>of the 28th International Conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="492" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The emerging role of data scientists on software development teams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Begel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 38th International Conference on Software Engineering</title>
		<meeting>of the 38th International Conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="96" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data scientists in software teams: State of the art and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Begel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1024" to="1038" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Trials and tribulations of developers of intelligent systems: A field study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Languages and Human-Centric Computing (VL/HCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="162" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Machine learning workflow</title>
		<ptr target="https://cloud.google.com/ml-engine/docs/tensorflow/ml-solutions-overview" />
		<imprint>
			<biblScope unit="volume">accessed</biblScope>
			<biblScope unit="page" from="2018" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Investigating statistical machine learning as a tool for software development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>of the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Team Data Science Process</title>
		<ptr target="https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/" />
		<imprint>
			<biblScope unit="page" from="2018" to="2027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The KDD process for extracting useful knowledge from volumes of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CRISP-DM: Towards a standard process model for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Intl. Conference on Practical Applications of Knowledge Discovery and Data mining</title>
		<meeting>4th Intl. Conference on Practical Applications of Knowledge Discovery and Data mining</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How do scientists develop and use scientific software?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Langtangen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in Proc. of the 2009 ICSE workshop on Software Engineering for Computational Science and Engineering</title>
		<meeting>of the 2009 ICSE workshop on Software Engineering for Computational Science and Engineering</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hardware/software co-design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Michell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the IEEE</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="365" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rapid development of multimodal interactive systems: a demonstration of platform for situated intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jalobeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>of the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="493" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hidden technical debt in machine learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Crespo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dennison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On human intellect and machine failures: Troubleshooting integrative machine learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kossmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What went wrong and why? diagnosing situated interaction failures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSR</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An analysis of ISO 26262: Using machine learning safely in automotive software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Queiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02435</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TFX: A tensorflow-based production-scale machine learning platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haykal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd ACM SIGKDD</title>
		<meeting>of the 23rd ACM SIGKDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1387" to="1395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Model governance: Reducing the anarchy of production ML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sundararaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Talagala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX. USENIX Association</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="351" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Machine learning in manufacturing: advantages, challenges, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wuest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Irgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-D</forename><surname>Thoben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Production &amp; Manufacturing Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="45" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">App-directed learning: An exploratory study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sillito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Begel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE)</title>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="81" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The capability maturity model, guidelines for improving the software process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Chrissis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Addison Wesley</publisher>
			<pubPlace>Harlow</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Six Sigma: The breakthrough management strategy revolutionizing the world&apos;s top corporations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Taylor &amp; Francis</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data management challenges in production machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2017 ACM SIGMOD</title>
		<meeting>of the 2017 ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1723" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Principles of explanatory debugging to personalize interactive machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th International Conference on Intelligent User Interfaces</title>
		<meeting>of the 20th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="126" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeltracker: Redesigning performance analysis tools for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>of the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards accountable AI: Hybrid human-machine analyses for characterizing system failure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in HCOMP</title>
		<imprint>
			<biblScope unit="page" from="126" to="135" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Defense Advanced Research Projects Agency (DARPA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gunning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Explainable artificial intelligence (XAI)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Intelligible artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04263</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Datasheets for datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vecchione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crawford</surname></persName>
		</author>
		<idno>abs/1803.09010</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data diff: Interpretable, executable summaries of changes in distributions for data wrangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geddes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 24th ACM SIGKDD</title>
		<meeting>of the 24th ACM SIGKDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2279" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Breaking the Code&quot;, moving between private and public work in collaborative software development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R B</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Redmiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dourish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2003 International ACM SIGGROUP Conference on Supporting Group Work</title>
		<meeting>of the 2003 International ACM SIGGROUP Conference on Supporting Group Work</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sometimes you need to see through walls: A field study of application programming interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R B</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Redmiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Millen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2004 ACM Conference on Computer Supported Cooperative Work</title>
		<meeting>of the 2004 ACM Conference on Computer Supported Cooperative Work</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
