<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric deep learning: going beyond Euclidean data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
						</author>
						<title level="a" type="main">Geometric deep learning: going beyond Euclidean data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>"Deep learning" refers to learning complicated concepts by building them from simpler ones in a hierarchical or multi-layer manner. Artificial neural networks are popular realizations of such deep multi-layer hierarchies. In the past few years, the growing computational power of modern GPUbased computers and the availability of large training datasets have allowed successfully training neural networks with many layers and degrees of freedom <ref type="bibr" target="#b0">[1]</ref>. This has led to qualitative breakthroughs on a wide variety of tasks, from speech recognition <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and machine translation <ref type="bibr" target="#b3">[4]</ref> to image analysis and computer vision <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> (the reader is referred to <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> for many additional examples of successful applications of deep learning). Nowadays, deep learning has matured into a technology that is widely used in commercial applications, including Siri speech recognition in Apple iPhone, Google text translation, and Mobileye visionbased technology for autonomously driving cars.</p><p>One of the key reasons for the success of deep neural networks is their ability to leverage statistical properties of MB is with USI Lugano, Switzerland, Tel Aviv University, and Intel Perceptual Computing, Israel. JB is with Courant Institute, NYU and UC Berkeley, USA. YL with with Facebook AI Research and NYU, USA. AS is with Facebook AI Research, USA. PV is with EPFL, Switzerland.</p><p>the data such as stationarity and compositionality through local statistics, which are present in natural images, video, and speech <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. These statistical properties have been related to physics <ref type="bibr" target="#b15">[16]</ref> and formalized in specific classes of convolutional neural networks (CNNs) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In image analysis applications, one can consider images as functions on the Euclidean space (plane), sampled on a grid. In this setting, stationarity is owed to shift-invariance, locality is due to the local connectivity, and compositionality stems from the multi-resolution structure of the grid. These properties are exploited by convolutional architectures <ref type="bibr" target="#b19">[20]</ref>, which are built of alternating convolutional and downsampling (pooling) layers. The use of convolutions has a two-fold effect. First, it allows extracting local features that are shared across the image domain and greatly reduces the number of parameters in the network with respect to generic deep architectures (and thus also the risk of overfitting), without sacrificing the expressive capacity of the network. Second, the convolutional architecture itself imposes some priors about the data, which appear very suitable especially for natural images <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>While deep learning models have been particularly successful when dealing with signals such as speech, images, or video, in which there is an underlying Euclidean structure, recently there has been a growing interest in trying to apply learning on non-Euclidean geometric data. Such kinds of data arise in numerous applications. For instance, in social networks, the characteristics of users can be modeled as signals on the vertices of the social graph <ref type="bibr" target="#b21">[22]</ref>. Sensor networks are graph models of distributed interconnected sensors, whose readings are modelled as time-dependent signals on the vertices. In genetics, gene expression data are modeled as signals defined on the regulatory network <ref type="bibr" target="#b22">[23]</ref>. In neuroscience, graph models are used to represent anatomical and functional structures of the brain. In computer graphics and vision, 3D objects are modeled as Riemannian manifolds (surfaces) endowed with properties such as color texture.</p><p>The non-Euclidean nature of such data implies that there are no such familiar properties as global parameterization, common system of coordinates, vector space structure, or shift-invariance. Consequently, basic operations like convolution that are taken for granted in the Euclidean case are even not well defined on non-Euclidean domains. The purpose of our paper is to show different methods of translating the key ingredients of successful deep learning methods such as convolutional neural networks to non-Euclidean data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1611.08097v2 [cs.CV] 3 May 2017</head><p>II. GEOMETRIC LEARNING PROBLEMS Broadly speaking, we can distinguish between two classes of geometric learning problems. In the first class of problems, the goal is to characterize the structure of the data. The second class of problems deals with analyzing functions defined on a given non-Euclidean domain. These two classes are related, since understanding the properties of functions defined on a domain conveys certain information about the domain, and vice-versa, the structure of the domain imposes certain properties on the functions on it.</p><p>Structure of the domain: As an example of the first class of problems, assume to be given a set of data points with some underlying lower dimensional structure embedded into a high-dimensional Euclidean space. Recovering that lower dimensional structure is often referred to as manifold learning <ref type="bibr" target="#b0">1</ref> or non-linear dimensionality reduction, and is an instance of unsupervised learning. Many methods for nonlinear dimensionality reduction consist of two steps: first, they start with constructing a representation of local affinity of the data points (typically, a sparsely connected graph). Second, the data points are embedded into a low-dimensional space trying to preserve some criterion of the original affinity. For example, spectral embeddings tend to map points with many connections between them to nearby locations, and MDS-type methods try to preserve global information such as graph geodesic distances. Examples of manifold learning include different flavors of multidimensional scaling (MDS) <ref type="bibr" target="#b25">[26]</ref>, locally linear embedding (LLE) <ref type="bibr" target="#b26">[27]</ref>, stochastic neighbor embedding (t-SNE) <ref type="bibr" target="#b27">[28]</ref>, spectral embeddings such as Laplacian eigenmaps <ref type="bibr" target="#b28">[29]</ref> and diffusion maps <ref type="bibr" target="#b29">[30]</ref>, and deep models <ref type="bibr" target="#b30">[31]</ref>. Most recent approaches <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> tried to apply the successful word embedding model <ref type="bibr" target="#b34">[35]</ref> to graphs. Instead of embedding the vertices, the graph structure can be processed by decomposing it into small sub-graphs called motifs <ref type="bibr" target="#b35">[36]</ref> or graphlets <ref type="bibr" target="#b36">[37]</ref>.</p><p>In some cases, the data are presented as a manifold or graph at the outset, and the first step of constructing the affinity structure described above is unnecessary. For instance, in computer graphics and vision applications, one can analyze 3D shapes represented as meshes by constructing local geometric descriptors capturing e.g. curvature-like properties <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In network analysis applications such as computational sociology, the topological structure of the social graph representing the social relations between people carries important insights allowing, for example, to classify the vertices and detect communities <ref type="bibr" target="#b39">[40]</ref>. In natural language processing, words in a corpus can be represented by the co-occurrence graph, where two words are connected if they often appear near each other <ref type="bibr" target="#b40">[41]</ref>.</p><p>Data on a domain: Our second class of problems deals with analyzing functions defined on a given non-Euclidean domain. We can further break down such problems into two subclasses: problems where the domain is fixed and those where multiple domains are given. For example, assume that we are given the geographic coordinates of the users of a social network, Note that the notion of "manifold" in this setting can be considerably more general than a classical smooth manifold; see e.g. <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> represented as a time-dependent signal on the vertices of the social graph. An important application in location-based social networks is to predict the position of the user given his or her past behavior, as well as that of his or her friends <ref type="bibr" target="#b41">[42]</ref>. In this problem, the domain (social graph) is assumed to be fixed; methods of signal processing on graphs, which have previously been reviewed in this Magazine <ref type="bibr" target="#b42">[43]</ref>, can be applied to this setting, in particular, in order to define an operation similar to convolution in the spectral domain. This, in turn, allows generalizing CNN models to graphs <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>.</p><p>In computer graphics and vision applications, finding similarity and correspondence between shapes are examples of the second sub-class of problems: each shape is modeled as a manifold, and one has to work with multiple such domains. In this setting, a generalization of convolution in the spatial domain using local charting <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref> appears to be more appropriate.</p><p>Brief history: The main focus of this review is on this second class of problems, namely learning functions on non-Euclidean structured domains, and in particular, attempts to generalize the popular CNNs to such settings. First attempts to generalize neural networks to graphs we are aware of are due to Scarselli et al. <ref type="bibr" target="#b48">[49]</ref>, who proposed a scheme combining recurrent neural networks and random walk models. This approach went almost unnoticed, re-emerging in a modern form in <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref> due to the renewed recent interest in deep learning. The first formulation of CNNs on graphs is due to Bruna et al. <ref type="bibr" target="#b51">[52]</ref>, who used the definition of convolutions in the spectral domain. Their paper, while being of conceptual importance, came with significant computational drawbacks that fell short of a truly useful method. These drawbacks were subsequently addressed in the followup works of Henaff et al. <ref type="bibr" target="#b43">[44]</ref> and Defferrard et al. <ref type="bibr" target="#b44">[45]</ref>. In the latter paper, graph CNNs allowed achieving some state-of-the-art results.</p><p>In a parallel effort in the computer vision and graphics community, Masci et al. <ref type="bibr" target="#b46">[47]</ref> showed the first CNN model on meshed surfaces, resorting to a spatial definition of the convolution operation based on local intrinsic patches. Among other applications, such models were shown to achieve stateof-the-art performance in finding correspondence between deformable 3D shapes. Followup works proposed different construction of intrinsic patches on point clouds <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b47">[48]</ref> and general graphs <ref type="bibr" target="#b53">[54]</ref>.</p><p>The interest in deep learning on graphs or manifolds has exploded in the past year, resulting in numerous attempts to apply these methods in a broad spectrum of problems ranging from biochemistry <ref type="bibr" target="#b54">[55]</ref> to recommender systems <ref type="bibr" target="#b55">[56]</ref>. Since such applications originate in different fields that usually do not cross-fertilize, publications in this domain tend to use different terminology and notation, making it difficult for a newcomer to grasp the foundations and current state-of-the-art methods. We believe that our paper comes at the right time attempting to systemize and bring some order into the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure of the paper:</head><p>We start with an overview of traditional Euclidean deep learning in Section III, summarizing the important assumptions about the data, and how they are realized in convolutional network architectures. <ref type="bibr" target="#b1">2</ref> Going to the non-Euclidean world in Section IV, we then define basic notions in differential geometry and graph theory. These topics are insufficiently known in the signal processing community, and to our knowledge, there is no introductorylevel reference treating these so different structures in a common way. One of our goals is to provide an accessible overview of these models resorting as much as possible to the intuition of traditional signal processing.</p><p>In Sections V-VIII, we overview the main geometric deep learning paradigms, emphasizing the similarities and the differences between Euclidean and non-Euclidean learning methods. The key difference between these approaches is in the way a convolution-like operation is formulated on graphs and manifolds. One way is to resort to the analogy of the Convolution Theorem, defining the convolution in the spectral domain. An alternative is to think of the convolution as a template matching in the spatial domain. Such a distinction is, however, far from being a clear-cut: as we will see, some approaches though draw their formulation from the spectral domain, essentially boil down to applying filters in the spatial domain. It is also possible to combine these two approaches resorting to spatio-frequency analysis techniques, such as wavelets or the windowed Fourier transform.</p><p>In Section IX, we show examples of selected problems from the fields of network analysis, particle physics, recommender systems, computer vision, and graphics. In Section X, we draw conclusions and outline current main challenges and potential future research directions in geometric deep learning. To make the paper more readable, we use inserts to illustrate important concepts. Finally, the readers are invited to visit a dedicated website geometricdeeplearning.com for additional materials, data, and examples of code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEEP LEARNING ON EUCLIDEAN DOMAINS</head><p>Geometric priors:</p><formula xml:id="formula_0">Consider a compact d-dimensional Euclidean domain Ω = [0, 1] d ⊂ R d on which square- integrable functions f ∈ L 2 (Ω)</formula><p>are defined (for example, in image analysis applications, images can be thought of as functions on the unit square Ω = [0, 1] 2 ). We consider a generic supervised learning setting, in which an unknown function y :</p><formula xml:id="formula_1">L 2 (Ω) → Y is observed on a training set {f i ∈ L 2 (Ω), y i = y(f i )} i∈I .<label>(1)</label></formula><p>In a supervised classification setting, the target space Y can be thought discrete with |Y| being the number of classes. In a multiple object recognition setting, we can replace Y by the K-dimensional simplex, which represents the posterior class probabilities p(y|x). In regression tasks, we may consider Y = R m . In the vast majority of computer vision and speech analysis tasks, there are several crucial prior assumptions on the unknown function y. As we will see in the following, these assumptions are effectively exploited by convolutional neural network architectures. </p><formula xml:id="formula_2">f ∈ L 2 (Ω) Square-integrable function on Ω δ x (x), δ ij Delta function at x , Kronecker delta {f i , y i } i∈I Training set T v Translation operator τ, L τ Deformation field, operator f Fourier transform of f f g Convolution of f and g X , T X , T x X</formula><p>Manifold, its tangent bundle, tangent</p><formula xml:id="formula_3">space at x •, •, T X Riemannian metric f ∈ L 2 (X ) Scalar field on manifold X F ∈ L 2 (T X ) Tangent vector field on manifold X A * Adjoint of operator A ∇, div, ∆ Gradient, divergence, Laplace operators V, E, F</formula><p>Vertices and edges of a graph, faces of a mesh</p><formula xml:id="formula_4">w ij , W Weight matrix of a graph, f ∈ L 2 (V) Functions on vertices of a graph F ∈ L 2 (E) Functions on edges of a graph φ i , λ i Laplacian eigenfunctions, eigenvalues h t (•, •) Heat kernel Φ k Matrix of first k Laplacian eigenvectors Λ k</formula><p>Diagonal matrix of first k Laplacian eigenvalues ξ Point-wise nonlinearity (e.g. ReLU)</p><formula xml:id="formula_5">γ l,l (x), Γ l,l</formula><p>Convolutional filter in spatial and spectral domain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stationarity:</head><p>Let</p><formula xml:id="formula_6">T v f (x) = f (x − v), x, v ∈ Ω,<label>(2)</label></formula><p>be a translation operator 3 acting on functions f ∈ L 2 (Ω).</p><p>Our first assumption is that the function y is either invariant or equivariant with respect to translations, depending on the task. In the former case, we have y(</p><formula xml:id="formula_7">T v f ) = y(f ) for any f ∈ L 2 (Ω)</formula><p>and v ∈ Ω. This is typically the case in object classification tasks. In the latter, we have y(</p><formula xml:id="formula_8">T v f ) = T v y(f )</formula><p>, which is welldefined when the output of the model is a space in which translations can act upon (for example, in problems of object localization, semantic segmentation, or motion estimation). Our definition of invariance should not be confused with the traditional notion of translation invariant systems in signal processing, which corresponds to translation equivariance in our language (since the output translates whenever the input translates).</p><p>Local deformations and scale separation: Similarly, a deformation L τ , where τ : Ω → Ω is a smooth vector field, acts on</p><formula xml:id="formula_9">L 2 (Ω) as L τ f (x) = f (x − τ (x)).</formula><p>Deformations can model local translations, changes in point of view, rotations and frequency transpositions <ref type="bibr" target="#b17">[18]</ref>.</p><p>Most tasks studied in computer vision are not only translation invariant/equivariant, but also stable with respect to local deformations <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b17">[18]</ref>. In tasks that are translation invariant we have</p><formula xml:id="formula_10">|y(L τ f ) − y(f )| ≈ ∇τ ,<label>(3)</label></formula><p>for all f, τ . Here, ∇τ measures the smoothness of a given deformation field. In other words, the quantity to be predicted does not change much if the input image is slightly deformed. In tasks that are translation equivariant, we have</p><formula xml:id="formula_11">|y(L τ f ) − L τ y(f )| ≈ ∇τ .<label>(4)</label></formula><p>This property is much stronger than the previous one, since the space of local deformations has a high dimensionality, as opposed to the d-dimensional translation group. It follows from (3) that we can extract sufficient statistics at a lower spatial resolution by downsampling demodulated localized filter responses without losing approximation power. An important consequence of this is that long-range dependencies can be broken into multi-scale local interaction terms, leading to hierarchical models in which spatial resolution is progressively reduced. To illustrate this principle, denote by</p><formula xml:id="formula_12">Y (x 1 , x 2 ; v) = Prob(f (u) = x 1 and f (u + v) = x 2 ) (5)</formula><p>the joint distribution of two image pixels at an offset v from each other. In the presence of long-range dependencies, this joint distribution will not be separable for any v. However, the deformation stability prior states that</p><formula xml:id="formula_13">Y (x 1 , x 2 ; v) ≈ Y (x 1 , x 2 ; v(1 + )) for small .</formula><p>In other words, whereas long-range dependencies indeed exist in natural images and are critical to object recognition, they can be captured and down-sampled at different scales. This principle of stability to local deformations has been exploited in the computer vision community in models other than CNNs, for instance, deformable parts models <ref type="bibr" target="#b57">[58]</ref>.</p><p>In practice, the Euclidean domain Ω is discretized using a regular grid with n points; the translation and deformation operators are still well-defined so the above properties hold in the discrete setting.</p><p>Convolutional neural networks: Stationarity and stability to local translations are both leveraged in convolutional neural networks (see insert IN1). A CNN consists of several convolutional layers of the form g = C Γ (f ), acting on a pdimensional input f (x) = (f 1 (x), . . . , f p (x)) by applying a bank of filters Γ = (γ l,l ), l = 1, . . . , q, l = 1, . . . , p and point-wise non-linearity ξ,</p><formula xml:id="formula_14">g l (x) = ξ p l =1 (f l γ l,l )(x) ,<label>(6)</label></formula><p>producing a q-dimensional output g(x) = (g 1 (x), . . . , g q (x)) often referred to as the feature maps. Here,</p><formula xml:id="formula_15">(f γ)(x) = Ω f (x − x )γ(x )dx<label>(7)</label></formula><p>denotes the standard convolution. According to the local deformation prior, the filters Γ have compact spatial support.</p><p>Additionally, a downsampling or pooling layer g = P (f ) may be used, defined as <ref type="bibr" target="#b7">(8)</ref> where N (x) ⊂ Ω is a neighborhood around x and P is a permutation-invariant function such as a L p -norm (in the latter case, the choice of p = 1, 2 or ∞ results in average-, energy-, or max-pooling).</p><formula xml:id="formula_16">g l (x) = P ({f l (x ) : x ∈ N (x)}), l = 1, . . . , q,</formula><p>A convolutional network is constructed by composing several convolutional and optionally pooling layers, obtaining a generic hierarchical representation</p><formula xml:id="formula_17">U Θ (f ) = (C Γ (K) • • • P • • • • C Γ (2) • C Γ (1) )(f )<label>(9)</label></formula><p>where Θ = {Γ <ref type="bibr" target="#b0">(1)</ref> , . . . , Γ (K) } is the hyper-vector of the network parameters (all the filter coefficients). The model is said to be deep if it comprises multiple layers, though this notion is rather vague and one can find examples of CNNs with as few as a couple and as many as hundreds of layers <ref type="bibr" target="#b10">[11]</ref>. The output features enjoy translation invariance/covariance depending on whether spatial resolution is progressively lost by means of pooling or kept fixed. Moreover, if one specifies the convolutional tensors to be complex wavelet decomposition operators and uses complex modulus as pointwise nonlinearities, one can provably obtain stability to local deformations <ref type="bibr" target="#b16">[17]</ref>. Although this stability is not rigorously proved for generic compactly supported convolutional tensors, it underpins the empirical success of CNN architectures across a variety of computer vision applications <ref type="bibr" target="#b0">[1]</ref>. In supervised learning tasks, one can obtain the CNN parameters by minimizing a task-specific cost L on the training</p><formula xml:id="formula_18">set {f i , y i } i∈I , min Θ i∈I L(U Θ (f i ), y i ),<label>(10)</label></formula><p>for instance, L(x, y) = x − y . If the model is sufficiently complex and the training set is sufficiently representative, when applying the learned model to previously unseen data, one expects U (f ) ≈ y(f ). Although (10) is a non-convex optimization problem, stochastic optimization methods offer excellent empirical performance. Understanding the structure of the optimization problems (10) and finding efficient strategies for its solution is an active area of research in deep learning <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>. A key advantage of CNNs explaining their success in numerous tasks is that the geometric priors on which CNNs are based result in a learning complexity that avoids the curse of dimensionality. Thanks to the stationarity and local deformation priors, the linear operators at each layer have a constant number of parameters, independent of the input size n (number of pixels in an image). Moreover, thanks to the multiscale hierarchical property, the number of layers grows at a rate O(log n), resulting in a total learning complexity of O(log n) parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE GEOMETRY OF MANIFOLDS AND GRAPHS</head><p>Our main goal is to generalize CNN-type constructions to non-Euclidean domains. In this paper, by non-Euclidean</p><p>[IN1] Convolutional neural networks: CNNs are currently among the most successful deep learning architectures in a variety of tasks, in particular, in computer vision. A typical CNN used in computer vision applications (see FIGS1) consists of multiple convolutional layers (6), passing the input image through a set of filters Γ followed by point-wise nonlinearity ξ (typically, half-rectifiers ξ(z) = max(0, z) are used, although practitioners have experimented with a diverse range of choices <ref type="bibr" target="#b12">[13]</ref>). The model can also include a bias term, which is equivalent to adding a constant coordinate to the input. A network composed of K convolutional layers put together  <ref type="bibr" target="#b7">[8]</ref> or motion estimation <ref type="bibr" target="#b58">[59]</ref>. In applications requiring invariance, such as image classification <ref type="bibr" target="#b6">[7]</ref>, the convolutional layers are typically interleaved with pooling layers (8) progressively reducing the resolution of the image passing through the network. Alternatively, one can integrate the convolution and downsampling in a single linear operator (convolution with stride). Recently, some authors have also experimented with convolutional layers which increase the spatial resolution using interpolation kernels <ref type="bibr" target="#b59">[60]</ref>. These kernels can be learnt efficiently by mimicking the socalled algorithmeà trous <ref type="bibr" target="#b60">[61]</ref>, also referred to as dilated convolution.</p><formula xml:id="formula_19">U (f ) = (C Γ (K) . . . • C Γ (2) • C Γ (1) )(f ) produces</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Red</head><p>Green Blue [FIGS1] Typical convolutional neural network architecture used in computer vision applications (figure reproduced from <ref type="bibr" target="#b0">[1]</ref>).</p><p>domains, we refer to two prototypical structures: manifolds and graphs. While arising in very different fields of mathematics (differential geometry and graph theory, respectively), in our context, these structures share several common characteristics that we will try to emphasize throughout our review.</p><p>Manifolds: Roughly, a manifold is a space that is locally Euclidean. One of the simplest examples is a spherical surface modeling our planet: around a point, it seems to be planar, which has led generations of people to believe in the flatness of the Earth. Formally speaking, a (differentiable) d-dimensional manifold X is a topological space where each point x has a neighborhood that is topologically equivalent (homeomorphic) to a d-dimensional Euclidean space, called the tangent space and denoted by T x X (see <ref type="figure" target="#fig_1">Figure 1</ref>, top). The collection of tangent spaces at all points (more formally, their disjoint union) is referred to as the tangent bundle and denoted by T X . On each tangent space, we define an inner product</p><formula xml:id="formula_20">•, • TxX : T x X × T x X → R,</formula><p>which is additionally assumed to depend smoothly on the position x. This inner product is called a Riemannian metric in differential geometry and allows performing local measurements of angles, distances, and volumes. A manifold equipped with a metric is called a Riemannian manifold.</p><p>It is important to note that the definition of a Riemannian manifold is completely abstract and does not require a geometric realization in any space. However, a Riemannian manifold can be realized as a subset of a Euclidean space (in which case it is said to be embedded in that space) by using the structure of the Euclidean space to induce a Riemannian metric. The celebrated Nash Embedding Theorem guarantees that any sufficiently smooth Riemannian manifold can be realized in a Euclidean space of sufficiently high dimension <ref type="bibr" target="#b66">[67]</ref>. An embedding is not necessarily unique; two different realizations of a Riemannian metric are called isometries.</p><p>Two-dimensional manifolds (surfaces) embedded into R 3 are used in computer graphics and vision to describe boundary surfaces of 3D objects, colloquially referred to as '3D shapes'. This term is somewhat misleading since '3D' here refers to the dimensionality of the embedding space rather than that of the manifold. Thinking of such a shape as made of infinitely thin material, inelastic deformations that do not stretch or tear it are isometric. Isometries do not affect the metric structure of the manifold and consequently, preserve any quantities that can be expressed in terms of the Riemannian metric (called intrinsic).</p><formula xml:id="formula_21">TxX x F (x) T x X x F (x )</formula><p>Conversely, properties related to the specific realization of the manifold in the Euclidean space are called extrinsic.</p><p>As an intuitive illustration of this difference, imagine an insect that lives on a two-dimensional surface <ref type="figure" target="#fig_1">(Figure 1</ref>, bottom). A human observer, on the other hand, sees a surface in 3D space -this is an extrinsic point of view.</p><p>Calculus on manifolds: Our next step is to consider functions defined on manifolds. We are particularly interested in two types of functions: A scalar field is a smooth real function f : X → R on the manifold. A tangent vector field F : X → T X is a mapping attaching a tangent vector F (x) ∈ T x X to each point x. As we will see in the following, tangent vector fields are used to formalize the notion of infinitesimal displacements on the manifold. We define the Hilbert spaces of scalar and vector fields on manifolds, denoted by L 2 (X ) and L 2 (T X ), respectively, with the following inner products:</p><formula xml:id="formula_22">f, g L 2 (X ) = X f (x)g(x)dx; (15) F, G L 2 (T X ) = X F (x), G(x) TxX dx;<label>(16)</label></formula><p>dx denotes here a d-dimensional volume element induced by the Riemannian metric.</p><p>In calculus, the notion of derivative describes how the value of a function changes with an infinitesimal change of its argument. One of the big differences distinguishing classical calculus from differential geometry is a lack of vector space structure on the manifold, prohibiting us from naïvely using expressions like f (x+dx). The conceptual leap that is required to generalize such notions to manifolds is the need to work locally in the tangent space.</p><p>To this end, we define the differential of f as an operator df : T X → R acting on tangent vector fields. At each point x, the differential can be defined as a linear functional</p><formula xml:id="formula_23">(1-form) df (x) = ∇f (x), • TxX acting on tangent vectors F (x) ∈ T x X , which model a small displacement around x.</formula><p>The change of the function value as the result of this displacement is given by applying the form to the tangent vector, df (x)F (x) = ∇f (x), F (x) TxX , and can be thought of as an extension of the notion of the classical directional derivative.</p><p>The operator ∇f :</p><formula xml:id="formula_24">L 2 (X ) → L 2 (T X )</formula><p>in the definition above is called the intrinsic gradient, and is similar to the classical notion of the gradient defining the direction of the steepest change of the function at a point, with the only difference that the direction is now a tangent vector. Similarly, the intrinsic divergence is an operator div :</p><formula xml:id="formula_25">L 2 (T X ) → L 2 (X )</formula><p>acting on tangent vector fields and (formal) adjoint to the gradient operator <ref type="bibr" target="#b70">[71]</ref>,</p><formula xml:id="formula_26">F, ∇f L 2 (T X ) = ∇ * F, f L 2 (X ) = −divF, f L 2 (X ) . (17)</formula><p>Physically, a tangent vector field can be thought of as a flow of material on a manifold. The divergence measures the net flow of a field at a point, allowing to distinguish between field 'sources' and 'sinks'. Finally, the Laplacian (or Laplace-Beltrami operator in differential geometric jargon) ∆ :</p><formula xml:id="formula_27">L 2 (X ) → L 2 (X ) is an operator ∆f = −div(∇f )<label>(18)</label></formula><p>acting on scalar fields. Employing relation <ref type="bibr" target="#b16">(17)</ref>, it is easy to see that the Laplacian is self-adjoint (symmetric),</p><formula xml:id="formula_28">∇f, ∇f L 2 (T X ) = ∆f, f L 2 (X ) = f, ∆f L 2 (X ) .<label>(19)</label></formula><p>The lhs in equation <ref type="formula" target="#formula_28">19</ref>is known as the Dirichlet energy in physics and measures the smoothness of a scalar field on the manifold (see insert IN3). The Laplacian can be interpreted as the difference between the average of a function on an infinitesimal sphere around a point and the value of the function at the point itself. It is one of the most important operators in mathematical physics, used to describe phenomena as diverse as heat diffusion (see insert IN4), quantum mechanics, and wave propagation. As we will see in the following, the Laplacian plays a center role in signal processing and learning on non-Euclidean domains, as its eigenfunctions generalize the classical Fourier bases, allowing to perform spectral analysis on manifolds and graphs.</p><p>It is important to note that all the above definitions are coordinate free. By defining a basis in the tangent space, it is possible to express tangent vectors as d-dimensional vectors and the Riemannian metric as a d × d symmetric positivedefinite matrix.</p><p>Graphs and discrete differential operators: Another type of constructions we are interested in are graphs, which are popular models of networks, interactions, and similarities [IN2] Laplacian on discrete manifolds: In computer graphics and vision applications, two-dimensional manifolds are commonly used to model 3D shapes. There are several common ways of discretizing such manifolds. First, the manifold is assumed to be sampled at n points. Their embedding coordinates x 1 , . . . , x n are referred to as point cloud. Second, a graph is constructed upon these points, acting as its vertices. The edges of the graph represent the local connectivity of the manifold, telling whether two points belong to a neighborhood or not, e.g. with Gaussian edge weights</p><formula xml:id="formula_29">w ij = e − xi−x j 2 /2σ 2 .<label>(11)</label></formula><p>This simplest discretization, however, does not capture correctly the geometry of the underlying continuous manifold (for example, the graph Laplacian would typically not converge to the continuous Laplacian operator of the manifold with the increase of the sampling density <ref type="bibr" target="#b67">[68]</ref>). A geometrically consistent discretization is possible with an additional structure of faces F ∈</p><formula xml:id="formula_30">V × V × V, where (i, j, k) ∈ F implies (i, j), (i, k), (k, j) ∈ E.</formula><p>The collection of faces represents the underlying continuous manifold as a polyhedral surface con-sisting of small triangles glued together. The triplet (V, E, F) is referred to as triangular mesh. To be a correct discretization of a manifold (a manifold mesh), every edge must be shared by exactly two triangular faces; if the manifold has a boundary, any boundary edge must belong to exactly one triangle. On a triangular mesh, the simplest discretization of the Riemannian metric is given by assigning each edge a length ij &gt; 0, which must additionally satisfy the triangle inequality in every triangular face. The mesh Laplacian is given by formula <ref type="bibr" target="#b24">(25)</ref> with</p><formula xml:id="formula_31">w ij = − 2 ij + 2 jk + 2 ik 8a ijk + − 2 ij + 2 jh + 2 ih 8a ijh ;<label>(12)</label></formula><formula xml:id="formula_32">a i = 1 jk:(i,j,k)∈F a ijk ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_33">a ijk = s ijk (s ijk − ij )(s ijk − jk )(s ijk − ik )</formula><p>is the area of triangle ijk given by the Heron formula, and</p><formula xml:id="formula_34">s ijk = 1 ( ij + jk + ki )</formula><p>is the semi-perimeter of triangle ijk. The vertex weight a i is interpreted as the local area element (shown in red in FIGS2). Note that the weights <ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref> are expressed solely in terms of the discrete metric and are thus intrinsic. When the mesh is infinitely refined under some technical conditions, such a construction can be shown to converge to the continuous Laplacian of the underlying manifold <ref type="bibr" target="#b68">[69]</ref>. An embedding of the mesh (amounting to specifying the vertex coordinates x 1 , . . . , x n ) induces a discrete metric ij = x i − x j 2 , whereby (12) become the cotangent weights</p><formula xml:id="formula_35">w ij = 1 2 (cot α ij + cot β ij )<label>(14)</label></formula><p>ubiquitously used in computer graphics <ref type="bibr" target="#b69">[70]</ref>.</p><p>between different objects. For simplicity, we will consider weighted undirected graphs, formally defined as a pair (V, E), where V = {1, . . . , n} is the set of n vertices, and E ⊆ V × V is the set of edges, where the graph being undirected implies that (i, j) ∈ E iff (j, i) ∈ E. Furthermore, we associate a weight a i &gt; 0 with each vertex i ∈ V, and a weight w ij ≥ 0 with each edge (i, j) ∈ E. Real functions f : V → R and F : E → R on the vertices and edges of the graph, respectively, are roughly the discrete analogy of continuous scalar and tangent vector fields in differential geometry. <ref type="bibr" target="#b3">4</ref> We can define Hilbert spaces L 2 (V) and L 2 (E) of such functions by specifying the respective inner products,</p><formula xml:id="formula_36">f, g L 2 (V) = i∈V a i f i g i ; (20) F, G L 2 (E) = i∈E w ij F ij G ij .<label>(21)</label></formula><p>Let f ∈ L 2 (V) and F ∈ L 2 (E) be functions on the <ref type="bibr" target="#b3">4</ref> It is tacitly assumed here that F is alternating, i.e., F ij = −F ji .</p><p>vertices and edges of the graphs, respectively. We can define differential operators acting on such functions analogously to differential operators on manifolds <ref type="bibr" target="#b71">[72]</ref>. The graph gradient is an operator ∇ :</p><formula xml:id="formula_37">L 2 (V) → L 2 (E)</formula><p>mapping functions defined on vertices to functions defined on edges,</p><formula xml:id="formula_38">(∇f ) ij = f i − f j ,<label>(22)</label></formula><p>automatically satisfying (∇f ) ij = −(∇f ) ji . The graph divergence is an operator div :</p><formula xml:id="formula_39">L 2 (E) → L 2 (V) doing the converse, (divF ) i = 1 a i j:(i,j)∈E w ij F ij .<label>(23)</label></formula><p>It is easy to verify that the two operators are adjoint w.r.t. the inner products (20)</p><formula xml:id="formula_40">-(21), F, ∇f L 2 (E) = ∇ * F, f L 2 (V) = −divF, f L 2 (V) .<label>(24)</label></formula><p>The graph Laplacian is an operator ∆ : <ref type="bibr" target="#b22">(23)</ref>, it can be expressed in the familiar form</p><formula xml:id="formula_41">L 2 (V) → L 2 (V) defined as ∆ = −div ∇. Combining definitions (22)-</formula><formula xml:id="formula_42">(∆f ) i = 1 a i (i,j)∈E w ij (f i − f j ).<label>(25)</label></formula><p>Note that formula <ref type="bibr" target="#b24">(25)</ref> captures the intuitive geometric interpretation of the Laplacian as the difference between the local average of a function around a point and the value of the function at the point itself.</p><p>Denoting by W = (w ij ) the n × n matrix of edge weights (it is assumed that w ij = 0 if (i, j) / ∈ E), by A = diag(a 1 , . . . , a n ) the diagonal matrix of vertex weights, and by D = diag( j:j =i w ij ) the degree matrix, the graph Laplacian application to a function f ∈ L 2 (V) represented as a column vector f = (f 1 , . . . , f n ) can be written in matrixvector form as</p><formula xml:id="formula_43">∆f = A −1 (D − W)f .<label>(26)</label></formula><p>The choice of A = I in (26) is referred to as the unnormalized graph Laplacian; another popular choice is A = D producing the random walk Laplacian <ref type="bibr" target="#b72">[73]</ref>.</p><p>Discrete manifolds: As we mentioned, there are many practical situations in which one is given a sampling of points arising from a manifold but not the manifold itself. In computer graphics applications, reconstructing a correct discretization of a manifold from a point cloud is a difficult problem of its own, referred to a meshing (see insert IN2).</p><p>In manifold learning problems, the manifold is typically approximated as a graph capturing the local affinity structure. We warn the reader that the term "manifold" as used in the context of generic data science is not geometrically rigorous, and can have less structure than a classical smooth manifold we have defined beforehand. For example, a set of points that "looks locally Euclidean" in practice may have self intersections, infinite curvature, different dimensions depending on the scale and location at which one looks, extreme variations in density, and "noise" with confounding structure.</p><p>Fourier analysis on non-Euclidean domains: The Laplacian operator is a self-adjoint positive-semidefinite operator, admitting on a compact domain <ref type="bibr" target="#b4">5</ref> an eigendecomposition with a discrete set of orthonormal eigenfunctions φ 0 , φ 1 , . . . (satisfying φ i , φ j L 2 (X ) = δ ij ) and non-negative real eigenvalues 0 = λ 0 ≤ λ 1 ≤ . . . (referred to as the spectrum of the Laplacian),</p><formula xml:id="formula_44">∆φ i = λ i φ i , i = 0, 1, . . .<label>(31)</label></formula><p>The eigenfunctions are the smoothest functions in the sense of the Dirichlet energy (see insert IN3) and can be interpreted as a generalization of the standard Fourier basis (given, in fact, by the eigenfunctions of the 1D Euclidean Laplacian,</p><formula xml:id="formula_45">− d 2</formula><p>x 2 e iωx = ω 2 e iωx ) to a non-Euclidean domain. It is important to emphasize that the Laplacian eigenbasis is intrinsic due to the intrinsic construction of the Laplacian itself.</p><p>A square-integrable function f on X can be decomposed into Fourier series as</p><formula xml:id="formula_46">f (x) = i≥0 f, φ i L 2 (X ) f i φ i (x),<label>(32)</label></formula><p>where the projection on the basis functions producing a discrete set of Fourier coefficients (f i ) generalizes the analysis (forward transform) stage in classical signal processing, and summing up the basis functions with these coefficients is the synthesis (inverse transform) stage. A centerpiece of classical Euclidean signal processing is the property of the Fourier transform diagonalizing the convolution operator, colloquially referred to as the Convolution Theorem. This property allows to express the convolution f g of two functions in the spectral domain as the element-wise product of their Fourier transforms,</p><formula xml:id="formula_47">( f g)(ω) = ∞ −∞ f (x)e −iωx dx ∞ −∞ g(x)e −iωx dx.(33)</formula><p>Unfortunately, in the non-Euclidean case we cannot even define the operation x − x on the manifold or graph, so the notion of convolution (7) does not directly extend to this case. One possibility to generalize convolution to non-Euclidean domains is by using the Convolution Theorem as a definition,</p><formula xml:id="formula_48">(f g)(x) = i≥0 f, φ i L 2 (X ) g, φ i L 2 (X ) φ i (x). (34)</formula><p>One of the key differences of such a construction from the classical convolution is the lack of shift-invariance. In terms of signal processing, it can be interpreted as a position-dependent filter. While parametrized by a fixed number of coefficients in the frequency domain, the spatial representation of the filter can vary dramatically at different points (see FIGS4).</p><p>The discussion above also applies to graphs instead of manifolds, where one only has to replace the inner product in equations <ref type="bibr" target="#b31">(32)</ref> and <ref type="bibr" target="#b33">(34)</ref> with the discrete one <ref type="bibr" target="#b19">(20)</ref>. All the sums over i would become finite, as the graph Laplacian ∆ has n eigenvectors. In matrix-vector notation, the generalized convolution f g can be expressed as Gf = Φ diag(ĝ)Φ f , whereĝ = (ĝ 1 , . . . ,ĝ n ) is the spectral representation of the filter and Φ = (φ 1 , . . . , φ n ) denotes the Laplacian eigenvectors <ref type="bibr" target="#b29">(30)</ref>. The lack of shift invariance results in the absence of circulant (Toeplitz) structure in the matrix G, which characterizes the Euclidean setting. Furthermore, it is easy to see that the convolution operation commutes with the Laplacian, G∆f = ∆Gf .</p><p>Uniqueness and stability: Finally, it is important to note that the Laplacian eigenfunctions are not uniquely defined. To start with, they are defined up to sign, i.e., ∆(±φ) = λ(±φ). Thus, even isometric domains might have different Laplacian eigenfunctions. Furthermore, if a Laplacian eigenvalue has multiplicity, then the associated eigenfunctions can be defined as orthonormal basis spanning the corresponding eigensubspace (or said differently, the eigenfunctions are defined up to an orthogonal transformation in the eigen-subspace). A small perturbation of the domain can lead to very large changes in the Laplacian eigenvectors, especially those associated with high frequencies. At the same time, the definition of heat kernels <ref type="bibr" target="#b35">(36)</ref> and diffusion distances <ref type="bibr" target="#b37">(38)</ref> does not suffer from these ambiguities -for example, the sign ambiguity disappears as the eigenfunctions are squared. Heat kernels also appear to be robust to domain perturbations.</p><p>[IN3] Physical interpretation of Laplacian eigenfunctions: Given a function f on the domain X , the Dirichlet energy <ref type="bibr" target="#b26">(27)</ref> measures how smooth it is (the last identity in <ref type="bibr" target="#b26">(27)</ref> stems from <ref type="bibr" target="#b18">(19)</ref>). We are looking for an orthonormal basis on X , containing k smoothest possible functions (FIGS3), by solving the optimization problem</p><formula xml:id="formula_49">E Dir (f ) = X ∇f (x) 2 TxX dx = X f (x)∆f (x)dx,</formula><formula xml:id="formula_50">min φ0 E Dir (φ 0 ) s.t. φ 0 = 1 (28) min φi E Dir (φ i ) s.t. φ i = 1, i = 1, 2, . . . k − 1 φ i ⊥ span{φ 0 , . . . , φ i−1 }.</formula><p>In the discrete setting, when the domain is sampled at n points, problem (28) can be rewritten as</p><formula xml:id="formula_51">min Φ k ∈R n×k trace(Φ k ∆Φ k ) s.t. Φ k Φ k = I,<label>(29)</label></formula><p>where</p><formula xml:id="formula_52">Φ k = (φ 0 , . . . φ k−1 )</formula><p>. The solution of (29) is given by the first k eigenvectors of ∆ satisfying</p><formula xml:id="formula_53">∆Φ k = Φ k Λ k ,<label>(30)</label></formula><p>where Λ k = diag(λ 0 , . . . , λ k−1 ) is the diagonal matrix of corresponding eigenvalues. The eigenvalues = λ 0 ≤ λ 1 ≤ . . . λ k−1 are non-negative due to the positive-semidefiniteness of the Laplacian and can be interpreted as 'frequencies', where φ 0 = const with the corresponding eigenvalue λ 0 = 0 play the role of the DC. The Laplacian eigendecomposition can be carried out in two ways. First, equation <ref type="formula" target="#formula_53">30</ref>can be rewritten as a generalized eigenproblem (D − W)Φ k = AΦ k Λ k , resulting in A-orthogonal eigenvectors, Φ k AΦ k = I. Alternatively, introducing a change of variables Ψ k = A 1/2 Φ k , we can obtain a standard eigendecomposition problem </p><formula xml:id="formula_54">A −1/2 (D − W)A −1/2 Ψ k = Ψ k Λ k with orthogonal eigen- vectors Ψ k Ψ k = I. When A = D is used, the matrix ∆ = A −1/2 (D − W)A −1/</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SPECTRAL METHODS</head><p>We have now finally got to our main goal, namely, constructing a generalization of the CNN architecture on non-Euclidean domains. We will start with the assumption that the domain on which we are working is fixed, and for the rest of this section will use the problem of classification of a signal on a fixed graph as the prototypical application.</p><p>We have seen that convolutions are linear operators that commute with the Laplacian operator. Therefore, given a weighted graph, a first route to generalize a convolutional architecture is by first restricting our interest to linear operators that commute with the graph Laplacian. This property, in turn, implies operating on the spectrum of the graph weights, given by the eigenvectors of the graph Laplacian.</p><p>Spectral CNN (SCNN) <ref type="bibr" target="#b51">[52]</ref>: Similarly to the convolutional layer (6) of a classical Euclidean CNN, Bruna et al. <ref type="bibr" target="#b51">[52]</ref> define</p><p>[IN4] Heat diffusion on non-Euclidean domains: An important application of spectral analysis, and historically, the main motivation for its development by Joseph Fourier, is the solution of partial differential equations (PDEs). In particular, we are interested in heat propagation on non-Euclidean domains. This process is governed by the heat diffusion equation, which in the simplest setting of homogeneous and isotropic diffusion has the form</p><formula xml:id="formula_55">f t (x, t) = −c∆f (x, t) f (x, 0) = f 0 (x) (Initial condition)<label>(35)</label></formula><p>with additional boundary conditions if the domain has a boundary. f (x, t) represents the temperature at point x at time t. Equation <ref type="formula" target="#formula_55">35</ref>encodes the Newton's law of cooling, according to which the rate of temperature change of a body (lhs) is proportional to the difference between its own temperature and that of the surrounding (rhs). The proportion coefficient c is referred to as the thermal diffusivity constant; here, we assume it to be equal to one for the sake of simplicity.</p><p>The solution of (35) is given by applying the heat operator H t = e −t∆ to the initial condition and can be expressed in the spectral domain as The 'cross-talk' between two heat kernels positioned at points</p><formula xml:id="formula_56">f (x, t) = e −t∆ f 0 (x) = i≥0 f 0 , φ i L 2 (X ) e −tλi φ i (x)(36) = X f 0 (x ) i≥0 e −tλi φ i (x)φ i (x ) ht(x,x ) dx . h t (x, x )</formula><p>x and x allows to measure an intrinsic distance</p><formula xml:id="formula_57">d 2 t (x, x ) = X (h t (x, y) − h t (x , y)) 2 dy (37) = i≥0 e −2tλi (φ i (x) − φ i (x )) 2<label>(38)</label></formula><p>referred to as the diffusion distance <ref type="bibr" target="#b29">[30]</ref>. Note that interpreting <ref type="bibr" target="#b36">(37)</ref> and <ref type="bibr" target="#b37">(38)</ref> as spatial-and frequency-domain norms • L 2 (X ) and • 2 , respectively, their equivalence is the consequence of the Parseval identity. Unlike geodesic distance that measures the length of the shortest path on the manifold or graph, the diffusion distance has an effect of averaging over different paths. It is thus more robust to perturbations of the domain, for example, introduction or removal of edges in a graph, or 'cuts' on a manifold. max 0</p><p>[FIGS4] Examples of heat kernels on non-Euclidean domains (manifold, top; and graph, bottom). Observe how moving the heat kernel to a different location changes its shape, which is an indication of the lack of shift-invariance. a spectral convolutional layer as</p><formula xml:id="formula_58">g l = ξ q l =1 Φ k Γ l,l Φ k f l ,<label>(39)</label></formula><p>where the n × p and n × q matrices F = (f 1 , . . . , f p ) and G = (g 1 , . . . , g q ) represent the pand q-dimensional input and output signals on the vertices of the graph, respectively (we use n = |V| to denote the number of vertices in the graph), Γ l,l is a k × k diagonal matrix of spectral multipliers representing a filter in the frequency domain, and ξ is a nonlinearity applied on the vertex-wise function values. Using only the first k eigenvectors in (39) sets a cutoff frequency which depends on the intrinsic regularity of the graph and also the sample size. Typically, k n, since only the first Laplacian eigenvectors describing the smooth structure of the graph are useful in practice.</p><p>If the graph has an underlying group invariance, such a construction can discover it. In particular, standard CNNs can be redefined from the spectral domain (see insert IN5). However, in many cases the graph does not have a group structure, or the group structure does not commute with the Laplacian, and so we cannot think of each filter as passing a template across V and recording the correlation of the template with that location.</p><p>We should stress that a fundamental limitation of the spectral construction is its limitation to a single domain. The reason is that spectral filter coefficients <ref type="bibr" target="#b38">(39)</ref> are basis dependent. It implies that if we learn a filter w.r.t. basis Φ k on one domain, and then try to apply it on another domain with another basis Ψ k , the result could be very different (see <ref type="figure" target="#fig_4">Figure 2</ref> and insert IN6). It is possible to construct compatible orthogonal bases across different domains resorting to a joint diagonalization procedure <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref>. However, such a construction requires the knowledge of some correspondence between the domains. In applications such as social network analysis, for example, where dealing with two time instances of a social graph in which new vertices and edges have been added, such a correspondence can be easily computed and is therefore a reasonable assumption. Conversely, in computer graphics applications, finding correspondence between shapes is in itself a very hard problem, so assuming known correspondence between the domains is a rather unreasonable assumption. A toy example illustrating the difficulty of generalizing spectral filtering across non-Euclidean domains. Left: a function defined on a manifold (function values are represented by color); middle: result of the application of an edge-detection filter in the frequency domain; right: the same filter applied on the same function but on a different (nearly-isometric) domain produces a completely different result. The reason for this behavior is that the Fourier basis is domain-dependent, and the filter coefficients learnt on one domain cannot be applied to another one in a straightforward manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Basis Signal</head><formula xml:id="formula_59">X Φ f X Φ ΦWΦ f Y Ψ ΨWΨ f</formula><p>Assuming that k = O(n) eigenvectors of the Laplacian are kept, a convolutional layer (39) requires pqk = O(n) parameters to train. We will see next how the global and local regularity of the graph can be combined to produce layers with constant number of parameters (i.e., such that the number of learnable parameters per layer does not depend upon the size of the input), which is the case in classical Euclidean CNNs.</p><p>The non-Euclidean analogy of pooling is graph coarsening, in which only a fraction α &lt; 1 of the graph vertices is retained. The eigenvectors of graph Laplacians at two different resolutions are related by the following multigrid property: Let Φ,Φ denote the n × n and αn × αn matrices of Laplacian eigenvectors of the original and the coarsened graph, respectively. Then,Φ</p><formula xml:id="formula_60">≈ PΦ I αn 0 ,<label>(40)</label></formula><p>where P is a αn × n binary matrix whose ith row encodes the position of the ith vertex of the coarse graph on the original graph. It follows that strided convolutions can be generalized using the spectral construction by keeping only the low-frequency components of the spectrum. This property also allows us to interpret (via interpolation) the local filters at deeper layers in the spatial construction to be low frequency. However, since in (39) the non-linearity is applied in the spatial domain, in practice one has to recompute the graph Laplacian eigenvectors at each resolution and apply them directly after each pooling step. The spectral construction (39) assigns a degree of freedom for each eigenvector of the graph Laplacian. In most graphs, individual high-frequency eigenvectors become highly unstable. However, similarly as the wavelet construction in Euclidean domains, by appropriately grouping high frequency eigenvectors in each octave one can recover meaningful and stable information. As we shall see next, this principle also entails better learning complexity.</p><p>Spectral CNN with smooth spectral multipliers <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b43">[44]</ref>: In order to reduce the risk of overfitting, it is important to adapt the learning complexity to reduce the number of free parameters of the model. On Euclidean domains, this is achieved by learning convolutional kernels with small spatial support, which enables the model to learn a number of parameters independent of the input size. In order to achieve a similar learning complexity in the spectral domain, it is thus necessary to restrict the class of spectral multipliers to those corresponding to localized filters.</p><p>For that purpose, we have to express spatial localization of filters in the frequency domain. In the Euclidean case, smoothness in the frequency domain corresponds to spatial decay, since</p><formula xml:id="formula_61">+∞ −∞ |x| 2k |f (x)| 2 dx = +∞ −∞ ∂ kf (ω) ∂ω k 2 dω,<label>(42)</label></formula><p>by virtue of the Parseval Identity. This suggests that, in order to learn a layer in which features will be not only shared across locations but also well localized in the original domain, one can learn spectral multipliers which are smooth. Smoothness can be prescribed by learning only a subsampled set of frequency multipliers and using an interpolation kernel to obtain the rest, such as cubic splines. However, the notion of smoothness also requires some geometry in the spectral domain. In the Euclidean setting, such a geometry naturally arises from the notion of frequency; for example, in the plane, the similarity between two Fourier atoms e iω x and e iω x can be quantified by the distance ω − ω , where x denotes the two-dimensional planar coordinates, and ω is the two-dimensional frequency vector. On graphs, such a relation can be defined by means of a dual graph with weightsw ij encoding the similarity between two eigenvectors φ i and φ j .</p><p>A particularly simple choice consists in choosing a onedimensional arrangement, obtained by ordering the eigenvec-</p><formula xml:id="formula_62">[IN5]</formula><p>Rediscovering standard CNNs using correlation kernels: In situations where the graph is constructed from the data, a straightforward choice of the edge weights (11) of the graph is the covariance of the data. Let F denote the input data distribution and</p><formula xml:id="formula_63">Σ = E(F − EF)(F − EF)<label>(41)</label></formula><p>be the data covariance matrix. If each point has the same variance σ ii = σ 2 , then diagonal operators on the Laplacian simply scale the principal components of F. In natural images, since their distribution is approximately stationary, the covariance matrix has a circulant structure σ ij ≈ σ i−j and is thus diagonalized by the standard Discrete Cosine Transform (DCT) basis. It follows that the principal components of F roughly correspond to the DCT basis vectors ordered by frequency. Moreover, natural images exhibit a power spectrum E| f (ω)| 2 ∼ |ω| −2 , since nearby pixels are more correlated than far away pixels <ref type="bibr" target="#b13">[14]</ref>. It results that principal components of the covariance are essentially ordered from low to high frequencies, which is consistent with the standard group structure of the Fourier basis. When applied to natural images represented as graphs with weights defined by the covariance, the spectral CNN construction recovers the standard CNN, without any prior knowledge <ref type="bibr" target="#b75">[76]</ref>. Indeed, the linear operators ΦΓ l,l Φ in <ref type="bibr" target="#b38">(39)</ref> are by the previous argument diagonal in the Fourier basis, hence translation invariant, hence classical convolutions. Furthermore, Section VI explains how spatial subsampling can also be obtained via dropping the last part of the spectrum of the Laplacian, leading to pooling, and ultimately to standard CNNs.</p><p>[FIG5a] Two-dimensional embedding of pixels in 16 × 16 image patches using a Euclidean RBF kernel. The RBF kernel is constructed as in <ref type="bibr" target="#b10">(11)</ref>, by using the covariance σij as Euclidean distance between two features. The pixels are embedded in a 2D space using the first two eigenvectors of the resulting graph Laplacian. The colors in the left and right figure represent the horizontal and vertical coordinates of the pixels, respectively. The spatial arrangement of pixels is roughly recovered from correlation measurements.</p><p>tors according to their eigenvalues. <ref type="bibr" target="#b5">6</ref> In this setting, the spectral multipliers are parametrized as</p><formula xml:id="formula_64">diag(Γ l,l ) = Bα l,l ,<label>(43)</label></formula><p>where B = (b ij ) = (β j (λ i )) is a k × q fixed interpolation kernel (e.g., β j (λ) can be cubic splines) and α is a vector of q interpolation coefficients. In order to obtain filters with constant spatial support (i.e., independent of the input size n), one should choose a sampling step γ ∼ n in the spectral domain, which results in a constant number nγ −1 = O(1) of coefficients α l,l per filter. Therefore, by combining spectral layers with graph coarsening, this model has O(log n) total trainable parameters for inputs of size n, thus recovering the same learning complexity as CNNs on Euclidean grids. Even with such a parametrization of the filters, the spectral CNN (39) entails a high computational complexity of performing forward and backward passes, since they require an expensive step of matrix multiplication by Φ k and Φ k . While on Euclidean domains such a multiplication can be efficiently carried in O(n log n) operations using FFT-type algorithms, for general graphs such algorithms do not exist and the complexity is O(n 2 ). We will see next how to alleviate</p><p>In the mentioned 2D example, this would correspond to ordering the Fourier basis function according to the sum of the corresponding frequencies ω 1 + ω 2 . Although numerical results on simple low-dimensional graphs show that the 1D arrangement given by the spectrum of the Laplacian is efficient at creating spatially localized filters <ref type="bibr" target="#b51">[52]</ref>, an open fundamental question is how to define a dual graph on the eigenvectors of the Laplacian in which smoothness (obtained by applying the diffusion operator) corresponds to localization in the original graph.</p><p>this cost by avoiding explicit computation of the Laplacian eigenvectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SPECTRUM-FREE METHODS</head><p>A polynomial of the Laplacian acts as a polynomial on the eigenvalues. Thus, instead of explicitly operating in the frequency domain with spectral multipliers as in equation <ref type="formula" target="#formula_64">43</ref>, it is possible to represent the filters via a polynomial expansion:</p><formula xml:id="formula_65">g α (∆) = Φg α (Λ)Φ ,<label>(44)</label></formula><p>corresponding to</p><formula xml:id="formula_66">g α (λ) = r−1 j=0 α j λ j .<label>(45)</label></formula><p>Here α is the r-dimensional vector of polynomial coefficients, and g α (Λ) = diag(g α (λ 1 ), . . . , g α (λ n )), resulting in filter matrices Γ l,l = g α l,l (Λ) whose entries have an explicit form in terms of the eigenvalues. An important property of this representation is that it automatically yields localized filters, for the following reason. Since the Laplacian is a local operator (working on 1-hop neighborhoods), the action of its jth power is constrained to j-hops. Since the filter is a linear combination of powers of the Laplacian, overall (45) behaves like a diffusion operator limited to r-hops around each vertex.</p><p>Graph CNN (GCNN) a.k.a. ChebNet <ref type="bibr" target="#b44">[45]</ref>: Defferrard et al. used Chebyshev polynomial generated by the recurrence relation</p><formula xml:id="formula_67">T j (λ) = 2λT j−1 (λ) − T j−2 (λ); (46) T 0 (λ) = 1; T 1 (λ) = λ.</formula><p>A filter can thus be parameterized uniquely via an expansion of order r − 1 such that Denotingf (j) = T j (∆)f , we can use the recurrence relation <ref type="bibr" target="#b45">(46)</ref> to computef (j) = 2∆f (j−1) −f (j−2) with f (0) = f andf <ref type="bibr" target="#b0">(1)</ref> =∆f . The computational complexity of this procedure is therefore O(rn) operations and does not require an explicit computation of the Laplacian eigenvectors.</p><formula xml:id="formula_68">g α (∆) = r−1 j=0 α j ΦT j (Λ)Φ (47) = r−1 j=0 α j T j (∆), where∆ = 2λ −1 n ∆ − I andΛ = 2λ −1 n Λ − I</formula><p>Graph Convolutional Network (GCN) <ref type="bibr" target="#b76">[77]</ref>: Kipf and Welling simplified this construction by further assuming r = 2 and λ n ≈ 2, resulting in filters of the form</p><formula xml:id="formula_69">g α (f ) = α 0 f + α 1 (∆ − I)f = α 0 f − α 1 D −1/2 WD −1/2 f .<label>(48)</label></formula><p>Further constraining α = α 0 = −α 1 , one obtains filters represented by a single parameter,</p><formula xml:id="formula_70">g α (f ) = α(I + D −1/2 WD −1/2 )f .<label>(49)</label></formula><p>Since the eigenvalues of I + D −1/2 WD −1/2 are now in the range [0, 2], repeated application of such a filter can result in numerical instability. This can be remedied by a renormalization</p><formula xml:id="formula_71">g α (f ) = αD −1/2WD−1/2 f ,<label>(50)</label></formula><p>whereW = W + I andD = diag( j =iw ij ).</p><p>Note that though we arrived at the constructions of ChebNet and GCN starting in the spectral domain, they boil down to applying simple filters acting on the ror 1-hop neighborhood of the graph in the spatial domain. We consider these constructions to be examples of the more general Graph Neural Network (GNN) framework:</p><p>Graph Neural Network (GNN) <ref type="bibr" target="#b77">[78]</ref>: Graph Neural Networks generalize the notion of applying the filtering operations directly on the graph via the graph weights. Similarly as Euclidean CNNs learn generic filters as linear combinations of localized, oriented bandpass and lowpass filters, a Graph Neural Network learns at each layer a generic linear combination of graph low-pass and high-pass operators. These are given respectively by f → Wf and f → ∆f , and are thus generated by the degree matrix D and the diffusion matrix W. Given a p-dimensional input signal on the vertices of the graph, represented by the n × p matrix F, the GNN considers a generic nonlinear function η θ : R p ×R p → R q , parametrized by trainable parameters θ that is applied to all nodes of the graph,</p><formula xml:id="formula_72">g i = η θ ((Wf ) i , (Df ) i ) .<label>(51)</label></formula><p>In particular, choosing η(a, b) = b−a one recovers the Laplacian operator ∆f , but more general, nonlinear choices for η yield trainable, task-specific diffusion operators. Similarly as with a CNN architecture, one can stack the resulting GNN layers g = C θ (f ) and interleave them with graph pooling operators. Chebyshev polynomials T r (∆) can be obtained with r layers of (51), making it possible, in principle, to consider ChebNet and GCN as particular instances of the GNN framework.</p><p>Historically, a version of GNN was the first formulation of deep learning on graphs, proposed in <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b77">[78]</ref>. These works optimized over the parameterized steady state of some diffusion process (or random walk) on the graph. This can be interpreted as in equation <ref type="formula" target="#formula_72">51</ref>, but using a large number of layers where each C θ is identical, as the forwards through the C θ approximate the steady state. Recent works <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref> relax the requirements of approaching the steady state or using repeated applications of the same C θ .</p><p>Because the communication at each layer is local to a vertex neighborhood, one may worry that it would take many layers to get information from one part of the graph to another, requiring multiple hops (indeed, this was one of the reasons for the use of the steady state in <ref type="bibr" target="#b77">[78]</ref>). However, for many applications, it is not necessary for information to completely traverse the graph. Furthermore, note that the graphs at each layer of the network need not be the same. Thus we can replace the original neighborhood structure with one's favorite multiscale coarsening of the input graph, and operate on that to obtain the same flow of information as with the convolutional nets above (or rather more like a "locally connected network" <ref type="bibr" target="#b80">[81]</ref>). This also allows producing a single output for the whole graph (for "translation-invariant" tasks), rather than a pervertex output, by connecting each to a special output node. Alternatively, one can allow η to use not only Wf and ∆f at each node, but also W s f for several diffusion scales s &gt; 1, (as in <ref type="bibr" target="#b44">[45]</ref>), giving the GNN the ability to learn algorithms such as the power method, and more directly accessing spectral properties of the graph.</p><p>The GNN model can be further generalized to replicate other operators on graphs. For instance, the point-wise nonlinearity η can depend on the vertex type, allowing extremely rich architectures <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CHARTING-BASED METHODS</head><p>We will now consider the second sub-class of non-Euclidean learning problems, where we are given multiple domains. A prototypical application the reader should have in mind throughout this section is the problem of finding correspondence between shapes, modeled as manifolds (see insert IN7). As we have seen, defining convolution in the frequency domain has an inherent drawback of inability to adapt the model across different domains. We will therefore need to resort to an alternative generalization of the convolution in the spatial domain that does not suffer from this drawback.</p><p>Furthermore, note that in the setting of multiple domains, there is no immediate way to define a meaningful spatial pooling operation, as the number of points on different domains can vary, and their order be arbitrary. It is however possible to pool point-wise features produced by a network by aggregating all the local information into a single vector. One possibility for such a pooling is computing the statistics of the point-wise features, e.g. the mean or covariance <ref type="bibr" target="#b46">[47]</ref>. Note that after such a pooling all the spatial information is lost.</p><p>On a Euclidean domain, due to shift-invariance the convolution can be thought of as passing a template at each point of the domain and recording the correlation of the template with the function at that point. Thinking of image filtering, this amounts to extracting a (typically square) patch of pixels, multiplying it element-wise with a template and summing up the results, then moving to the next position in a sliding window manner. Shift-invariance implies that the very operation of extracting the patch at each position is always the same.</p><p>One of the major problems in applying the same paradigm to non-Euclidean domains is the lack of shift-invariance, implying that the 'patch operator' extracting a local 'patch' would be position-dependent. Furthermore, the typical lack of meaningful global parametrization for a graph or manifold forces to represent the patch in some local intrinsic system of coordinates. Such a mapping can be obtained by defining a set of weighting functions</p><formula xml:id="formula_73">v 1 (x, •), . . . , v J (x, •)</formula><p>localized to positions near x (see examples in <ref type="figure" target="#fig_6">Figure 3</ref>). Extracting a patch amounts to averaging the function f at each point by these weights,</p><formula xml:id="formula_74">D j (x)f = X f (x )v j (x, x )dx , j = 1, . . . , J, (52)</formula><p>providing for a spatial definition of an intrinsic equivalent of convolution</p><formula xml:id="formula_75">(f g)(x) = j g j D j (x)f,<label>(53)</label></formula><p>where g denotes the template coefficients applied on the patch extracted at each point. Overall, (52)-(53) act as a kind of nonlinear filtering of f , and the patch operator D is specified by defining the weighting functions v 1 , . . . , v J . Such filters are localized by construction, and the number of parameters is equal to the number of weighting functions J = O(1). Several frameworks for non-Euclidean CNNs essentially amount to different choice of these weights. The spectrum-free methods (ChebNet and GCN) described in the previous section can also be thought of in terms of local weighting functions, as it is easy to see the analogy between formulae (53) and <ref type="bibr" target="#b46">(47)</ref>. Geodesic CNN <ref type="bibr" target="#b46">[47]</ref>: Since manifolds naturally come with a low-dimensional tangent space associated with each point, it is natural to work in a local system of coordinates in the tangent space. In particular, on two-dimensional manifolds one can create a polar system of coordinates around x where the radial coordinate is given by some intrinsic distance ρ(x ) = <ref type="figure">d(x, x )</ref>, and the angular coordinate θ(x) is obtained by ray shooting from a point at equi-spaced angles. The weighting functions in this case can be obtained as a product of Gaussians</p><formula xml:id="formula_76">v ij (x, x ) = e −(ρ(x )−ρi) 2 /2σ 2 ρ e −(θ(x )−θj ) /2σ 2 θ ,<label>(54)</label></formula><p>where i = 1, . . . , J and j = 1, . . . , J denote the indices of the radial and angular bins, respectively. The resulting JJ weights are bins of width σ ρ × σ θ in the polar coordinates <ref type="figure" target="#fig_6">(Figure 3, right)</ref>. Anisotropic CNN <ref type="bibr" target="#b47">[48]</ref>: We have already seen the non-Euclidean heat equation <ref type="formula" target="#formula_55">35</ref>, whose heat kernel h t (x, •) produces localized blob-like weights around the point x (see FIGS4). Varying the diffusion time t controls the spread of the kernel. However, such kernels are isotropic, meaning that the heat flows equally fast in all the directions. A more general anisotropic diffusion equation on a manifold</p><formula xml:id="formula_77">f t (x, t) = −div(A(x)∇f (x, t)),<label>(55)</label></formula><p>involves the thermal conductivity tensor A(x) (in case of twodimensional manifolds, a 2 × 2 matrix applied to the intrinsic gradient in the tangent plane at each point), allowing modeling heat flow that is position-and direction-dependent <ref type="bibr" target="#b81">[82]</ref>. A particular choice of the heat conductivity tensor proposed in <ref type="bibr" target="#b52">[53]</ref> is</p><formula xml:id="formula_78">A αθ (x) = R θ (x) α 1 R θ (x),<label>(56)</label></formula><p>where the 2 × 2 matrix R θ (x) performs rotation of θ w.r.t. to some reference (e.g. the maximum curvature) direction and α &gt; 0 is a parameter controlling the degree of anisotropy (α = 1 corresponds to the classical isotropic case). The heat kernel of such anisotropic diffusion equation is given by the spectral expansion</p><formula xml:id="formula_79">h αθt (x, x ) = i≥0 e −tλ αθi φ αθi (x)φ αθi (x ),<label>(57)</label></formula><p>where φ αθ0 (x), φ αθ1 (x), . . . are the eigenfunctions and λ αθ0 , λ αθ1 , . . . the corresponding eigenvalues of the anisotropic Laplacian</p><formula xml:id="formula_80">∆ αθ f (x) = −div(A αθ (x)∇f (x)).<label>(58)</label></formula><p>The discretization of the anisotropic Laplacian is a modification of the cotangent formula (14) on meshes or graph Laplacian (11) on point clouds <ref type="bibr" target="#b47">[48]</ref>. The anisotropic heat kernels h αθt (x, •) look like elongated rotated blobs (see <ref type="figure" target="#fig_6">Figure 3, center)</ref>, where the parameters α, θ and t control the elongation, orientation, and scale, respectively. Using such kernels as weighting functions v in the construction of the patch operator (52), it is possible to obtain a charting similar to the geodesic patches (roughly, θ plays the role of the angular coordinate and t of the radial one).</p><p>Mixture model network (MoNet) <ref type="bibr" target="#b53">[54]</ref>: Finally, as the most general construction of patches, Monti et al. <ref type="bibr" target="#b53">[54]</ref> proposed defining at each point a local system of d-dimensional pseudocoordinates u(x, x ) around x. On these coordinates, a set of parametric kernels v 1 (u), . . . , v J (u)) is applied, producing the weighting functions in (52). Rather than using fixed kernels as in the previous constructions, Monti et al. use Gaussian kernels</p><formula xml:id="formula_81">v j (u) = exp − 1 (u − µ j ) Σ −1 j (u − µ j )</formula><p>whose parameters (d × d covariance matrices Σ 1 , . . . , Σ J and d×1 mean vectors µ 1 , . . . , µ J ) are learned. <ref type="bibr" target="#b6">7</ref> Learning not only the filters but also the patch operators in (53) affords additional degrees of freedom to the MoNet architecture, which makes it currently the state-of-the-art approach in several applications. It is also easy to see that this approach generalizes the previous models, and e.g. classical Euclidean CNNs as well as Geodesic-and Anisotropic CNNs can be obtained as particular instances thereof <ref type="bibr" target="#b53">[54]</ref>. MoNet can also be applied on general graphs using as the pseudo-coordinates u some local graph features such as vertex degree, geodesic distance, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion distance Anisotropic heat kernel</head><p>Geodesic polar coordinates Top: examples of intrinsic weighting functions used to construct a patch operator at the point marked in black (different colors represent different weighting functions). Diffusion distance (left) allows to map neighbor points according to their distance from the reference point, thus defining a one-dimensional system of local intrinsic coordinates. Anisotropic heat kernels (middle) of different scale and orientations and geodesic polar weights (right) are twodimensional systems of coordinates. Bottom: representation of the weighting functions in the local polar (ρ, θ) system of coordinates (red curves represent the 0.5 level set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. COMBINED SPATIAL/SPECTRAL METHODS</head><p>The third alternative for constructing convolution-like operations of non-Euclidean domains is jointly in spatial-frequency domain.</p><p>Windowed Fourier transform: One of the notable drawbacks of classical Fourier analysis is its lack of spatial localization. By virtue of the Uncertainty Principle, one of the fundamental properties of Fourier transforms, spatial localization comes at the expense of frequency localization, and vice versa. In classical signal processing, this problem is remedied This choice allow interpreting intrinsic convolution (53) as a mixture of Gaussians, hence the name of the approach. by localizing frequency analysis in a window g(x), leading to the definition of the Windowed Fourier Transform (WFT, also known as short-time Fourier transform or spectrogram in signal processing),</p><formula xml:id="formula_82">(Sf )(x, ω) = ∞ −∞ f (x ) g(x − x)e −iωx gx,ω(x ) dx (59) = f, g x,ω L 2 (R) .<label>(60)</label></formula><p>The WFT is a function of two variables: spatial location of the window x and the modulation frequency ω. The choice of the window function g allows to control the tradeoff between spatial and frequency localization (wider windows result in better frequency resolution). Note that WFT can be interpreted as inner products <ref type="bibr" target="#b59">(60)</ref> of the function f with translated and modulated windows g x,ω , referred to as the WFT atoms.</p><p>The generalization of such a construction to non-Euclidean domains requires the definition of translation and modulation operators <ref type="bibr" target="#b82">[83]</ref>. While modulation simply amounts to multiplication by a Laplacian eigenfunction, translation is not welldefined due to the lack of shift-invariance. It is possible to resort again to the spectral definition of a convolution-like operation <ref type="bibr" target="#b33">(34)</ref>, defining translation as convolution with a deltafunction,</p><formula xml:id="formula_83">(g δ x )(x) = i≥0 g, φ i L 2 (X ) δ x , φ i L 2 (X ) φ i (x) = i≥0ĝ i φ i (x )φ i (x).<label>(61)</label></formula><p>The translated and modulated atoms can be expressed as</p><formula xml:id="formula_84">g x ,j (x) = φ j (x ) i≥0ĝ i φ i (x)φ i (x ),<label>(62)</label></formula><p>where the window is specified in the spectral domain by its Fourier coefficientsĝ i ; the WFT on non-Euclidean domains thus takes the form</p><formula xml:id="formula_85">(Sf )(x , j) = f, g x ,j L (X ) = i≥0ĝ i φ i (x ) f, φ i φ j L 2 (X ) .<label>(63)</label></formula><p>Due to the intrinsic nature of all the quantities involved in its definition, the WFT is also intrinsic. Wavelets: Replacing the notion of frequency in timefrequency representations by that of scale leads to wavelet decompositions. Wavelets have been extensively studied in general graph domains <ref type="bibr" target="#b83">[84]</ref>. Their objective is to define stable linear decompositions with atoms well localized both in space and frequency that can efficiently approximate signals with isolated singularities. Similarly to the Euclidean setting, wavelet families can be constructed either from its spectral constraints or from its spatial constraints.</p><p>The simplest of such families are Haar wavelets. Several bottom-up wavelet constructions on graphs were studied in <ref type="bibr" target="#b84">[85]</ref> and <ref type="bibr" target="#b85">[86]</ref>. In <ref type="bibr" target="#b86">[87]</ref>, the authors developed an unsupervised method that learns wavelet decompositions on graphs by optimizing a sparse reconstruction objective. In <ref type="bibr" target="#b87">[88]</ref>, ensembles of Haar wavelet decompositions were used to define deep wavelet scattering transforms on general domains, obtaining excellent numerical performance. Learning amounts to finding optimal pairings of nodes at each scale, which can be efficiently solved in polynomial time.</p><p>Localized Spectral CNN (LSCNN) <ref type="bibr" target="#b88">[89]</ref>: Boscaini et al. used the WFT as a way of constructing patch operators (52) on manifolds and point clouds and used in an intrinsic convolutionlike construction <ref type="bibr" target="#b52">(53)</ref>. The WFT allows expressing a function around a point in the spectral domain in the form D j (x)f = (Sf )(x, j). Applying learnable filters to such 'patches' (which in this case can be interpreted as spectral multipliers), it is possible to extract meaningful features that also appear to generalize across different domains. An additional degree of freedom is the definition of the window, which can also be learned <ref type="bibr" target="#b88">[89]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dichotomy of Geometric deep learning methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Domain Data Spectral CNN <ref type="bibr" target="#b51">[52]</ref> spectral graph GCNN/ChebNet <ref type="bibr" target="#b44">[45]</ref> spec. free graph GCN <ref type="bibr" target="#b76">[77]</ref> spec. free graph GNN <ref type="bibr" target="#b77">[78]</ref> spec. free graph Geodesic CNN <ref type="bibr" target="#b46">[47]</ref> charting mesh Anisotropic CNN <ref type="bibr" target="#b47">[48]</ref> charting mesh/point cloud MoNet <ref type="bibr" target="#b53">[54]</ref> charting graph/mesh/point cloud LSCNN <ref type="bibr" target="#b88">[89]</ref> combined mesh/point cloud IX. APPLICATIONS Network analysis: One of the classical examples used in many works on network analysis are citation networks. Citation network is a graph where vertices represent papers and there is a directed edge (i, j) if paper i cites paper j. Typically, vertex-wise features representing the content of the paper (e.g. histogram of frequent terms in the paper) are available. A prototypical classification application is to attribute each paper to a field. Traditional approaches work vertex-wise, performing classification of each vertex's feature vector individually. More recently, it was shown that classification can be considerably improved using information from neighbor vertices, e.g. with a CNN on graphs <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b76">[77]</ref>. Insert IN6 shows an example of application of spectral and spatial graph CNN models on a citation network.</p><p>Another fundamental problem in network analysis is ranking and community detection. These can be estimated by solving an eigenvalue problem on an appropriately defined operator on the graph: for instance, the Fiedler vector (the eigenvector associated with the smallest non-trivial eigenvalue of the Laplacian) carries information on the graph partition with minimal cut <ref type="bibr" target="#b72">[73]</ref>, and the popular PageRank algorithm approximates page ranks with the principal eigenvector of a modified Laplacian operator. In some contexts, one may want develop data-driven versions of such algorithms, that can adapt to model mismatch and perhaps provide a faster alternative to diagonalization methods. By unrolling power iterations, one obtains a Graph Neural Network architecture whose parameters can be learnt with backpropagation from labeled examples, similarly to the Learnt Sparse Coding paradigm <ref type="bibr" target="#b90">[91]</ref>. We are</p><p>[IN6] Citation network analysis application: The CORA citation network <ref type="bibr" target="#b89">[90]</ref> is a graph containing vertices representing papers and 5429 edges representing citations. Each paper is described by a 1433-dimensional bag-of-words feature vector and belongs to seven classes. For simplicity, the network is treated as an undirected graph. Applying the spectral CNN with two spectral convolutional layers parametrized according to <ref type="bibr" target="#b49">(50)</ref>, the authors of <ref type="bibr" target="#b76">[77]</ref> obtained classification accuracy of 81.6% (compared to 75.7% previous best result). In <ref type="bibr" target="#b53">[54]</ref>, this result was slightly improved further, reaching 81.7% accuracy with the use of MoNet architecture.</p><p>[FIGS6a] Classifying research papers in the CORA dataset with MoNet. Shown is the citation graph, where each node is a paper, and an edge represents a citation. Vertex fill and outline colors represents the predicted and groundtruth labels, respectively; ideally, the two colors should coincide. ( <ref type="figure">Figure  reproduced</ref> from <ref type="bibr" target="#b53">[54]</ref>).</p><p>currently exploring this connection by constructing multiscale versions of graph neural networks.</p><p>Recommender systems: Recommending movies on Netflix, friends on Facebook, or products on Amazon are a few examples of recommender systems that have recently become ubiquitous in a broad range of applications. Mathematically, a recommendation method can be posed as a matrix completion problem <ref type="bibr" target="#b91">[92]</ref>, where columns and rows represent users and items, respectively, and matrix values represent a score determining whether a user would like an item or not. Given a small subset of known elements of the matrix, the goal is to fill in the rest. A famous example is the Netflix challenge <ref type="bibr" target="#b92">[93]</ref> offered in 2009 and carrying a 1M$ prize for the algorithm that can best predict user ratings for movies based on previous ratings. The size of the Netflix matrix is 480K movies × 18K users (8.5B elements), with only 0.011% known entries.</p><p>Several recent works proposed to incorporate geometric structure into matrix completion problems <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b96">[97]</ref> in the form of column-and row graphs representing similarity of users and items, respectively (see <ref type="figure">Figure 4)</ref>. Such a geometric matrix completion setting makes meaningful e.g. the notion of smoothness of the matrix values, and was shown beneficial for the performance of recommender systems. In a recent work, Monti et al. <ref type="bibr" target="#b55">[56]</ref> proposed addressing the geometric matrix completion problem by means of a learnable model combining a Multi-Graph CNN (MGCNN) and a recurrent neural network (RNN). Multi-graph convolution can be thought of a generalization of the standard bidimensional image convolution, where the domains of the rows and the columns are now different (in our case, user-and item graphs). The features extracted from the score matrix by means of the MGCNN are then passed to an RNN, which produces a sequence of incremental updates of the score values. Overall, the model can be considered as a learnable diffusion of the scores, with the main advantage compared to traditional approach being a fixed number of variables independent of the matrix size. MGCNN achieved state-of-theart results on several classical matrix completion challenges and, on a more conceptual level, could be a very interesting practical application of geometric deep learning to a classical signal processing problem. Computer vision and graphics: The computer vision community has recently shown an increasing interest in working with 3D geometric data, mainly due to the emergence of affordable range sensing technology such as Microsoft Kinect or Intel RealSense. Many machine learning techniques successfully working on images were tried "as is" on 3D geometric data, represented for this purpose in some way "digestible" by standard frameworks, e.g. as range images <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b98">[99]</ref> or rasterized volumes <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b100">[101]</ref>. The main drawback of such approaches is their treatment of geometric data as Euclidean structures. First, for complex 3D objects, Euclidean representations such as depth images or voxels may lose significant parts of the object or its fine details, or even break its topological structure. Second, Euclidean representations are not intrinsic, and vary when changing pose or deforming the object. Achieving invariance to shape deformations, a common requirement in many vision applications, demands very complex models and huge training sets due to the large number of degrees of freedom involved in describing non-rigid deformations ( <ref type="figure" target="#fig_8">Figure 5, left)</ref>. In the domain of computer graphics, on the other hand, working intrinsically with geometric shapes is a standard practice. In this field, 3D shapes are typically modeled as Riemannian manifolds and are discretized as meshes. Numerous studies (see, e.g. <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b102">[103]</ref>, <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b105">[106]</ref>) have been devoted to designing local and global features e.g. for establishing similarity or correspondence between deformable shapes with guaranteed invariance to isometries.</p><p>Furthermore, different applications in computer vision and graphics may require completely different features: for instance, in order to establish feature-based correspondence between a collection of human shapes, one would desire the descriptors of corresponding anatomical parts (noses, mouths, etc.) to be as similar as possible across the collection. In other words, such descriptors should be invariant to the collection variability. Conversely, for shape classification, one would like descriptors that emphasize the subject-specific characteristics, and for example, distinguish between two different nose shapes (see <ref type="figure">Figure 6</ref>). Deciding a priori which structures should be used and which should be ignored is often hard or sometimes even impossible. Moreover, axiomatic modeling of geometric noise such as 3D scanning artifacts turns out to be extremely hard.</p><p>Correspondence Similarity <ref type="figure">Fig. 6</ref>. Left: features used for shape correspondence should ideally manifest invariance across the shape class (e.g., the "knee feature" shown here should not depend on the specific person). Right: on the contrary, features used for shape retrieval should be specific to a shape within the class to allow distinguishing between different people. Similar features are marked with same color. Hand-crafting the right feature for each application is a very challenging task.</p><p>By resorting to intrinsic deep neural networks, the invariance to isometric deformations is automatically built into the model, thus vastly reducing the number of degrees of freedom required to describe the invariance class. Roughly speaking, the intrinsic deep model will try to learn 'residual' deformations that deviate from the isometric model. Geometric deep learning can be applied to several problems in 3D shape analysis, which can be divided in two classes. First, problems such as local descriptor learning <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b52">[53]</ref> or correspondence learning <ref type="bibr" target="#b47">[48]</ref> (see example in the insert IN7), in which the output of the network is point-wise. The inputs to the network are some point-wise features, for example, color texture or simple geometric features such as normals. Using a CNN architecture with multiple intrinsic convolutional layers, it is possible to produce non-local features that capture the context around each point. The second type of problems such as shape recognition require the network to produce a global shape descriptor, aggregating all the local information into a single vector using e.g. the covariance pooling <ref type="bibr" target="#b46">[47]</ref>.</p><p>Particle physics and Chemistry: Many areas of experimental science are interested in studying systems of discrete particles defined over a low-dimensional phase space. For instance, the chemical properties of a molecule are determined by the relative positions of its atoms, and the classification of events in particle accelerators depends upon position, momentum, and spin of all the particles involved in the collision.</p><p>The behavior of an N -particle system is ultimately derived from solutions of the Schrödinger equation, but its exact solution involves diagonalizing a linear system of exponential size. In this context, an important question is whether one can approximate the dynamics with a tractable model that incorporates by construction the geometric stability postulated by the Schrödinger equation, and at the same time has enough flexibility to adapt to data-driven scenarios and capture complex interactions.</p><p>An instance l of an N l -particle system can be expressed as</p><formula xml:id="formula_86">f l (t) = N l j=1 α j,l δ(t − x j,l ) ,</formula><p>where (α j,l ) model particle-specific information such as the spin, and (x j,l ) are the locations of the particles in a given phase-space. Such system can be recast as a signal defined over a graph with |V l | = N l vertices and edge weights W l = (φ(α i,l , α j,l , x i,l , x j,l )) expressed through a similarity kernel capturing the appropriate priors. Graph neural networks are currently being applied to perform event classification, energy regression, and anomaly detection in high-energy physics experiments such as the Large Hadron Collider (LHC) and neutrino detection in the IceCube Observatory. Recently, models based on graph neural networks have been applied to predict the dynamics of N -body systems <ref type="bibr" target="#b110">[111]</ref>, <ref type="bibr" target="#b111">[112]</ref> showing excellent prediction performance.</p><p>Molecule design: A key problem in material-and drug design is predicting the physical, chemical, or biological properties of a novel molecule (such as solubility of toxicity) from its structure. State-of-the-art methods rely on hand-crafted molecule descriptors such as circular fingerprints <ref type="bibr" target="#b112">[113]</ref>, <ref type="bibr" target="#b113">[114]</ref>, <ref type="bibr" target="#b114">[115]</ref>. A recent work from Harvard university <ref type="bibr" target="#b54">[55]</ref> proposed modeling molecules as graphs (where vertices represents atoms and edges represent chemical bonds) and employing graph convolutional neural networks to learn the desired molecule properties. Their approach has significantly outperformed hand-crafted features. This work opens a new avenue in molecule design that might revolutionize the field.</p><p>Medical imaging: An application area where signals are naturally collected on non-Euclidean domains and where the methodologies we reviewed could be very useful is brain imaging. A recent trend in neuroscience is to associate functional MRI traces with a pre-computed connectivity rather than inferring it from the traces themselves <ref type="bibr" target="#b115">[116]</ref>. In this case, the challenge consists in processing and analyzing an array of signals collected over a complex topology, which results in subtle dependencies. In a recent work from Imperial College <ref type="bibr" target="#b116">[117]</ref>, graph CNNs were used to detect disruptions of the brain functional networks associated with autism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. OPEN PROBLEMS AND FUTURE DIRECTIONS</head><p>The recent emergence of geometric deep learning methods in various communities and application domains, which we tried to overview in this paper, allow us to proclaim, perhaps with some caution, that we might be witnessing a new field being born. We expect the following years to bring exciting new approaches and results, and conclude our review with a few observations of current key difficulties and potential directions of future research.</p><p>Many disciplines dealing with geometric data employ some empirical models or "handcrafted" features. This is a typical situation in geometry processing and computer graphics, where axiomatically-constructed features are used to analyze 3D shapes, or computational sociology, where it is common to first come up with a hypothesis and then test it on the data <ref type="bibr" target="#b21">[22]</ref>. Yet, such models assume some prior knowledge (e.g. isometric shape deformation model), and often fail to correctly capture the full complexity and richness of the data. In computer vision, departing from "handcrafted" features towards generic models learnable from the data in a task-specific manner</p><p>[IN7] 3D shape correspondence application: Finding intrinsic correspondence between deformable shapes is a classical tough problem that underlies a broad range of vision and graphics applications, including texture mapping, animation, editing, and scene understanding <ref type="bibr" target="#b106">[107]</ref>. From the machine learning standpoint, correspondence can be thought of as a classification problem, where each point on the query shape is assigned to one of the points on a reference shape (serving as a "label space") <ref type="bibr" target="#b107">[108]</ref>. It is possible to learn the correspondence with a deep intrinsic network applied to some input feature vector f (x) at each point x of the query shape X , producing an output U Θ (f (x))(y), which is interpreted as the conditional probability p(y|x) of x being mapped to y. Using a training set of points with their ground-truth correspondence {x i , y i } i∈I , supervised learning is performed minimizing the multinomial regression loss</p><formula xml:id="formula_87">min Θ − i∈I log U Θ (f (x i ))(y i )<label>(64)</label></formula><p>w.r.t. the network parameters Θ. The loss penalizes for the deviation of the predicted correspondence from the groundtruth. We note that, while producing impressive result, such an approach essentially learns point-wise correspondence, which then has to be post-processed in order to satisfy certain properties such as smoothness or bijectivity. Correspondence is an example of structured output, where the output of the network at one point depends on the output in other points (in the simplest setting, correspondence should be smooth, i.e., the output at nearby points should be similar). Litany et al. <ref type="bibr" target="#b108">[109]</ref> proposed intrinsic structured prediction of shape correspondence by integrating a layer computing functional correspondence <ref type="bibr" target="#b105">[106]</ref> into the deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UΘ xi yi</head><p>X Y</p><p>[FIGS7a] Learning shape correspondence: an intrinsic deep network U Θ is applied point-wise to some input features defined at each point. The output of the network at each point x of the query shape X is a probability distribution of the reference shape Y that can be thought of as a soft correspondence.</p><p>[FIGS7b] Intrinsic correspondence established between human shapes using intrinsic deep architecture (MoNet <ref type="bibr" target="#b53">[54]</ref> with three convolutional layers). SHOT descriptors capturing the local normal vector orientations <ref type="bibr" target="#b109">[110]</ref> were used in this example as input features. The correspondence is visualized by transferring texture from the leftmost reference shape. For additional examples, see <ref type="bibr" target="#b53">[54]</ref>.</p><p>has brought a breakthrough in performance and led to an overwhelming trend in the community to favor deep learning methods. Such a shift has not occurred yet in the fields dealing with geometric data due to the lack of adequate methods, but there are first indications of a coming paradigm shift.</p><p>Generalization: Generalizing deep learning models to geometric data requires not only finding non-Euclidean counterparts of basic building blocks (such as convolutional and pooling layers), but also generalization across different domains. Generalization capability is a key requirement in many applications, including computer graphics, where a model is learned on a training set of non-Euclidean domains (3D shapes) and then applied to previously unseen ones. Spectral formulation of convolution allows designing CNNs on a graph, but the model learned this way on one graph cannot be straightforwardly applied to another one, since the spectral representation of convolution is domain-dependent. A possible remedy to the generalization problem of spectral methods is the recent architecture proposed in <ref type="bibr" target="#b117">[118]</ref>, applying the idea of spatial transformer networks <ref type="bibr" target="#b118">[119]</ref> in the spectral domain. This approach is reminiscent of the construction of compatible orthogonal bases by means of joint Laplacian diagonalization <ref type="bibr" target="#b74">[75]</ref>, which can be interpreted as an alignment of two Laplacian eigenbases in a k-dimensional space.</p><p>The spatial methods, on the other hand, allow generalization across different domains, but the construction of low-dimensional local spatial coordinates on graphs turns to be rather challenging. In particular, the construction of anisotropic diffusion on general graphs is an interesting research direction.</p><p>The spectrum-free approaches also allow generalization across graphs, at least in terms of their functional form. However, if multiple layers of equation <ref type="bibr" target="#b50">(51)</ref> used with no nonlinearity or learned parameters θ, simulating a high power of the diffusion, the model may behave differently on different kinds of graphs. Understanding under what circumstances and to what extent these methods generalize across graphs is currently being studied.</p><p>Time-varying domains: An interesting extension of geometric deep learning problems discussed in this review is coping with signals defined over a dynamically changing structure. In this case, we cannot assume a fixed domain and must track how these changes affect signals. This could prove useful to tackle applications such as abnormal activity detection in social or financial networks. In the domain of computer graphics and vision, potential applications deal with dynamic shapes (e.g. 3D video captured by a range sensor).</p><p>Directed graphs: Dealing with directed graphs is also a challenging topic, as such graphs typically have nonsymmetric Laplacian matrices that do not have orthogonal eigendecomposition allowing easily interpretable spectraldomain constructions. Citation networks, which are directed graphs, are often treated as undirected graphs (including in our example in IN7) considering citations between two papers without distinguishing which paper cites which. This obviously may loose important information.</p><p>Synthesis problems: Our main focus in this review was primarily on analysis problems on non-Euclidean domains. Not less important is the question of data synthesis. There have been several recent attempts to try to learn a generative model allowing to synthesize new images <ref type="bibr" target="#b119">[120]</ref> and speech waveforms <ref type="bibr" target="#b120">[121]</ref>. Extending such methods to the geometric setting seems a promising direction, though the key difficulty is the need to reconstruct the geometric structure (e.g., an embedding of a 2D manifold in the 3D Euclidean space modeling a deformable shape) from some intrinsic representation <ref type="bibr" target="#b121">[122]</ref>.</p><p>Computation: The final consideration is a computational one. All existing deep learning software frameworks are primarily optimized for Euclidean data. One of the main reasons for the computational efficiency of deep learning architectures (and one of the factors that contributed to their renaissance) is the assumption of regularly structured data on 1D or 2D grid, allowing to take advantage of modern GPU hardware. Geometric data, on the other hand, in most cases do not have a grid structure, requiring different ways to achieve efficient computations. It seems that computational paradigms developed for large-scale graph processing are more adequate frameworks for such applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>fox (1.0) ; Eskimo dog (0.6) ; White wolf (0.4) ; Siberian husky (0.4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Top: tangent space and tangent vectors on a two-dimensional manifold (surface). Bottom: Examples of isometric deformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Two commonly used discretizations of a two-dimensional manifold: a graph and a triangular mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>is known as the heat kernel and represents the solution of the heat equation with an initial condition f 0 (x) = δ x (x), or, in signal processing terms, an 'impulse response'. In physical terms, h t (x, x ) describes how much heat flows from a point x to point x in time t. In the Euclidean case, the heat kernel is shift-invariant, h t (x, x ) = h t (x − x ), allowing to interpret the integral in (36) as a convolution f (x, t) = (f 0 h t )(x). In the spectral domain, convolution with the heat kernel amounts to low-pass filtering with frequency response e −tλ . Larger values of diffusion time t result in lower effective cutoff frequency and thus smoother solutions in space (corresponding to the intuition that longer diffusion smoothes more the initial heat distribution).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>denotes a rescaling of the Laplacian mapping its eigenvalues from the interval [0, λ n ] to [−1, 1] (necessary since the Chebyshev polynomials form an orthonormal basis in [−1, 1]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 Fig. 4 .</head><label>14</label><figDesc>Geometric matrix completion exemplified on the famous Netflix movie recommendation problem. The column and row graphs represent the relationships between users and items, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of the difference between classical CNN (left) applied to a 3D shape (checkered surface) considered as a Euclidean object, and a geometric CNN (right) applied intrinsically on the surface. In the latter case, the convolutional filters (visualized as a colored window) are deformation-invariant by construction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc><ref type="bibr" target="#b1">2</ref> is referred to as the normalized symmetric Laplacian.</figDesc><table><row><cell>0.2</cell><cell>φ0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>max</cell></row><row><cell></cell><cell>φ3</cell><cell>φ2</cell><cell>φ1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell></row><row><cell>0 −0.2</cell><cell></cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>80</cell><cell>90</cell><cell>φ0</cell><cell>φ1</cell><cell>φ2</cell><cell>φ3</cell><cell>min</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Euclidean</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Manifold</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>max</cell></row><row><cell></cell><cell></cell><cell></cell><cell>φ0</cell><cell></cell><cell></cell><cell></cell><cell>φ1</cell><cell>φ2</cell><cell></cell><cell></cell><cell>φ3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>min</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Graph</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>[FIGS3] Example of the first four Laplacian eigenfunctions φ0, . . . , φ3 on a Euclidean domain (1D line, top left) and non-Euclidean domains (human shape modeled as a 2D manifold, top right; and Minnesota road graph, bottom). In the Euclidean case, the result is the standard Fourier basis comprising sinusoids of increasing frequency. In all cases, the eigenfunction φ0 corresponding to zero eigenvalue is constant ('DC').</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For a more in-depth review of CNNs and their applications, we refer the reader to<ref type="bibr" target="#b11">[12]</ref>,<ref type="bibr" target="#b0">[1]</ref>,<ref type="bibr" target="#b12">[13]</ref> and references therein.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We assume periodic boundary conditions to ensure that the operation is well-defined over L 2 (Ω).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In the Euclidean case, the Fourier transform of a function defined on a finite interval (which is a compact set) or its periodic extension is discrete. In practical settings, all domains we are dealing with are compact.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Strategies for training large scale neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc. Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional networks and applications in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCAS</title>
		<meeting>ISCAS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning: methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="197" to="387" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Natural image statistics and neural representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1193" to="1216" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What the statistics of natural images tell us about visual coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An exact mapping between the variational renormalization group and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3831</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Group invariant scattering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1331" to="1398" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A mathematical motivation for complex-valued convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tygert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Piantino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten ZIP code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Life in the network: the coming age of computational social science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lazer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">5915</biblScope>
			<biblScope unit="page">721</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A genomic regulatory network for development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="issue">5560</biblScope>
			<biblScope unit="page" from="1669" to="1678" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The multiscale structure of non-differentiable image manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Which spatial partition trees are adaptive to intrinsic dimension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kpotufe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Uncertainty in Artificial Intelligence</title>
		<meeting>Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diffusion maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">App. and Comp. Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DeepWalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LINE: Large-scale information network embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GraRep: Learning graph representations with global structural information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IKM</title>
		<meeting>IKM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Biological network comparison using graphlet degree distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pržulj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A concise and provably informative multi-scale signature based on heat diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1383" to="1392" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spectral descriptors for deformable shape correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="180" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Community detection in graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fortunato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Reports</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="75" to="174" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Friendship and mobility: user movement in location-based social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Proc. Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02136v2</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3DRR</title>
		<meeting>3DRR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07736</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Anisotropic diffusion descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="431" to="441" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06803</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. Trans. R. Soc. A</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="issue">2065</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A wavelet tour of signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The loss surfaces of multilayer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Safran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04210</idno>
		<title level="m">On the quality of the initial basin in overspecified neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep learning without poor local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05641</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Topology and geometry of half-rectified network optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The imbedding problem for Riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="63" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Discrete laplace operators: no free lunch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wardetzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kälberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grinspun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SGP</title>
		<meeting>SGP</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Convergence of the cotangent formula: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wardetzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discrete Differential Geometry</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="275" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Computing discrete minimal surfaces and their conjugates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pinkall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Polthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Mathematics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="36" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">The Laplacian on a Riemannian manifold: an introduction to analysis on manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Hodge Laplacians on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05379</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Coupled quasi-harmonic bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovnatsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Glashoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="439" to="448" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Multimodal manifold analysis by simultaneous diagonalization of Laplacians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovnatsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Glashoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2505" to="2517" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning the 2-d topology of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joliveau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Anisotropic Laplace-Beltrami operators for shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NORDIA</title>
		<meeting>NORDIA</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Vertex-frequency analysis on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ricaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">App. and Comp. Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="291" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Diffusion wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">App. and Comp. Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="94" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Diffusion-driven multiscale analysis on manifolds and graphs: topdown and bottom-up constructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Bremerjr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optics &amp; Photonics 2005. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="59" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Multiscale wavelets on trees, graphs and high dimensional data: Theory and applications to semi supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gavish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rustamov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Deep Haar scattering networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Inference</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="105" to="133" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Melzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Castellani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="13" to="23" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commu. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="111" to="119" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Web Search and Data Mining</title>
		<meeting>Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalofolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.1717</idno>
	</analytic>
	<monogr>
		<title level="j">Matrix completion on graphs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Collaborative filtering with graph information: Consistency and scalable methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">A harmonic extension approach for collaborative ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bertozzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05127</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Dense human body correspondences using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vouga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">3D shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view CNNs for object classification on 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Generalized multidimensional scaling: a framework for isometry-invariant partial surface matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1168" to="1172" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Scale-invariant heat kernel signatures for non-rigid shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Blended intrinsic maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">ShapeGoogle: Geometric words and expressions for invariant shape retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Functional maps: a flexible representation of maps between shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Recent trends, applications, and perspectives in 3D shape similarity assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biasotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cerri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Dense non-rigid shape correspondence using random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Windheuser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vestner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Deep functional maps: Structured prediction for dense shape correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08686</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00341</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">The generation of a unique machine description for chemical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chemical Documentation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">The generation of a unique machine description for chemical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Glem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Arnby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investigational Drugs</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="204" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">The dynamic functional connectome: State-of-the-art and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Preti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Distance metric learning using graph convolutional networks: Application to functional brain networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02161</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Learning to generate chairs, tables and cars with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Shapefrom-operator: Recovering shapes from intrinsic operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kourounis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
