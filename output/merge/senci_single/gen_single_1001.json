{"1001": "recurrent neural networks ( rnns) have had considerable success in classifying and predicting sequences. we demonstrate that rnns can be effectively used in order to encode sequences and provide effective representations. the methodology we use is based on fisher vectors, where the rnns are the generative probabilistic models and the partial derivatives are computed using backpropagation. state of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. we also show a surprising transfer learning result from the task of image annotation to the task of video action recognition. this paper proposes a novel pooling technique that takes a multiset of vectors and computes its mean : v 1n n i1 xi. clearly, the vector v that results from the pooling is in xi. this pooling technique takes a multiset of vectors , x x1, x2, , xn r d, and computes its mean : v 1n n i1 xi. the rnn is trained to predict the next element in a sequence, given the previous elements. given a sequence of input vectors x, the rnn is trained to predict the next vector in the sequence, i.e., the sequence y the output layer of the network is a fully-connected layer, the size of which would be d, i.e., the dimension of the input vector space. this special element is used to denote the beginning of the input sequence, and we use xstart throughout this paper. the rnn is trained to predict , at each time step i , the next element xi1 of the sequence, given the previous elements x0, ..., xi. therefore, given the input sequence, the target sequence would be : y ( x1 , x2, ... xn) in this paper , we present a new method for classifying video sequences . the method is based on a probabilistic model which predicts the sequence u ( w1 , w2 , ... , wn) from the sequence x ( x0, x1 , denote by m the size of our symbol alphabet, i.e. , the number of unique symbols in the input sequences, i.e. , the cross-entropy loss at time step i is derived from the probability which rnn gives to j th symbol at time step i. the loss function for the training of the rnn is the cross-entropy loss. after the rnn is trained, it is ready to be used as a feature vector extractor for new sequences. the classification application is applicable for predicting a sequence of symbols w1 , w2 , ... , wn that have matching vector representations r ( w1 , w2 , ... , r ( wn) xn the rnn predicts the sequence u ( w1 , w2 , ... , r ( wn) xn this paper presents two network alternatives for the image-sentence retrieval tasks : classification and regression. a sentence, being an ordered sequence of words, can be represented as a vector using the rnn-fv scheme. after each rnn epoch, we extract the rnn-fv representation as described above, we extract the rnn-fv representation as described above, we train a linear svm classifier on the training set and evaluate the performance on the validation set. the early stopping point is chosen at the epoch with highest recognition accuracy on the validation set. after choosing our model this way, we train an svm classifier on all training samples and report our performance on the test set. the training data may be any large set of sentences. these sentences may be extracted from the dataset of a specific benchmark, or, in order to obtain a generic representation, any external corpus, e.g., wikipedia, may be used. the two network alternatives are explored: classification and regression. in this paper, we propose a new classification method based on the regularized cca (regularized cca) algorithm. the regularized cca algorithm 47, where the regularization parameter is selected based on the validation set, is used to match the sentence rnn-fv representation with the sentence rnn-fv representation. in the shared cca space, the cosine similarity is used. not shown is the performance obtained when using the activations of the rnn as a feature vector. for matching images and text, each image is represented as a 4096-dimensional vector extracted using the 19-layer vgg. for gmm-fv, the only parameter is k, which is the number of components in the mixture. the state of the art on the bidirectional image and sentence retrieval task is ucf101 and hmdb51. in the bidirectional image and sentence retrieval task, we show that rnn-fv outperforms ucf101 and hmdb51. this paper introduces a novel fv representation for sequences that is derived from rnns. the proposed representation is sensitive to the element ordering in the sequence and provides a richer model than the additive bag model typically used for conventional fvs. the proposed representation surpasses the state-of-theart results for video action recognition on two challenging datasets. when used for representing sentences, the rnnfv representation achieves state-of-theart or competitive results on image annotation and image search tasks. since the length of the sentences in these tasks is usually short and, therefore, the ordering is less crucial, we believe that using the rnn-fv representation for tasks that use longer text will provide an even larger gap between the conventional fv and the rnn-fv . the paper proposes a novel approach for fv representation of sequences using a recurrent neural network ( rnn). the paper explores two different approaches for training the rnn for the image annotation and image search tasks. one is based on training a regression problem, and the other on training a classification problem. the rnn-fv is capable of encoding the sequence properties, and as underlying features, we rely on video encodings that are based on single frames or on fixed length blocks of frames. word embedding a word was represented either by word2vec, or by the gmmhglmm representation of 18, projected to a 300d sentence to vgg-encoded-image cca space. the vgg pipeline is cropped in ten different ways into by pixel images, the center, and their xaxis mirror image. the mean intensity is then subtracted in each color channel and the resulting images are encoded by the network. the rnn model consists of three layers: a 200d fullyconnected layer units with leaky-relu activation ( 0.1) , a 200-units long short-term memory ( lstm) layer, and a 500d linear fullyconnected layer. when training the rnn for regression, the same 300d input is used, followed by an lstm layer of size the output layer, in this case, is fullyconnected, where the ( 300 dimensional) word embedding of next word is predicted. the rnn-fv representation surpasses the state-of-the-art results for video action recognition on two challenging datasets. a transfer learning result from the image annotation task to the video action recognition task was shown. the con-ceptual distance between these two tasks makes this result both interesting and surprising.", "1000": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1007": "###", "1008": "###", "1009": "###", "1010": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1018": "###", "1019": "###", "1020": "###", "1021": "###"}