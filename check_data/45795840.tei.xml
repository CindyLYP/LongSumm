<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyuan</forename><surname>Ruan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencong</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanwei</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Putnam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ustc</forename><forename type="middle">‡</forename><surname>Ucla</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">•</forename><surname>Beihang University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KV-Direct: High-Performance In-Memory Key-Value Store with Programmable NIC</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3132747.3132756</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Key-Value Store</term>
					<term>Programmable Hardware</term>
					<term>Performance</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Performance of in-memory key-value store (KVS) continues to be of great importance as modern KVS goes beyond the traditional object-caching workload and becomes a key infrastructure to support distributed main-memory computation in data centers. Recent years have witnessed a rapid increase of network bandwidth in data centers, shifting the bottleneck of most KVS from the network to the CPU. RDMA-capable NIC partly alleviates the problem, but the primitives provided by RDMA abstraction are rather limited. Meanwhile, programmable NICs become available in data centers, enabling in-network processing. In this paper, we present KV-Direct, a high performance KVS that leverages programmable NIC to extend RDMA primitives and enable remote direct key-value access to the main host memory. We develop several novel techniques to maximize the throughput and hide the latency of the PCIe connection between the NIC and the host memory, which becomes the new bottleneck. Combined, these mechanisms allow a single NIC KV-Direct to achieve up to 180 M key-value operations per second, equivalent to the throughput of tens of CPU cores. Compared with CPU based KVS implementation, KV-Direct improves power efficiency by 3x, while keeping tail latency below 10 µs. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a commodity server, we achieve 1.22 billion KV operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In-memory key-value store (KVS) is a key distributed system component in many data centers. KVS enables access to a shared key-value hash table among distributed clients. Historically, KVS such as Memcached <ref type="bibr" target="#b24">[25]</ref> gained popularity as an object caching system for web services. Large web service providers such as Amazon <ref type="bibr" target="#b16">[17]</ref> and Facebook <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b56">57]</ref>, have deployed distributed key-value stores at scale. More recently, as main-memory based computing becomes a major trend in the data centers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b57">58]</ref>, KVS starts to go beyond caching and becomes an infrastructure to store shared data structure in a distributed system. Many data structures can be expressed in a key-value hash table, e.g., data indexes in NoSQL databases <ref type="bibr" target="#b11">[12]</ref>, model parameters in machine learning <ref type="bibr" target="#b45">[46]</ref>, nodes and edges in graph computing <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b74">74]</ref> and sequencers in distributed synchronization <ref type="bibr" target="#b36">[37]</ref>. For most of these applications, the performance of the KVS is the key factor that directly determines the system efficiency. Due to its importance, over the years significant amount of research effort has been invested on improving KVS performance.</p><p>Earlier key-value systems <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b56">57]</ref> are built on top of traditional OS abstractions such as OS lock and TCP/IP stack. This puts considerable stress on the performance of <ref type="figure">Figure 1</ref>: Design space of KVS data path and processing device. Line indicates data path. One KV operation (thin line) may require multiple address-based memory accesses (thick line). Black box indicates where KV processing takes place.</p><p>the OS, especially the networking stack. The bottleneck is exacerbated by the fact that physical network transport speed has seen huge improvements in the last decade due to heavy bandwidth demand from data center applications.</p><p>More recently, as both the single core frequency scaling and multi-core architecture scaling are slowing down <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b69">69]</ref>, a new research trend in distributed systems is to leverage Remote Direct Memory Access (RDMA) technology on NIC to reduce network processing cost. One line of research <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> uses two-sided RDMA to accelerate communication <ref type="figure">(Figure 1a)</ref>. KVS built with this approach are bounded by CPU performance of the KVS servers. Another line of research uses one-sided RDMA to bypass remote CPU and shift KV processing workload to clients <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b54">55]</ref>  <ref type="figure">(Figure 1b</ref>). This approach achieves better GET performance but degrades performance for PUT operations due to high communication and synchronization overhead. Due to lack of transactional support, the abstraction provided by RDMA is not a perfect fit for building efficient KVS.</p><p>In the meantime, another trend is emerging in data center hardware evolution. More and more servers in data centers are now equipped with programmable NICs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b63">64]</ref>. At the heart of a programmable NIC is a field-programmable gate array (FPGA) with an embedded NIC chip to connect to the network and a PCIe connector to attach to the server. Programmable NIC is initially designed to enable network virtualization <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref>. However, many found that FPGA resources can be used to offload some workloads of CPU and significantly reduce CPU resource usage <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b59">60]</ref>. Our work takes this general approach.</p><p>We present KV-Direct, a new in-memory key-value system that takes advantage of programmable NIC in data center. KV-Direct, as its name implies, directly fetches data and applies updates in the host memory to serve KV requests, bypassing host CPU <ref type="figure">(Figure 1c</ref>). KV-Direct extends the RDMA primitives from memory operations (READ and WRITE) to key-value operations (GET, PUT, DELETE and ATOMIC ops). Compared with one-sided RDMA based systems, KV-Direct deals with the consistency and synchronization issues at server-side, thus removes computation overhead in client and reduces network traffic. In addition, to support vectorbased operations and reduce network traffic, KV-Direct also provides new vector primitives UPDATE, REDUCE, and FILTER, allowing users to define active messages <ref type="bibr" target="#b18">[19]</ref> and delegate certain computation to programmable NIC for efficiency.</p><p>Since the key-value operations are offloaded to the programmable NIC, we focus our design on optimizing the PCIe traffic between the NIC and host memory. KV-Direct adopts a series of optimizations to fully utilize PCIe bandwidth and hide latency. Firstly, we design a new hash table and memory allocator to leverage parallelism available in FPGA and minimize the number of PCIe DMA requests. On average, KV-Direct achieves close to one PCIe DMA per READ operation and two PCIe DMAs per WRITE operation. Secondly, to guarantee consistency among dependent KV operations, KV-Direct includes an out-of-order execution engine to track operation dependencies while maximizing the throughput of independent requests. Thirdly, KV-Direct exploits on-board DRAM buffer available on programmable NIC by implementing a hardware-based load dispatcher and caching component in FPGA to fully utilize on-board DRAM bandwidth and capacity.</p><p>A single NIC KV-Direct is able to achieve up to 180 M KV operations per second (Ops), equivalent to the throughput of CPU cores <ref type="bibr" target="#b46">[47]</ref>. Compared with state-of-art CPU KVS implementations, KV-Direct reduces tail latency to as low as 10 µs while achieving a 3x improvement on power efficiency. Moreover, KV-Direct can achieve near linear scalability with multiple NICs. With 10 programmable NIC cards in a server, we achieve 1.22 billion KV operations per second in a single commodity server, which is more than an order of magnitude improvement over existing systems.</p><p>KV-Direct supports general atomic operations up to 180 Mops, equal to normal KV operation and significantly outperforms the number reported in state-of-art RDMA-based system: 2.24 Mops <ref type="bibr" target="#b35">[36]</ref>. The atomic operation agnostic performance is mainly a result of our out-of-order execution engine that can efficiently track the dependency among KV operations without explicitly stalling the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 Workload Shift in KVS</head><p>Historically, KVS such as Memcached <ref type="bibr" target="#b24">[25]</ref> gained popularity as an object caching system for web services. In the era of in-memory computation, KVS goes beyond caching and becomes an infrastructure service to store shared data structure in a distributed system. Many data structures can be expressed in a key-value hash table, e.g., data indexes in NoSQL databases <ref type="bibr" target="#b11">[12]</ref>, model parameters in machine learning <ref type="bibr" target="#b45">[46]</ref>, nodes and edges in graph computing <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b74">74]</ref> and sequencers in distributed synchronization <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>The workload shifts from object cache to generic data structure store implies several design goals for KVS.</p><p>High batch throughput for small KV. In-memory computations typically access small key-value pairs in large batches, e.g., sparse parameters in linear regression <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b74">74]</ref> or all neighbor nodes in graph traversal <ref type="bibr" target="#b67">[67]</ref>, therefore a KVS should be able to benefit from batching and pipelining.</p><p>Predictable low latency. For many data-parallel computation tasks, the latency of an iteration is determined by the slowest operations <ref type="bibr" target="#b58">[59]</ref>. Therefore, it is important to control the tail latency of KVS. CPU based implementations often have large fluctuations under heavy load due to scheduling irregularities and inflated buffering.</p><p>High efficiency under write-intensive workload. For cache workloads, KVS often has much more reads than writes <ref type="bibr" target="#b2">[3]</ref>, but it is no longer the case for distributed computation workloads such as graph computation <ref type="bibr" target="#b60">[61]</ref>, parameter servers <ref type="bibr" target="#b45">[46]</ref>. These workloads favor hash table structures that can handle both read and write operations efficiently.</p><p>Fast atomic operations. Atomic operations on several extremely popular keys appear in applications such as centralized schedulers <ref type="bibr" target="#b62">[63]</ref>, sequencers <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref>, counters <ref type="bibr" target="#b76">[76]</ref> and short-term values in web applications <ref type="bibr" target="#b2">[3]</ref>. This requires high throughput on single-key atomics.</p><p>Support vector-type operations. Machine learning and graph computing workloads <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b74">74]</ref> often require operating on every element in a vector, e.g., incrementing every element in a vector with a scalar or reducing a vector into the sum of its elements. KVS without vector support requires the client to either issue one KVS operation per element, or retrieve the vector back to the client and perform the operation. Supporting vector data type and operations in KVS can greatly reduce network communication and CPU computation overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Achilles' Heel of High-Performance KVS Systems</head><p>Building a high performance KVS is a non-trivial exercise of optimizing various software and hardware components in a computer system. Characterized by where the KV processing takes place, state-of-the-art high-performance KVS systems basically falls into three categories: on the CPU of KVS server <ref type="figure">(Figure 1a</ref>), on KVS clients <ref type="figure">(Figure 1b</ref>) or on a hardware accelerator ( <ref type="figure">Figure 1c</ref>). When pushed to the limit, in high performance KVS systems the throughput bottleneck can be attributed to the computation in KV operation and the latency in random memory access. CPU-based KVS needs to spend CPU cycles for key comparison and hash slot computation. Moreover, KVS hash table is orders of magnitude larger than the CPU cache, therefore the memory access latency is dominated by cache miss latency for practical access patterns.</p><p>By our measurement, a 64-byte random read latency for a contemporary computer is ∼110 ns. A CPU core can issue several memory access instructions concurrently when they fall in the instruction window, limited by the number of loadstore units in a core (measured to be 3∼4 in our CPU) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b75">75]</ref>. In our CPU, we measure a max throughput of 29.3 M random 64B access per second per core. On the other hand, an operation to access 64-byte KV pair typically requires ∼100 ns computation or ∼500 instructions, which is too large to fit in the instruction window (measured to be 100∼200). When interleaved with computation, the performance of a CPU core degrades to only 5.5 M KV operations per second (Mops). An optimization is to batch memory accesses in a KV store by clustering the computation for several operations together before issuing the memory access all at once <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b55">56]</ref>. This improves the per-core throughput to 7.9 MOps in our CPU, which is still far less than the random 64B throughput of host DRAM.</p><p>Observing the limited capacity of CPU in KV processing, recent work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b70">70]</ref> leverage one-sided RDMA to offload KV processing to clients and effectively using the KVS server as a shared memory pool. Despite the high message rate (8∼150 Mops <ref type="bibr" target="#b36">[37]</ref>) provided by RDMA NICs, it is challenging to find an efficient match between RDMA primitives and key-value operations. For a write (PUT or atomic) operation, multiple network round-trips and multiple memory accesses may be required to query the hash index, handle hash collisions and allocate variable-size memory. RDMA does not support transactions. Clients must synchronize with each other to ensure consistency using RDMA atomics or distributed atomic broadcast <ref type="bibr" target="#b70">[70]</ref>, both incurring communication overhead and synchronization latency <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b54">55]</ref>. Therefore, most RDMA-based KVS <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b54">55]</ref> recommend using onesided RDMA for GET operations only. For PUT operations, they fall back to the server CPU. The throughput of writeintensive workload is still bottlenecked by CPU cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Programmable NIC with FPGA</head><p>Ten years ago, processor frequency scaling slowed down and people turned to multi-core and concurrency <ref type="bibr" target="#b69">[69]</ref>. Nowadays,   power ceiling implies that multi-core scaling has also met difficulties <ref type="bibr" target="#b21">[22]</ref>. People are now turning to domain-specific architectures (DSAs) for better performance. Due to the increasing mismatch of network speed and CPU network processing capacity, programmable NICs with FPGA <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref> now witness large-scale deployment in datacenters. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the heart of the programmable NIC we use is an FPGA, with an embedded NIC chip to connect to the network. Programmable NICs typically come with on-board DRAM as packet buffers and runtime memory for NIC firmware <ref type="bibr" target="#b43">[44]</ref>, but the DRAM is typically not large enough to hold the entire key-value store.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Challenges for Remote Direct Key-Value Access</head><p>KV-Direct moves KV processing from the CPU to the programmable NIC in the server <ref type="figure">(Figure 1c</ref>  <ref type="figure" target="#fig_2">Figure 3a</ref>. On the other hand, with 40 Gbps network and 64-byte KV pairs, the throughput ceiling is 78 Mops with client-side batching. In order to saturate the network with GET operations, the KVS on NIC must make full use of PCIe bandwidth and achieve close to one average memory access per GET. This boils down to three challenges:</p><p>Minimize DMA requests per KV operation. Hash table and memory allocation are two major components in KVS that require random memory access. Previous works propose hash tables <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> with close to 1 memory access per GET operation even under high load factors. However, under higher than 50% load factor, these tables need multiple memory accesses per PUT operation on average with large variance. This not only consumes PCIe throughput, but also leads to latency variations for write-intensive workloads.</p><p>In addition to hash Hide PCIe latency while maintaining consistency. An efficient KVS on NIC must pipeline KV operations and DMA requests to hide the PCIe latency. However, KV operations may have dependencies. A GET following PUT on a same key needs to return the updated value. This requires tracking KV operations being processed and stall the pipeline on data hazard, or better, design an out-of-order executor to resolve data dependency without explicitly stalling the pipeline.</p><p>Dispatch load between NIC DRAM and host memory. An obvious idea is to use the DRAM on NIC as a cache for host memory, but in our NIC, the DRAM throughput (12.8 GB/s) is on par with the achievable throughput (13.2 GB/s) of two PCIe Gen3 x8 endpoints. It is more desirable to distribute memory access between DRAM and host memory in order to utilize both of their bandwidths. However, the onboard DRAM is small (4 GiB) compared to the host memory (64 GiB), calling for a hybrid caching and load-dispatching approach. </p><formula xml:id="formula_0">update scalar2scalar (k, ∆, λ( , ∆) → ) →</formula><p>Atomically update the value of key k using function λ on scalar ∆, and return the original value.</p><formula xml:id="formula_1">update scalar2vector (k, ∆, λ( , ∆) → ) → [ ]</formula><p>Atomically update all elements in vector k using function λ and scalar ∆, and return the original vector.</p><formula xml:id="formula_2">update vector2vector (k, [∆], λ( , ∆) → ) → [ ]</formula><p>Atomically update each element in vector k using function λ on the corresponding element in vector [∆], and return the original vector.</p><formula xml:id="formula_3">reduce (k, Σ, λ( , Σ) → Σ) → Σ</formula><p>Reduce vector k to a scalar using function λ on initial value, and return the reduction result Σ.</p><formula xml:id="formula_4">filter (k, λ( ) → bool) → [ ]</formula><p>Filter elements in a vector k by function λ, and return the filtered vector.</p><p>In the following, we will present KV-Direct, a novel FPGAbased key-value store that satisfies all aforementioned goals and describe how we address the challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN 3.1 System Architecture</head><p>KV-Direct enables remote direct key-value access. Clients send KV-Direct operations ( §3.2) to KVS server while the programmable NIC processes the requests and sending back results, bypassing the CPU. The programmable NIC on KVS server is an FPGA reconfigured as a KV processor ( §3.3). <ref type="figure" target="#fig_0">Figure 2</ref> shows the architecture of KV-Direct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">KV-Direct Operations</head><p>KV-Direct extends one-sided RDMA operations to key-value operations, as summarized in <ref type="table" target="#tab_2">Table 1</ref>. In addition to standard KVS operations as shown in the top part of <ref type="table" target="#tab_2">Table 1</ref>, KV-Direct supports two types of vector operations: Sending a scalar to the NIC on the server and the NIC applies the update to each element in the vector; or send a vector to the server and the NIC updates the original vector element-by-element. Furthermore, KV-Direct supports user-defined update functions as a generalization to atomic operations. The update function needs to be pre-registered and compiled to hardware logic before executing. KV operations with user-defined update functions are similar to active messages <ref type="bibr" target="#b18">[19]</ref>, saving communication and synchronization cost. When a vector operation update, reduce or filter is operated on a key, its value is treated as an array of fixed-bit-width elements. Each function λ operates on one element in the vector, a client-specified parameter ∆, and/or an initial value Σ for reduction. The KV-Direct development toolchain duplicates the λ several times to leverage parallelism in FPGA and match computation throughput with PCIe throughput, then compiles it into reconfigurable hardware logic using an high-level synthesis (HLS) tool <ref type="bibr" target="#b1">[2]</ref>. The HLS tool automatically extracts data dependencies in the duplicated function and generates a fully pipelined programmable logic.</p><p>Update operations with user-defined functions are capable of general stream processing on a vector value. For example, a network processing application may interpret the vector as a stream of packets for network functions <ref type="bibr" target="#b43">[44]</ref> or a bunch of states for packet transactions <ref type="bibr" target="#b68">[68]</ref>. Single-object transaction processing completely in the programmable NIC is also possible, e.g., wrapping around S QUANTITY in TPC-C benchmark <ref type="bibr" target="#b15">[16]</ref>. Vector reduce operation supports neighbor weight accumulation in PageRank <ref type="bibr" target="#b60">[61]</ref>. Non-zero values in a sparse vector can be fetched with vector filter operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">KV Processor</head><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the KV processor in FPGA receives packets from the network, decodes vector operations and buffers KV operations in the reservation station ( §3.3.3). Next, the out-of-order engine ( §3.3.3) issues independent KV operations from reservation station into the operation decoder. Depending on the operation type, the KV processor looks up the hash table ( §3.3.1) and executes the corresponding operations. To minimize the number of memory accesses, small KV pairs are stored inline in the hash table, others are stored in dynamically allocated memory from the slab memory allocator ( §3.3.2). Both the hash index and the slaballocated memory are managed by a unified memory access engine ( §3.3.4), which accesses the host memory via PCIe DMA and caches a portion of host memory in NIC DRAM. After the KV operation completes, the result is sent back to the out-of-order execution engine ( §3.3.3) to find and execute matching KV operations in reservation station. As discussed in §2.4, the scarcity of PCIe operation throughput requires the KV processor to be frugal on DMA accesses. For GET operation, at least one memory read is required. For PUT or DELETE operation, one read and one write are minimal for hash tables. Log-based data structures can achieve one write per PUT, but it sacrifices GET performance. KV-Direct carefully designs the hash table to achieve close to ideal DMA accesses per lookup and insertion, as well as the memory allocator to achieve &lt; 0.1 amortized DMA operations per dynamic memory allocation. <ref type="table">Table.</ref> To store variable-sized KVs, the KV storage is partitioned into two parts. The first part is a hash index ( <ref type="figure" target="#fig_4">Figure 5</ref>), which consists a fixed number of hash buckets. Each hash bucket contains several hash slots and some metadata. The rest of the memory is dynamically allocated, and managed by a slab allocator ( §3.3.2). A hash index ratio configured at initialization time determines the percentage of the memory allocated for hash index. The choice of hash index ratio will be discussed in §5.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Hash</head><p>Each hash slot includes a pointer to the KV data in dynamically allocated memory and a secondary hash. Secondary hash is an optimization that enables parallel inline checking. The key is always checked to ensure correctness, at the cost of one additional memory access. Assuming a 64 GiB KV storage in host memory and 32-byte allocation granularity (a trade-off between internal fragmentation and allocation metadata overhead), the pointer requires 31 bits. A secondary hash of 9 bits gives a 1/512 false positive probability. Cumulatively, the hash slot size is 5 bytes. To determine the hash bucket size, we need to trade-off between the number of hash slots per bucket and the DMA throughput. <ref type="figure" target="#fig_2">Figure 3a</ref> shows that the DMA read throughput below 64B granularity is bound by PCIe latency and parallelism in the DMA engine. A bucket size less than 64B is suboptimal due to increased possibility of hash collision. On the other hand, increasing the bucket size above 64B would decrease hash lookup throughput. So we choose the bucket size to be bytes.</p><p>KV size is the combined size of key and value. KVs smaller than a threshold are stored inline in the hash index to save the additional memory access to fetch KV data. An inline KV may span multiple hash slots, whose pointer and secondary hash fields are re-purposed for storing KV data. It might not be optimal to inline all KVs that can fit in a bucket. To minimize average access time, assuming that smaller and larger keys are equally likely to be accessed, it is more desirable to inline KVs smaller than an inline threshold. To quantify the portion of used buckets in all buckets, we use memory utilization instead of load factor, because it relates more to the number of KVs that can fit in a fixed amount of memory. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, for a certain inline threshold, the average memory access count increases with memory utilization, due to more hash collisions. Higher inline threshold shows a more steep growth curve of memory access count, so an optimal inline threshold can be found to minimize memory accesses under a given memory utilization. As with hash index ratio, the inline threshold can also be configured at initialization time. When all slots in a bucket are filled up, there are several solutions to resolve hash collisions. Cuckoo hashing <ref type="bibr" target="#b61">[62]</ref> and hopscotch hashing <ref type="bibr" target="#b28">[29]</ref> guarantee constant-time lookup by moving occupied slots during insertion. However, in writeintensive workload, the memory access time under high load factor would experience large fluctuations. Linear probing may suffer from primary clustering, therefore its performance is sensitive to the uniformity of hash function. We choose chaining to resolve hash conflicts, which balances lookup and insertion, while being more robust to hash clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Slab Memory</head><p>Allocator. Chained hash slots and non-inline KVs need dynamic memory allocation. We choose slab memory allocator <ref type="bibr" target="#b6">[7]</ref> to achieve O(1) average memory access per allocation and deallocation. The main slab allocator logic runs on host CPU and communicates with the KV-processor through PCIe. Slab allocator rounds up allocation size to the nearest power of two, called slab size. It maintains a free slab pool for each possible slab size (32, 64, . . . , 512 bytes), and a global allocation bitmap to help to merge small free slabs back to larger slabs. Each free slab pool is an array of slab entries consisting of an address field and a slab type field indicating the size of the slab entry. The free slab pool can be cached on the NIC. The cache syncs with the host memory in batches of slab entries. Amortized by batching, less than 0.07 DMA operation is needed per allocation or deallocation. When a small slab pool is almost empty, larger slabs need to be split. Because the slab type is already included in a slab entry, in slab splitting, slab entries are simply copied from the larger pool to the smaller pool, without the need for computation. Including slab type in the slab entry also saves communication cost because one slab entry may contain multiple slots.</p><p>On deallocation, the slab allocator needs to check whether the freed slab can be merged with its neighbor, requiring at least one read and write to the allocation bitmap. Inspired by garbage collection, we propose lazy slab merging to merge free slabs in batch when a slab pool is almost empty and no larger slab pools have enough slabs to split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Out-of-Order Execution Engine.</head><p>Dependency between two KV operations with the same key in the KV processor will lead to data hazard and pipeline stall. This problem is magnified in single-key atomics where all operations are dependent, thus limiting the atomics throughput. We borrow the concept of dynamic scheduling from computer architecture and implement a reservation station to track all in-flight KV operations and their execution context. To saturate PCIe, DRAM and the processing pipeline, up to 256 in-flight KV operations are needed. However, comparing 256 16-byte keys in parallel would take 40% logic resource of our FPGA. Instead, we store the KV operations in a small hash table in on-chip BRAM, indexed by the hash of the key. To simplify hash collision resolution, we regard KV operations with the same hash as dependent, so there may be false positives, but it will never miss a dependency. Operations with the same hash are organized in a chain and examined sequentially. Hash collision would degrade the efficiency of chain examination, so the reservation station contains 1024 hash slots to make hash collision probability below 25%.</p><p>The reservation station not only holds pending operations, but also caches their latest values for data forwarding. When a KV operation is completed by the main processing pipeline, its result is returned to the client, and the latest value is forwarded to the reservation station. Pending operations in the same hash slot are checked one by one, and operations with matching key are executed immediately and removed from the reservation station. For atomic operations, the computation is performed in a dedicated execution engine. For write operations, the cached value is updated. The execution result is returned to the client directly. After scanning through the chain of dependent operations, if the cached value is updated, a PUT operation is issued to the main processing pipeline for cache write back. This data forwarding and fast execution path enable single-key atomics to be processed one operation per clock cycle (180 Mops), eliminate head-of-line blocking <ref type="figure">Figure 7</ref>: DRAM load dispatcher. under workload with popular keys, and ensure consistency because no two operations on the same key can be in the main processing pipeline simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">DRAM Load Dispatcher.</head><p>To further save the burden on PCIe, we dispatch memory accesses between PCIe and the NIC on-board DRAM. Our NIC DRAM has 4 GiB size and 12.8 GB/s throughput, which is an order of magnitude smaller than the KVS storage on host DRAM (64 GiB) and slightly slower than the PCIe link (14 GB/s). One approach is to put a fixed portion of the KVS in NIC DRAM. However, the NIC DRAM is too small to carry a significant portion of memory accesses. The other approach is to use the NIC DRAM as a cache for host memory, the throughput would degrade due to the limited throughput of our NIC DRAM.</p><p>We adopt a hybrid solution to use the DRAM as a cache for a fixed portion of the KVS in host memory, as shown in <ref type="figure">Figure 7</ref>. The cache-able part is determined by the hash of memory address, in granularity of 64 bytes. The hash function is selected so that a bucket in hash index and a dynamically allocated slab have an equal probability of being cache-able. The portion of cache-able part in entire host memory is called load dispatch ratio (l). Assume the cache hit probability is h(l). To balance load on PCIe and NIC DRAM, the load dispatch ratio l should be optimized so that:</p><formula xml:id="formula_5">l tput DRAM = (1 − l) + l • (1 − h(l))</formula><p>tput PC I e let k be the ratio of NIC memory size and host memory size. Under uniform workload, cache hit probability h(l) = NIC memory size cache-able corpus size = k l when k ≤ l. Caching under uniform workload is not efficient. Under long-tail workload with Zipf distribution, assume n is the total number of KVs, approximately h(l) = log(NIC memory size) log(cache-able corpus size) = log(kn) log(ln) when k ≤ l. Under long-tail workload, the cache hit probability is as high as 0.7 with 1M cache in 1G corpus. An optimal l can be solved numerically, as discussed in §6.3.1.  <ref type="figure">Figure 8</ref>, for each slab size, the slab cache on the NIC is synchronized with host DRAM using two double-ended stacks. For the NICside double-ended stack (left side in <ref type="figure">Figure 8</ref>), the left end is popped and pushed by the allocator and deallocator, and the right end is synchronized with the left end of the corresponding host-side stack via DMA. The NIC monitors the size of NIC stack and synchronizes to or from the host stack according to high and low watermarks. Host daemon periodically checks the size of host-side double-ended stack. If it grows above a high watermark, slab merging is triggered; when it drops below a low watermark, slab splitting is triggered. Because each end of a stack is either accessed by the NIC or the host, and the data is accessed prior to moving pointers, race conditions would not occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>DRAM Load Dispatcher. One technical challenge is the storage of metadata in DRAM cache, which requires additional 4 address bits and one dirty flag per 64-byte cache line. Cache valid bit is not needed because all KVS storage is accessed exclusively by the NIC. To store the 5 metadata bits per cache line, extending the cache line to 65 bytes would reduce DRAM performance due to unaligned access; saving the metadata elsewhere will double memory accesses. Instead, we leverage spare bits in ECC DRAM for metadata storage. ECC DRAM typically has 8 ECC bits per 64 bits of data. For Hamming code to correct one bit of error in 64 bits of data, only 7 additional bits are required. The 8th ECC bit is a parity bit for detecting double-bit errors. As we access DRAM in 64-byte granularity and alignment, there are 8 parity bits per 64B data. We increase the parity checking granularity from 64 data bits to 256 data bits, so double-bit errors can still be detected. This allows us to have 6 extra bits which can save our address bits and dirty flag.  Vector Operation Decoder. Compared with PCIe, network is a more scarce resource with lower bandwidth (5 GB/s) and higher latency (2 µs). An RDMA write packet over Ethernet has 88 bytes of header and padding overhead, while a PCIe TLP packet has only 26 bytes of overhead. This is why previous FPGA-based key-value stores <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> have not saturated the PCIe bandwidth, although their hash table designs are less efficient than KV-Direct. This calls for client-side batching in two aspects: batching multiple KV operations in one packet and supporting vector operations for a more compact representation. Towards this end, we implement a decoder in the KV-engine to unpack multiple KV operations from a single RDMA packet. Observing that many KVs have a same size or repetitive values, the KV format includes two flag bits to allow copying key and value size, or the value of the previous KV in the packet. Fortunately, many significant workloads (e.g. graph traversal, parameter server) can issue KV operations in batches. Looking forward, batching would be unnecessary if higher-bandwidth network is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>In this section, we first take a reductionist perspective to support our design choices with microbenchmarks of key components, then switch to a holistic approach to demonstrate the overall performance of KV-Direct in system benchmark.</p><p>We evaluate KV-Direct in a testbed of eight servers and one Arista DCS-7060CX-32S switch. Each server equips two 8 core Xeon E5-2650 v2 CPUs with hyper-threading disabled, forming two NUMA nodes connected through QPI Link. Each NUMA node is populated with 8 DIMMs of 8 GiB Samsung DDR3-1333 ECC RAM, resulting a total of 128 GiB of host memory on each server. A programmable NIC <ref type="bibr" target="#b9">[10]</ref> is connected to the PCIe root complex of CPU 0, and its 40 Gbps Ethernet port is connected to the switch. The programmable NIC has two PCIe Gen3 x8 links in a bifurcated Gen3 x16 physical connector. The tested server equips SuperMicro X9DRG-QF motherboard and one 120 GB SATA SSD running Archlinux (kernel 4.11.9-1).</p><p>For system benchmark, we use YCSB workload <ref type="bibr" target="#b14">[15]</ref>. For skewed Zipf workload, we choose skewness 0.99 and refer it as long-tail workload.      <ref type="table">Table.</ref> There are two free parameters in our hash table design: (1) inline threshold, (2) the ratio of hash index in the entire memory space. As shown in <ref type="figure" target="#fig_7">Figure 9a</ref>, when hash index ratio grows, more KV pairs can be stored inline, yielding a lower average memory access time. <ref type="figure" target="#fig_7">Figure 9b</ref> shows the increase of memory accesses as more memory is utilized. As shown in <ref type="figure" target="#fig_8">Figure 10</ref>, the maximal achievable memory utilization drops under higher hash index ratio, because less memory is available for dynamic allocation. Consequently, aiming to accommodate the entire corpus in a given memory size, the hash index ratio has an upper bound. We choose this upper bound and get a minimal average memory access times, shown as the dashed line in <ref type="figure" target="#fig_8">Figure 10</ref>.</p><p>In <ref type="figure" target="#fig_12">Figure 11</ref>, we plot the number of memory accesses per GET and PUT operation for three possible hash table designs: chaining in KV-Direct, bucket cuckoo hashing in MemC3 <ref type="bibr" target="#b22">[23]</ref> and chain-associative hopscotch hashing in FaRM <ref type="bibr" target="#b17">[18]</ref>. For KV-Direct, we make the optimal choice of inline threshold and hash index ratio for the given KV size and memory utilization requirement. For cuckoo and hopscotch hashing, we assume that keys are inlined and can be compared in parallel, while the values are stored in dynamically allocated slabs. Since the hash table of MemC3 and FaRM cannot support more than 55% memory utilization for 10B KV size, the three rightmost bars in <ref type="figure" target="#fig_12">Figure 11a</ref> and <ref type="figure" target="#fig_12">Figure 11b</ref> only show the performance of KV-Direct.</p><p>For inline KVs, KV-Direct has close to 1 memory access per GET and close to 2 memory accesses per PUT under non-extreme memory utilizations. GET and PUT for noninline KVs have one additional memory access. Comparing KV-Direct and chained hopscotch hashing under high memory utilization, hopscotch hashing performs better in GET, but significantly worse in PUT. Although KV-Direct cannot guarantee worst case DMA accesses, we strike for a balance between GET and PUT. Cuckoo hashing needs to access up to two hash slots on GET, therefore has more memory accesses than KV-Direct under most memory utilizations. Under high memory utilization, cuckoo hashing incurs large fluctuations in memory access times per PUT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Slab Memory</head><p>Allocator. The communication overhead of slab memory allocator comes from the NIC accessing available slab queues in host memory. To sustain the maximal throughput of 180M operations per second, in the worst case, 180M slab slots need to be transferred, consuming 720 MB/s PCIe throughput, i.e., 5% of total PCIe throughput of our NIC.</p><p>The computation overhead of slab memory allocator comes from slab splitting and merging on host CPU. Fortunately, they are not frequently invoked. For workloads with stable KV size distributions, newly freed slab slots are reused by subsequent allocations, therefore does not trigger splitting and merging.</p><p>Slab splitting requires moving continuous slab entries from one slab queue to another. When the workload shifts from large KV to small KV, in the worst case the CPU needs to move 90M slab entries per second, which only utilizes 10% of a core because it is simply continuous memory copy.</p><p>Merging free slab slots to larger slots is rather a timeconsuming task, because it involves filling the allocation bitmap with potentially random offsets and thus requiring random memory accesses. To sort the addresses of free slabs and merge continuous ones, radix sort <ref type="bibr" target="#b66">[66]</ref> scales better to multiple cores than simple bitmap. As shown in <ref type="figure" target="#fig_0">Figure 12</ref>,  merging all 4 billion free slab slots in a 16 GiB vector requires 30 seconds on a single core, or only 1.8 seconds on 32 cores using radix sort <ref type="bibr" target="#b66">[66]</ref>. Although garbage collecting free slab slots takes seconds, it runs in background without stalling the slab allocator, and practically only triggered when the workload shifts from small KV to large KV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Out-of-Order Execution Engine.</head><p>We evaluate the effectiveness of out-of-order execution by comparing the throughput with the simple approach that stalls the pipeline on key conflict, under atomics and long-tail workload. Onesided RDMA and two-sided RDMA <ref type="bibr" target="#b36">[37]</ref> throughputs are also shown as baselines.</p><p>Without this engine, an atomic operation needs to wait for PCIe latency and processing delay in the NIC, during which subsequent atomic operations on the same key cannot be executed. This renders a single-key atomics throughput of 0.94 Mops in <ref type="figure" target="#fig_2">Figure 13a</ref>, consistent with 2.24 Mops measured from an RDMA NIC <ref type="bibr" target="#b36">[37]</ref>. The higher throughput of RDMA NIC can be attributed to its higher clock frequency and lower processing delay. With out-of-order execution, single-key atomic operations in KV-Direct can be processed at peak throughput, i.e., one operation per clock cycle. In MICA <ref type="bibr" target="#b50">[51]</ref>, single-key atomics throughput cannot scale beyond a single core. Atomic fetch-and-add can be spread to multiple cores in <ref type="bibr" target="#b36">[37]</ref>, but it relies on the commutativity among the atomics and therefore does not apply to non-commutative atomics such as compare-and-swap.</p><p>With out-of-order execution, single-key atomics throughput improves by 191x and reaches the clock frequency bound of 180 Mops. When the atomic operations spread uniformly among multiple keys, the throughput of one-sided RDMA, two-sided RDMA and KV-Direct without out-of-order execution grow linearly with the number of keys, but still far from the optimal throughput of KV-Direct. <ref type="figure" target="#fig_2">Figure 13b</ref> shows the throughput under the long-tail workload. Recall that the pipeline is stalled when a PUT operation finds any in-flight operation with the same key. The long-tail workload has multiple extremely popular keys, so it is likely that two operations with the same popular key arrive closely in time. With higher PUT ratio, it is more likely that at least one of the two operations is a PUT, therefore triggering a pipeline stall.      <ref type="figure" target="#fig_3">Figure 14</ref> shows the throughput improvement of DRAM load dispatch over the baseline of using PCIe only. Under uniform workload, the caching effect of DRAM is negligible because its size is only 6% of host KVS memory. Under long-tail workload, ∼30% of memory accesses are served by the DRAM cache. Overall, the memory access throughput for 95% and 100% GET achieves the 180 Mops clock frequency bound. However, if DRAM is simply used as a cache, the throughput would be adversely impacted because the DRAM throughput is lower than PCIe throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5">Vector Operation</head><p>Decoder. To evaluate the efficiency of vector operations in KV-Direct, <ref type="table">Table compares</ref> the throughput of atomic vector increment with two alternative approaches: (1) If each element is stored as a unique key, the bottleneck is the network to transfer the KV operations. (2) If the vector is stored as a large opaque value, retrieving the vector to the client also overwhelms the network. Additionally, the two alternatives in Table do not ensure consistency within the vector. Adding synchronization would incur further overhead.</p><p>KV-Direct client packs KV operations in network packets to mitigate packet header overhead. <ref type="figure" target="#fig_4">Figure 15</ref> shows that network batching increases network throughput by up to 4x, while keeping networking latency below 3.5 µs.  Before each benchmark, we tune hash index ratio, inline threshold and load dispatch ratio according to the KV size, access pattern and target memory utilization. Then we generate random KV pairs with a given size. The key size in a given inline KV size is irrelevant to the performance of KV-Direct, because the key is padded to the longest possible inline KV size during processing. To test inline case, we use KV size that is a multiple of slot size (when size ≤ 50, i.e. slots). To test non-inline case, we use KV size that is a power of two minus 2 bytes (for metadata). As the last step of preparation, we issue PUT operations to insert the KV pairs into an idle KVS until 50% memory utilization. The performance under other memory utilizations can be derived from <ref type="figure" target="#fig_12">Figure 11</ref>.</p><p>During benchmark, we use an FPGA-based packet generator <ref type="bibr" target="#b43">[44]</ref> in the same ToR to generate batched KV operations, send them to the KV server, receive completions and measure sustainable throughput and latency. The processing delay of the packet generator is pre-calibrated via direct loop-back and removed from latency measurements. Error bars represent the 5 t h and 95 t h percentile. <ref type="figure" target="#fig_5">Figure 16</ref> shows the throughput of KV-Direct under YCSB uniform and long-tail (skewed Zipf) workload. Three factors may be the bottleneck of KV-Direct: clock frequency, network and PCIe/DRAM. For 5B∼15B KVs inlined in the hash index, most GETs require one PCIe/-DRAM access and PUTs require two PCIe/DRAM accesses. Such tiny KVs are prevalent in many systems. In PageRank, the KV size for an edge is 8B. In sparse logistic regression, the KV size is typically 8B-16B. For sequencers and locks in distributed systems, the KV size is 8B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Throughput.</head><p>Under same memory utilization, larger inline KVs have lower throughput, due to a higher probability of hash collision. 62B and larger KVs are not inlined, so they require an additional memory access. Long-tail workload has higher throughput than uniform workload and able to reach the clock frequency bound of 180 Mops under read-intensive workload, or reach the network throughput bound for ≥62B KV sizes.  Under long-tail workload, the out-of-order execution engine merges up to ∼15% operations on the most popular keys, and the NIC DRAM has ∼60% cache hit rate under 60% load dispatch ratio, which collectively lead to up to 2x throughput as uniform workload. As shown in <ref type="table" target="#tab_8">Table 3</ref>, the throughput of a KV-Direct NIC is on-par with a state-of-the-art KVS server with tens of CPU cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Power efficiency.</head><p>When the KV-Direct server is at peak throughput, the system power is 121.4 watts (measured on the wall). Compared with state-of-the-art KVS systems in <ref type="table" target="#tab_8">Table 3</ref>, KV-Direct is 3x more power efficient than other systems, being the first general-purpose KVS system to achieve 1 million KV operations per watt on commodity servers.</p><p>When the KV-Direct NIC is unplugged, an idle server consumes 87.0 watts power, therefore the combined power consumption of programmable NIC, PCIe, host memory and the daemon process on CPU is only 34 watts. The measured power difference is justified since the CPU is almost idle and the server can run other workloads when KV-Direct is operating (we use the same criterion for one-sided RDMA, shown at parentheses of <ref type="table" target="#tab_8">Table 3</ref>). In this regard, KV-Direct is 10x more power efficient than CPU-based systems. <ref type="figure" target="#fig_22">Figure 17</ref> shows the latency of KV-Direct under the peak throughput of YCSB workload. Without network batching, the tail latency ranges from 3∼9 µs depending on KV size, operation type and key distribution. PUT has higher latency than GET due to additional memory access. Skewed workload has lower latency than uniform due to more likelihood of being cached in NIC DRAM. Larger KV has higher latency due to additional network and PCIe transmission delay. Network batching adds less than 1 µs latency than non-batched operations, but significantly improves throughput, which has been evaluated in <ref type="figure" target="#fig_4">Figure 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Latency.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.5</head><p>Impact on CPU performance. KV-Direct is designed to bypass the server CPU and uses only a portion of host memory for KV storage. Therefore, the CPU can still run other applications. Our measurements find a minimal impact on other workloads on the server when a single NIC KV-Direct is at peak load.  <ref type="bibr" target="#b36">[37]</ref> Two-sided RDMA, 12 cores PCIe 98.3 ∼60 ∼490 ∼300 5 Xilinx'13 <ref type="bibr" target="#b4">[5]</ref> FPGA (with host) Network 3.5 4.5 Mega-KV <ref type="bibr" target="#b75">[75]</ref> GPU <ref type="formula">(4 GiB</ref>   peak throughput of KV-Direct. Except for sequential throughput of CPU 0 to access its own NUMA memory (the line marked in bold), the latency and throughput of CPU memory accesses are mostly unaffected. This is because 8 channels of host memory can provide far higher random access throughput than all CPU cores could consume, while the CPU can indeed stress the sequential throughput of the DRAM channels. The impact of the host daemon process is minimal when the distribution of KV sizes is relatively stable, because the garbage collector is invoked only when the number of available slots for different slab sizes are imbalanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXTENSIONS 6.1 CPU-based Scatter-Gather DMA</head><p>PCIe has 29% TLP header and padding overhead for 64B DMA operations ( §2.4) and the DMA engine may not have enough parallelism to saturate the PCIe bandwidth-delay product with small TLPs ( §4). Larger DMA operations with up to 256-byte TLP payload is supported by the PCIe root complex  in our system. In this case, the TLP head and padding overhead is only 9%, and the DMA engine has enough parallelism (64) to saturate the PCIe link with 27 in-flight DMA reads. To batch the DMA operations on PCIe link, we can leverage the CPU to perform scatter-gather ( <ref type="figure" target="#fig_7">Figure 19</ref>). First, the NIC DMAs addresses to a request queue in host memory. The host CPU polls the request queue, performs random memory access, put the data in response queue and writes MMIO doorbell to the NIC. The NIC then fetches the data from response queue via DMA. <ref type="figure" target="#fig_24">Figure 18</ref> shows that CPU-based scatter-gather DMA has up to 79% throughput improvement compared to the CPUbypassing approach. In addition to the CPU overhead, the primary drawback of CPU-based scatter-gather is the additional latency. To save MMIOs from the CPU to the NIC, we batch 256 DMA operations per doorbell, which requires 10 µs to complete. The overall latency for the NIC to access host memory using CPU-based scatter-gather is ∼20 µs, almost 20x higher than direct DMA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Multiple NICs per Server</head><p>In some cases, it may be desirable to build a dedicated keyvalue store with maximal throughput per server. Through simulation, <ref type="bibr" target="#b46">[47]</ref> showed the possibility of achieving a billion KV op/s in a single server with four (currently unavailable) 60-core CPUs. As shown in <ref type="table" target="#tab_8">Table 3</ref>, with 10 KV-Direct NICs   on a server, the one billion KV op/s performance is readily achievable with a commodity server. The KV-Direct server consumes 357 watts power (measured on the wall) to achieve 1.22 Gop/s GET or 0.61 Gop/s PUT. In order to saturate the 80 PCIe Gen3 lanes of two Xeon E5 CPUs, we replace the motherboard of the benchmarked server (Sec. 5) with a SuperMicro X9DRX+-F motherboard with 10 PCIe Gen3 x8 slots. We use PCIe x16 to x8 converters to connect 10 programmable NICs on each of the slots, and only one PCIe Gen3 x8 link is enabled on each NIC, so the throughput per NIC is lower than <ref type="figure" target="#fig_5">Figure 16</ref>. Each NIC owns an exclusive memory region in host memory and serves a disjoint partition of keys. Multiple NICs suffer the same load imbalance problem as a multi-core KVS implementation. Fortunately, for a small number of partitions (e.g. 10), the load imbalance is not significant <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b50">51]</ref>. Under YCSB longtail workload, the highest-loaded NIC has 1.5x load of the average, and the added load from extremely popular keys is served by the out-of-order execution engine <ref type="figure" target="#fig_2">(Sec. 3.3.</ref>3). <ref type="figure" target="#fig_0">Figure 20</ref> shows that KV-Direct throughput scales almost linearly with the number of NICs on a server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Discussion</head><p>6.3.1 NIC hardware with different capacity. The goal of KV-Direct is to leverage existing hardware in data centers to offload an important workload (KV access), instead of designing a special hardware to achieve maximal KVS performance. We use programmable NICs, which usually contain limited amount of DRAM for buffering and connection-state tracking. Large DRAMs are expensive in both die size and power consumption.</p><p>Even if future NICs have faster or larger on-board memory, under long-tail workload, our load dispatch design (Sec. 3.3.4)  <ref type="table">Table 6</ref>: Relative throughput of load dispatch compared to simple partitioning. Column titles are same as <ref type="table" target="#tab_11">Table 5</ref>.</p><p>still shows performance gain over the simple design of partitioning the keys uniformly according to NIC and host memory capacity. <ref type="table" target="#tab_11">Table 5</ref> shows the optimal load dispatch ratio for long-tail workload with a corpus of 1 billion keys, under different ratio of NIC DRAM and PCIe throughput and different ratio of NIC and host memory size. If a NIC has faster DRAM, more load will be dispatched to the NIC. A load dispatch ratio of 1 means the NIC memory behaves exactly like a cache of host memory. If a NIC has larger DRAM, a slightly less portion of load will be dispatched to the NIC. As shown in <ref type="table">Table 6</ref>, even when the size of NIC DRAM is a tiny fraction of host memory, the throughput gain is significant. The hash table and slab allocator design (Sec. 3.3.1) is generally applicable to hash-based storage systems that need to be frugal of random accesses for both lookup and insertion.</p><p>The out-of-order execution engine (Sec. 3.3.3) can be applied to all kinds of applications in need of latency hiding, and we hope future RDMA NICs to support that for atomics. In 40 Gbps networks, network bandwidth bounds nonbatched KV throughput, so we use client-side batching (Sec.4). With higher network bandwidth, batch size can be reduced, thus reducing latency. In a 200 Gbps network, a KV-Direct NIC could achieve 180 Mop/s without batching.</p><p>KV-Direct leverages widely-deployed programmable NICs with FPGAs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b63">64]</ref>. FlexNIC <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> is another promising architecture for programmable NICs with Reconfigurable Match-action Tables (RMT) <ref type="bibr" target="#b7">[8]</ref>. NetCache <ref type="bibr" target="#b34">[35]</ref> implements a KV cache in RMT-based programmable switches, showing potential for building KV-Direct in an RMT-based NIC.</p><p>6.3.2 Implications for real-world applications. Backof-the-envelope calculations show potential performance gains when KV-Direct is applied in end-to-end applications. In PageRank <ref type="bibr" target="#b60">[61]</ref>, because each edge traversal can be implemented with one KV operation, KV-Direct supports 1.22G TEPS on a server with 10 programmable NICs. In comparison, GRAM <ref type="bibr" target="#b73">[73]</ref> supports 250M TEPS per server, bound by interleaved computation and random memory access.</p><p>KV-Direct supports user-defined functions and vector operations <ref type="table" target="#tab_2">(Table 1</ref>) that can further optimize PageRank by offloading client computation to hardware. Similar arguments hold for parameter server <ref type="bibr" target="#b45">[46]</ref>. We expect future work to leverage hardware-accelerated key-value stores to improve distributed application performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>As an important infrastructure, the research and development of distributed key-value store systems have been driven by performance. A large body of distributed KVS are based on CPU. To reduce the computation cost, Masstree <ref type="bibr" target="#b52">[53]</ref>, MemC3 <ref type="bibr" target="#b22">[23]</ref> and libcuckoo <ref type="bibr" target="#b47">[48]</ref> optimize locking, caching, hashing and memory allocation algorithms, while KV-Direct comes with a new hash table and memory management mechanism specially designed for FPGA to minimize the PCIe traffic. MICA <ref type="bibr" target="#b50">[51]</ref> partitions the hash table to each core thus completely avoids synchronization. This approach, however, introduces core imbalance for skewed workloads.</p><p>To get rid of the OS kernel overhead, <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b65">65]</ref> directly poll network packets from NIC and <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b53">54]</ref> process them with the user space lightweight network stack. Key-value store systems <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref> benefit from such optimizations for high performance. As a further step towards this direction, recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b36">37]</ref> leverage the hardware-based network stack of RDMA NIC, using two-sided RDMA as an RPC mechanism between KVS client and server to further improve per-core throughput and reduce latency. Still, these systems are CPU bound ( §2.2).</p><p>Another different approach is to leverage one-sided RDMA. Pilaf <ref type="bibr" target="#b54">[55]</ref> and FaRM <ref type="bibr" target="#b17">[18]</ref> adopt one-sided RDMA read for GET operation and FaRM achieves throughput that saturates the network. Nessie <ref type="bibr" target="#b70">[70]</ref>, DrTM <ref type="bibr" target="#b72">[72]</ref>, DrTM+R <ref type="bibr" target="#b12">[13]</ref> and FaSST <ref type="bibr" target="#b37">[38]</ref> leverage distributed transactions to implement both GET and PUT with one-sided RDMA. However, the performance of PUT operation suffer from unavoidable synchronization overhead for consistency guarantee, limited by RDMA primitives <ref type="bibr" target="#b36">[37]</ref>. Moreover, client-side CPU is involved in KV processing, limiting per-core throughput to ∼10 Mops on the client side. In contrast, KV-Direct extends the RDMA primitives to key-value operations while guarantees the consistency in server side, leaving the KVS client totally transparent while achieving high throughput and low latency even for PUT operation.</p><p>As a flexible and customizable hardware, FPGA is now widely deployed in datacenter-scale <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b63">64]</ref> and greatly improved for programmability <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b43">44]</ref>. Several early works have explored building KVS on FPGA. But some of them are not practical by limiting the data storage in on-chip (about several MB memory) <ref type="bibr" target="#b49">[50]</ref> or on-board DRAM (typically 8GB memory) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. <ref type="bibr" target="#b5">[6]</ref> focuses on improving system capacity rather than throughput, and adopts SSD as the secondary storage out of on-board DRAM. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">50]</ref> limit their usage in fixed size key-value pairs, which can only work for the special purpose rather than a general key-value store. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43]</ref> uses host DRAM to store the hash table, and <ref type="bibr" target="#b71">[71]</ref> uses NIC DRAM as a cache of host DRAM, but they did not optimize for network and PCIe DMA bandwidth, resulting in poor performance. KV-Direct fully utilizes both NIC DRAM and host DRAM, making our FPGA-based key-value store system general and capable of large-scale deployment. Furthermore, our careful hardware and software co-design, together with optimizations for PCIe and networking push the performance to the physical limitation, advancing state-of-art solutions.</p><p>Secondary index is an important feature to retrieve data by keys other than the primary key in data storage system <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">42]</ref>. SLIK <ref type="bibr" target="#b41">[42]</ref> supports multiple secondary keys using a B+ tree algorithm in key-value store system. It would be interesting to explore how to support secondary index to help KV-Direct step towards a general data storage system. SwitchKV <ref type="bibr" target="#b48">[49]</ref> leverages content-based routing to route requests to backend nodes based on cached keys, and Net-Cache <ref type="bibr" target="#b34">[35]</ref> takes a further step to cache KV in the switches. Such load balancing and caching will also benefit our system. Eris <ref type="bibr" target="#b44">[45]</ref> leverages network sequencers to achieve efficient distributed transactions, which may give a new life to the one-sided RDMA approach with client synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we describe the design and evaluation of KV-Direct, a high performance in-memory key-value store. Following a long history in computer system design, KV-Direct is another exercise in leveraging reconfigurable hardware to accelerate an important workload. KV-Direct is able to obtain superior performance by carefully co-designing hardware and software in order to remove bottlenecks in the system and achieve performance that is close to the physical limits of the underlying hardware.</p><p>After years of broken promises, FPGA-based reconfigurable hardware finally becomes widely available in main stream data centers. Many significant workloads will be scrutinized to see whether they can benefit from reconfigurable hardware, and we expect much more fruitful work in this general direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Programmable NIC with FPGA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>PCIe random DMA performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>KV processor architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Hash index structure. Each line is a hash bucket containing 10 hash slots, 3 bits of slab memory type per hash slot, one bitmap marking the beginning and end of inline KV pairs and a pointer to the next chained bucket on hash collision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Average memory access count under varying inline thresholds (10B, 15B, 20B, 25B) and memory utilizations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fix hash index ratio 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Memory access count under different memory utilizations or hash index ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Determine the optimal hash index ratio for a required memory utilization and KV size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Memory accesses per KV operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Execution time of merging 4 billion slab slots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Effectiveness of out-of-order execution engine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>DMA throughput with load dispatch (load dispatch ratio = 0.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 :</head><label>15</label><figDesc>Efficiency of network batching.5.1.4 DRAM Load Dispatcher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 16 :</head><label>16</label><figDesc>Throughput of KV-Direct under YCSB work-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 17 :</head><label>17</label><figDesc>Latency of KV-Direct under peak throughput of YCSB workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 18 :</head><label>18</label><figDesc>Scatter-gather performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 19 Figure 20 :</head><label>1920</label><figDesc>Performance of multiple NICs per server.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>KV-Direct operations.</figDesc><table /><note>get (k) → Get the value of key k.put (k, ) → bool Insert or replace a (k, ) pair.delete (k) → bool Delete key k.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Our hardware platform is built on an Intel Stratix V FPGA based programmable NIC ( §2.3). The programmable NIC is attached to the server through two PCIe Gen3 x8 links in ] to synthesize hardware logic from OpenCL. Our KV processor is implemented in 11K lines of OpenCL code and all kernels are fully pipelined, i.e., the throughput is one operation per clock cycle. With 180 MHz clock frequency, our design can process KV operations at 180 M op/s if not bottlenecked by network, DRAM or PCIe.</figDesc><table><row><cell>Hashtable</cell><cell>32B stack 512B stack</cell><cell>Sync</cell><cell>32B stack 512B stack</cell><cell>Host Daemon merger Splitter</cell></row><row><cell></cell><cell>NIC side</cell><cell></cell><cell cols="2">Host side</cell></row><row><cell></cell><cell cols="3">Figure 8: Slab memory allocator.</cell><cell></cell></row><row><cell cols="5">a bifurcated x16 physical connector, and contains 4 GiB of</cell></row><row><cell cols="5">on-board DRAM with a single DDR3-1600 channel.</cell></row><row><cell cols="5">For development efficiency, we use Intel FPGA SDK for</cell></row><row><cell>OpenCL [2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Below highlights several implementation details. Slab Memory Allocator. As shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="7">Throughput (GB/s) of vector operations with</cell></row><row><cell cols="5">vector update or alternatives.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Throughput (MOps)</cell><cell>20 180 200</cell><cell>8</cell><cell>16</cell><cell>32 No Batching 64 128 256 Batching</cell><cell>Latency (us)</cell><cell>2 2.2 2.4 2.6 2.8 3 3.2 3.4</cell><cell>8</cell><cell>16</cell><cell>32 No Batching 64 128 Batching</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Batched KV Size (B)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Batched KV Size (B)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Throughput.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table quantifies</head><label>quantifies</label><figDesc></figDesc><table><row><cell>KVS</cell><cell>Comment</cell><cell>Bottleneck</cell><cell cols="6">Tput (Mops) Power Efficiency (Kops/W) Avg Delay (µs)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">GET PUT</cell><cell>GET</cell><cell cols="2">PUT GET</cell><cell>PUT</cell></row><row><cell>Memcached [25]</cell><cell>Traditional</cell><cell>Core synchronization</cell><cell>1.5</cell><cell>1.5</cell><cell>∼5</cell><cell cols="2">∼5 ∼50</cell><cell>∼50</cell></row><row><cell>MemC3 [23]</cell><cell>Traditional</cell><cell>OS network stack</cell><cell>4.3</cell><cell>4.3</cell><cell>∼14</cell><cell cols="2">∼14 ∼50</cell><cell>∼50</cell></row><row><cell>RAMCloud [59]</cell><cell>Kernel bypass</cell><cell>Dispatch thread</cell><cell></cell><cell>1</cell><cell>∼20</cell><cell>∼3.3</cell><cell></cell><cell>14</cell></row><row><cell>MICA [51]</cell><cell cols="2">Kernel bypass, 24 cores, 12 NIC ports CPU KV processing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FaRM [18]</cell><cell>One-sided RDMA for GET</cell><cell>RDMA NIC</cell><cell></cell><cell>3</cell><cell>∼30 (261)</cell><cell>∼15</cell><cell>4.5</cell><cell>∼10</cell></row><row><cell>DrTM-KV [72]</cell><cell>One-sided RDMA and HTM</cell><cell>RDMA NIC</cell><cell cols="3">115.2 14.3 ∼500 (3972)</cell><cell>∼60</cell><cell>3.4</cell><cell>6.3</cell></row><row><cell>HERD'16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">this impact at the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Comparison of KV-Direct with other KVS systems under long-tail (skewed Zipf) workload of 10B tiny KVs. For metrics not reported in the papers, we emulate the systems using similar hardware and report our approximate measurements. For CPU-bypass systems, numbers in parentheses report power difference under peak load and idle.</figDesc><table><row><cell cols="2">KV-Direct status →</cell><cell>Idle</cell><cell>Busy</cell></row><row><cell></cell><cell>CPU0-0</cell><cell>82.2 ns</cell><cell>83.5 ns</cell></row><row><cell>Random</cell><cell>CPU0-1</cell><cell>129.3 ns</cell><cell>129.9 ns</cell></row><row><cell>Latency</cell><cell>CPU1-0</cell><cell>122.3 ns</cell><cell>122.2 ns</cell></row><row><cell></cell><cell>CPU1-1</cell><cell>84.2 ns</cell><cell>84.3 ns</cell></row><row><cell></cell><cell>CPU0-0</cell><cell cols="2">60.3 GB/s 55.8 GB/s</cell></row><row><cell>Sequential</cell><cell>CPU0-1</cell><cell>25.7 GB/s</cell><cell>25.6 GB/s</cell></row><row><cell>Throughput</cell><cell>CPU1-0</cell><cell>25.5 GB/s</cell><cell>25.9 GB/s</cell></row><row><cell></cell><cell>CPU1-1</cell><cell>60.2 GB/s</cell><cell>60.3 GB/s</cell></row><row><cell></cell><cell cols="3">32B read 10.53 GB/s 10.46 GB/s</cell></row><row><cell>Random</cell><cell cols="3">64B read 14.41 GB/s 14.42 GB/s</cell></row><row><cell>Throughput</cell><cell>32B write</cell><cell>9.01 GB/s</cell><cell>9.04 GB/s</cell></row><row><cell></cell><cell cols="3">64B write 12.96 GB/s 12.94 GB/s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Impact on CPU memory access performance when KV-Direct is at peak throughput. Measured with Intel Memory Latency Checker v3.4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Optimal load dispatch ratio for long-tail workload under different NIC DRAM/PCIe throughput ratio (vertical) and NIC/host memory size ratio (horizontal).</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank Kun Tan, Ningyi Xu, Ming Wu, Jiansong Zhang and Anuj Kalia for all technical discussions and valuable comments. We'd also like to thank the whole Catapult v-team at Microsoft for support on the FPGA platform. We thank our shepherd, Simon Peter, and other anonymous reviewers for their valuable feedback and comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">InfiniBand Architecture Specification: Release 1.0. InfiniBand Trade Association</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Altera SDK for OpenCL</title>
		<ptr target="http://www.altera.com/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Workload analysis of a large-scale key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Paleczny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="2012" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rodric Rabbah, and Sunil Shukla</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bacon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="56" to="63" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>FPGA programming for the masses</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Achieving 10Gbps Line-rate Key-value Stores with FPGAs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimon</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 5th USENIX Workshop on Hot Topics in Cloud Computing. USENIX</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Kees Vissers, Jeremia Bär, and Zsolt István</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling Out to a Single-Node 80Gbps Memcached Server with 40Ter-abytes of Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotStorage &apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Kimon Karras, and Kees A Vissers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Slab Allocator: An Object-Caching Kernel Memory Allocator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bonwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX summer</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Forwarding metamorphosis: Fast programmable match-action processing in hardware for SDN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Bosshart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Gibb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hun-Seok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Izzard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Mujica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="110" />
			<date type="published" when="2013" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Horton tables: fast hash tables for in-memory data-intensive computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Breslow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Greathouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuwan</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A cloud-scale acceleration architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Adrian M Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note>49th Annual</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An FPGA memcached appliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Sai Rahul Chalamalasetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Auyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Margala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/SIGDA international symposium on Field programmable gate arrays (FPGA)</title>
		<meeting>the ACM/SIGDA international symposium on Field programmable gate arrays (FPGA)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bigtable: A distributed storage system for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fay</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><forename type="middle">A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast and general distributed transactions using RDMA and HTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingda</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurosys &apos;16</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Invited -Heterogeneous Datacenters: Options and Opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody Hao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Design Automation Conference (DAC &apos;16)</title>
		<meeting>the 53rd Annual Design Automation Conference (DAC &apos;16)<address><addrLine>New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>6 pages</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with YCSB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM symposium on Cloud computing</title>
		<meeting>the 1st ACM symposium on Cloud computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">tpc-c benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tpc Council</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>revision 5.11.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Swaminathan Sivasubramanian, Peter Vosshall, and Werner Vogels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madan</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunavardhan</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pilchin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="205" to="220" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Dynamo: amazon&apos;s highly available key-value store</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Miguel Castro, and Orion Hodson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Dragojević</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyanth</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>FaRM: fast remote memory</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Active messages: a mechanism for integrated communication and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tv Eicken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><forename type="middle">Copen</forename><surname>Culler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">Erik</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 19th Annual International Symposium on. IEEE</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="256" to="266" />
		</imprint>
	</monogr>
	<note>Computer Architecture, 1992. Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HyperDex: A distributed, searchable key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Escriva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emin Gün</forename><surname>Sirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dark silicon and the end of multicore scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Hadi Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renee</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthikeyan</forename><surname>St Amant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture (ISCA), 2011 38th Annual International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Power challenges may end the multicore era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Hadi Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renée</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthikeyan</forename><surname>St Amant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="93" to="102" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MemC3: Compact and concurrent memcache with dumber caching and smarter hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="371" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VFP: A Virtual Switch Platform for Host SDN in the Public Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Firestone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;17</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="315" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed caching with memcached</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux journal</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Hiding memory latency using dynamic scheduling in shared-memory multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kourosh</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hennessy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>ACM</publisher>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SDN for the Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Keynote in the 2015 ACM Conference on Special Interest Group on Data Communication</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PacketShader: a GPU-accelerated software router</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangjin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sue</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="195" to="206" />
			<date type="published" when="2010" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hopscotch hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Herlihy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moran</forename><surname>Tzafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Distributed Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="350" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Programming and Runtime Support to Blaze FPGA Accelerator Deployment at Datacenter Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenman</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Interlandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyson</forename><surname>Condie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh ACM Symposium on Cloud Computing (SoCC &apos;16)</title>
		<meeting>the Seventh ACM Symposium on Cloud Computing (SoCC &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="456" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Data plane development kit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dpdk Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A flexible hash table design for 10gbps key-value stores on fpgas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>István</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Vissers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Field programmable Logic and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A hash table for line-rate data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>István</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Vissers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Reconfigurable Technology and Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015" />
			<publisher>TRETS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">mTCP: a Highly Scalable User-level TCP Stack for Multicore Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunyoung</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinae</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Asim</forename><surname>Jamshed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haewon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Ihm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="489" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">NetCache: Balancing Key-Value Stores with Fast In-Network Caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Soule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongkeun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;17</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Changhoon Kim, and Ion Stoica</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using RDMA efficiently for key-value services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Design Guidelines for High Performance RDMA Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">FaSST: fast, scalable and simple distributed transactions with two-sided RDMA datagram RPCs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="185" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chronos: predictable low latency for data center applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malveeka</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM Symposium on Cloud Computing. ACM</title>
		<meeting>the Third ACM Symposium on Cloud Computing. ACM</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">FlexNIC: Rethinking Network DMA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ho-tOS &apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">High Performance Packet Processing with FlexNIC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navven</forename><forename type="middle">Kumar</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 21th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SLIK: Scalable low-latency indexes for a key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankita</forename><surname>Kejriwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An FPGAbased in-line accelerator for Memcached</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maysam</forename><surname>Lavasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Chiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="57" to="60" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ClickNP: Highly flexible and High-performance Network Processing with Reconfigurable Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Layong</forename><surname>Larry Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM &apos;16. ACM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Eris: Coordination-Free Consistent Transactions Using In-Network Concurrency Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellis</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">R K</forename><surname>Ports</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;17</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Scaling Distributed Machine Learning with the Parameter Server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Woo</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Full-Stack Architecting to Achieve a Billion Requests Per Second Throughput on a Single Key-Value Store Server Platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Ho Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukhan</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Pradeep Dubey, and others</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Algorithmic improvements for fast concurrent cuckoo hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurosys &apos;14</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Be fast, cheap and in control with SwitchKV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;16</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Memory efficient and high performance key-value store on FPGA using Cuckoo hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingli</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Field Programmable Logic and Applications (FPL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">MICA: a holistic approach to fast in-memory key-value storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="429" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">FPGA-Accelerated Transactional Execution of Graph Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Chiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<editor>Jonathan W. Greene and Jason Helge Anderson</editor>
		<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays<address><addrLine>Monterey, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-02-22" />
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cache craftiness for fast multicore key-value storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert Tappan</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM european conference on Computer Systems</title>
		<meeting>the 7th ACM european conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Network stack specialization for performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilias</forename><surname>Marinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Handley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="175" to="186" />
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Using One-Sided RDMA Reads to Build a Fast, CPU-Efficient Key-Value Store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Phase Reconciliation for Contended In-Memory Transactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neha</forename><surname>Narula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="511" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mike Paleczny, Daniel Peek, Paul Saab, and others</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Nishtala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Fugal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcelroy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Scaling memcache at facebook</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The case for RAMClouds: scalable high-performance storage entirely in DRAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Leverich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mazières</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhasish</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Parulkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="92" to="105" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Mendel Rosenblum, and others</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Mendel Rosenblum, and others. 2015. The ramcloud storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankita</forename><surname>Kejriwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnam</forename><surname>Montazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">SDA: Software-defined accelerator for large-scale DNN systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiding</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Hot Chips 26 Symposium (HCS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Stanford InfoLab</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flemming Friche Rodler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cuckoo hashing. Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="122" to="144" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fastpass: A centralized zero-queue datacenter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Fugal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="307" to="318" />
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Gopi Prashanth Gopal, Jan Gray, and others. 2014. A reconfigurable fabric for accelerating large-scale datacenter services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kypros</forename><surname>Constantinides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Demme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Fowers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m">ACM/IEEE 41st International Symposium on Computer Architecture (ISCA). IEEE</title>
		<imprint>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Netmap: a novel framework for fast packet I/O</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st USENIX Security Symposium (USENIX Security 12</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Fast sort on CPUs and GPUs: a case for bandwidth oblivious SIMD sort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jatin</forename><surname>Chhugani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM SIGMOD International Conference on Management of data</title>
		<meeting>the 2010 ACM SIGMOD International Conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="351" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Trinity: A distributed graph engine on a memory cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2013 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="505" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Packet transactions: High-level programming for line-rate switches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2016 Conference</title>
		<meeting>the ACM SIGCOMM 2016 Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
	<note>Nick McKeown, and Steve Licking</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The free lunch is over: A fundamental turn toward concurrency in software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herb</forename><surname>Sutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dr. Dobbs journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="202" to="210" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Designing a low-latency cuckoo hash table for write-intensive workloads using RDMA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Szepesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Workshop on Rack-scale Computing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A multilevel NOSQL cache design combining In-NIC and In-Kernel caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Tokusashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Matsutani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High-Performance Interconnects (HOTI &apos;16)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="60" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fast in-memory transaction processing using RDMA and HTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingda</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;15</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="87" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">GraM: scaling graph computation to the trillions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM Symposium on Cloud Computing</title>
		<meeting>the Sixth ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="408" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">TuX2: Distributed Graph Computation for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;17</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Mega-KV: a case for GPUs to maximize the throughput of in-memory key-value stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rubao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1226" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Packet-level telemetry in large datacenter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratul</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="479" to="491" />
			<date type="published" when="2015" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
