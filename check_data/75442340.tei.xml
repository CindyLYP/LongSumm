<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
							<email>bengio@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
							<email>vinyals@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
							<email>ndjaitly@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research Mountain View</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks can be used to process sequences, either as input, output or both. While they are known to be hard to train when there are long term dependencies in the data <ref type="bibr" target="#b0">[1]</ref>, some versions like the Long Short-Term Memory (LSTM) <ref type="bibr" target="#b1">[2]</ref> are better suited for this. In fact, they have recently shown impressive performance in several sequence prediction problems including machine translation <ref type="bibr" target="#b2">[3]</ref>, contextual parsing <ref type="bibr" target="#b3">[4]</ref>, image captioning <ref type="bibr" target="#b4">[5]</ref> and even video description <ref type="bibr" target="#b5">[6]</ref>.</p><p>In this paper, we consider the set of problems that attempt to generate a sequence of tokens of variable size, such as the problem of machine translation, where the goal is to translate a given sentence from a source language to a target language. We also consider problems in which the input is not necessarily a sequence, like the image captioning problem, where the goal is to generate a textual description of a given image.</p><p>In both cases, recurrent neural networks (or their variants like LSTMs) are generally trained to maximize the likelihood of generating the target sequence of tokens given the input. In practice, this is done by maximizing the likelihood of each target token given the current state of the model (which summarizes the input and the past output tokens) and the previous target token, which helps the model learn a kind of language model over target tokens. However, during inference, true previous target tokens are unavailable, and are thus replaced by tokens generated by the model itself, yielding a discrepancy between how the model is used at training and inference. This discrepancy can be mitigated by the use of a beam search heuristic maintaining several generated target sequences, but for continuous state space models like recurrent neural networks, there is no dynamic programming approach, so the effective number of sequences considered remains small, even with beam search.</p><p>The main problem is that mistakes made early in the sequence generation process are fed as input to the model and can be quickly amplified because the model might be in a part of the state space it has never seen at training time.</p><p>Here, we propose a curriculum learning approach <ref type="bibr" target="#b6">[7]</ref> to gently bridge the gap between training and inference for sequence prediction tasks using recurrent neural networks. We propose to change the training process in order to gradually force the model to deal with its own mistakes, as it would have to during inference. Doing so, the model explores more during training and is thus more robust to correct its own mistakes at inference as it has learned to do so during training. We will show experimentally that this approach yields better performance on several sequence prediction tasks.</p><p>The paper is organized as follows: in Section 2, we present our proposed approach to better train sequence prediction tasks with recurrent neural networks; this is followed by Section 3 which draws links to some related approaches. We then present some experimental results in Section 4 and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Proposed Approach</head><p>We are considering supervised tasks where the training set is given in terms of N input/output pairs</p><formula xml:id="formula_0">{X i , Y i } N i=1</formula><p>, where X i is the input and can be either static (like an image) or dynamic (like a sequence) while the target output Y i is a sequence y</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , y i</head><p>Ti of a variable number of tokens that belong to a fixed known dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model</head><p>Given a single input/output pair (X, Y ), the log probability P (Y |X) can be computed as:</p><formula xml:id="formula_1">log P (Y |X) = log P (y T 1 |X) = T t=1 log P (y t |y t−1 1 , X)<label>(1)</label></formula><p>where Y is a sequence of length T represented by tokens y 1 , y 2 , . . . , y T . The latter term in the above equation is estimated by a recurrent neural network with parameters θ by introducing a state vector, h t , that is a function of the previous state, h t−1 , and the previous output token, y t−1 , i.e. log P (y t |y t−1 1 , X; θ) = log P (y t |h t ; θ)</p><p>where h t is computed by a recurrent neural network as follows:</p><formula xml:id="formula_3">h t = f (X; θ) if t = 1, f (h t−1 , y t−1 ; θ) otherwise.<label>(3)</label></formula><p>P (y t |h t ; θ) is often implemented as a linear projection 1 of the state vector h t into a vector of scores, one for each token of the output dictionary, followed by a softmax transformation to ensure the scores are properly normalized (positive and sum to 1). f (h, y) is usually a non-linear function that combines the previous state and the previous output in order to produce the current state.</p><p>This means that the model focuses on learning to output the next token given the current state of the model AND the previous token. Thus, the model represents the probability distribution of sequences in the most general form -unlike Conditional Random Fields <ref type="bibr" target="#b7">[8]</ref> and other models that assume independence between between outputs at different time steps, given latent variable states.</p><p>The capacity of the model is only limited by the representational capacity of the recurrent and feedforward layers. LSTMs, with their ability to learn long range structure are especially well suited to this task and make it possible to learn rich distributions over sequences.</p><p>In order to learn variable length sequences, a special token, &lt;EOS&gt;, that signifies the end of a sequence is added to the dictionary and the model. During training, &lt;EOS&gt; is concatenated to the end of each sequence. During inference, the model generates tokens until it generates &lt;EOS&gt;.</p><p>Although one could also use a multi-layered non-linear projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>Training recurrent neural networks to solve such tasks is usually accomplished by using mini-batch stochastic gradient descent to look for a set of parameters θ that maximizes the log likelihood of producing the correct target sequence Y i given the input data X i for all training pairs (X i , Y i ):</p><formula xml:id="formula_4">θ = arg max θ (X i ,Y i ) log P (Y i |X i ; θ) .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference</head><p>During inference the model can generate the full sequence y T 1 given X by generating one token at a time, and advancing time by one step. When an &lt;EOS&gt; token is generated, it signifies the end of the sequence. For this process, at time t, the model needs as input the output token y t−1 from the last time step in order to produce y t . Since we do not have access to the true previous token, we can instead either select the most likely one given our model, or sample according to it.</p><p>Searching for the sequence Y with the highest probability given X is too expensive because of the combinatorial growth in the number of sequences. Instead we use a beam searching procedure to generate k "best" sequences. We do this by maintaining a heap of m best candidate sequences. At each time step new candidates are generated by extending each candidate by one token and adding them to the heap. At the end of the step, the heap is re-pruned to only keep m candidates. The beam searching is truncated when no new sequences are added, and k best sequences are returned.</p><p>While beam search is often used for discrete state based models like Hidden Markov Models where dynamic programming can be used, it is harder to use efficiently for continuous state based models like recurrent neural networks, since there is no way to factor the followed state paths in a continuous space, and hence the actual number of candidates that can be kept during beam search decoding is very small.</p><p>In all these cases, if a wrong decision is taken at time t − 1, the model can be in a part of the state space that is very different from those visited from the training distribution and for which it doesn't know what to do. Worse, it can easily lead to cumulative bad decisions -a classic problem in sequential Gibbs sampling type approaches to sampling, where future samples can have no influence on the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Bridging the Gap with Scheduled Sampling</head><p>The main difference between training and inference for sequence prediction tasks when predicting token y t is whether we use the true previous token y t−1 or an estimateŷ t−1 coming from the model itself.</p><p>We propose here a sampling mechanism that will randomly decide, during training, whether we use y t−1 orŷ t−1 . Assuming we use a mini-batch based stochastic gradient descent approach, for every token to predict y t ∈ Y of the i th mini-batch of the training algorithm, we propose to flip a coin and use the true previous token with probability i , or an estimate coming from the model itself with probability (1 − i ) <ref type="bibr" target="#b1">2</ref> The estimate of the model can be obtained by sampling a token according to the probability distribution modeled by P (y t−1 |h t−1 ), or can be taken as the arg max s P (y t−1 = s|h t−1 ). This process is illustrated in <ref type="figure">Figure 1</ref>.</p><p>When i = 1, the model is trained exactly as before, while when i = 0 the model is trained in the same setting as inference. We propose here a curriculum learning strategy to go from one to the other: intuitively, at the beginning of training, sampling from the model would yield a random token since the model is not well trained, which could lead to very slow convergence, so selecting more often the true previous token should help; on the other hand, at the end of training, i should favor sampling from the model more often, as this corresponds to the true inference situation, and one expects the model to already be good enough to handle it and sample reasonable tokens. We thus propose to use a schedule to decrease i as a function of i itself, in a similar manner used to decrease the learning rate in most modern stochastic gradient descent approaches. Examples of such schedules can be seen in <ref type="figure">Figure 2</ref> as follows:</p><p>• Linear decay: i = max( , k − ci) where 0 ≤ &lt; 1 is the minimum amount of truth to be given to the model and k and c provide the offset and slope of the decay, which depend on the expected speed of convergence. • Exponential decay: i = k i where k &lt; 1 is a constant that depends on the expected speed of convergence. • Inverse sigmoid decay: i = k/(k +exp(i/k)) where k ≥ 1 depends on the expected speed of convergence.</p><p>We call our approach Scheduled Sampling. Note that when we sample the previous tokenŷ t−1 from the model itself while training, we could back-propagate the gradient of the losses at times t → T through that decision. This was not done in the experiments described in this paper and is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>The discrepancy between the training and inference distributions has already been noticed in the literature, in particular for control and reinforcement learning tasks.</p><p>SEARN <ref type="bibr" target="#b8">[9]</ref> was proposed to tackle problems where supervised training examples might be different from actual test examples when each example is made of a sequence of decisions, like acting in a complex environment where a few mistakes of the model early in the sequential decision process might compound and yield a very poor global performance. Their proposed approach involves a meta-algorithm where at each meta-iteration one trains a new model according to the current policy (essentially the expected decisions for each situation), applies it on a test set and modifies the next iteration policy in order to account for the previous decisions and errors. The new policy is thus a combination of the previous one and the actual behavior of the model.</p><p>In comparison to SEARN and related ideas <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, our proposed approach is completely online: a single model is trained and the policy slowly evolves during training, instead of a batch approach, which makes it much faster to train <ref type="bibr" target="#b2">3</ref> Furthermore, SEARN has been proposed in the context of reinforcement learning, while we consider the supervised learning setting trained using stochastic gradient descent on the overall objective.</p><p>Other approaches have considered the problem from a ranking perspective, in particular for parsing tasks <ref type="bibr" target="#b11">[12]</ref> where the target output is a tree. In this case, the authors proposed to use a beam search both during training and inference, so that both phases are aligned. The training beam is used to find</p><p>In fact, in the experiments we report in this paper, our proposed approach was not meaningfully slower (nor faster) to train than the baseline. the best current estimate of the model, which is compared to the guided solution (the truth) using a ranking loss. Unfortunately, this is not feasible when using a model like a recurrent neural network (which is now the state-of-the-art technique in many sequential tasks), as the state sequence cannot be factored easily (because it is a multi-dimensional continuous state) and thus beam search is hard to use efficiently at training time (as well as inference time, in fact).</p><p>Finally, <ref type="bibr" target="#b12">[13]</ref> proposed an online algorithm for parsing problems that adapts the targets through the use of a dynamic oracle that takes into account the decisions of the model. The trained model is a perceptron and is thus not state-based like a recurrent neural network, and the probability of choosing the truth is fixed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We describe in this section experiments on three different tasks, in order to show that scheduled sampling can be helpful in different settings. We report results on image captioning, constituency parsing and speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Captioning</head><p>Image captioning has attracted a lot of attention in the past year. The task can be formulated as a mapping of an image onto a sequence of words describing its content in some natural language, and most proposed approaches employ some form of recurrent network structure with simple decoding schemes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. A notable exception is the system proposed in <ref type="bibr" target="#b16">[17]</ref>, which does not directly optimize the log likelihood of the caption given the image, and instead proposes a pipelined approach.</p><p>Since an image can have many valid captions, the evaluation of this task is still an open problem. Some attempts have been made to design metrics that positively correlate with human evaluation <ref type="bibr" target="#b17">[18]</ref>, and a common set of tools have been published by the MSCOCO team <ref type="bibr" target="#b18">[19]</ref>.</p><p>We used the MSCOCO dataset from <ref type="bibr" target="#b18">[19]</ref> to train our model. We trained on 75k images and report results on a separate development set of 5k additional images. Each image in the corpus has 5 different captions, so the training procedure picks one at random, creates a mini-batch of examples, and optimizes the objective function defined in <ref type="bibr" target="#b3">(4)</ref>. The image is preprocessed by a pretrained convolutional neural network (without the last classification layer) similar to the one described in <ref type="bibr" target="#b19">[20]</ref>, and the resulting image embedding is treated as if it was the first word from which the model starts generating language. The recurrent neural network generating words is an LSTM with one layer of 512 hidden units, and the input words are represented by embedding vectors of size 512. The number of words in the dictionary is 8857. We used an inverse sigmoid decay schedule for i for the scheduled sampling approach. <ref type="table" target="#tab_0">Table 1</ref> shows the results on various metrics on the development set. Each of these metrics is a variant of estimating the overlap between the obtained sequence of words and the target one. Since there were 5 target captions per image, the best result is always chosen. To the best of our knowledge, the baseline results are consistent (slightly better) with the current state-of-the-art on that task. While dropout helped in terms of log likelihood (as expected but not shown), it had a negative impact on the real metrics. On the other hand, scheduled sampling successfully trained a model more resilient to failures due to training and inference mismatch, which likely yielded higher quality captions according to all the metrics. Ensembling models also yielded better performance, both for the baseline and the schedule sampling approach. It is also interesting to note that a model trained while always sampling from itself (hence in a regime similar to inference), dubbed Always Sampling in the table, yielded very poor performance, as expected because the model has a hard time learning the task in that case. We also trained a model with scheduled sampling, but instead of sampling from the model, we sampled from a uniform distribution, in order to verify that it was important to build on the current model and that the performance boost was not just a simple form of regularization. We called this Uniform Scheduled Sampling and the results are better than the baseline, but not as good as our proposed approach. We also experimented with flipping the coin once per sequence instead of once per token, but the results were as poor as the Always Sampling approach. It's worth noting that we used our scheduled sampling approach to participate in the 2015 MSCOCO image captioning challenge <ref type="bibr" target="#b20">[21]</ref> and ranked first in the final leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Constituency Parsing</head><p>Another less obvious connection with the any-to-sequence paradigm is constituency parsing. Recent work <ref type="bibr" target="#b3">[4]</ref> has proposed an interpretation of a parse tree as a sequence of linear "operations" that build up the tree. This linearization procedure allowed them to train a model that can map a sentence onto its parse tree without any modification to the any-to-sequence formulation.</p><p>The trained model has one layer of 512 LSTM cells and words are represented by embedding vectors of size 512. We used an attention mechanism similar to the one described in <ref type="bibr" target="#b21">[22]</ref> which helps, when considering the next output token to produce y t , to focus on part of the input sequence only by applying a softmax over the LSTM state vectors corresponding to the input sequence. The input word dictionary contained around 90k words, while the target dictionary contained 128 symbols used to describe the tree. We used an inverse sigmoid decay schedule for i in the scheduled sampling approach.</p><p>Parsing is quite different from image captioning as the function that one has to learn is almost deterministic. In contrast to an image having a large number of valid captions, most sentences have a unique parse tree (although some very difficult cases exist). Thus, the model operates almost deterministically, which can be seen by observing that the train and test perplexities are extremely low compared to image captioning (1.1 vs. 7).</p><p>This different operating regime makes for an interesting comparison, as one would not expect the baseline algorithm to make many mistakes. However, and as can be seen in <ref type="table" target="#tab_1">Table 2</ref>, scheduled sampling has a positive effect which is additive to dropout. In this table we report the F1 score on the WSJ 22 development set <ref type="bibr" target="#b22">[23]</ref>. We should also emphasize that there are only 40k training instances, so overfitting contributes largely to the performance of our system. Whether the effect of sampling during training helps with regard to overfitting or the training/inference mismatch is unclear, but the result is positive and additive with dropout. Once again, a model trained by always sampling from itself instead of using the groundtruth previous token as input yielded very bad results, in fact so bad that the resulting trees were often not valid trees (hence the "-" in the corresponding F1 metric). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Speech Recognition</head><p>For the speech recognition experiments, we used a slightly different setting from the rest of the paper. Each training example is an input/output pair (X, Y ), where X is a sequence of T input vectors x 1 , x 2 , • • • x T and Y is a sequence of T tokens y 1 , y 2 , • • • y T so each y t is aligned with the corresponding x t . Here, x t are the acoustic features represented by log Mel filter bank spectra at frame t, and y t is the corresponding target. The targets used were HMM-state labels generated from a GMM-HMM recipe, using the Kaldi toolkit <ref type="bibr" target="#b23">[24]</ref> but could very well have been phoneme labels. This setting is different from the other experiments in that the model we used is the following:</p><formula xml:id="formula_5">log P (Y |X; θ) = log P (y T 1 |x T 1 ; θ) = T t=1 log P (y t |y t−1 1 , x t 1 ; θ) = T t=1 log P (y t |h t ; θ)<label>(5)</label></formula><p>where h t is computed by a recurrent neural network as follows:</p><formula xml:id="formula_6">h t = f (o h , S, x 1 ; θ) if t = 1, f (h t−1 , y t−1 , x t ; θ) otherwise.<label>(6)</label></formula><p>where o h is a vector of 0's with same dimensionality as h t 's and S is an extra token added to the dictionary to represent the start of each sequence.</p><p>We generated data for these experiments using the TIMIT 4 corpus and the KALDI toolkit as described in <ref type="bibr" target="#b24">[25]</ref>. Standard configurations were used for the experiments -40 dimensional log Mel filter banks and their first and second order temporal derivatives were used as inputs to each frame. 180 dimensional targets were generated for each time frame using forced alignment to transcripts using a trained GMM-HMM system. The training, validation and test sets have 3696, 400 and 192 sequences respectively, and their average length was 304 frames. The validation set was used to choose the best epoch in training, and the model parameters from that epoch were used to evaluate the test set.</p><p>The trained models had two layers of 250 LSTM cells and a softmax layer, for each of five configurations -a baseline configuration where the ground truth was always fed to the model, a configuration (Always Sampling) where the model was only fed in its own predictions from the last time step, and three scheduled sampling configurations (Scheduled Sampling 1-3), where i was ramped linearly from a maximum value to a minimum value over ten epochs and then kept constant at the final value. For each configuration, we trained 3 models and report average performance over them. Training of each model was done over frame targets from the GMM. The baseline configurations typically reached the best validation accuracy after approximately 14 epochs whereas the sampling models reached the best accuracy after approximately 9 epochs, after which the validation accuracy decreased. This is probably because the way we trained our models is not exact -it does not account for the gradient of the sampling probabilities from which we sampled our targets. Future effort at tackling this problem may further improve results.</p><p>Testing was done by finding the best sequence from beam search decoding (using a beam size of 10 beams) and computing the error rate over the sequences. We also report the next step error rate (where the model was fed in the ground truth to predict the class of the next frame) for each of the models on the validation set to summarize the performance of the models on the training objective. <ref type="table">Table 3</ref> shows a summary of the results</p><p>It can be seen that the baseline performs better next step prediction than the models that sample the tokens for input. This is to be expected, since the former has access to the groundtruth. However, it can be seen that the models that were trained with sampling perform better than the baseline during decoding. It can also be seen that for this problem, the "Always Sampling" model performs quite https://catalog.ldc.upenn.edu/LDC93S1.</p><p>well. We hypothesize that this has to do with the nature of the dataset. The HMM-aligned states have a lot of correlation -the same state appears as the target for several frames, and most of the states are constrained only to go to a subset of other states. Next step prediction with groundtruth labels on this task ends up paying disproportionate attention to the structure of the labels (y t−1 1 ) and not enough to the acoustics input (x t 1 ). Thus it achieves very good next step prediction error when the groundtruth sequence is fed in with the acoustic information, but is not able to exploit the acoustic information sufficiently when the groundtruth sequence is not fed in. For this model the testing conditions are too far from the training condition for it to make good predictions. The model that is only fed its own prediction (Always Sampling) ends up exploiting all the information it can find in the acoustic signal, and effectively ignores its own predictions to influence the next step prediction. Thus at test time, it performs just as well as it does during training. A model such as the attention model of <ref type="bibr" target="#b25">[26]</ref> which predicts phone sequences directly, instead of the highly redundant HMM state sequences, would not suffer from this problem because it would need to exploit both the acoustic signal and the language model sufficiently to make predictions. Nevertheless, even in this setting, adding scheduled sampling still helped to improve the decoding frame error rate.</p><p>Note that typically speech recognition experiments use HMMs to decode predictions from neural networks in a hybrid model. Here we avoid using an HMM altogether and hence we do not have the advantage of the smoothing that results from the HMM architecture and the language models. Thus the results are not directly comparable to the typical hybrid model results. <ref type="table">Table 3</ref>: Frame Error Rate (FER) on the speech recognition experiments. In next step prediction (reported on validation set) the ground truth is fed in to predict the next target like it is done during training. In decoding experiments (reported on test set), beam searching is done to find the best sequence. We report results on four different linear schedulings of sampling, where i was ramped down linearly from s to e . For the baseline, the model was only fed in the ground truth. See Section 4.3 for an analysis of the results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Using recurrent neural networks to predict sequences of tokens has many useful applications like machine translation and image description. However, the current approach to training them, predicting one token at a time, conditioned on the state and the previous correct token, is different from how we actually use them and thus is prone to the accumulation of errors along the decision paths. In this paper, we proposed a curriculum learning approach to slowly change the training objective from an easy task, where the previous token is known, to a realistic one, where it is provided by the model itself. Experiments on several sequence prediction tasks yield performance improvements, while not incurring longer training times. Future work includes back-propagating the errors through the sampling decisions, as well as exploring better sampling strategies including conditioning on some confidence measure from the model itself.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Illustration of the Scheduled Sampling approach, where one flips a coin at every time step to decide to use the true previous token or one sampled from the model itself. Examples of decay schedules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Various metrics (the higher the better) on the MSCOCO development set for the image captioning task.</figDesc><table><row><cell>Approach vs Metric</cell><cell cols="3">BLEU-4 METEOR CIDER</cell></row><row><cell>Baseline</cell><cell>28.8</cell><cell>24.2</cell><cell>89.5</cell></row><row><cell>Baseline with Dropout</cell><cell>28.1</cell><cell>23.9</cell><cell>87.0</cell></row><row><cell>Always Sampling</cell><cell>11.2</cell><cell>15.7</cell><cell>49.7</cell></row><row><cell>Scheduled Sampling</cell><cell>30.6</cell><cell>24.3</cell><cell>92.1</cell></row><row><cell>Uniform Scheduled Sampling</cell><cell>29.2</cell><cell>24.2</cell><cell>90.9</cell></row><row><cell>Baseline ensemble of 10</cell><cell>30.7</cell><cell>25.1</cell><cell>95.7</cell></row><row><cell>Scheduled Sampling ensemble of 5</cell><cell>32.3</cell><cell>25.4</cell><cell>98.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>F1 score (the higher the better) on the validation set of the parsing task.</figDesc><table><row><cell>Approach</cell><cell>F1</cell></row><row><cell>Baseline LSTM</cell><cell>86.54</cell></row><row><cell>Baseline LSTM with Dropout</cell><cell>87.0</cell></row><row><cell>Always Sampling</cell><cell>-</cell></row><row><cell>Scheduled Sampling</cell><cell>88.08</cell></row><row><cell cols="2">Scheduled Sampling with Dropout 88.68</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Note that in the experiments, we flipped the coin for every token. We also tried to flip the coin once per sequence, but the results were much worse, most probably because consecutive errors are amplified during the first rounds of training.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long term dependencies is hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7449</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ICML</title>
		<meeting>the International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Search-based structured prediction as classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname><genName>III</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Artificial Intelligence and Statistics</title>
		<meeting>the Workshop on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>AISTATS</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving multi-step prediction of learned time series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics, ACL</title>
		<meeting>the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dynamic oracle for arc-eager dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING</title>
		<meeting>COL-ING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft coco: Common objects in context</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ICML</title>
		<meeting>the International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Microsoft coco captioning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<ptr target="http://mscoco.org/dataset/#captions-challenge2015" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ontonotes: The 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Short Papers</title>
		<meeting>the Human Language Technology Conference of the NAACL, Short Papers<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="2011-12" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>IEEE Catalog</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploring Deep Learning Methods for discovering features in speech signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-to-end continuous speech recognition using attention-based recurrent nn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1602</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">First results. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
