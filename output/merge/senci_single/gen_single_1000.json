{"1000": "unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for train. unfortunately, the best completely unsupervised english pos tagger, making its practical usability questionable at best. to bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. algorithm bilingual pos induction require : parallel english and foreign language data de and df, unlabeled foreign training data f; english tagger. ensure: f, a set of parameters learned using a constrained unsupervised model. this paper proposes a graph - based framework for unsupervised pos tagging. in this framework, the vertices of the graph are labeled and unlabeled examples, and the edge weights encode the degree to which the examples they link have the same label graph construction for structured prediction problems such as pos tagging is nontrivial: on the one hand, using individual words as the vertices throws away the context. the word alignment methods do not use pos information. necessary for disambiguation; on the other hand, it is unclear how to define proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning. to define a similarity function between the english and the foreign vertices, we rely on high-confidence word alignments. since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the english sentences de 5note that many combinations are impossible giving a pmi value of 0; e.g., when the trigram type and the feature instantiation don t have words in common. and their foreign language translations df label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence alignments in this paper , we propose a supervised tagger that propagates the label distribution from the english vertices to the connected foreign language vertices (v lf) at the periphery of the graph. in the first stage, we run a single step of label propagation, which transfers the label distributions from the english vertices to the connected foreign language vertices (v lf) at the periphery of the graph. note that many foreign vertices will not be connected to any english vertices. we use a squared loss to neighboring vertices that have different label distributions: qi qj2 y ( qi ( y) qj ( y) 2, and additionally regularize the label distributions towards the uniform distribution u over all possible labels y it can be shown that this objective is convex in q. the first term regularizer term is the graph smoothness which encourages the distributions of similar vertices ( large wij) to be similar. the second term regularizer term is a regularizer and encourages all type marginals to be uniform. if an unlabeled vertex does not have a path to any labeled vertex, this term ensures that the converged marginal for this vertex will be uniform over all tags, allowing the middle word of such an unlabeled vertex to take on any of the possible tags. this paper proposes a new approach to pos induction in languages for which no labeled resources are available. in this paper, we propose a graph - based model for language classification that is able to extract the constraint feature for all foreign word types. the graph is constructed using million trigrams; we chose these by truncating the parallel datasets up to the number of sentence pairs that contained million trigrams. we used c as the l2 regularization constant in (eq. 10) and trained both em and l-bfgs for iterations. when extracting the vector tx used to compute the constraint feature from the graph, it performs better than the hitherto state-of-theart feature-hmm baseline, and better than the no lp setting overall, when we macro-average the accuracy over all languages. our full model outperforms the no lp setting because it has better vocabulary coverage and allows the extraction of the constraint feature. for comparison, the completely unsupervised feature-hmm baseline accuracy on the universal pos tags for english is 79.4, and goes up to with a treebank dictionary. the paper proposes a graph - based approach to pos taggers for languages which do not have any annotated data, but have translations into a resource-rich language. our method does not assume any knowledge about the target language across eight european languages, our approach results in an average absolute improvement of over a state-of-the-art baseline, and over vanilla hidden markov models induced with the expectation maximization algorithm. this paper proposes a new model for pos taggers in languages for which no labeled resources are available. the model is based on the feature-hmm of berg-kirkpatrick et al., which is a generalization of the traditional hmm model.", "1001": "###", "1002": "###", "1003": "###", "1004": "###", "1005": "###", "1006": "###", "1007": "###", "1008": "###", "1009": "###", "1010": "###", "1011": "###", "1012": "###", "1013": "###", "1014": "###", "1015": "###", "1016": "###", "1017": "###", "1018": "###", "1019": "###", "1020": "###", "1021": "###"}