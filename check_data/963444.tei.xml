<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diversity Networks: Neural Network Compression Using Determinantal Point Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-04-18">18 Apr 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelda</forename><surname>Mariet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
							<email>suvrit@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology Cambridge</orgName>
								<address>
									<postCode>02139</postCode>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diversity Networks: Neural Network Compression Using Determinantal Point Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-04-18">18 Apr 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1511.05077v6[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Training neural networks requires setting several hyper-parameters to adequate values: number of hidden layers, number of neurons per hidden layer, learning rate, momentum, dropout rate, etc. Although tuning such hyper-parameters via parameter search has been recently investigated by <ref type="bibr" target="#b18">Maclaurin et al. (2015)</ref>, doing so remains extremely costly, which makes it imperative to develop more efficient techniques.</p><p>Of the many hyper-parameters, those that determine the network's architecture are among the hardest to tune, especially because changing them during training is more difficult than adjusting more dynamic parameters such as the learning rate or momentum. Typically, the architecture parameters are set once and for all before training begins. Thus, assigning them correctly is paramount: if the network is too small, it will not learn well; if it is too large, it may take significantly longer to train while running the risk of overfitting. Networks are therefore usually trained with more parameters than necessary, and pruned once the training is complete. This paper introduces Divnet, a new technique for reducing the size of a network. Divnet decreases the amount of redundancy in a neural network (and hence its size) in two steps: first, it samples a diverse subset of neurons; then, it merges the remaining neurons with the ones previously selected.</p><p>Specifically, Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) <ref type="bibr" target="#b8">(Hough et al., 2006)</ref> over neurons in a layer, which is then used to select a subset of diverse neurons. Subsequently, Divnet "fuses" information from the dropped neurons into the selected ones through a reweighting procedure. Together, these steps reduce network size (and act as implicit regularization), without requiring any further training or significantly hurting performance. Divnet is fast and runs in time negligible compared to the network's prior training time. Moreover, it is agnostic to other network parameters such as activation functions, number of hidden layers, and learning rates.</p><p>For simplicity, we describe and analyze Divnet for feed-forward neural networks, however Divnet is not limited to this setting. Indeed, since Divnet operates on a layer fully connected to the following one in a network's hierarchy, it applies equally well to other architectures with fully connected layers. For example, it can be applied without any further modification to Deep Belief Nets and to the fully-connected layers in Convolutional Neural Networks. As these layers are typically responsible for the large majority of the CNNs' memory footprint <ref type="bibr" target="#b23">(Yang et al., 2014)</ref>, Divnet is particularly well suited for such networks.</p><p>Contributions. The key contributions of this paper are the following:</p><p>-Introduction of DPPs as a flexible, powerful tool for modeling layerwise neuronal diversity ( ยง2.1). Specifically, we present a practical method for creating DPPs over neurons, which enables diversity promoting sampling and thereby leads to smaller network sizes.</p><p>-A simple but crucial "fusing" step that minimizes the adverse effects of removing neurons. Specifically, we introduce a reweighting procedure for a neuron's connections that transfers the contributions of the pruned neurons to the ones that are retained ( ยง2.2).</p><p>The combination of both ideas is called Divnet. We perform several experiments to validate Divnet and compare to previous neuron pruning approaches, which Divnet consistently outperforms. Notably, Divnet's reweighting strategy benefits other pruning approaches.</p><p>Related work. Due to their large number of parameters, deep neural networks typically have a heavy memory footprint. Moreover, in many deep neural network models parameters show a significant amount of redundancy <ref type="bibr" target="#b2">(Denil et al., 2013)</ref>. Consequently, there has been significant interest in developing techniques for reducing a network's size without penalizing its performance.</p><p>A common approach to reducing the number of parameters is to remove connections between layers. In <ref type="bibr" target="#b14">(LeCun et al., 1990;</ref><ref type="bibr" target="#b5">Hassibi et al., 1993)</ref>, connections are deleted using information drawn from the Hessian of the network's error function. <ref type="bibr" target="#b20">Sainath et al. (2013)</ref> reduce the number of parameters by analyzing the weight matrices, and applying low-rank factorization to the final weight layer. <ref type="bibr" target="#b4">Han et al. (2015)</ref> remove connections with weights smaller than a given threshold before retraining the network. These methods focus on deleting parameters whose removal influences the network the least, while Divnet seeks diversity and merges similar neurons; these methods can thus be used in conjunction with ours. Although methods such as <ref type="bibr" target="#b14">(LeCun et al., 1990</ref>) that remove connections between layers may also delete neurons from the network by removing all of their outgoing or incoming connections, it is likely that the overall impact on the size of the network will be lesser than approaches such as Divnet that remove entire neurons: indeed, removing a neuron decreases the number of rows or columns of the weight matrices connecting the neuron's layer to both the previous and following layers.</p><p>Convolutional Neural Networks (LeCun et al., 1998) replace fully-connected layers with convolution and subsampling layers, which significantly decreases the number of parameters. However, as CNNs still maintain fully-connected layers, they also benefit from Divnet.</p><p>Closer to our approach of reducing the number of hidden neurons is <ref type="bibr" target="#b6">(He et al., 2014)</ref>, which evaluates each hidden neuron's importance and deletes neurons with the smaller importance scores. In <ref type="bibr" target="#b22">(Srinivas and Babu, 2015)</ref>, a neuron is pruned when its weights are similar to those of another neuron in its layer, leading to a weight update procedure that is somewhat similar in idea (albeit simpler) to our reweighting step: where <ref type="bibr" target="#b22">(Srinivas and Babu, 2015)</ref> removes neurons with equal or similar weights, we consider the more complicated task of merging neurons that, as a group, perform redundant calculations based on their activations.</p><p>Other recent approaches consider network compression without pruning: in <ref type="bibr" target="#b7">(Hinton et al., 2015)</ref>, a new, smaller network is trained on the outputs of the large network; Chen et al.</p><p>use hashing to reduce the size of the weight matrices by forcing all connections within the same hash bucket to have the same weight. <ref type="bibr" target="#b1">Courbariaux et al. (2014)</ref> and <ref type="bibr" target="#b3">Gupta et al. (2015)</ref> show that networks can be trained and run using limited precision values to store the network parameters, thus reducing the overall memory footprint.</p><p>We emphasize that Divnet's focus on neuronal diversity is orthogonal and complementary to prior network compression techniques. Consequently, Divnet can be combined, in most cases trivially, with previous approaches to memory footprint reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Diversity and redundancy reduction</head><p>In this section we introduce our technique for modeling neuronal diversity more formally.</p><p>Let T denote the training data, a layer of n neurons, a ij the activation of the i-th neuron on input t j , and v i = (a i1 , . . . , a in ) the activation vector of the i-th neuron obtained by feeding the training data through the network. To enforce diversity in layer , we must determine which neurons are computing redundant information and remove them. Doing so requires finding a maximal subset of (linearly) independent activation vectors in a layer and retaining only the corresponding neurons. In practice, however, the number of items in the training set (or the number of batches) can be much larger than the number of neurons in a layer, so the activation vectors v 1 , . . . , v n are likely linearly independent. Merely selecting by the maximal subset may thus lead to a trivial solution that selects all neurons.</p><p>Reducing redundancy therefore requires a more careful approach to sampling. We propose to select a subset of neurons whose activation patterns are diverse while contributing to the network's overall computation (i.e., their activations are not saturated at 0). We achieve this diverse selection by formulating the neuron selection task as sampling from a Determinantal Point Process (DPP). We describe the details below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neuronal diversity via Determinantal Point Processes</head><p>DPPs are probability measures over subsets of a ground set of items. Originally introduced to model the repulsive behavior of fermions <ref type="bibr" target="#b17">(Macchi, 1975)</ref>, they have since been used fruitfully in machine-learning <ref type="bibr" target="#b11">(Kulesza and Taskar, 2012)</ref>. Interestingly, they have also been recently applied to modeling inter-neuron inhibitions in neural spiking behavior in the rat hippocampus <ref type="bibr" target="#b21">(Snoek et al., 2013)</ref>.</p><p>DPPs present an elegant mathematical technique to model diversity: the probability mass associated to each subset is proportional to the determinant (hence the name) of a DPP kernel matrix. The determinant encodes negative associations between variables, and thus DPPs tend to assign higher probability mass to diverse subsets (corresponding to diverse submatrices of the DPP kernel). Formally, a ground set of N items Y = {1, . . . , N } and a probability P :</p><formula xml:id="formula_1">2 Y โ [0, 1] such that P(Y ) = det(L Y ) det(L + I) ,<label>(1)</label></formula><p>where L is a N -by-N positive definite matrix, form a DPP. L is called the DPP kernel ; here, L Y indicates the |Y | ร |Y | principal submatrix of L indexed by the elements of Y .</p><p>The key ingredient that remains to be specified is the DPP kernel, which we now describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Constructing the DPP kernel</head><p>There are numerous potential choices for the DPP kernel. We found that experimentally a well-tuned Gaussian RBF kernel provided a good balance between simplicity and quality: for instance, it provides much better results that simple linear kernels (obtained via the outer product of the activation vectors) and is easier to use than more complex Gaussian RBF kernels with additional parameters. A more thorough evaluation of kernel choice is future work.</p><p>Recall that layer has activations v 1 , . . . , v n . Using these, we first create an n ร n kernel L with bandwidth parameter ฮฒ by setting</p><formula xml:id="formula_2">L ij = exp(โฮฒ v i โ v j 2 ) 1 โค i, j โค n .<label>(2)</label></formula><p>To ensure strict positive definiteness of the kernel matrix L , we add a small diagonal perturbation ฮตI to L (ฮต = 0.01). The choice of the bandwidth parameter could be done by cross-validation, but that would greatly increase the training cost. Therefore, we use the fixed choice ฮฒ = 10/|T |, which was experimentally seen to work well.</p><p>Finally, in order to limit rounding errors, we introduce a final scaling operation: suppose we wish to obtain a desired size, say k, of sampled subsets (in which case we are said to be using a k-DPP <ref type="bibr">(Kulesza and Taskar, 2011)</ref>). To that end, we can scale the kernel L + ฮตI by a factor ฮณ, so that its expected sample size becomes k. For a DPP with kernel L, the expected sample size is given by <ref type="bibr">(Kulesza and Taskar, 2012, Eq. 34)</ref>:</p><formula xml:id="formula_3">E[|Y |] = Tr(L(I + L) โ1 ).</formula><p>Therefore, we scale the kernel to ฮณ(L + ฮตI) with ฮณ such that</p><formula xml:id="formula_4">ฮณ = k n โ k โข n โ k k ,</formula><p>where k is the expected sample size for the kernel L + ฮตI.</p><p>Finally, generating and then sampling from L = ฮณ(L + ฮตI) has O(n 3 + n 2 |T |) cost. In our experiments, this sampling cost was negligible compared with the cost of training. For networks with very large hidden layers, one can avoiding the n 3 cost by using more scalable sampling techniques <ref type="bibr" target="#b16">(Li et al., 2015;</ref><ref type="bibr" target="#b9">Kang, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fusing redundant neurons</head><p>Simply excising the neurons that are not sampled by the DPP drastically alters the neuron inputs to the next layer. Intuitively, since activations of neurons marked redundant are not arbitrary, throwing them away is wasteful. Ideally we should preserve the total information of a given layer, which suggests that we should "fuse" the information from unselected neurons into the selected ones. We achieve this via a reweighting procedure as outlined below.</p><p>Without loss of generality, let neurons 1 through k be the ones sampled by the DPP and v 1 , . . . , v k their corresponding activation vectors. Let w ij be the weights connecting the i-th neuron (1 โค i โค k) in the current layer to the j-th neuron in the next layer; let w ij = ฮด ij +w ij denote the updated weights after merging the contributions from the removed neurons.</p><p>We seek to minimize the impact of removing n โ k neurons from layer . To that end, we minimize the difference in inputs to neurons in the subsequent layer before ( iโคn w ij v i ) and after ( i=1โคk w ij v i ) DPP pruning. That is, we wish to solve for all neurons in the next layer (indexed by j, 1 โค j โค n +1 ):</p><formula xml:id="formula_5">min wij โR k i=1 w ij v i โ n i=1 w ij v i 2 = min ฮดij โR k i=1 ฮด ij v i โ n i=k+1 w ij v i 2 .<label>(3)</label></formula><p>Eq. 3 is minimized when iโคk ฮด ij v i is the projection of i&gt;k w ij v i onto the linear space generated by {v 1 , . . . , v k }. Thus, to minimize Eq. 3, we obtain the coefficients ฮฑ ij that for</p><formula xml:id="formula_6">j &gt; k minimize v j โ k i=1 ฮฑ ij v i 2</formula><p>and then update the weights by setting</p><formula xml:id="formula_7">โi, 1 โค i โค k, w ij = w ij + n r=k+1 ฮฑ ir w rj<label>(4)</label></formula><p>Using ordinary least squares to obtain ฮฑ, the reweighting procedure runs in O(|T |n 2 + n 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results</head><p>To quantify the performance of our algorithm, we present below the results of experiments 1 on common datasets for neural network evaluation: MNIST (LeCun and Cortes, 2010), MNIST ROT <ref type="bibr" target="#b12">(Larochelle et al., 2007)</ref> and <ref type="bibr">CIFAR-10 (Krizhevsky, 2009)</ref>.</p><p>All networks were trained up until a certain training error threshold, using softmax activation on the output layer and sigmoids on other layers; see <ref type="table" target="#tab_0">Table 1</ref> for more details. In all following plots, error bars represent standard deviations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pruning and reweighting analysis</head><p>To validate our claims on the benefits of using DPPs and fusing neurons, we compare these steps separately and also simultaneously against random pruning, where a fixed number of neurons are chosen uniformly at random from a layer and then removed, with and without our fusing step. We present performance results on the test data; of course, both DPP selection and reweighting are based solely on information drawn from the training data.  <ref type="figure" target="#fig_0">Figure 1a</ref> shows the activations of the 50 neurons sampled using a k-DPP (k = 50) defined over the first hidden layer, whereas <ref type="figure" target="#fig_0">Figure 1b</ref> shows the activations of the first 50 neurons of the same layer. <ref type="figure" target="#fig_0">Figure 1b</ref> contains multiple similar columns: for example, there are 3 entirely green columns, corresponding to three neurons that saturate to 1 on each of the 10 instances. In contrast, the DPP samples neurons with diverse activations, and <ref type="figure" target="#fig_0">Figure 1a</ref> shows no similar redundancy.</p><p>Figures 2 through 4 illustrate the impact of each step of Divnet separately. <ref type="figure" target="#fig_4">Figure 2</ref> shows the impact of pruning on test error using DPP pruning and random pruning (in which a fixed number of neurons are selected uniformly at random and removed from the network). DPPpruned networks have consistently better training and test errors than networks pruned at random for the same final size. As expected, the more neurons are maintained, the less the error suffers from the pruning procedure; however, the pruning is in both cases destructive, and is seen to significantly increase the error rate.</p><p>This phenomenon can be mitigated by our reweighting procedure, as shown in <ref type="figure" target="#fig_5">Figure 3</ref>. By fusing and reweighting neurons after pruning, the training and test errors are considerably reduced, even when 90% of the layer's neurons are removed. Pruning also reduces variability of the results: the standard deviation for the results of the reweighted networks is significantly smaller than for the non-reweighted networks, and may be thus seen as a way to regularize neural networks.</p><p>Finally, <ref type="figure" target="#fig_6">Figure 4</ref> illustrates the performance of Divnet (DPP pruning and reweighting) compared to random pruning with reweighting. Although Divnet's performance is ultimately better, the reweighting procedure also dramatically benefits the networks that were pruned randomly.</p><p>We also ran these experiments on networks for shrinking the second layer while maintaining the first layer intact. The results are similar, and may be found in Appendix A. Notably, we found that the gap between Divnet and random pruning's performances was much wider  when pruning the last layer. We believe this is due to the connections to the output layer being learned much faster, thus letting a small, diverse subset of neurons (hence well suited to DPP sampling) in the last hidden layer take over the majority of the computational effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance analysis</head><p>Much attention has been given to reducing the size of neural networks in order to reduce memory consumption. When using neural nets locally on devices with limited memory, it is crucial that their memory footprint be as small as possible.</p><p>Node importance-based pruning (henceforth "importance pruning") is one of the most intuitive ways to cut down on network size. Introduced to deep networks by <ref type="bibr" target="#b6">He et al. (2014)</ref>, this method removes the neurons whose calculations impact the network the least. Among the three solutions to estimating a neuron's importance discussed in <ref type="bibr" target="#b6">He et al. (2014)</ref>, the sum the output weights of each neuron (the 'onorm' function) provided the best results:</p><formula xml:id="formula_8">onorm(n i ) := 1 n +1 n j=1 |w +1</formula><p>ij |. <ref type="figure" target="#fig_8">Figure 5</ref> compares the test data error of the networks after being pruned using importance pruning that uses onorm as a measure of relevance against Divnet. Since importance pruning deletes neurons that contribute the least to the next layer's computations, it performs well up to a certain point; however, when pruning a significant amount of neurons, this pruning procedure even removes neurons performing essential calculations, hurting the network's performance significantly. However, since Divnet fuses redundant neurons, instead of merely deleting them its resulting network delivers much better performance even with very large amounts of pruning.</p><p>In order to illustrate numerically the impact of Divnet on network performance, <ref type="table" target="#tab_1">Table 2</ref> shows network training and test errors (between 0 and 1) under various compression rates obtained with Divnet, without additional retraining (that is, the pruned network is not retrained to re-optimize its weights).       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion and Remarks</head><p>โข In all experiments, sampling and reweighting ran several orders of magnitude faster than training; reweighting required significantly more time than sampling. If Divnet must be further sped up, a fraction of the training set can be used instead of the entire set, at the possible cost of subsequent network performance. โข When using DPPs with a Gaussian RBF kernel, sampled neurons need not have linearly independent activation vectors: not only is the DPP sampling probabilistic, the kernel itself is not scale invariant. Indeed, for two collinear but unequal activation vectors, the corresponding coefficient in the kernel will not be 1 (or ฮณ with the L โ ฮณL update). โข In our work, we selected a subset of neurons by sampling once from the DPP. Alternatively, one could sample a fixed amount of times, using the subset with the highest likelihood (i.e., largest det(L Y )), or greedily approximate the mode of the DPP distribution. โข Our reweighting procedure benefits competing pruning methods as well (see <ref type="figure" target="#fig_6">Figure 4</ref>).</p><p>โข We also investigated DPP sampling for pruning concurrently with training iterations, hoping that this might allow us to detect superfluous neurons before convergence, and thus reduce training time. However, we observed that in this case DPP pruning, with or without reweighting, did not offer a significant advantage over random pruning.</p><p>โข Consistently over all datasets and networks, the expected sample size from the kernel L was much smaller for the last hidden layer than for other layers. We hypothesize that this is caused by the connections to the output layer being learned faster than the others, allowing a small subset of neurons to take over the majority of the computational effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Future work and conclusion</head><p>Divnet leverages similarities between the behaviors of neurons in a layer to detect redundant parameters and merge them, thereby enforcing neuronal diversity within each hidden layer. Using Divnet, large, redundant networks can be shrunk to much smaller structures without impacting their performance and without requiring further training. We believe that the performance profile of Divnet will remain similar even when scaling to larger scale datasets and networks, and hope to include results on bigger problems (e.g., Imagenet <ref type="bibr" target="#b19">(Russakovsky et al., 2015)</ref>) in the future.</p><p>Many hyper-parameters can be tuned by a user as per need include: the number of remaining neurons per layer can be fixed manually; the precision of the reweighting and the sampling procedure can be tuned by choosing how many training instances are used to generate the DPP kernel and the reweighting coefficients, creating a trade-off between accuracy, memory management, and computational time. Although Divnet requires the user to select the size of the final network, we believe that a method where no parameter explicitly needs to be tuned is worth investigating. The fact that DPPs can be augmented to also reflect different distributions over the sampled set sizes <ref type="bibr">(Kulesza and Taskar, 2012, ยง5.1</ref>.1) might be leveraged to remove the burden of choosing the layer's size from the user.</p><p>Importantly, Divnet is agnostic to most parameters of the network, as it only requires knowledge of the activation vectors. Consequently, Divnet can be easily used jointly with other pruning/memory management methods to reduce size. Further, the reweighting procedure is agnostic to how the pruning is done, as shown in our experiments.</p><p>Furthermore, the principles behind Divnet can theoretically also be leveraged in non fullyconnected settings. For example, the same diversifying approach may also be applicable to filters in CNNs: if a layer of the CNN is connected to a simple, feed-forward layer -such as the S4 layer in LeCun et al. (1998) -by viewing each filter's activation values as an vector and applying Divnet on the resulting activation matrix, one may be able to remove entire filters from the network, thus significantly reducing CNN's memory footprint.</p><p>Finally, we believe that investigating DPP pruning with different kernels, such as kernels invariant to the scaling of the activation vectors, or even kernels learned from data, may provide insight into which interactions between neurons of a layer contain the information necessary for obtaining good representations and accurate classification. This marks an interesting line of future investigation, both for training and representation learning.</p><p>A Pruning the second layer    -10 -5 10 -4 10 -3 10 -2 10 -1 10 0 Figure 10: Influence of ฮฒ on the number of neurons that remain after pruning networks trained on MNIST (when pruning non-parametrically, using a DPP instead of a k-DPP.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>visualizes neuron activations in the first hidden layer of a network trained on the MNIST dataset. Each column in the plotted heat maps represents the activation of a neuron on instances of digits 0 through 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>First 50 neurons of the first hidden layer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Heat map of the activation of subsets of 50 neurons for one instance of each class of the MNIST dataset. The rows correspond to digits 0 through 9. Each column corresponds to the activation values of one neuron in the network's first layer on images of digits 0 through 9. Green values are activations close to 1, red values are activations close to 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of random and k-DPP pruning procedures. pruning k-DPP + reweighting (b) MNIST ROT dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of Divnet (k-DPP + reweighting) to simple k-DPP pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of random and k-DPP pruning when both are followed by reweighting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of random pruning, importance pruning, and Divnet's impact on the network's performance after decreasing the number of neurons in the first hidden layer of a network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of random and k-DPP pruning procedures. pruning k-DPP + reweighting (b) MNIST ROT dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of Divnet to simple k-DPP pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Comparison of random and k-DPP pruning when both are followed by reweighting.B Influence of the ฮฒ parameter on network size and error Influence of ฮฒ on training error (using the networks trained on MNIST). The dotted lines show min and max errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of the sets of networks used in the experiments. We train each class of networks until the first iteration of backprop for which the training error reaches a predefined threshold.</figDesc><table><row><cell>Dataset</cell><cell>Instances Trained up until</cell><cell>Architecture</cell></row><row><cell>MNIST</cell><cell>&lt; 1% error</cell><cell>784 -500 -500 -10</cell></row><row><cell>MNIST ROT</cell><cell>&lt; 1% error</cell><cell>784 -500 -500 -10</cell></row><row><cell>CIFAR-10</cell><cell>&lt; 50% error</cell><cell>3072 -1000 -1000 -1000 -10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Training and test error for different percentages of remaining neurons (mean ยฑ standard deviation). Initially, MNIST and MNIST ROT nets have 1000 hidden neurons, and CIFAR-10 have 3000.</figDesc><table><row><cell cols="2">Remaining hidden neurons</cell><cell>10%</cell><cell>25%</cell><cell>50%</cell><cell>75%</cell><cell>100 %</cell></row><row><cell>MNIST</cell><cell cols="6">training error 0.76 ยฑ 0.06 0.28 ยฑ 0.12 0.15 ยฑ 0.04 0.06 ยฑ 0.04 0.01 ยฑ0.001 test error 0.76 ยฑ 0.07 0.29 ยฑ 0.12 0.17 ยฑ 0.05 0.07 ยฑ 0.03 0.03 ยฑ 0.002</cell></row><row><cell>MNIST ROT</cell><cell cols="6">training error 0.74 ยฑ 0.08 0.54 ยฑ 0.09 0.34 ยฑ 0.06 0.20 ยฑ 0.03 0.01 ยฑ 0.003 test error 0.73 ยฑ 0.09 0.49 ยฑ 0.11 0.25 ยฑ 0.07 0.06 ยฑ 0.03 0.15 ยฑ0.008</cell></row><row><cell>CIFAR-10</cell><cell cols="6">training error 0.84 ยฑ 0.05 0.61 ยฑ 0.01 0.52 ยฑ 0.01 0.50 ยฑ 0.01 0.49 ยฑ 0.004 test error 0.85 ยฑ 0.05 0.62 ยฑ 0.02 0.54 ยฑ 0.01 0.52 ยฑ 0.01 0.51 ยฑ 0.005</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Run in MATLAB, based on the code from DeepLearnToolBox (https://github.com/ rasmusbergpalm/DeepLearnToolbox) and Alex Kulesza's code for DPPs (http://web.eecs.umich. edu/~kulesza/), on a Linux Mint system with 16GB of RAM and an i7-4710HQ CPU @ 2.50GHz.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partly supported by NSF grant: IIS-1409802.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno>abs/1504.04788</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Low precision arithmetic for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<idno>abs/1412.7024</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting parameters in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shakibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno>abs/1306.0543</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
		<idno>abs/1502.02551</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dally</surname></persName>
		</author>
		<idno>abs/1506.02626</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C R</forename><surname>Com</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 5</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reshaping deep neural network for fast decoding by node-pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="245" to="249" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Determinantal processes and independence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krishnapur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Virรกg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability Surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="206" to="229" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast determinantal point process sampling with application to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2319" to="2327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A. Kulesza and B. Taskar. k-DPPs: Fixed-size determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Learning multiple layers of features from tiny images</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Determinantal point processes for machine learning. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
		<meeting>the 24th International Conference on Machine Learning, ICML &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient sampling for k-determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1509.01618" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The coincidence approach to stochastic point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Macchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based hyperparameter optimization through reversible learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low-rank matrix factorization for deep neural network training with high-dimensional output targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6655" to="6659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A determinantal point process latent variable model for inhibition in neural spiking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1932" to="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Data-free parameter pruning for deep neural networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<idno>abs/1507.06149</idno>
		<ptr target="http://arxiv.org/abs/1507.06149" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1412.7149</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
