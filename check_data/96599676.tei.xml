<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hypercolumns for Object Segmentation and Fine-grained Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
							<email>bharath2@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbeláez</surname></persName>
							<email>pa.arbelaez@uniandes.edu.co</email>
							<affiliation key="aff1">
								<orgName type="institution">Universidad de los Andes</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Redmond</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<email>malik@eecs.berkeley.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hypercolumns for Object Segmentation and Fine-grained Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a feature representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localization but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hypercolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [22], where we improve state-of-the-art from 49.7 mean AP r [22] to 60.0, keypoint localization, where we get a 3.3 point boost over [20], and part labeling, where we show a 6.6 point gain over a strong baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Features based on convolutional networks (CNNs) <ref type="bibr" target="#b28">[29]</ref> have now led to the best results on a range of vision tasks: image classification <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref>, object segmentation and detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>, action classification <ref type="bibr" target="#b34">[35]</ref>, pose estimation <ref type="bibr" target="#b36">[37]</ref> and fine-grained category recognition <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b5">6]</ref>. We have thus moved from the era of HOG and SIFT to the era of convolutional network features. Therefore, understanding these features and how best to exploit them is of wide applicability.</p><p>Typically, recognition algorithms use the output of the last layer of the CNN. This makes sense when the task is assigning category labels to images or bounding boxes: the last layer is the most sensitive to category-level semantic information and the most invariant to "nuisance" variables such as pose, illumination, articulation, precise location and so on. However, when the task we are interested in is finer-grained, such as one of segmenting the detected object or estimating its pose, these nuisance variables are precisely what we are interested in. For such applications, the top layer is thus not the optimal representation.</p><p>The information that is generalized over in the top layer is present in intermediate layers, but intermediate layers are also much less sensitive to semantics. For instance, bar detectors in early layers might localize bars precisely, but cannot discriminate between bars that are horse legs and bars that are tree trunks. This observation suggests that reasoning at multiple levels of abstraction and scale is necessary, mirroring other problems in computer vision where reasoning across multiple levels has proven beneficial. For example, in optical flow, coarse levels of the image pyramid are good for correspondence, but finer levels are needed for accurate measurement, and a multiscale strategy is used to get the best of both worlds <ref type="bibr" target="#b6">[7]</ref>.</p><p>In this paper, we think of the layers of a convolutional network as a non-linear counterpart of the image pyramids used in optical flow and other vision tasks. Our hypothesis is that the information of interest is distributed over all levels of the CNN and should be exploited in this way. We define the "hypercolumn" at a given input location as the outputs of all units above that location at all layers of the CNN, stacked into one vector. (Because adjacent layers are strongly correlated, in practice we need not consider all layers but can simply sample a few.) <ref type="figure" target="#fig_0">Figure 1</ref> shows a visualization of the idea. We borrow the term "hypercolumn" from neuroscience, where it is used to describe a set of V1 neurons sensitive to edges at multiple orientations and multiple frequencies arranged in a columnar structure <ref type="bibr" target="#b23">[24]</ref>. However, our hypercolumn includes not just edge detectors but also more semantic units and is thus a more general notion.</p><p>We on two kinds of problems that require precise localization.</p><p>The first problem is simultaneous detection and segmentation (SDS) <ref type="bibr" target="#b21">[22]</ref>, where the aim is to both detect and segment every instance of an object category in the image. The second problem deals with detecting an object and localizing its parts. We consider two variants of this: one, locating the keypoints <ref type="bibr" target="#b42">[43]</ref>, and two, segmenting out each part <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31]</ref>. We present a general framework for tackling these and other fine-grained localization tasks by framing them as pixel classification and using hypercolumns as pixel descriptors. We formulate our entire system as a neural network, allowing end-to-end training for particular tasks simply by changing the target labels. Our empirical results are:</p><p>1. On SDS, the previous state-of-the-art was 49.7 mean AP r <ref type="bibr" target="#b21">[22]</ref>. Substituting hypercolumns into the pipeline of <ref type="bibr" target="#b21">[22]</ref> improves this to 52.8. We also propose a more efficient pipeline that allows us to use a larger network, pushing up the performance to 60.0.</p><p>2. On keypoint prediction, we show that a simple keypoint prediction scheme using hypercolumns achieves a 3.3 point gain in the APK metric <ref type="bibr" target="#b42">[43]</ref> over prior approaches working with only the top layer features <ref type="bibr" target="#b19">[20]</ref>. While there isn't much prior work on labeling parts of objects, we show that the hypercolumn framework is significantly better (by 6.6 points on average) than a strong baseline based on the top layer features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Combining features across multiple levels: Burt and Adelson introduced Laplacian pyramids <ref type="bibr" target="#b7">[8]</ref>, a representation that is widely used in computer vision. Koenderink and van Doorn <ref type="bibr" target="#b26">[27]</ref> used "jets", which are sets of partial derivatives of intensity up to a particular order, to estimate edge orientation, curvature, etc. Malik and Perona <ref type="bibr" target="#b31">[32]</ref> used the output of a bank of filters as a representation for texture discrimination. This representation also proved useful for optical flow <ref type="bibr" target="#b38">[39]</ref> and stereo <ref type="bibr" target="#b25">[26]</ref>. While the filter banks in these works cover multiple scales, they are still restricted to simple linear filters, whereas many of the features in the hypercolumn representation are highly non-linear functions of the image. There has also been work in convolutional networks that combines multiple levels of abstraction and scale. Farabet et al. <ref type="bibr" target="#b14">[15]</ref> combine CNN outputs from multiple scales of an image to do semantic segmentation. Tompson et al. <ref type="bibr" target="#b36">[37]</ref> use a similar idea for detecting parts and estimating pose. However, the features being combined still come from the same level of the CNN and hence have similar invariance. Sermanet et al. <ref type="bibr" target="#b33">[34]</ref> combine subsampled intermediate layers with the top layer for pedestrian detection. In contrast, since we aim for precise localization, we maintain the high resolution of the lower layers and upsample the higher layers instead. In contemporary work, Long et al. <ref type="bibr" target="#b29">[30]</ref> also use multiple layers for their fully convolutional semantic segmentation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detection and segmentation:</head><p>The task of simultaneous detection and segmentation task, introduced in <ref type="bibr" target="#b21">[22]</ref>, requires one to detect and segment every instance of a category in the image. SDS differs from classical bounding box detection in its requirement of a segmentation and from classical semantic segmentation in its requirement of separate instances. There has been other prior work on segmenting out instances of a category, mostly starting from bounding box detections. Borenstein and Ullman <ref type="bibr" target="#b3">[4]</ref> first suggested the idea of using class-specific knowledge for segmentation. Yang et al. <ref type="bibr" target="#b41">[42]</ref> use figure ground masks associated with DPM detectors <ref type="bibr" target="#b15">[16]</ref> to segment out detected objects and reason about depth orderings. Parkhi et al. <ref type="bibr" target="#b32">[33]</ref> use color models extracted from the detected cat and dog heads to segment them out. Dai and Hoiem <ref type="bibr" target="#b11">[12]</ref> generalize this reasoning to all categories. Fidler et al. <ref type="bibr" target="#b16">[17]</ref> and Dong et al. <ref type="bibr" target="#b12">[13]</ref> combine object detections from DPM <ref type="bibr" target="#b15">[16]</ref> with semantic segmentation outputs from O 2 P [9] to improve both systems. Current leading methods use CNNs to score bottom-up object proposals, both for object detection <ref type="bibr" target="#b17">[18]</ref> and for SDS <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Pose estimation and part labeling: Current best performers for pose estimation are based on CNNs. Toshev and Szegedy <ref type="bibr" target="#b37">[38]</ref> use a CNN to regress to keypoint locations. Tompson et al. <ref type="bibr" target="#b36">[37]</ref> show large improvements over stateof-the-art by predicting a heatmap for each keypoint, where the value of the heatmap at a location is the probability of the keypoint at that location. These algorithms show results in the setting where the rough location of the person is known. Yang and Ramanan <ref type="bibr" target="#b42">[43]</ref> propose a more realistic setting where the location of the person is not known and one has to both detect the person and identify his/her keypoints. Gkioxari et al. <ref type="bibr" target="#b20">[21]</ref> show some results in this setting using HOG-based detectors, but in their later work <ref type="bibr" target="#b19">[20]</ref> show large gains using CNNs.</p><p>Related to pose estimation is the task of segmenting out the different parts of a person, a task typically called "object parsing". Yamaguchi et al. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40]</ref> parse fashion photographs into clothing items. There has also been work on parsing pedestrians <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref>. Ionescu et al. <ref type="bibr" target="#b24">[25]</ref> jointly infer part segmentations and pose. However, the setting is typically tightly cropped bounding boxes of pedestrians, while we are interested in the completely unconstrained case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pixel classification using hypercolumns</head><p>Problem setting: We assume an object detection system that gives us a set of detections. Each detection comes with a bounding box, a category label and a score (and sometimes an initial segmentation hypothesis). The detections have already been subjected to non-maximum suppression. For every detection, we want to segment out the object, segment its parts or predict its keypoints.</p><p>For each task, we expand the bounding box of the detection slightly and predict a heatmap on this expanded box. The type of information encoded by this heatmap depends on the particular task. For segmentation, the heatmap encodes the probability that a particular location is inside the object. For part labeling, we predict a separate heatmap for each part, where each heatmap is the probability a location belongs to that part. For keypoint prediction, again we output a separate heatmap for each keypoint, with each heatmap encoding the probability that the keypoint is at a particular location.</p><p>In each case, we predict a 50×50 heatmap that we resize to the size of the expanded bounding box and splat onto the image. Thus, in our framework, these diverse fine-grained localization problems are addressed as the unified task of assigning a probability to each of the 50 × 50 locations or, in other words, of classifying each location. We solve this classification problem using the hypercolumn representation as described in detail below.</p><p>Computing the hypercolumn representation: We take the cropped bounding box, resize it to a fixed size and feed it into a CNN as in <ref type="bibr" target="#b17">[18]</ref>. For each location, we extract features from a set of layers by taking the outputs of the units that are "above" the location (as shown in <ref type="figure" target="#fig_0">Figure 1</ref>). All the intermediate outputs in a CNN are feature maps (the output of a fully connected layer can be seen as a 1 × 1 feature map). However, because of subsampling and pooling operations in the CNN, these feature maps need not be at the same resolution as the input or the target output size. So which unit lies above a particular location is ambiguous. We get around this by simply resizing each feature map to the size we want with bilinear interpolation. If we denote the feature map by F and the upsampled feature map by f , then the feature vector for the ith location has the form:</p><formula xml:id="formula_0">f i = k α ik F k (1)</formula><p>α ik depends on the position of i and k in the box and feature map respectively. We concatenate features from some or all of the feature maps in the network into one long vector for every location which we call the hypercolumn at that location. As an example, using pool2 (256 channels), conv4 (384 channels) and fc7 (4096 channels) from the architecture of <ref type="bibr" target="#b27">[28]</ref> would lead to a 4736 dimensional vector.</p><p>Interpolating into a grid of classifiers: Because these feature maps are the result of convolutions and poolings, they do not encode any information about where in the bounding box a given pixel lies. However, location can be an important feature. For instance, in a person bounding box, the head is more likely to be at the top of the bounding box than at the bottom. Thus a pixel that looks like a nose should be considered as part of the person if it occurs at the top of the box and should be classified as background otherwise. The reasoning should be the opposite for a foot-like pixel. This is a highly non-linear effect of location, and such reasoning cannot be achieved simply by a location-specific bias. (Indeed, our classifiers include (x, y) as features but assign negligible weight to them). Such reasoning requires different classifiers for each location.</p><p>Location is also needed to make better use of the features from the fully connected layers at the top. Since these features are shared by all the locations in the bounding box, they can at best contribute a global instance-specific bias. However, with a different classifier at each location, we can have a separate instance-specific bias for each location. Thus location-specific classifiers in conjunction with the global, instance-level features from the fully connected layer produce an instance-specific prior.</p><p>The simplest way to get a location-specific classifier is to train separate classifiers for each of the 50 × 50 locations. However, doing so has three problems. One, it dramatically reduces the amount of data each classifier sees during training. In our training sets, some categories may have only a few hundred instances, while the dimensionality of the feature vector is of the order of several thousand. Thus, having fewer parameters and more sharing of data is necessary to prevent overfitting. Two, training this many classifiers is computationally expensive, since we will have to train 2500 classifiers for 20 categories. Three, while we do want the classifier to vary with location, the classifier should change slowly: two adjacent pixels that are similar to each other in appearance should also be classified similarly.</p><p>Our solution is to train a coarse K × K grid of classifiers and interpolate between them. In our experiments we use K = 5 or 10. For the interpolation, we use an extension of bilinear interpolation where we interpolate a grid of functions instead of a grid of values. Concretely, each classifier in the grid is a function g k (•) that takes in a feature vector and outputs a probability between 0 and 1. We use this coarse grid of functions to define the function h i at each pixel i as a linear combination of the nearby grid functions, analogous to Equation 1:</p><formula xml:id="formula_1">h i (•) = k α ik g k (•)<label>(2)</label></formula><p>If the feature vector at the ith pixel is f i , then the score of the ith pixel is:</p><formula xml:id="formula_2">p i = k α ik g k (f i ) = k α ik p ik (3)</formula><p>where p ik is the probability output by the kth classifier for the ith pixel. Thus, at test time we run all our K 2 classifiers on all the pixels. Then, at each pixel, we linearly combine the outputs of all classifiers at that pixel using the above equation to produce the final prediction. Note that the coefficients of the linear combination depend on the location. Training this interpolated classifier is a hard optimization problem. We use a simple heuristic and ignore the interpolation at train time, using it only at test time.We divide each training bounding box into a K × K grid. The training data for the kth classifier consists only of pixels from the kth grid cell across all training instances. Each classifier is trained using logistic regression. This training methodology does not directly optimize the loss we would encounter at test time, but allows us to use off-the-shelf code such as liblinear <ref type="bibr" target="#b13">[14]</ref> to train the logistic regressor.</p><p>Efficient classification using convolutions and upsampling: Our system requires us to resize every feature map to 50 × 50 and then classify each location. But resizing feature maps with hundreds of channels can be expensive. However, we know we are going to run several linear classifiers on top of the hypercolumn features and we can use this knowledge to save computation as follows: each feature map with c channels will give rise to a c-dimensional block of features in the hypercolumn representation of a location, and this block will have a corresponding block of weights in the classifiers. Thus if f i is the feature vector at location i, then f i will be composed of blocks f (j) i corresponding to the jth feature map. A linear classifier w will decompose similarly. The dot product between w and f i can then be written as:</p><formula xml:id="formula_3">w T f i = j w (j)T f (j) i (4)</formula><p>The jth term in the decomposition corresponds to a linear classifier on top of the upsampled jth feature map. However, since the upsampling is a linear operation, we can first apply the classifier and then upsample using Equation 1:</p><formula xml:id="formula_4">f (j) i = k α (j) ik F (j) k (5) w (j)T f (j) i = k α (j) ik w (j)T F (j) k (6)</formula><p>We note that this insight was also used by Barron et al. <ref type="bibr" target="#b1">[2]</ref> in their volumetric semantic segmentation system.</p><p>Observe that applying a classifier to each location in a feature map is the same as a 1 × 1 convolution. Thus, to run a linear classifier on top of hypercolumn features, we break it into blocks corresponding to each feature map, run 1 × 1 convolutions on each feature map to produce score maps, upsample all score maps to the target resolution, and sum.</p><p>We consider a further modification to this pipeline where we replace the 1 × 1 convolution with a general n × n convolution. This corresponds to looking not only at the unit directly above a pixel but also the neighborhood of the unit. This captures the pattern of activations of a whole neighborhood, which can be more informative than a single unit, especially in the lower layers of the network.</p><p>Representation as a neural network: We can write our final hypercolumn classifier using additional layers grafted onto the original CNN as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. For each feature map, we stack on an additional convolutional layer. Each such convolutional layer has K 2 channels, corresponding to the K 2 classifiers we want to train. We can choose any kernel size for the convolutions as described above, although for fully connected layers that produce 1×1 feature maps, we are restricted to 1 × 1 convolutions. We take the outputs of all these layers, upsample them using bilinear interpolation and sum them. Finally, we pass these outputs through a sigmoid, and combine the K 2 heatmaps using equation 3 to give our final output. Each operation is differentiable and can be back-propagated over.</p><p>Representing our pipeline as a neural network allows us to train the whole network (including the CNN from which we extract features) for this task. For such training, we feed in the target 50 × 50 heatmap as a label. The loss is the sum of logistic losses (or equivalently, the sum of the negative log likelihoods) over all the 50 × 50 locations. We found that treating the sigmoids, the linear combination and the log likelihood as a single composite function and computing the gradient with respect to that led to simpler, more numerically stable expressions. Instead of training the network from scratch, we use a pretrained network and finetune, i.e., do backpropagation with a small learning rate. Finally, this representation as a neural network also allows us to train the grid classifiers together and use classifier in- terpolation during training, instead of training separate grid classifiers independent of each other.</p><p>Training classifiers for segmentation and part localization: For each category we take bottom-up MCG candidates <ref type="bibr" target="#b0">[1]</ref> that overlap a ground truth instance by 70% or more. For each such candidate, we find the ground truth instance it overlaps most with, and crop that ground truth instance to the expanded bounding box of the candidate. Depending on the task we are interested in (SDS, keypoint prediction or part labeling), we then use the labeling of the cropped ground truth instance to label locations in the expanded bounding box as positive or negative. For SDS, locations inside the instance are considered positive, while locations outside are considered negative. For part labeling, locations inside a part are positive and all other locations are negative. For keypoint prediction, the true keypoint location is positive and locations outside a certain radius (we use 10% of the bounding box diagonal) of the true location are labeled negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on SDS</head><p>Our first testbed is the SDS task. Our baseline for this task is the algorithm presented in <ref type="bibr" target="#b21">[22]</ref>. This pipeline scores bottom-up region proposals from <ref type="bibr" target="#b0">[1]</ref> using CNN features computed on both the cropped bounding box of the region and the cropped region foreground. The regions are subjected to non-max suppression. Finally, the surviving candidates are refined using figure-ground predictions based on the top layer features. As our first system for SDS, we use the same pipeline as above, but replace the refinement step with one based on hypercolumns. (We also add a bounding box regression step <ref type="bibr" target="#b17">[18]</ref> so as to start from the best available bounding box). We present results with this pipeline in section 4.1, where we show that hypercolumn-based refinement is significantly better than the refinement in <ref type="bibr" target="#b21">[22]</ref>, and is especially accurate when it comes to capturing fine details of the segmentation. We also evaluate several ablations of our system to unpack this performance gain. For ease of reference, we call this System 1.</p><p>One issue with this system is its computational cost. Extracting features from region foregrounds is expensive and doubles the time taken. Further, while CNN-based bounding box detection <ref type="bibr" target="#b17">[18]</ref> can be speeded up dramatically using approaches such as <ref type="bibr" target="#b22">[23]</ref>, no such speedups exist for region classification. To address these drawbacks, we propose as our second system the pipeline shown in <ref type="figure" target="#fig_3">Figure 3</ref>. This pipeline starts with bounding box detections after nonmaximum suppression. We expand this set of detections by adding nearby high-scoring boxes that were removed by non-maximum suppression but may be better localized (explained in detail below). This expanded set is only twice as large as the original set, and about two orders of magnitude smaller than the full set of bottom-up proposals. For each candidate in this set, we predict a segmentation, and score this candidate using CNN features computed on the segmentation. Because region-based features are computed only on a small set, the pipeline is much more efficient. We call this system System 2.</p><p>This pipeline relies crucially on our ability to predict a good segmentation from just bounding boxes. We use hypercolumns to make this prediction. In section 4.2, we show that these predictions are accurate, and significantly better than predictions based on the top layer of the CNN.</p><p>Finally, the efficiency of this pipeline also allows us to experiment with larger but more expressive architectures. While <ref type="bibr" target="#b21">[22]</ref> used the architecture proposed by Krizhevsky et al. <ref type="bibr" target="#b27">[28]</ref> (referred to as "T-Net" henceforth, following <ref type="bibr" target="#b18">[19]</ref>) for both the box features and the region features, we show in section 4.2 that the architecture proposed by Simonyan and Zisserman <ref type="bibr" target="#b35">[36]</ref> (referred to as "O-Net" henceforth <ref type="bibr" target="#b18">[19]</ref>) is significantly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">System 1: Refinement using hypercolumns</head><p>In our first set of experiments, we compare a hypercolumn-based refinement to that proposed in <ref type="bibr" target="#b21">[22]</ref>. We use the ranked hypotheses produced by <ref type="bibr" target="#b21">[22]</ref> and refine each hypothesis using hypercolumns. For the CNN, we use the same network that was used for the region classification (described as C in <ref type="bibr" target="#b21">[22]</ref>). This network consists of two pathways, each based on T-Net. It takes in both the cropped bounding box as well as the cropped foreground. For the hypercolumn representation we use the top-level fc7 features, the conv4 features from both pathways using a 1 × 1 neighborhood, and the pool2 features from the box pathway with a 3 × 3 neighborhood. We choose these layers because they are spread out evenly in the network and capture a diverse  set of features. In addition, for each location, we add as features a 0 or 1 encoding if the location was inside the original region candidate, and a coarse 10 × 10 discretization of the original candidate flattened into a 100-dimensional vector. This is to be commensurate with <ref type="bibr" target="#b21">[22]</ref> where these features were used in the refinement step. We use a 10 × 10 grid of classifiers. As a last step, we project our predictions to superpixels by averaging the prediction over each superpixel.</p><p>We train on VOC2012 Train and evaluate on VOC2012 Val. <ref type="table">Table 1</ref> shows the results of our experiments. The first two columns show the performance reported in <ref type="bibr" target="#b21">[22]</ref> with and without the refinement step. "Hyp" is the result we get using hypercolumns, without bounding box regression or finetuning. Our mean AP r at 0.5 is 1.5 points higher, and at 0.7 is 6.3 points higher, indicating that our refinement is much better than that of <ref type="bibr" target="#b21">[22]</ref> and is a large improvement over the original candidate. Bounding box regression and finetuning the network both provide significant gains, and with both of these, our mean AP r at 0.5 is 3.1 points higher and at 0.7 is 8.4 points higher than <ref type="bibr" target="#b21">[22]</ref>. <ref type="table">Table 1</ref> also shows the results of several ablations of our model (all without bounding box regression or finetuning):</p><p>1. Only fc7 uses only fc7 features and is thus similar to the refinement step in <ref type="bibr" target="#b21">[22]</ref>. We include this baseline to confirm that we can replicate those results.</p><p>2. fc7+pool2, fc7+conv4 and pool2+conv4 are refinement systems that use hypercolumns but leave out features from conv4, pool2 and fc7 respectively. Each of these baselines performs worse than our full system. In each case the difference is statistically significant at a confidence threshold of 0.05, computed using paired sample permutation tests.</p><p>3. The 1 × 1, 2 × 2 and 5 × 5 models use different grid resolutions, with the 1 × 1 grid amounting to a single classifier. There is a significant loss in performance (2.4 points at 0.7 overlap) when using a 1 × 1 grid. However this baseline still outperforms <ref type="bibr" target="#b21">[22]</ref> indicating that even without our grid classifiers (and without fc7, since the global fc7 features are ineffectual without the grid), the hypercolumn representation by itself is quite powerful. A 5 × 5 grid is enough to recover full performance.</p><p>Finally, following <ref type="bibr" target="#b21">[22]</ref>, we take our Hyp+FT+bbox-reg system and use the pasting scheme of <ref type="bibr" target="#b8">[9]</ref> to obtain a semantic segmentation. We get a mean IU of 54.6 on VOC2012 Segmentation Test, 3 points higher than <ref type="bibr" target="#b21">[22]</ref> (51.6 mean IU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">System 2: SDS from bounding box detections</head><p>For our experiments with System 2, we use the detections of R-CNN <ref type="bibr" target="#b17">[18]</ref> as the starting point. R-CNN uses CNNs to classify bounding box proposals from selective search. We use the final output after non-max suppression and bounding box regression. However, to allow direct comparison with our previous experiments, we retrained R-CNN to work with box proposals from MCG <ref type="bibr" target="#b0">[1]</ref>. We do all training on VOC2012 Train.</p><p>We first evaluate our segmentation predictions. As before, we use the same network as the detector to compute the hypercolumn transform features. We first experiment with the T-Net architecture. We use the layers fc7, conv4 with a neighborhood of 1, and pool2 with a neighborhood of 3. For computational reasons we do not do any finetuning. We use superpixel projection as before.</p><p>We show results in <ref type="table" target="#tab_0">Table 2</ref>. Since we use only one network operating on bounding boxes instead of two working on both the box and the region, we expect a drop in performance. We find that this is the case, but the loss is small: we get a mean AP r of 49.1 at 0.5 and 29.1 at 0.7, compared to 51.9 and 32.4 when we have the region features. In fact, our performance is nearly as good as <ref type="bibr" target="#b21">[22]</ref> at 0.5 and about 4 points better at 0.7, and we get this accuracy starting from just the bounding box.</p><p>To see how much of this performance is coming from the hypercolumn representation, we also run a baseline using just fc7 features. As expected, this baseline is only able to output a fuzzy segmentation, compared to the sharp delineation we get using hypercolumns. It performs considerably worse, losing 5 points at 0.5 overlap and almost 13 points at 0.7 overlap. <ref type="figure" target="#fig_4">Figure 4</ref> shows example segmentations.</p><p>We now replace the T-Net architecture for the O-Net architecture. This architecture is significantly larger, but provides an 8 point gain in detection AP <ref type="bibr" target="#b18">[19]</ref>. We again retrain the R-CNN system using this architecture on MCG bounding box proposals. Again, for the hypercolumn representation we use the same network as the detector. We use the layers fc7, conv4 with a neighborhood of 1 and pool3 with a neighborhood of 3. (We use pool3 instead of pool2 because the pool3 feature map has about half the resolution and is thus easier to work with.)</p><p>We observe that the O-Net architecture is significantly better than the T-Net: we get a boost of 7.5 points at the 0.5 overlap threshold and 8 points at the 0.7 threshold. We also find that this architecture gives us the best performance on the SDS task so far: with simple bounding box detection followed by our hypercolumn-based mask prediction, we achieve a mean AP r of 56.5 at an overlap threshold of 0.5 and a mean AP r of 37.0 at an overlap threshold of 0.7. These numbers are about 6.8 and 11.7 points better than the results of <ref type="bibr" target="#b21">[22]</ref>. Last but not the least, we observe that the large gap between our hypercolumn system and the only-fc7 baseline persists, and is equally large for the O-Net architecture. This implies that the gain provided by hypercolumns is not specific to a particular network architecture. <ref type="figure" target="#fig_4">Figure 4</ref> visualizes our O-Net results.</p><p>We now implement the full pipeline proposed in <ref type="figure">Figure</ref> 3. For this, we expand the initial pool of detections as follows. We pick boxes with score higher than a threshold that were suppressed by NMS but that overlap the detections by less than 0.7. We then do a non-max suppression with a lenient threshold of 0.7 to get a pool of candidates to rescore. Starting from 20K initial detections per category across the dataset, our expanded pool is typically less than 50K per category, and less than 600K in total.</p><p>Next we segment each candidate using hypercolumns and score it using a CNN trained to classify regions. This network has the same architecture as O-Net. However, instead of a bounding box, this network takes as input the bounding box with the region background masked out. This network is trained as described in <ref type="bibr" target="#b21">[22]</ref>. We use features from the topmost layer of this network and concatenate them with the features from the top layer of the detection network, and feed these into an SVM. For training data, we use our expanded pool of candidates on the training set, and take all candidates for which segmentation predictions overlap groundtruth by more than 70% as positive and those with overlap less than 50% as negative. After rescoring, we do a non-max suppression using region overlap to get the final set of detections (we use an overlap threshold of 0.3).</p><p>We get 60.0 mean AP r at 0.5, and 40.4 mean AP r at 0.7. These numbers are state-of-the-art on the SDS benchmark (in contemporary work, <ref type="bibr" target="#b10">[11]</ref> get slightly higher performance at 0.5 but do not report the performance at 0.7; our gains are orthogonal to theirs). Finally, on the semantic segmentation benchmark, we get a mean IU of 62.6, which is comparable to state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on part localization</head><p>We evaluate part localization in the unconstrained detection setting, where the task is to both detect the object and  label its keypoints/segment its parts. This is different from most prior work on these problems <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31]</ref>, which operates on the immediate vicinity of ground-truth instances. We start from the detections of <ref type="bibr" target="#b21">[22]</ref>. We use the same features and network as in Section 4.1. As before, we do all training on VOC2012 Train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keypoint prediction</head><p>We evaluate keypoint prediction on the "person" category using the protocol described in <ref type="bibr" target="#b20">[21]</ref>. The test set for evaluating keypoints is the person images in the second half of VOC2009 val. We use the APK metric <ref type="bibr" target="#b42">[43]</ref>, which evaluates keypoint predictions in a detection setting. Each detection comes with a keypoint prediction and a score. A predicted keypoint within a threshold distance (0.2 of the torso height) of the ground-truth keypoint is a true positive, and is a false positive otherwise. The area under the PR curve gives the APK for that keypoint. We start from the person detections of <ref type="bibr" target="#b21">[22]</ref>. We use bounding box regression to start from a better bounding box. As described in Section 3 we train a separate system for each keypoint using the hypercolumn representation. We use keypoint annotations collected by <ref type="bibr" target="#b4">[5]</ref>. We produce a heatmap for each keypoint and then take the highest scoring location of the heatmap as the keypoint prediction.</p><p>The APK metric requires us to attach a score with each keypoint prediction. This score must combine the confidence in the person detection and the confidence in the keypoint prediction, since predicting a keypoint when the keypoint is invisible counts as a false positive. For this score we multiply the value of the keypoint heatmap at the predicted location with the score output by the person detector (which we pass through a sigmoid).</p><p>Results are shown in <ref type="table">Table 3</ref>. We compare our perfor- <ref type="table">Table 3</ref>. Results on keypoint prediction (APK on the Person subset of VOC2009 val). Our system is 3.3 points better than <ref type="bibr" target="#b19">[20]</ref> (Section 5). <ref type="figure">Figure 5</ref>. Keypoint prediction (left wrist). Top row: baseline using fc7, bottom row: ours (hypercolumns without finetuning). In black is the bounding box and the predicted heatmap is in red. We normalize each heatmap so that the maximum value is 1.  <ref type="table">Table 4</ref>. Results on part labeling. Our approach (Hyp) is almost uniformly better than using top level features (Section 5).</p><p>mance to <ref type="bibr" target="#b19">[20]</ref>, the previous best on this dataset. Gkioxari et al. <ref type="bibr" target="#b19">[20]</ref> finetuned a network for pose, person detection and action classification, and then trained an SVM to assign a score to the keypoint predictions. Without any finetuning for pose, our system achieves a 1.8 point boost. A baseline system trained using our pipeline but with just the fc7 features performs significantly worse than our system, and is even worse than a HOG-based method <ref type="bibr" target="#b20">[21]</ref>. This confirms that the gains we get are from the hypercolumn representation. <ref type="figure">Figure 5</ref> shows some example predictions. Finetuning the network as described in Section 3 gives an additional 1.5 point gain, raising mean APK to 18.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part labeling</head><p>We evaluate part labeling on the articulated object categories in PASCAL VOC: person, horse, cow, sheep, cat, dog, bird. We use the part annotations provided by <ref type="bibr" target="#b9">[10]</ref>. We group the parts into top-level parts: head, torso, <ref type="figure">Figure 6</ref>. Part labeling. Top: baseline using fc7, bottom: ours (hypercolumns). Both rows use the same figure-ground segmentation. Red: head, green: torso, blue: legs, magenta: arms. arms and legs for person, head, torso, legs and tail for the four-legged animals and head, torso, legs, wings, tail for the bird. We train separate classifiers for each part. At test time, we use the Hyp+bbox-reg+FT system from Section 4.1 to predict a figure-ground mask for each detection, and to every pixel in the figure-ground mask, we assign the part with the highest score at that pixel.</p><p>For evaluation, we modify the definition of intersectionover-union in the AP r metric <ref type="bibr" target="#b21">[22]</ref>: we count in the intersection only those pixels for which we also get the part label correct. We call this metric AP r part . As before, we evaluate both our system and a baseline that uses only fc7 features. <ref type="table">Table 4</ref> shows our results. We get a large gain in almost all categories by using hypercolumns. Note that this gain is entirely due to improvements in the part labeling, since both methods use the same figure-ground mask. <ref type="figure">Figure 6</ref> shows some example part labelings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have shown that the hypercolumn representation provides large gains in three different tasks. We also believe that this representation might prove useful for other finegrained tasks such as attribute or action classification. We leave an investigation of this to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>HypercolumnFigure 1 .</head><label>1</label><figDesc>show the utility of the hypercolumn representation arXiv:1411.5752v2 [cs.CV] 25 Apr 2015 Convolutional Network The hypercolumn representation. The bottom image is the input, and above it are the feature maps of different layers in the CNN. The hypercolumn at a pixel is the vector of activations of all units that lie above that pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Representing our hypercolumn classifiers as a neural network. Layers of the original classification CNN are shown in red, and layers that we add are in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>An alternative pipeline for SDS starting from bounding box detections (Section 4)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure groundsegmentations starting from bounding box detections. Top row: baseline using fc7, bottom row: Ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Results on SDS on VOC 2012 val using System 2. Our final pipeline is state-of-the-art on SDS. (Section 4.2)</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported by ONR MURI N000141010933, a Google Research Grant and a Microsoft Research fellowship. We thank NVIDIA for providing GPUs through their academic program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Volumetric semantic segmentation using pyramid context features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V E</forename><surname>Keränen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Biggin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape-based pedestrian parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Class-specific, top-down segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved bird species recognition using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code. Communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to localize detected objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards unified object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bottom-up segmentation for top-down detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">R-CNNs for pose estimation and action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Iterated second-order label sensitive pooling for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Determining three-dimensional shape from orientation and spatial frequency disparities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representation of local geometry in the visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pedestrian parsing via deep decompositional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Preattentive texture discrimination with early vision mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America A</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The truth about cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2199</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Robust computation of optical flow in a multi-scale differential framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">IJCV</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Paper doll parsing: Retrieving similar styles to parse clothing items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Layered object models for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Partbased R-CNNs for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
