<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards a Neural Statistician</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-06-07">7 Jun 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
							<email>h.l.edwards@sms.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
							<email>a.storkey@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards a Neural Statistician</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-06-07">7 Jun 2016</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1606.02185v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. 2 Problem Statement We are given datasets D i for i ∈ I. Each dataset D i = {x 1 ,. .. , x ki } consists of a number of i.i.d samples from an associated distribution p i over R n. The task can be split into learning and inference components. The learning component is to produce a generative modelp i for each dataset D i. We assume there is a common underlying generative process p such that p i = p(•|c i) for c i ∈ R l drawn from p(c). We refer to c as the context. The inference component is to give an approximate posterior over the context q(c|D) for a given dataset produced by a statistic network.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The machine learning community is well-practised at learning representations of data-points and sequences. A middle-ground between these two is representing, or summarizing, datasets -unordered collections of vectors, such as photos of a particular person, recordings of a given speaker or a document as a bag-of-words. Where these sets take the form of i.i.d samples from some distribution, such summaries are called statistics. We explore the idea of using neural networks to learn statistics and we refer to our approach as the neural statistician. The key result of our approach is a statistic network that takes as input a set of vectors and outputs a vector of summary statistics specifying a generative model of that set. The advantages of our approach are that it is:</p><p>• Unsupervised: It provides principled and unsupervised way to learn summary statistics as a variational encoder of a generative model. • Data efficient: If one has a large number of small but related datasets, modelling the datasets jointly enables us to gain statistical strength. • Parameter Efficient: By using summary statistics instead of say categorical labellings of each dataset, we decouple the number of parameters of the model from the number of datasets.  Left: basic hierarchical model. Right: full neural statistician model with three latent layers z 1 , z 2 , z 3 . Each collection of incoming edges to a node is implemented as a neural network, the input of which is the concatenation of the edges' sources, the output of which is a parameterization of a distribution over the random variable represented by that node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variational Autoencoder</head><p>The variational autoencoder is a latent variable model p(x|z; θ) with parameters θ. For each observed</p><p>x, a corresponding latent variable z is drawn from p(z) so that</p><formula xml:id="formula_0">p(x) = p(x|z; θ)p(z) dz<label>(1)</label></formula><p>The generative parameters θ are learned by introducing a recognition network q(z|x; φ) with parameters φ. The recognition network gives an approximate posterior over the latent variables that can then be used to give a variational lower bound on the log-likelihood</p><formula xml:id="formula_1">L x = E q(z|x,φ) [log p(x|z; θ)] − D KL (q(z|x; φ) p(z))<label>(2)</label></formula><p>We can then optimize L x with respect to φ and θ using the reparameterization trick introduced by Kingma and Welling <ref type="bibr" target="#b15">[16]</ref> and Rezende et al. <ref type="bibr" target="#b26">[27]</ref> to take gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Basic Model</head><p>We extend this model as shown on the left in <ref type="figure" target="#fig_6">Figure 3</ref> to include a latent variable c, the context, that varies per dataset D. Now the likelihood of a particular dataset D is given by</p><formula xml:id="formula_2">p(D) = p(c) x∈D p(x|z; θ)p(z|c; θ) dz dc (3)</formula><p>The prior p(c) is chosen to be a spherical Gaussian with zero-mean and unit variances. The conditional p(z|c; θ) is Gaussian with diagonal covariance, where the mean and variance depend on c through a neural network. Similarly the observation model p(x|z; θ) will be a simple likelihood function appropriate to the data modality with dependence on z parameterized by a neural network. We use approximate inference networks q(z|x, c, φ), q(c|D; φ) with parameters φ to calculate and optimize a variational lower bound on the log-likelihood, where again the likelihood forms are diagonal Gaussians parameterized by neural networks:</p><formula xml:id="formula_3">L D = E q(c|D;φ) x∈d E q(z,|c,x;φ) [log p(x|z; θ)] − D KL (q(z|c, x; φ) p(z|c; θ)) − D KL (q(c|D; φ) p(c))<label>(4)</label></formula><p>Note that q(c|D, φ) accepts as input a dataset D and we refer to this as the statistic network. We describe this in Subsection 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Full Model</head><p>Algorithm 1 Sampling a Dataset of size k sample c ∼ p(c)</p><formula xml:id="formula_4">for i = 1 to k do sample z i,L ∼ p(z L |c, θ) for j = L − 1 to 1 do sample z i,j ∼ p(z j |z i,j+1 , c, θ) end for sample x i ∼ p(x|z i,1 , . . . , z i,L , c, θ) end for</formula><p>The basic model works well for modelling simple datasets, but struggles when the datasets have complex internal structure. To increase the sophistication of the model we use multiple stochastic layers z 1 , . . . , z k and introduce skipconnections for both the inference and generative networks. The model is shown graphically in <ref type="figure" target="#fig_6">Figure 3</ref>. The probability of a dataset D is then given by</p><formula xml:id="formula_5">p(D) = p(c) x∈D p(x|c, z 1:L ; θ)p(z L |c; θ) L−1 i=1 p(z i |z i+1 , c; θ) dz 1:L dc<label>(5)</label></formula><p>and the generative process for the full model is described in Algorithm 1.</p><p>The full approximate posterior factorizes analogously as</p><formula xml:id="formula_6">q(c, z 1:L |D; φ) = q(c|D; φ) x∈D q(z L |x, c; φ) L−1 i=1 q(z i |z i+1 , x, c; φ)<label>(6)</label></formula><p>For convenience we give the variational lower bound as sum of a three parts</p><formula xml:id="formula_7">L D = R D + C D + L D<label>(7)</label></formula><p>the reconstruction term</p><formula xml:id="formula_8">R D = E q(c|D;φ) x∈D E q(z 1:L |c,x;φ) log p(x|z 1:L , c; θ)<label>(8)</label></formula><p>the context divergence</p><formula xml:id="formula_9">C D = D KL (q(c|D; φ) p(c))<label>(9)</label></formula><p>and the latent divergences</p><formula xml:id="formula_10">L D = E q(c,z 1:L |D;φ)</formula><p>x∈D</p><formula xml:id="formula_11">D KL (q(z L |c, x; φ) p(z L |c; θ)) + L−1 i=1 D KL (q(z i |z i+1 , c, x; φ) p(z i |z i+1 , c; θ))<label>(10)</label></formula><p>The skip-connections p(z i |z i+1 , c, θ) and q(z i |z i+1 , x) allow the context to specify a more precise distribution for each latent variable by explaining-away more generic aspects of the dataset at each stochastic layer. This architecture was inspired by recent work on probabilistic ladder networks in Kaae Sønderby et al. <ref type="bibr" target="#b13">[14]</ref>. Complementing these are the skip-connections from each latent variable to the observation p(x|z 1:L , c; θ), the intuition here is that each stochastic layer can focus on representing a certain level of abstraction, since its information does not need to be copied into the next layer, a similar approach was used in Maaløe et al. <ref type="bibr" target="#b21">[22]</ref>. Note that we are optimizing over many datasets D: we want to maximize the expectation of L D over all datasets. We do this optimization using stochastic gradient descent. In contrast to a variational autoencoder where a minibatch would consist of a subsample of datapoints from the dataset, we use minibatches consisting of a subsample of datasets -tensors of shape (batch size, sample size, number of features).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Statistic Network</head><p>In addition to the standard recognition networks we use a statistic network q(c|D; φ) to give an approximate posterior over the context c given a dataset D = {x 1 , . . . , x k } . This is a feedforward neural network consisting of three main elements:</p><p>• An instance encoder E that takes each individual datapoint x i to a vector e i = E(x i ).</p><p>• An exchangeable instance pooling layer that collapses the matrix (e 1 , . . . , e k ) to a single prestatistic vector v. Examples include elementwise means, sums, products, geometric means and maximum. We use the sample mean for all experiments. • A final post-pooling network that takes v to a parameterization of a diagonal Gaussian. We note that the humble sample mean already gives the statistic network a great deal of representational power due to the fact that the instance encoder can learn a representation where averaging makes sense. For example since the instance encoder can approximate a polynomial on a compact domain, and so can the post-pooling network, a statistic network can approximate any moment of a distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Due to the general nature of the problem considered, our work touches on many different topics which we now attempt to summarize.</p><p>Topics models and graphical models We note the resemblance of the graphical model in <ref type="figure" target="#fig_6">Figure  3</ref> to the form of a topic model. In contrast to traditional topic models we do not use discrete latent variables, or restrict to discrete data. In addition we use more flexible conditional distributions and dependency structures parameterized by deep neural networks, although recent work has explored this for document models in Miao et al. <ref type="bibr" target="#b22">[23]</ref>. Along related lines are 'structured variational autoencoders' <ref type="bibr" target="#b12">[13]</ref> where they treat the general problem of integrating graphical models with variational autoencoders.</p><p>Transfer learning There is a considerable literature on transfer learning, for a survey see Pan and Yang <ref type="bibr" target="#b24">[25]</ref>. There they discuss 'parameter-transfer' approaches whereby parameters or priors are shared across datasets, and our work fits into that paradigm. For examples see Lawrence and Platt <ref type="bibr" target="#b19">[20]</ref> where share they priors between Gaussian processes, and Evgeniou and Pontil <ref type="bibr" target="#b4">[5]</ref> where they take an SVM-like approach to share kernels.</p><p>One-shot Learning Learning quickly from small amounts of data is a topic of great interest. In particular we have seen Lake et al. <ref type="bibr" target="#b17">[18]</ref> where they use Bayesian program induction, and <ref type="bibr" target="#b16">[17]</ref> where they train a Siamese ( <ref type="bibr" target="#b2">[3]</ref>) convolutional network for one-shot image classification We note the relation to the recent work <ref type="bibr" target="#b27">[28]</ref> in which they use a conditional recurrent variational autoencoder capable of one-shot generalization by taking as extra input a conditioning data point. The important differences here are that we jointly model datasets and datapoints and consider datasets of any size.</p><p>Multiple-Instance Learning There is previous work on classifying sets in multiple-instance learning, for a useful survey see Cheplygina et al. <ref type="bibr" target="#b1">[2]</ref>. Typical approaches involve adapting kernel based methods such as support measure machines <ref type="bibr" target="#b23">[24]</ref>, support distribution machines <ref type="bibr" target="#b25">[26]</ref> and multiple-instance-kernels <ref type="bibr" target="#b6">[7]</ref>.</p><p>Set2Seq A highly related work is Vinyals et al. <ref type="bibr" target="#b30">[31]</ref> where they explore architectures for mapping sets to sequences. There they use an LSTM to repeatedly compute weighted-averages of the datapoints. They use this to tackle problems such as sorting a list of numbers. The main difference between their work and ours is that they primarily consider supervised problems, whereas we present a general unsupervised method for learning representations of sets of i.i.d instances. In future work we may also explore recurrently computing statistics.</p><p>ABC There has also been work on learning summary statistics for Approximate Bayesian Computation by either learning to predict the parameters generating a sample as a supervised problem, or by using kernel embeddings as infinite dimensional summary statistics. See <ref type="bibr" target="#b5">[6]</ref> for instance for kernel-based approaches. More recently Jiang et al. <ref type="bibr" target="#b11">[12]</ref> used deep neural networks to predict the parameters generating the data. The crucial differences are that their problem is supervised, they do not leverage any exchangeability properties the data may have, nor can it deal with varying sample sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>Given an input set x 1 , . . . x k we can use the statistic network to calculate an approximate posterior over contexts q(c|x 1 , . . . , x k ; φ). Each context c specifies a generative model p(x|c; θ). To get samples from the model we set c to the mean of the approximate posterior and then sample directly We sample from both the true posterior and the approximate posterior and fit a kernel density. from the conditional distributions. We use the Adam optimization algorithm <ref type="bibr" target="#b14">[15]</ref> for all experiments, batch normalization <ref type="bibr" target="#b9">[10]</ref> for models with mulitple stochastic layers and we always use a batch size of 16. We primarily use the Theano <ref type="bibr" target="#b28">[29]</ref> framework with the Lasagne <ref type="bibr" target="#b3">[4]</ref> library, but the final experiments with face data were done using Tensorflow <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Simple 1-D Distributions</head><p>In our first experiment we wanted to know if the neural statistician will learn to cluster synthetic 1-D datasets by distribution family. We generated a collection of synthetic 1-D datasets each containing respectively. Training data contains 10K sets. We used a model with a single stochastic layer with 32 units for z and 3 units for c. We used three dense layers before and after pooling in the statistic network each with 128 units with Rectified Linear Unit (ReLU) activations. For q(z|x, c) and p(x|z, c) we used three dense layers with ReLU activations and 128 units. <ref type="figure" target="#fig_2">Figure 2</ref> on the left shows a 3-D scatter plot of the summary statistics learned. Notice that the different families of distribution cluster. It is interesting to observe that the Exponential cluster is differently orientated to the others, perhaps reflecting the fact that it is the only non-symmetric distribution. We also see that between the Gaussian and Laplacian cluster there is an area of ambiguity which is as one might expect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Bernoulli Data with Varying Sample Size</head><p>If we observe one coin flip come up heads we gain little information about the coin; if we see fifty heads in a row we are all but certain that the coin is biased. But in both cases the observed proportion of heads is 100%. The lesson is that the sample size can be important for accurate inference of the generative parameters, and so a neural statistician should be able to take sample size into account. The problem is that the sample mean records only the relative proportions of values, for instance: the sample means = 1 x i is equal to the sample mean of the single datapoint x 1 =s. The approximate posterior over the context given by the statistic network is a function of a sample mean. Hence the approximate posterior is also partially invariant to the sample size in the above sense. This means that the statistic network q(c|D) has no way to represent uncertainty about the context resulting from differing sample sizes. In order to handle uncertainty over the context resulting from differing sample sizes, we augment our recognition model by introducing a prior-interpolation layer. The prior-interpolation layer modifies the output of the statistic network q(c|D) described in Subsection 3.4 as follows: given that the output of q(c|D) is µ c and σ 2 c parameterizing the approximate posterior N (c; µ c , σ 2 c ), we define µ * and (σ 2 c ) * , the interpolated parameters by To demonstrate that this simple intervention can work we perform an experiment on synthetic data where we can calculate the true posterior analytically. The datasets consist of samples from Bernoulli distributions with probability p ∼ U [0, 1]. Sample sizes are between 1 and 20 with equal probability. We chose f α (n) = exp(− exp(α) • n) and f β (n) = exp(− exp(β) • n) as our prior-interpolation layer. We use a single stochastic layer with 2 units for c and 2 units for z. There is no instance encoder in the statistic network, the pre-statistic vector is simply the average of each dataset. There are three dense post-pooling layers with 16 ReLU units. The other subnetworks each have 2 dense layers with 32 ReLU units. We use a Bernoulli likelihood for the decoder p(x|z, c).</p><formula xml:id="formula_12">µ * c = f α (n) • 0 + (1 − f α (n)) • µ c and (σ 2 c ) * = f β (n) • 1 + (1 − f β (n)) • σ 2 c<label>(11)</label></formula><p>Since the Beta distribution is a conjugate prior for the Bernoulli, we know the exact posterior for p given observations d. In <ref type="figure" target="#fig_2">Figure 2</ref> on the right we compare samples from the approximate posterior given by the model and the true posterior across a range of sample sizes, and we see that the model is indeed able to account for uncertainty. In order to sample the probability from our model we sample a context from the approximate posterior and then sample x ∼ p(x|c; θ) 100 times and average the result to give p. This simple modification improves the average log-likelihood per data point from −0.627 when training without the prior-interpolation layer to −0.597, for comparison the true likelihood of the test data is −0.595. Building on the previous experiments we investigate 2-D datasets that have complex structure, but the datapoints contain little information by themselves. We created a dataset called spatial MNIST. In spatial MNIST each image from MNIST <ref type="bibr" target="#b20">[21]</ref> is turned into a dataset by interpreting the normalised pixel intensities as a probability density and sampling coordinate values. An example is shown in <ref type="figure" target="#fig_6">Figure 3</ref>. This creates two-dimensional spatial datasets. We used a sample size of 50. Note that since the pixel coordinates are discrete, it is necessary to dequantize them by adding uniform noise u ∼ U [0, 1] to the coordinates if one models them as real numbers, else you can get arbitrarily high densities (see <ref type="bibr" target="#b29">[30]</ref> for a discussion of this point). We used 3 stochastic layers with 2 units for each z and 64 units for c. Each subnetwork contained 3 dense layers with 256 ReLU units each. We used a Gaussian likelihood for p(x|z 1:3 , c θ) . In addition to being able to sample from the model conditioned on a set of inputs, we can also to summarize a dataset by choosing a subset S ⊆ d to minimise the KL divergence of q(C|d; φ) from q(C|S; φ). We do this greedily by iteratively discarding points from the full sample. The results are shown in <ref type="figure" target="#fig_7">Figure 5</ref>.3. We see that the model is capable of handling complex arrangements of datapoints. We also see that it can select sensible subsets of a dataset as a summary, notice that the model tends to select 'critical points' of the shape just as a human might do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Spatial MNIST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">MNIST</head><p>We now move on to modelling high dimensional data. A natural source of such datasets are images of a particular class. We begin with sets of MNIST digits of the same class with sample sizes of 10. We treat the data as binary by interpreting the pixel intensities as Bernoulli probabilities and sampling. We resample the training data at each epoch to combat overfitting following <ref type="bibr" target="#b13">[14]</ref>. Since the MNIST digits exhibit relatively small amounts of variation, we use a single stochastic layer with 32 units for both z and c. The statistic network has three pre and post pooling dense layers with 64 ReLU units each. The other subnetworks each have three dense layers with 256 ReLU units each. We used a Bernoulli likelihood for the decoder. Conditioned samples are shown in <ref type="figure" target="#fig_7">Figure 5</ref> on the right. The samples appear well-formed and so we have demonstrated that it is possible to flexibly condition generative models of high-dimensional data on-the-fly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Omniglot</head><p>Next we work with the OMNIGLOT data <ref type="bibr" target="#b17">[18]</ref>. This contains 1628 classes of handwritten characters but with just 20 examples per class. This makes it an excellent test-bed for transfer / small-shot learning. Here the datasets correspond to samples from a given class, and we use a sample size of 5. We save 5 examples per class for the test set, and in addition we hold out 20 entire classes to test generalization to new classes. We created new classes with data augmentation by rotating and reflecting characters. We resized the images to 28 × 28. As with MNIST we treated the data as binary and sampled a binarization of the image at each epoch. We used three stochastic layers with 16 units for each z and 64 units for c. The decoder p(x|z 1 , z 2 , z 3 , c) consisted of: three dense layers with 256 ReLU units, followed a dense layer H with 10 × 7 × 7 linear units, followed by ReLU convolutional layers with 64 → 64 → 32 filters of size 3 × 3 interspersed with 2 × 2 upsampling layers, followed by a 1 × 1 convolution with a single filter and a sigmoid activation. The output was then passed through a spatial transformer network <ref type="bibr" target="#b10">[11]</ref> effecting an affine change of coordinates with parameters a linear projection of H. The decoder used a Bernoulli likelihood. All other subnetworks had 3 dense layers with 256 ReLU units. In <ref type="figure" target="#fig_7">Figure 5</ref> on the left we show samples from the model conditioned on classes seen in the training data. The good quality of the samples indicate that the model is indeed able to represent a large variety of different datasets. In <ref type="figure" target="#fig_8">Figure 6</ref> we show two examples of small-shot learning by conditioning on samples of unseen characters from OMNIGLOT, and conditioning on samples of digits from MNIST. The OMNIGLOT samples are mostly of a high-quality, and this shows that the neural statistician can generalize even to new datasets. The transfer to MNIST represents a harder task and produces less attractive samples, but they are still recognisably the correct digit. We believe that the results could be improved further through use of a convolutional encoder and/or more aggressive data augmentation. As a further test we considered small-shot classification of both unseen OMNIGLOT characters and MNIST digits. Given a sets of labelled examples of each class D 0 , . . . , D 9 for MNIST say we computed the approximate posteriors q(C|D i ; φ) using the statistic network. Then for each test image x we also computed the posterior q(C|x; φ) and classified it according to training dataset D i minimizing the KL divergence from the test context to the training context. We tried this with either 1 or 5 labelled examples per class. We implemented two simple baselines, 1-nearest neighbour on the raw features, and 1-nearest neighbour on features learned by an autoencoder trained on the OMNIGLOT training data. The autoencoder had 5 hidden layers with 256,256,64,256,256 respective  ReLU units, the middle layer was used as the feature embedding. The results are shown in <ref type="table" target="#tab_0">Table 1</ref>. We also include similar results from Koch <ref type="bibr" target="#b16">[17]</ref> a discriminatively trained model, but note that they are not directly comparable since they used different splits of characters when training their model. We include their results to give a rough idea of how our model compares to one trained for this task. We find that the performance is remarkably good, especially considering that the model itself was not discriminatively trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Youtube Faces</head><p>Finally, we provide a proof of concept for generating faces of a particular person. We use the Youtube Faces Database from Wolf et al. <ref type="bibr" target="#b31">[32]</ref>. It contains 3, 245 videos of 1, 595 different people. We use the aligned and cropped to face version, resize to 64 × 64. The validation and test sets contain 100 unique people each, and there is no overlap of persons between data splits. The sets were created by sampling frames randomly without replacement from each video, we use a set size of 5 frames. We resample the sets for the training data each epoch. Our architecture for this problem is based on one presented in <ref type="bibr" target="#b18">[19]</ref>. We used a single stochastic layer with 500 dimensional latent c and 16 dimensional z variable. The statistic network and the inference network q(z|x, c) share a common convolutional encoder consisting of layers with (filter sizes, numbers of filters, strides) given by <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b0">1)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b0">1)</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b0">1)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b0">1)</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr">64,</ref><ref type="bibr" target="#b1">2)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">128,</ref><ref type="bibr" target="#b0">1)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">128,</ref><ref type="bibr" target="#b0">1)</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr">128,</ref><ref type="bibr" target="#b1">2)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">256,</ref><ref type="bibr" target="#b0">1)</ref>, <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">256,</ref><ref type="bibr" target="#b0">1)</ref>, <ref type="bibr" target="#b1">(2,</ref><ref type="bibr">256,</ref><ref type="bibr" target="#b1">2)</ref> respectively, all with ReLU activations. The statistic network continues with a dense ReLU layer with 1000 units, followed by average pooling and a dense layer to c. The inference network continues by concatenating with the sample from the statistic network, then a 1000 unit ReLU layer then a linear mapping to z. The decoder begins with a 1000 unit ReLU layer, followed by a linear layer with 256 × 8 × 8 units followed by convolutional layers with (filter sizes, numbers of filters, strides) given by <ref type="bibr" target="#b2">(3,</ref><ref type="bibr">256</ref>  <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2)</ref>, (1,3,1) respectively, with stride 1 layers being convolutions and stride 2 layers being transpose-convolutional layers. All activations are ReLUs except for the last layer which is a sigmoid. The likelihood function is a Gaussian, but where the variance parameters are shared across all datapoints, this was found to make training faster and more stable. The results are shown in <ref type="figure" target="#fig_9">Figure 7</ref>. Whilst there is room for improvement, we see that it is possible to specify a highly complex distribution on-the-fly with a set of photos of a previously unseen person. The samples conditioned on an input set have a reasonable likeness of the input faces. We also show the ability of the model to generate new datasets and see that the samples have a consistent identity and varied poses. Our approach can easily benefit from advances in deep generative models by upgrading our base generative model, and so in the future we could for instance use a recurrent generative model as in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>Our goal was to demonstrate that it is both possible and profitable to work at a level of abstraction of datasets rather than just datapoints. We have shown how it is possible to learn to represent datasets using a statistic network, and that these statistics enable highly flexible and efficient models that can do transfer learning, small shot classification, cluster distributions, summarize datasets and more. Avenues for future research are engineering, methodological and application based. In terms of engineering we believe that there are gains to be had by more thorough exploration of different (larger) architectures. In terms of methodology we want to look at: improved methods of representing uncertainty resulting from sample size; models explicitly designed trained for small-shot classification; supervised and semi-supervised approaches to classifiying either datasets or datapoints within the dataset. One advantage we have yet to explore is that by specifying classes implicitly in terms of sets, we can combine multiple data sources with potentially different labels, or multiple labels. We can also easily train on any unlabelled data because this corresponds to sets of size one. We also want to consider questions such as: What are desirable properties for statistics to have as representations? How can we enforce these? Can we use ideas from classical work on estimators? In terms of applications we are interested in applying this framework to learning embeddings of speakers for speech problems or customer embeddings in commercial problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>Capable of small-shot learning: If the datasets correspond to examples from different classes, class embeddings, (summary statistics associated with examples from a class) allow us to handle new classes at test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: basic hierarchical model. Right: full neural statistician model with three latent layers z 1 , z 2 , z 3 . Each collection of incoming edges to a node is implemented as a neural network, the input of which is the concatenation of the edges' sources, the output of which is a parameterization of a distribution over the random variable represented by that node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Left: A 3-D scatter plot of the summary statistics learned on synthetic data, coloured by distribution family. Right: We show the posterior distributions over parameter p of Bernoulli distributions given varying numbers of samples. Each sample consists of equal numbers of 0s and 1s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>200 samples. Datasets consist of samples from either an Exponential, Gaussian, Uniform or Laplacian distribution with equal probability. Means and variances are sampled from U [−1, 1] and U [0.5, 2]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>is equal to the sample mean of the repeated data 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Conditioned samples from spatial MNIST data. Blue and red digits are the input sets, black digits above correspond to samples given the input. Red points correspond to a 6-sample summary of the dataset giving an interpolated approximate posterior N (c; µ * c , (σ 2 c ) * ), where n is the sample size, f α , f β are monotonically decreasing functions with outputs in [0, 1] with parameters α and β (learned through optimizing the variational lower bound in Equation 7), and such that f α (0) = f β (0) = 1 and f α (n), f β (n) → 0 as n → ∞. This allows the statistic network to interpolate its proposed approximate posterior coordinate-wise with the prior p(c) = N (c; 0, 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>An image from MNIST on the left, transformed to a set of 50 (x, y) coordinates, shown as a scatter plot on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Left: Left rows show input set, right rows show corresponding samples. Input sets are test examples from previously seen classes from OMNIGLOT. Right: Samples from model trained on MNIST. Left rows show the input set. Corresponding right rows show samples given the input set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Small shot learning Left: Small-shot learning from OMNIGLOT to MNIST. Left rows are input sets, right rows are samples given the inputs. Right: Small-shot learning from with OMNIGLOT data to unseen classes. Left rows are input sets, right rows are samples given the inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Small shot learning for face data. Samples are from model trained on Youtube Faces Database. Left: Each row shows an input set of size 5. Center: Each row shows 5 samples from the model corresponding to the input set on the left. Right: Imagined new faces generated by sampling contexts from the prior. Each row consists of 5 samples from the model given a particular sampled context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>,1), (3,256,1), (3,256,2), (3,128,1), (3,128,1), (2,128,2), (3,64,1), (3,64,1),(2,64,2), (3,32,1), (3,32,1),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The table shows the classification accuracies of various small-shot learning tasks. Models are trained on OMNIGLOT data and tested on either unseen OMNIGLOT classes or MNIST with varying numbers of samples per class. *Note these results are not directly comparable since we do not use the same class splits.</figDesc><table><row><cell>Model</cell><cell>Data</cell><cell cols="2">Samples Per Class Accuracy (%)</cell></row><row><cell>Raw</cell><cell>MNIST</cell><cell>1</cell><cell>37</cell></row><row><cell cols="2">Autoencoder MNIST</cell><cell>1</cell><cell>40</cell></row><row><cell>Ours</cell><cell>MNIST</cell><cell>1</cell><cell>70</cell></row><row><cell>Raw</cell><cell>MNIST</cell><cell>5</cell><cell>60</cell></row><row><cell cols="2">Autoencoder MNIST</cell><cell>5</cell><cell>56</cell></row><row><cell>Ours</cell><cell>MNIST</cell><cell>5</cell><cell>82</cell></row><row><cell>Raw</cell><cell cols="2">OMNIGLOT 1</cell><cell>21</cell></row><row><cell cols="3">Autoencoder OMNIGLOT 1</cell><cell>33</cell></row><row><cell>Ours</cell><cell cols="2">OMNIGLOT 1</cell><cell>88</cell></row><row><cell>Raw</cell><cell cols="2">OMNIGLOT 5</cell><cell>42</cell></row><row><cell cols="3">Autoencoder OMNIGLOT 5</cell><cell>54</cell></row><row><cell>Ours</cell><cell cols="2">OMNIGLOT 5</cell><cell>95</cell></row><row><cell>[17]*</cell><cell cols="2">OMNIGLOT 1</cell><cell>92</cell></row><row><cell>[17]*</cell><cell>MNIST</cell><cell>1</cell><cell>70</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On classification with bags, groups and sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Sander Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eben</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kelly</surname></persName>
		</author>
		<title level="m">First release</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel Bayes&apos; rule: Bayesian inference with positive definite kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3753" to="3783" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-instance kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">A</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kowalczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th International Conf. on Machine Learning</title>
		<meeting>19th International Conf. on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="179" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.08772</idno>
	</analytic>
	<monogr>
		<title level="j">Towards conceptual compression</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning summary statistic for approximate Bayesian computation via deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02175</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">B</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sandeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06277</idno>
		<title level="m">Structured vaes: Composing probabilistic graphical models and variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">How to train deep variational autoencoders and probabilistic ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Casper Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Søren Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02282</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations (ICLR</title>
		<meeting>the 2nd International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Siamese neural networks for one-shot image recognition. Doctoral dissertation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Discriminative regularization for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03220</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to learn with the informative vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Søren Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05473</idno>
		<title level="m">Auxiliary deep generative models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06038</idno>
		<title level="m">Neural variational inference for text processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from distributions via support measure machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Dinuzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>P. Bartlett, FCN. Pereira, CJC. Burges, L. Bottou, and KQ. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge and Data Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>A survey on transfer learning</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Support distribution machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dougal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneider</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1202.0302" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">One-shot generalization in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theano</forename><surname>Development Team</surname></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Order matters: sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
