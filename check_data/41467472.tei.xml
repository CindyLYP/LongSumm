<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning End-to-end Video Classification with Rank-Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
							<email>basura.fernando@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Research School of Engineering</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<postCode>2601</postCode>
									<region>ACT</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
							<email>stephen.gould@anu.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<postCode>2601</postCode>
									<region>ACT</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning End-to-end Video Classification with Rank-Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a new model for representation learning and classification of video sequences. Our model is based on a convolutional neural network coupled with a novel temporal pooling layer. The temporal pooling layer relies on an inner-optimization problem to efficiently encode temporal semantics over arbitrarily long video clips into a fixed-length vector representation. Importantly, the representation and classification parameters of our model can be estimated jointly in an end-to-end manner by formulating learning as a bilevel optimization problem. Furthermore, the model can make use of any existing convolutional neural network architecture (e.g., AlexNet or VGG) without modification or introduction of additional parameters. We demonstrate our approach on action and activity recognition tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Representation learning from sequence data has many applications including, but not limited to, action and activity recognition from video, gesture recognition, music classification, and gene regulatory network analysis. Neural network-based supervised learning of representations from sequence data has many advantages compared to handcrafted feature engineering. However, capturing the discriminative behaviour of sequence data is a very challenging problem; especially when neural network-based supervised learning is used. In this paper we present a principled method to jointly learn discriminative dynamic representations and classification model parameters from video data using convolutional neural networks (CNNs).</p><p>In recent years CNNs have become very popular for automatically learning representations from large collections Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48. Copyright 2016 by the author(s). of static images. Many application domains, such as image classification, image segmentation and object detection, have benefited from such automatic representation learning <ref type="bibr" target="#b18">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b12">Girshick et al., 2014)</ref>. However, it is unclear how one may extend these highly successful CNNs to sequence data; especially, when the intended task requires capturing dynamics of video sequences (e.g., action and activity recognition). Indeed, capturing the discriminative dynamics of a video sequence remains an open problem. Some authors have propose to use recurrent neural networks (RNNs) <ref type="bibr" target="#b8">(Du et al., 2015)</ref> or extensions, such as long short term memory (LSTM) networks <ref type="bibr" target="#b27">(Srivastava et al., 2015)</ref>, to classify video sequences. However, CNN-RNN/LSTM models introduce a large number of additional parameters to capture sequence information. Consequently, these methods need much more training data. For sequence data such as videos, obtaining labelled data is more costly than obtaining labels for static images. This is reflected in the size of datasets used in action and activity recognition research today. Even though there are datasets that consist of millions of labelled images (e.g., ImageNet <ref type="bibr" target="#b4">(Deng et al., 2009)</ref>), the largest fully labelled action recognition dataset, UCF101, consists of barely more than 13,000 videos <ref type="bibr" target="#b26">(Soomro et al., 2012)</ref>. It is highly desirable, therefore, to develop frameworks that can learn discriminative dynamics from video data without the cost of additional training data or model complexity.</p><p>Perhaps the most straightforward CNN-based method for encoding video sequence data is to apply temporal max pooling or temporal average pooling over the video frames. However, these methods do not capture any valuable time varying information of the video sequences <ref type="bibr" target="#b16">(Karpathy et al., 2014)</ref>. In fact, an arbitrary reshuffling of the frames would produce an identical video representation under these pooling schemes. Rank-pooling <ref type="bibr" target="#b9">(Fernando et al., 2015)</ref>, on the other hand, attempts to encode time varying information by learning a linear ranking machine, one for each video, to produce a chronological ordering of the video's frames based on their appearance (i.e., the CNN features). The parameters of the ranking machine are then used as the video representation. However, unlike max and average pooling, it is unclear how the CNN parameters can be fine-tuned to give a more discriminative representation when rank-pooling is used.</p><p>In this paper, we present a novel approach for learning discriminative representations for videos using rank-pooling over a sequence CNN feature vectors (derived from the video frames). We do this by formulating an optimization problem that jointly learns the video representation and classifier parameters. A key technical challenge, however, is that the optimization problem contains the rankpooling linear ranking machine as a subproblem-itself a non-trivial optimization problem. This leads to a largescale bilevel optimization problem <ref type="bibr" target="#b0">(Bard, 1998)</ref> with convex inner-problem, which we propose to solve by stochastic gradient descent.</p><p>Moreover, because we use support vector regression to solve the inner-optimization problem and obtain our video representation, there are theoretical stability guarantees on the learned temporal representation <ref type="bibr" target="#b2">(Bousquet &amp; Elisseeff, 2002)</ref>. That is, even if the input sequence is perturbed the output of the temporal encoding layer produces stable video representations leading to a robust model in contrast to CNN-RNN/LSTM models, which can be very sensitive to changes in the input.</p><p>Our contributions are two-fold: First, we present an elegant method for encoding temporal information in video sequences from frame-based CNN features. Second, we show that the video representation and classifier parameters can be learned jointly in an end-to-end fashion using a bilevel optimization formulation of the problem. We demonstrate the effectiveness of our method on two challenging video classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Temporal information encoding of video sequences using neural networks is an active field of research in both the machine learning and computer vision communities. Recently, several methods have been proposed to tackle the problem of video sequence encoding using neural network architectures <ref type="bibr" target="#b14">(Ji et al., 2013;</ref><ref type="bibr" target="#b7">Donahue et al., 2015;</ref><ref type="bibr" target="#b27">Srivastava et al., 2015;</ref><ref type="bibr" target="#b31">Yue-Hei Ng et al., 2015;</ref><ref type="bibr" target="#b32">Zha et al., 2015;</ref><ref type="bibr" target="#b25">Simonyan &amp; Zisserman, 2014)</ref>. Some authors (e.g., <ref type="bibr" target="#b14">(Ji et al., 2013)</ref> and <ref type="bibr" target="#b28">(Tran et al., 2014))</ref> propose to use 3D convolutions to incorporate spatial and motion information. However, it is not clear if temporal information of videos can be processed in a similar manner to the spatial information of images. Therefore the use of 3D-convolutions to capture motion may not be the ideal solution. Moreover, these 3D filers are applied over very short video clips capturing only local motion information. Con-sequently, they are not able to capture long-range dynamics of complex activities. Most of the CNN-based methods use fixed-length short video clips to learn video representations ignoring long-range dynamics (e.g., <ref type="bibr" target="#b14">(Ji et al., 2013;</ref><ref type="bibr" target="#b28">Tran et al., 2014;</ref><ref type="bibr" target="#b25">Simonyan &amp; Zisserman, 2014)</ref>). This is not ideal as it is essential use all available temporal information to learn good video representations, especially in tasks such as activity recognition.</p><p>LSTM-based methods have also been proposed to learn video representations. For example, unsupervised video representation learning method is presented in <ref type="bibr" target="#b27">Srivastava et al. (2015)</ref>. In that work temporal information is learned by encoding a video using an LSTM model and by decoding the encoded vector to reconstruct the video in the reverse order. However, it is unclear how to adapt such a strategy to encode discriminative dynamics of a video. At the same time the LSTM model is trained on extracted feature activations from CNNs. However, the LSTM and CNN parameters are not trained jointly, leading to suboptimal parameter settings. The LRCN method proposed by <ref type="bibr" target="#b7">Donahue et al. (2015)</ref> has several nice properties such as the ability to train an end-to-end model and handle variable length sequences. However, for inference this method takes the average prediction over all frames, which can destroy valuable dynamical information found by the LSTM component of the LRCN model. In comparison to these methods, our model is trained in a principled end-to-end fashion using a single convolutional neural network. It captures the time varying information using rank pooling <ref type="bibr" target="#b9">(Fernando et al., 2015;</ref><ref type="bibr" target="#b11">2016b)</ref> and has the ability to handle variable length sequences. Importantly, our method captures video-wide temporal information and does not require the ad hoc assembly of disjoint components typical of other video classification frameworks. A schematic illustration of our model is shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Our learning algorithm introduces a bilevel optimization method for encoding temporal dynamics of video sequences using convolutional neural networks. Bilevel optimization <ref type="bibr" target="#b0">(Bard, 1998</ref>) is a large and active research field derived from the study of non-cooperative games with much work focusing on efficient techniques for solving non-smooth problems <ref type="bibr" target="#b22">(Ochs et al., 2015)</ref> or studying replacement of the lower level problem with necessary conditions for optimality <ref type="bibr" target="#b3">(Dempe &amp; Franke, 2015)</ref>. It has recently gained interest in the machine learning community in the context of hyperparameter learning <ref type="bibr" target="#b17">(Klatzer &amp; Pock, 2015;</ref><ref type="bibr" target="#b5">Do et al., 2007)</ref> and in the computer vision community in the context of image denoising <ref type="bibr" target="#b6">(Domke, 2012;</ref><ref type="bibr" target="#b19">Kunisch &amp; Pock, 2013)</ref>. Unlike these works we take a gradient-based approach, which the structure of our problem admits. We also address the problem of encoding and classification of temporal sequences, in particular action and activity recognition in video.</p><p>Two recent rank-pooling methods with learning have also been proposed <ref type="bibr" target="#b10">(Fernando et al., 2016a;</ref><ref type="bibr" target="#b1">Bilen et al., 2016)</ref>. <ref type="bibr" target="#b10">Fernando et al. (2016a)</ref> propose a discriminative hierarchical rank pooling method on extracted CNN features. However, this method does not learn an end-to-end network. In contrast, <ref type="bibr" target="#b1">Bilen et al. (2016)</ref> does learn an end-to-end network, but the rank-pooling operator is simplified for efficiency and only applied to input RGB data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Sequence Classification</head><p>In this section we describe our method for sequence classification and the associated end-to-end parameter learning problem. We start by formalising our sequence representation and prediction pipeline. We then present our main contribution-jointly learning the representation of the elements in the sequence and the parameters of the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Representation</head><p>We consider the problem of classifying a sequence x = x t | t = 1, . . . , T by assigning it a label y from some discrete set of classes Y. For example, the sequence can be a video and the label can be the action occurring in the video. Here Y is the set of recognizable actions such as "running", "jumping", "skipping", etc. We assume that each element x t of the sequence is an object from some input domain X (e.g., a video frame).</p><p>Our first task is to transform the arbitrary-length sequence into a form that is amenable to classification. To this end we first map each element of the sequence into a p-dimensional feature vector via a parameterizable feature function</p><formula xml:id="formula_0">ψ θ (•), v t = ψ θ (x t ) ∈ R p .<label>(1)</label></formula><p>The feature function can be, for example, a convolutional neural network (CNN) applied to a video frame with features extracted from the final activation layers in the network. We introduce the shorthand v = v 1 , . . . , v T to denote the sequence of element feature vectors.</p><p>Next we map the sequence of element feature vectors into a single q-dimensional feature vector describing the entire sequence via a temporal encoding function φ,</p><formula xml:id="formula_1">u = φ( v) ∈ R q .<label>(2)</label></formula><p>The vector u is now a fixed-length representation of the sequence, which can be used for classification.</p><p>Typical temporal encoding functions include sufficient statistics calculations or simple pooling operations, such as max or avg. However, the temporal encoding function can be much more sophisticated, such as the recently introduced rank-pool operator <ref type="bibr" target="#b9">(Fernando et al., 2015)</ref>. Unlike max and avg pooling operators, which can be expressed in closed-form, rank-pool requires an optimization problem to be solved in order to determine the representation. Mathematically, we have</p><formula xml:id="formula_2">u ∈ argmin u f ( v, u )<label>(3)</label></formula><p>where f (•, •) is some measure of how well a sequence is described by each representation and we seek the best representation. It is this type of temporal encoding function that we are interested in this paper.</p><p>Note also, that the optimization problem is very general and can include constraints on the solution in addition to just minimizing the objective f . Moreover, many standard pooling operations can be formulated in this way. For example, avg pooling can be written (somewhat offensively) as</p><formula xml:id="formula_3">avg( v) = argmin u 1 2 T t=1 u − v t 2 .<label>(4)</label></formula><p>Importantly, the rank-pool operator encodes temporal dynamics of the sequence, which max and avg pooling operators do not. Specifically, the rank-pool operator attempts to capture the order of elements in the sequence by finding a vector u such that u</p><formula xml:id="formula_4">T v a &lt; u T v b for all a &lt; b, i.e., the function v → u T v</formula><p>honors the relative order of the elements in the sequence. This is achieved by regressing the element feature vector onto its index in the sequence and solved using regularized support vector regression (SVR) to give a point-wise ranking function <ref type="bibr" target="#b21">(Liu, 2009)</ref>. Concretely, we define rank-pool( v) as</p><formula xml:id="formula_5">argmin u 1 u 2 + C T t=1 |t − u T v t | − 2 ≥0 (5)</formula><p>where [•] ≥0 = max{•, 0} projects onto the positive reals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prediction</head><p>With a fixed-length sequence descriptor u ∈ R q in hand the prediction task is to map u to one of the discrete class labels. Let h β be a predictor parameterized by β. We can summarize our classification pipeline of an arbitrary-length sequence x to a label y ∈ Y as:</p><formula xml:id="formula_6">x = x t ψ θ −→ v t φ −→ u h β −→ y<label>(6)</label></formula><p>Typical predictors include (linear) support vector machines (SVM) and soft-max classifiers. For the latter-which we use in this work-the probability of a label y given the sequence x can be written as  Here h β (u) represents the (discrete) probability distribution over all labels and β = {β y } are the learned parameters of the model.</p><formula xml:id="formula_7">P (y | x) = exp(β T y u) y exp(β T y u) .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning</head><p>Given a dataset of sequence-label pairs, {(</p><formula xml:id="formula_8">x (i) , y (i) )} n i=1</formula><p>, our goal is to learn both the parameters of the classifier and representation of the elements in the sequence. Let ∆(•, •) be a loss function. For example, when using the soft-max classifier a typical choice would be the cross-entropy loss</p><formula xml:id="formula_9">∆(y, h β (u)) = − log P (y | x).<label>(8)</label></formula><p>We jointly estimate the parameters of the feature function and prediction function by minimizing the regularized empirical risk. Our learning problem is</p><formula xml:id="formula_10">minimize θ,β n i=1 ∆ y (i) , h β (u (i) ) + R(θ, β) subject to u (i) ∈ argmin u f ( v (i) , u)<label>(9)</label></formula><p>where R(•, •) is some regularization function, typically the 2 -norm of the parameters, and θ also appears in the definition of the v (i) by Eq. 1.</p><p>Eq. 9 is an instance of a bilevel optimization problem, which have recently been explored in the context of support vector machine (SVM) hyper-parameter learning <ref type="bibr" target="#b17">(Klatzer &amp; Pock, 2015)</ref>. Here an upper level problem is solved subject to constraints enforced by a lower level problem. A number of solution methods have been proposed for bilevel optimization problems. Given our interest in learning video representations from powerful CNN features, gradient-based techniques are most appropriate in allowing the fine-tuning of the CNN parameters.</p><p>When the temporal encoding function φ can be evaluated in closed-form (e.g., max or avg) we can substitute the constraints in Eq. 9 directly into the objective and use (sub-)gradient descent to solve for (locally or globally) optimal parameters.</p><p>Fortunately, when the lower level objective is twice differentiable we can compute the gradient of the argmin function as other authors have also observed <ref type="bibr" target="#b22">(Ochs et al., 2015;</ref><ref type="bibr" target="#b6">Domke, 2012;</ref><ref type="bibr" target="#b5">Do et al., 2007)</ref> and the following lemmas show. It is then simply a matter of applying the chain rule to obtain the derivative of the loss function with respect to any parameter in the model. We begin by considering the scalar case and then extend to the vector case. Lemma 1. : Let f : R × R → R be a continuous function with first and second derivatives. Let g(x) =</p><formula xml:id="formula_11">argmin y f (x, y). Then dg(x) dx = − f XY (x, g(x)) f Y Y (x, g(x))</formula><p>where f XY . = ∂ 2 f ∂x∂y and f Y Y</p><formula xml:id="formula_12">. = ∂ 2 f ∂y 2 .</formula><p>Proof. Differentiating f with respect to y we have</p><formula xml:id="formula_13">∂f (x, y) ∂y y=g(x) = 0 (since g(x) = argmin y f (x, y)) (10) ∴ d dx ∂f (x, g(x)) ∂y = 0</formula><p>(differentiating lhs and rhs)</p><formula xml:id="formula_14">(11) But d dx ∂f (x, g(x)) ∂y = ∂ 2 f (x, g(x)) ∂x∂y + ∂ 2 f (x, g(x)) ∂y 2 dg(x) dx<label>(12)</label></formula><p>Equating to zero and rearranging gives the desired result</p><formula xml:id="formula_15">dg(x) dx = − ∂ 2 f (x, g(x)) ∂y 2 −1 ∂ 2 f (x, g(x)) ∂x∂y (13) = − f XY (x, g(x)) f Y Y (x, g(x))<label>(14)</label></formula><p>Lemma 2. : Let f : R × R n → R be a continuous function with first and second derivatives. Let g(x) = argmin y∈R n f (x, y). Then</p><formula xml:id="formula_16">g (x) = −f Y Y (x, g(x)) −1 f XY (x, g(x)).</formula><p>where</p><formula xml:id="formula_17">f Y Y . = ∇ 2 yy f (x, y) ∈ R n×n and f XY . = ∂ ∂x ∇ y f (x, y) ∈ R n .</formula><p>Proof. Similar to Lemma 1, we have:</p><formula xml:id="formula_18">f Y (x, g(x)) . = ∇ Y f (x, y)| y=g(x) = 0 (15) d dx f Y (x, g(x)) = 0 (16) ∴ f XY (x, g(x)) + f Y Y (x, g(x))g (x) = 0 (17) d dx g(x) = −f Y Y (x, g(x)) −1 f XY (x, g(x))<label>(18)</label></formula><p>Interestingly, replacing argmin with argmax yields the same gradient, which follows from the proof above that only requires that g(x) be a stationary point.</p><p>Consider again the learning problem defined in Eq. 9. Using the result of Lemma 2 we can compute du <ref type="bibr">(i)</ref> dθ for each training example and hence the gradient of the objective via the chain rule. <ref type="bibr">1</ref> We then use stochastic gradient descent (SGD) to learn all parameters jointly.</p><p>Gradient of the rank-pool function: For completeness let us now derive the gradient of the rank-pool function. Assume a scalar parameter θ for the element feature function ψ (the extension to a vector of parameters can be derived elementwise). Let</p><formula xml:id="formula_19">f (θ, u) = 1 u 2 + C T t=1 |t − u T v t | − 2 ≥0<label>(19)</label></formula><formula xml:id="formula_20">where v t = ψ θ (x t ). Let e t . =      u T v t − t + , if t − u T v t ≥ u T v t − t − , if u T v t − t ≥ 0, otherwise.<label>(20)</label></formula><p>Then by Lemma 2 we have</p><formula xml:id="formula_21">d dθ argmin u f (θ, u) =   I + C et =0 v t v T t   −1   C et =0 e t ψ θ (x t ) − u T ψ θ (x t )v t   (21)</formula><p>where with slight abuse of notation the u on the right-hand side is the optimal u. Here ψ θ (x t ) is the derivative of the element feature function. In the context of CNN-based features for encoding video frames the derivative can be computed by back-propagation through the network.</p><p>Note that the rank-pool objective function is convex but includes a zero-measure set of non-differentiable points. However, this does not cause any practical problems during optimization in our experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Optimization difficulties</head><p>One of the main difficulties for learning the parameters of high-dimensional temporal encoding functions (such as those based on CNN features) is that the gradient update in Eq. 21 requires the inversion of the hessian f Y Y . One solution is to use a diagonal approximation of the hessian, which is trivial to invert. Fortunately, for temporal encoding functions with certain structure like ours, namely where the hessian can be expressed as a diagonal plus the sum of rank-one matrices, the inverse can be computed efficiently using the Sherman-Morrison formula <ref type="bibr" target="#b13">(Golub &amp; Loan, 1996)</ref>,</p><p>The derivative with respect to β, which only appears in the upper-level problem, is straightforward. <ref type="table">Table 1</ref>. Classification accuracies for action recognition on the ten-class UCF-sports dataset <ref type="bibr" target="#b24">(Rodriguez et al., 2008</ref> </p><formula xml:id="formula_22">Let H = I + n i=1 u i v T i ∈ R p×p be in- vertible. Define H 0 = I and H m = H m−1 + u m v T m for m = 1, . . . , n. Then H −1 m = H −1 m−1 − H −1 m−1 u m v T m H −1 m−1 + v T m H −1 m−1 u m (22) whenever v T m H −1 m−1 u m = −1.</formula><p>Proof. Follows from repeated application of the Sherman-Morrison formula.</p><p>Since each update in Eq. 22 can be performed in O(p 2 ) the inverse of H can be computed in O(np 2 ), which is acceptable for many applications. In the Section 4 we present experimental results and discuss the overall running time of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on action and activity recognition tasks in video using two real-world datasets, and compare our approach against some strong baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and tasks</head><p>First, we use UCF-sports dataset <ref type="bibr" target="#b24">(Rodriguez et al., 2008)</ref> for the task of action classification. The dataset consists of a set of short video clips depicting actions collected from various sports. The clips were typically sourced from footage on broadcast television channels such as the BBC and ESPN. The video collection represents a natural pool of actions featured in a wide range of scenes and viewpoints. The dataset includes a total of 150 sequences of resolution 720 × 480 pixels. Video clips are grouped into ten action categories as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Classification performance is measured using mean per-class accuracy. We use provided train-test splits for training and testing. Second, we use the Hollywood2 dataset <ref type="bibr" target="#b20">(Laptev et al., 2008)</ref> for the task of activity recognition. The dataset has been constructed from 69 different Hollywood movies and includes 12 activity classes. It has 1,707 videos in total with a pre-defined split of 823 training videos and 884 test videos. Training and test videos are selected from different movies. The dataset represents a very challenging video collection. The length of the video clips varies from hundreds to several thousand frames. As is standard on this dataset, performance is measured by mean average precision (mAP) over all classes. Different activity classes are shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baseline methods and implementation details</head><p>We compare our end-to-end training of the rank-pooling network against the following baseline methods.</p><p>avg pooling + svm: We extract FC7 feature activations from the pre-trained Caffe reference model <ref type="bibr" target="#b15">(Jia et al., 2014)</ref> using MatConvNet <ref type="bibr" target="#b29">(Vedaldi &amp; Lenc, 2015)</ref> for each frame of the video. Then we apply temporal average pooling to obtain a fixed-length feature vector per video (4096 dimensional). Afterwards, we use a linear SVM classifier (Lib-SVM) to train and test action and activity categories.</p><p>max pooling + svm: Similar to the above baseline, we extract FC7 feature activations for each frame of the video and then apply temporal max pooling to obtain a fixedlength feature vector per video. Again we use a linear SVM classifier to predict action and activity categories.</p><p>rank pooling + svm: We extract FC7 feature activations for each frame of the video. We then apply time varying mean vectors to smooth the signal as recommended by <ref type="bibr" target="#b9">(Fernando et al., 2015)</ref>, and L2-normalize all frame features. Next, we apply the rank-pooling operator to obtain a video representation using publicly available code <ref type="bibr" target="#b9">(Fernando et al., 2015)</ref>. We use a linear SVM classifier applied <ref type="table">Table 2</ref>. Classification performance in average precision for activity recognition on the Hollywood2 dataset <ref type="bibr" target="#b20">(Laptev et al., 2008</ref> on the L2-normalized representation to classify each video.</p><p>frame-level fine-tuning: We fine-tune the Caffe reference model on the frame data considering each frame as an instance from the respective action category. Then we sum the classifier scores from each frame belonging to a video to obtain the final prediction.</p><p>frame-level fine-tuning + rank-pooling: We use the pretrained model as before and fine-tune the Caffe reference model on the frame data considering each frame as an instance from the respective action category. Afterwards, we extract FC7 features from each video (frames). Then we encode temporal information of fine-tuned FC7 video data using rank-pooling. Afterwards, we use soft-max classifier to classify videos.</p><p>end-to-end baselines: We also compare our method with end-to-end trained max and average pooling variants. Here the pre-trained CNN parameters were fine-tuned using the classification loss.</p><p>state-of-the-art: Last, we benchmark our approach against combined state-of-the-art improved trajectory <ref type="bibr" target="#b30">(Wang &amp; Schmid, 2013)</ref> features <ref type="bibr">(MBH, HOG, HOG)</ref> and Fisher vectors <ref type="bibr" target="#b23">(Perronnin et al., 2010)</ref> with rank-pooling for temporal encoding <ref type="bibr" target="#b9">(Fernando et al., 2015)</ref>.</p><p>The first five baselines can all be viewed as variants of the CNN-base temporal pooling architecture of <ref type="figure" target="#fig_1">Figure 1</ref>. The differences being the pooling operation and whether endto-end training is applied. The last baseline represents a state-of-the-art video classification pipeline.</p><p>We compare the baseline methods against our rank-pooled CNN-based temporal architecture where training is done end-to-end. We do not sub-sample videos to generate fixedlength clips as typically done in the literature (e.g., <ref type="bibr" target="#b25">(Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b28">Tran et al., 2014)</ref>). Instead, we consider the entire video during training as well as testing. We use stochastic gradient descent method without batch updates (i.e., each batch consists of a single video).</p><p>We initialize the network with the Caffe reference model and use a variable learning rate starting from 0.01 down to 0.0001 over 60 epochs. We also use a weight decay of 0.0005 on an L2-regularizer over the model parameters. We explore two variants of the learning algorithm.</p><p>In the first variant we use the diagonal approximation to the rank-pool gradient during the back-propagation. In the second variant we use the full gradient update, which requires computing the inverse of matrices per video (see Section 3). For the UCF-sports dataset we use the crossentropy loss for all CNN-based methods (including the baselines). Whereas for the Hollywood2 dataset, where performance is measured by mAP, we use the hinge-loss (as is common practice for this dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental results</head><p>Results for experiments on the UCF-sports dataset are reported in <ref type="table">Table 1</ref>. Let us make several observations. First, the performance of max, average and rank-pooling are similar when CNN activation features are used without end-toend learning. Perhaps increasing the capacity of the model to better capture video dynamics (say, using a non-linear SVM) may improve results but that is beyond the scope of our study in this paper. Second, end-to-end training helps all three pooling methods. However, the improvement obtained by end-to-end training of rank-pooling is about 21%, significantly higher than the other two pooling approaches. Moreover, the performance using the diagonal approximation is the same as when full gradient is used. This suggests that the diagnoal approximation is driving the parameters in a desireable direction and may be sufficient for a stochastic gradient-based method. Last, and perhaps most interesting, is that using state-of-the-art improved trajectory <ref type="bibr" target="#b30">(Wang &amp; Schmid, 2013)</ref> features (MBH, HOG, HOG) and Fisher vectors <ref type="bibr" target="#b23">(Perronnin et al., 2010)</ref> with rank-pooling <ref type="bibr" target="#b9">(Fernando et al., 2015)</ref> obtains 87.2% on this dataset. This result is comparable with the results obtained with our method using end-to-end feature learning. Note, however, that the dimensionality of the feature vectors for the state-of-the-art method are extremely high (over 50,000 dimensional) compared to our 4,096-dimensional feature representation.</p><p>We now evaluate activity recognition performance on the Hollywood2 dataset. Results are reported in <ref type="table">Table 2</ref> as average precision performance for each class and we take the mean average precision (mAP) to compare methods. As before, for this task, the best results are obtained by end-to-end training using rank-pooling for temporal encoding. The improvement over non-end-to-end rank pooling is 9.6 mAP. One may ask whether this performance could be achieved without end-to-end training but just fine-tuning the frame-level features. To answer this question we ran two additional baselines. In the first we simply fine-tuned the CNN parameters to classify each video frame with the ground-truth activity and average the frame-level predictions at test time. In the second we apply rank-pooling to the fine-tuned frame features. On the test set we get 34.1 mAP and 36.3 mAP, respectively. Thus we observe gradual improvement from frame-level fine-tuning to fine-tuning with rank-pooling to end-to-end training (40.6 mAP).</p><p>For this dataset, one can obtain much higher accuracy using the state-of-the-art improved trajectory features (MBH, HOG, HOG) and Fisher vectors with rank-pooling <ref type="bibr" target="#b9">(Fernando et al., 2015)</ref>. Here <ref type="bibr" target="#b9">Fernando et al. (2015)</ref> used several kinds of data augmentations (forward reverse rank pooling and mirrored videos data) to get to 70.0 mAP after combining all features. Individually, HOG, HOF, MBH,and TRJ features obtains 45.3, 59.8, 60.5, and 49.8 mAP, respectively. We improve CNN feature performance from 31.0 (vanilla rank pooling) to 40.6 mAP using end-toend training, and note that here our objective is not to obtain state-of-the art but to show that rank-pooling operator of <ref type="bibr" target="#b9">(Fernando et al., 2015)</ref> can be improved in the context of CNN-based video classification.</p><p>That said, our end-to-end trained CNN features can be combined with HOG+HOF+MBH features to boost performance. Here, without any data augmentation, we obtain 73.4 mAP whereas combining the vanilla CNN features combined with HOG+HOF+MBH we only get 71.4 mAP. These results indicate that our end-to-end training is useful even when combined with hand-crafted HOG, HOF and MBH features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Diagonal approximation vs. full gradient</head><p>As we have seen, optimization using the diagonal approximation of the gradient obtains results that are on par with full gradient. Using the full gradient optimization is ten times slower than the approximate method, resulting in pro-cessing videos at 5 frames per second versus 50 frames per second (for the approximate method) during training on a Titan-X GPU. Even with the diagonal approximation endto-end training is currently prohibitively slow on very large video collections such as the UCF101 dataset. Here we estimate training to take over 340 hours for 60 epochs over the 1M frames in the dataset. However, we are hopeful the next-generation GPUs, promised to be ten times faster than the Titan-X, will make our method tractable on very large video collections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose an effective, clean, and principled temporal encoding method for convolutional neural network-based video sequence classification task. Our temporal pooling layer can sit above any CNN architecture and through a bilevel optimization formulation admits end-to-end learning of all model parameters. We demonstrated that this endto-end learning significantly improves performance over a traditional rank-pooling approach by 21% on the UCFsports dataset and 9.6 mAP on the Hollywood2 dataset.</p><p>We believe that the framework proposed in this paper will open the way for embedding other traditional optimization methods as subroutines inside CNN architectures. Our work also suggests a number of interesting future research directions. First, it would be interesting to explore more expressive variants of rank-pooling such as through kernalization. Second, our framework could be adapted to other sequence classification tasks (e.g., speech recognition) and we conjecture that as for video classification there may be accuracy gains for these other tasks too. Last, our ability to update model parameters at 50 frames per second suggests that an agent, provided with appropriate supervision, could learn to recognize activities in real-time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>The CNN network architecture used in this paper for learning end-to-end temporal representations of videos. Network takes a sequence of frames from a video as inputs and feed forward till the end of the temporal pooling layer. At the temporal polling layer, the sequence of vectors are encoded by rank-pooling operator to produce fixed length video representation. This fixed length vector is feed to the next layer in the network. Note that we do not introduce any new parameters to network architectures such as AlexNet or Caffe reference model. During back-propagation, the gradients are feed backwards through the rank-pooling operator to the rest of the CNN network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Example frames from UCF-sports (top) and Holly-wood2 (bottom) dataset from different action and activity classes.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the Australian Research Council (ARC) through the Centre of Excellence for Robotic Vision (CE140100016).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Practical Bilevel Optimization: Algorithms and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">F</forename><surname>Bard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efstratios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stability and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="499" to="526" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the solution of convex bilevel Learning End-to-end Video Classification with Rank-Pooling optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient multiple hyperparameter learning for log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuong</forename><forename type="middle">B</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuan-Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generic methods for optimization-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subhashini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efstratios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative hierarchical rank pooling for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rank pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Basura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efstratios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagannath</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Matrix Computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Loan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Johns Hopkins University Press</publisher>
		</imprint>
	</monogr>
	<note>3 edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shuiwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanketh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Continuous hyperparameter learning for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Klatzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Winter Workshop</title>
		<imprint>
			<publisher>CVWW</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A bilevel optimization approach for parameter learning in variational models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Kunisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="938" to="983" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilevel optimization with nonsmooth lower level problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Scale Space and Variational Methods in Computer Vision (SSVM)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with compressed fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action mach a spatio-temporal maximum average correlation height filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><forename type="middle">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04681</idno>
		<title level="m">Unsupervised learning of video representations using lstms</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Int. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Int. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sudheendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploiting image-trained CNN architectures for unconstrained video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shengxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
