<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Swapout: Learning an ensemble of deep architectures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
							<email>dhoiem@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Swapout: Learning an ensemble of deep architectures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a rich set of architectures including dropout [17], stochastic depth [6] and residual architectures [4, 5] as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper describes swapout, a stochastic training method for general deep networks. Swapout is a generalization of dropout <ref type="bibr" target="#b16">[17]</ref> and stochastic depth <ref type="bibr" target="#b5">[6]</ref> methods. Dropout zeros the output of individual units at random during training, while stochastic depth skips entire layers at random during training. In comparison, the most general swapout network produces the value of each output unit independently by reporting the sum of a randomly selected subset of current and all previous layer outputs for that unit. As a result, while some units in a layer may act like normal feedforward units, others may produce skip connections and yet others may produce a sum of several earlier outputs. In effect, our method averages over a very large set of architectures that includes all architectures used by dropout and all used by stochastic depth.</p><p>Our experimental work focuses on a version of swapout which is a natural generalization of the residual network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. We show that this results in improvements in accuracy over residual networks with the same number of layers. Improvements in accuracy are often sought by increasing the depth, leading to serious practical difficulties. The number of parameters rises sharply, although recent works such as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref> have addressed this by reducing the filter size <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref>. Another issue resulting from increased depth is the difficulty of training longer chains of dependent variables. Such difficulties have been addressed by architectural innovations that introduce shorter paths from input to loss either directly <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4]</ref> or with additional losses applied to intermediate layers <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b9">10]</ref>. At the time of writing, the deepest networks that have been successfully trained are residual networks (1001 layers <ref type="bibr" target="#b4">[5]</ref>). We show that increasing the depth of our swapout networks increases their accuracy.</p><p>There is compelling experimental evidence that these very large depths are helpful, though this may be because architectural innovations introduced to make networks trainable reduce the capacity of the layers. The theoretical evidence that a depth of 1000 is required for practical problems is thin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1605.06465v1 [cs.CV] 20 May 2016</head><p>Bengio and Dellaleau argue that circuit efficiency constraints suggest increasing depth is important, because there are functions that require exponentially large shallow networks to compute <ref type="bibr" target="#b0">[1]</ref>. Less experimental interest has been displayed in the width of the networks (the number of filters in a convolutional layer). We show that increasing the width of our swapout networks leads to significant improvements in their accuracy; an appropriately wide swapout network is competitive with a deep residual network that is 1.5 orders of magnitude deeper and has more parameters.</p><p>Contributions: Swapout is a novel stochastic training scheme that can sample from a rich set of architectures including dropout, stochastic depth and residual architectures as special cases. Swapout improves the performance of the residual networks for a model of the same depth. Wider but much shallower swapout networks are competitive with very deep residual networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Convolutional neural networks have a long history (see the introduction of <ref type="bibr" target="#b8">[9]</ref>). They are now intensively studied as a result of recent successes (e.g. <ref type="bibr" target="#b7">[8]</ref>). Increasing the number of layers in a network improves performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref> if the network can be trained. A variety of significant architectural innovations improve trainability, including: the ReLU <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2]</ref>; batch normalization <ref type="bibr" target="#b6">[7]</ref>; and allowing signals to skip layers.</p><p>Our method exploits this skipping process. Highway networks use gated skip connections to allow information and gradients to pass unimpeded across several layers <ref type="bibr" target="#b17">[18]</ref>. Residual networks use identity skip connections to further improve training <ref type="bibr" target="#b3">[4]</ref>; extremely deep residual networks can be trained, and perform well <ref type="bibr" target="#b4">[5]</ref>. In contrast to these architectures, our method skips at the unit level (below), and does so randomly.</p><p>Our method employs randomness at training time. For a review of the history of random methods, see the introduction of <ref type="bibr" target="#b13">[14]</ref>, which shows that entirely randomly chosen features can produce an SVM that generalizes well. Randomly dropping out unit values (dropout <ref type="bibr" target="#b16">[17]</ref>) discourages coadaptation between units. Randomly skipping layers (stochastic depth) <ref type="bibr" target="#b5">[6]</ref> during training reliably leads to improvements at test time, likely because doing so regularizes the network. The precise details of the regularization remain uncertain, but it appears that stochastic depth represents a form of tying between layers; when a layer is dropped, other layers are encouraged to be able to replace it. Each method can be seen as training a network that averages over a family of architectures during inference. Dropout averages over architectures with "missing" units and stochastic depth averages over architectures with "missing" layers. Other successful recent randomized methods include dropconnect <ref type="bibr" target="#b19">[20]</ref> which generalizes dropout by dropping individual connections instead of units (so dropping several connections together), and stochastic pooling <ref type="bibr" target="#b20">[21]</ref> (which regularizes by replacing the deterministic pooling by randomized pooling). In contrast, our method skips layers randomly at a unit level enjoying the benefits of each method.</p><p>Recent results show that (a) stochastic gradient descent with sufficiently few steps is stable (in the sense that changes to training data do not unreasonably disrupt predictions) and (b) dropout enhances that property, by reducing the value of a Lipschitz constant ( <ref type="bibr" target="#b2">[3]</ref>, Lemma 4.4). We show our method enjoys the same behavior as dropout in this framework.</p><p>Like dropout, the network trained with swapout depends on random variables. A reasonable strategy at test time with such a network is to evaluate multiple instances (with different samples used for the random variables) and average. Reliable improvements in accuracy are achievable by training distinct models (which have distinct sets of parameters), then averaging predictions <ref type="bibr" target="#b18">[19]</ref>, thereby forming an explicit ensemble. In contrast, each of the instances of our network in an average would draw from the same set of parameters (we call this an implicit ensemble). Srivastava et al. argue that, at test time, random values in a dropout network should be replaced with expectations, rather than taking an average over multiple instances <ref type="bibr" target="#b16">[17]</ref> (though they use explicit ensembles, increasing the computational cost). Considerations include runtime at test; the number of samples required; variance; and experimental accuracy results. For our model, accurate values of these expectations are not available. In Section 4, we show that (a) swapout networks that use estimates of these expectations outperform strong comparable baselines and (b) in turn, these are outperformed by swapout networks that use an implicit ensemble.</p><formula xml:id="formula_0">X + F (X) 0 F (X) X Swapout Y = ⇥1 X + ⇥2 F (X) (e) SkipForward Y = ⇥ X + (1 ⇥) F (X) (d) ResNet Y = X + F (X) (c) X (a) F (X) (b) FeedForward</formula><p>Input Output <ref type="figure">Figure 1</ref>: Visualization of architectural differences, showing computations for a block using various architectures. Each circle is a unit in a grid corresponding to spatial layout, and circles are colored to indicate what they report. Given input X (a), all units in a feed forward block emit F (X) (b). All units in a residual network block emit X + F (X) (c). A skipforward network randomly chooses between reporting X and F (X) per unit (d). Finally, swapout randomly chooses between reporting (and so dropping out the unit), X (skipping the unit), F (X) (imitating a feedforward network at the unit) and X + F (X) (imitating a residual network unit).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Swapout</head><p>Notation and terminology: We use capital letters to represent tensors and to represent elementwise product (broadcasted for scalars). We use boldface 0 and 1 to represent tensors of 0 and respectively. A network block is a set of simple layers in some specific configuration e.g. a convolution followed by a ReLU or a residual network block <ref type="bibr" target="#b3">[4]</ref>. Several such potentially different blocks can be connected in the form of a directed acyclic graph to form the full network model. Dropout kills individual units randomly; stochastic depth skips entire blocks of units randomly. Swapout allows individual units to be dropped, or to skip blocks randomly. Implementing swapout is a straightforward generalization of dropout. Let X be the input to some network block that computes F (X). The u'th unit produces F (u) (X) as output. Let Θ be a tensor of i.i.d. Bernoulli random variables. Dropout computes the output Y of that block as Y = Θ F (X).</p><p>(1)</p><p>It is natural to think of dropout as randomly selecting an output from the set F (u) = {0, F (u) (X)} for the u'th unit.</p><p>Swapout generalizes dropout by expanding the choice of F <ref type="bibr">(u)</ref> . Now write {Θ i } for N distinct tensors of iid Bernoulli random variables indexed by i and with corresponding parameters {θ i }. Let {F i } be corresponding tensors consisting of values already computed somewhere in the network. Note that one of these F i can be X itself (identity). However, F i are not restricted to being a function of X and we drop the X to indicate this. Most natural choices for F i are the outputs of earlier layers. Swapout computes the output of the layer in question by computing</p><formula xml:id="formula_1">Y = N i=1 Θ i F i<label>(2)</label></formula><p>and so, for unit u, we have</p><formula xml:id="formula_2">F (u) = {F (u) 1 , F (u) 2 , . . . , F (u) 1 + F (u) 2 , . . . , i F (u) i }.</formula><p>We study the simplest case where</p><formula xml:id="formula_3">Y = Θ 1 X + Θ 2 F (X)<label>(3)</label></formula><p>so that, for unit u, we have</p><formula xml:id="formula_4">F (u) = {0, X (u) , F (u) (X), X (u) + F (u) (X)}.</formula><p>Thus, each unit in the layer could be:</p><formula xml:id="formula_5">1) dropped (choose 0);</formula><p>2) a feedforward unit (choose F (u) (X));</p><p>3) skipped (choose X (u) ); 4) or a residual network unit (choose X (u) + F (u) (X)).</p><p>Since a swapout network can clearly imitate a residual network, and since residual networks are currently the best-performing networks on various standard benchmarks, we perform exhaustive experimental comparisons with them.</p><p>If one accepts the view of dropout and stochastic depth as averaging over a set of architectures, then swapout extends the set of architectures used. Appropriate random choices of Θ 1 and Θ 2 yield: all architectures covered by dropout; all architectures covered by stochastic depth; and block level skip connections. But other choices yield unit level skip and residual connections.</p><p>Swapout retains important properties of dropout. Swapout discourages co-adaptation by dropping units, but also by on occasion presenting units with inputs that have come from earlier layers. Dropout has been shown to enhance the stability of stochastic gradient descent ( <ref type="bibr" target="#b2">[3]</ref>, lemma 4.4). This applies to swapout in its most general form, too. We extend the notation of that paper, and write L for a Lipschitz constant that applies to the network, ∇f (v) for the gradient of the network f with parameters v, and D∇f (v) for the gradient of the dropped out version of the network.</p><p>The crucial point in the relevant enabling lemma is that E[|| Df </p><formula xml:id="formula_6">(v) ||] &lt; E[|| ∇f (v</formula><formula xml:id="formula_7">v) ||] &lt; E[|| ∇G [f ] (v) ||] ≤ L, improving stability ([3] Section 4).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inference in Stochastic Networks</head><p>A model trained with swapout represents an entire family of networks with tied parameters, where members of the family were sampled randomly during training. There are two options for inference. We could either replace random variables with their expected values, as recommended by Srivastava et al. <ref type="bibr" target="#b16">[17]</ref> (deterministic inference). Alternatively, we could sample several members of the family at random, and average their predictions (stochastic inference).</p><p>There is an important difference between swapout and dropout. In a dropout network, one can estimate expectations exactly (as long as the network isn't trained with batch normalization, below  <ref type="figure">(Figure 11</ref> in <ref type="bibr" target="#b16">[17]</ref>). Our experience of stochastic inference with swapout has been positive, with the number of samples needed for good behavior small <ref type="figure">(Figure 2</ref>). Furthermore, computational costs of inference are smaller when each instance of the network uses the same parameters  <ref type="bibr" target="#b5">[6]</ref> reports that dropout doesn't lead to any improvement when used in residual networks with batch normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline comparison methods</head><p>ResNets:</p><p>We compare with ResNet architectures as described in <ref type="bibr" target="#b3">[4]</ref>(referred to as v1) and in <ref type="bibr" target="#b4">[5]</ref>(referred to as v2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dropout:</head><p>We use standard dropout (replace equation 3 with equation 1).</p><p>Layer Dropout: We replace equation 3 by Y = X + Θ (1×1) F (X). Here Θ (1×1) is a single Bernoulli random variable shared across all units.</p><p>SkipForward: Equation 3 introduces two stochastic parameters Θ 1 and Θ 2 . We also explore and compare with a simpler architecture, SkipForward, that introduces only one parameter but samples from a smaller set F (u) = {X (u) , F (u) (X)} as below.</p><formula xml:id="formula_8">Y = Θ X + (1 − Θ) F (X)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We experiment extensively on the CIFAR-10 dataset and demonstrate that a model trained with swapout outperforms a comparable ResNet model. Further, a 32 layer wider model matches the performance of a 1001 layer ResNet on both CIFAR-10 and CIFAR-100 datasets.</p><p>Model: We experiment with ResNet architectures as described in <ref type="bibr" target="#b3">[4]</ref>(referred to as v1) and in <ref type="bibr" target="#b4">[5]</ref>(referred to as v2). However, our implementation (referred to as ResNet Ours) has the following modifications which improve the performance of the original model <ref type="table" target="#tab_3">(Table 1)</ref>. Between blocks of different feature sizes we subsample using average pooling instead of strided convolutions and use projection shortcuts with learned parameters. For final prediction we follow a scheme similar to Network in Network <ref type="bibr" target="#b10">[11]</ref>. We replace average pooling and fully connected layer by a 1x1 convolution layer followed by global average pooling to predict the logits that are fed into the softmax.</p><p>Layers in ResNets are arranged in three groups with all convolutional layers in a group containing equal number of filters. We represent the number of filters in each group as a tuple with the smallest size as (16, 32, 64) (as used in <ref type="bibr" target="#b3">[4]</ref>for CIFAR-10). We refer to this as width and experiment with various multiples of this base size represented as W × 1, W × 2 etc.</p><p>Training: We train using SGD with a batch size of 128, momentum of 0.9 and weight decay of 0.0001. Unless otherwise specified, we train all the models for a total 256 epochs. Starting from an initial learning rate of 0.1, we drop it by a factor of 10 after 196 epochs and then again after 224 epochs. We do the standard augmentation of left-right flips and random translations of up to four pixels. For translation, we pad the images by 4 pixels on all the sides and sample a random 32x32 crop. All the images in a mini-batch use the same crop. Note that dropout slows convergence ( <ref type="bibr" target="#b16">[17]</ref>, A.4), and swapout should do so too for similar reasons. Thus using the same training schedule for all the methods should disadvantage swapout.</p><p>Models trained with Swapout consistently outperform baselines: <ref type="table" target="#tab_3">Table 1</ref> compares Swapout with various 20 layer baselines. Models trained with Swapout consistently outperform all other models of similar architecture.</p><p>The stochastic training schedule matters: Different layers in a swapout network could be trained with different parameters of their Bernoulli distributions (the stochastic training schedule). <ref type="table">Table 2</ref> shows that different stochastic training schedules have a significant affect on the performance. We report the performance with deterministic as well as stochastic inference. These schedules differ in how the values of parameters θ 1 and θ 2 of the Bernoulli random variables in equation 3 are set for the different layers. Note that θ 1 = θ 2 = 0.5 corresponds to the maximum stochasticity. A schedule with less randomness in the early layers (bottom row) performs the best. This is expected because Swapout adds per unit noise and early layers have the largest number of units. Thus, low stochasticity in early layers significantly reduces the randomness in the system. We use this schedule for all the experiments unless otherwise stated. In comparison with fair baselines on CIFAR-10, swapout is always more accurate. We refer to the base width of (16, 32, 64) as W × 1 and others are multiples of it (See <ref type="table" target="#tab_5">Table 3</ref> for details on width). We report the width along with the number of parameters in each model. Models trained with swapout consistently outperform all other models of comparable architecture. All stochastic methods were trained using the Linear(1, 0.5) schedule <ref type="table">(Table 2</ref>). v1 and v2 represent residual block architectures in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref> respectively. W × 2 1.09M 5.68 <ref type="table">Table 2</ref>: The choice of stochastic training schedule matters. We evaluate the performance of a 20 layer swapout model (W × 2) trained with different stochasticity schedules on CIFAR-10. These schedules differ in how the parameters θ 1 and θ 2 of the Bernoulli random variables in equation 3 are set for the different layers. Linear(a, b) refers to linear interpolation from a to b from the first block to the last (see <ref type="bibr" target="#b5">[6]</ref>). Others use the same value for all the blocks. We report the performance for both the deterministic and stochastic inference (with 30 samples). Schedule with less randomness in the early layers (bottom row) performs the best. Swapout improves over ResNet architecture: From <ref type="table" target="#tab_5">Table 3</ref> it is evident that networks trained with Swapout consistently show better performance than corresponding ResNets, for most choices of width investigated, using just the deterministic inference. This difference indicates that the performance improvement is not just an ensemble effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Stochastic inference outperforms deterministic inference: <ref type="table" target="#tab_5">Table 3</ref> shows that the stochastic inference scheme outperforms the deterministic scheme in all the experiments. Prediction for each image is done by averaging the results of 30 stochastic forward passes. This difference is not just due to the widely reported effect that an ensemble of networks is better as networks in our ensemble share parameters. Instead, stochastic inference produces more accurate expectations and interacts better with batch normalization.</p><p>Stochastic inference needs few samples for a good estimate: <ref type="figure">Figure 2</ref> shows the estimated accuracies as a function of the number of forward passes per image. It is evident that relatively few samples are enough for a good estimate of the mean. Compare <ref type="figure">Figure-11</ref> of <ref type="bibr" target="#b16">[17]</ref>, which implies ∼ 50 samples are required.</p><p>Increase in width leads to considerable performance improvements: The number of filters in a convolutional layer is its width. <ref type="table" target="#tab_5">Table 3</ref> shows that the performance of a 20 layer model improves considerably as the width is increased both for the baseline ResNet v2 architecture as well as  the models trained with Swapout. Swapout is better able to use the available capacity than the corresponding ResNet with similar architecture and number of parameters. <ref type="table" target="#tab_6">Table 4</ref> compares models trained with Swapout with other approaches on CIFAR-10 while Experiments on CIFAR-100 confirm our results: <ref type="table" target="#tab_7">Table 5</ref> shows that Swapout is very effective as it improves the performance of a 20 layer model (ResNet Ours) by more than 2%. Widening the network and reducing the stochasticity leads to further improvements. Further, a wider but relatively shallow model trained with Swapout (22.72%; 7.46M params) is competitive with the best performing, very deep (1001 layer) latest ResNet model (22.71%;10.2M params).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion and future work</head><p>Swapout is a stochastic training method that shows reliable improvements in performance and leads to networks that use parameters efficiently. Relatively shallow swapout networks give comparable performance to extremely deep residual networks.</p><p>We have shown that different stochastic training schedules produce different behaviors, but have not searched for the best schedule in any systematic way. It may be possible to obtain improvements by doing so. We have described an extremely general swapout mechanism. It is straightforward using equation 2 to apply swapout to inception networks <ref type="bibr" target="#b18">[19]</ref> (by using several different functions of the input and a sufficiently general form of convolution); to recurrent convolutional networks <ref type="bibr" target="#b12">[13]</ref> (by  <ref type="figure">Figure 2</ref>: Stochastic inference needs few samples for a good estimate. We plot the mean error rate on the left as a function of the number of samples for two stochastic training schedules. Standard error of the mean is shown as the shaded interval on the left and magnified in the right plot. It is evident that relatively few samples are needed for a reliable estimate of the mean error. The mean and standard error was computed using 30 repetitions for each sample count.</p><p>choosing F i to have the form F • F • F . . .); and to gated networks. All our experiments focus on comparisons to residual networks because these are the current top performers on CIFAR-10 and CIFAR-100. It would be interesting to experiment with other versions of the method.</p><p>As with dropout and batch normalization, it is difficult to give a crisp explanation of why swapout works. We believe that our results support the idea that swapout causes some form of improvement in the optimization process. This is because relatively shallow networks with swapout reliably work as well as or better than quite deep alternatives; and because swapout is notably and reliably more efficient in its use of parameters than comparable deeper networks. Unlike dropout, swapout will often propagate gradients while still forcing units not to co-adapt. Furthermore, our swapout networks involve some form of tying between layers. When a unit sometimes sees layer i and sometimes layer i − j, the gradient signal will be exploited to encourage the two layers to behave similarly. The reason swapout is successful likely involves both of these points.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 = θ 2 =</head><label>12</label><figDesc>Linear(0.5, 1)) 7.34 6.52 Swapout (θ 1 = θ 2 = Linear(1, 0.5)) 6.43 5.68</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>A technically more delicate point is that both dropout and swapout networks interact poorly with batch normalization if one uses deterministic inference. The problem is that the estimates collected by batch normalization during training may not reflect test time statistics. To see this consider two random variables X and Y and let Θ 1 , Θ</figDesc><table /><note>2 ∼ Bernoulli(θ). While E[Θ 1 X + Θ 2 Y ] = E[θX + θY ] = θX + θY , it can be shown that Var[Θ 1 X + Θ 2 Y ] ≥ Var[θX + θY ] with equality holding only for θ = 0 and θ = 1. Thus, the variance estimates collected by Batch Normalization during training do not represent the statistics observed during testing if the expected values of Θ 1 and Θ 2 are used in a deterministic inference scheme. These errors in scale estimation accumulate as more and more layers are stacked. This may explain why</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Wider swapout models work better. We evaluate the effect of increasing the number of filters on CIFAR-10. ResNets<ref type="bibr" target="#b3">[4]</ref> contain three groups of layers with all convolutional layers in a group containing equal number of filters. We indicate the number of filters in each group as a tuple below and report the performance with deterministic as well as stochastic inference with 30 samples. For each size, model trained with Swapout outperforms the corresponding ResNet model.</figDesc><table><row><cell>Model</cell><cell>Width</cell><cell cols="2">#Params ResNet v2</cell><cell>Swapout</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Deterministic Stochastic</cell></row><row><cell cols="3">0.27M 1.09M Swapout v2 (20) W × 4 (64, 128, 256) 4.33M Swapout v2 (20) W × 1 (16, 32, 64) Swapout v2 (20) W × 2 (32, 64, 128) Swapout v2 (32) W × 4 (64, 128, 256) 7.43M</cell><cell>8.27 6.54 5.62 5.23</cell><cell>8.58 6.40 5.43 4.97</cell><cell>7.92 5.68 5.09 4.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">: Swapout outperforms comparable methods on CIFAR-10. Note that a 32 layer wider model</cell></row><row><cell cols="3">performs competitively in comparison to a 1001 layer ResNet model.</cell></row><row><cell>Method</cell><cell cols="2">#Params Error(%)</cell></row><row><cell>DropConnect [20]</cell><cell>-</cell><cell>9.32</cell></row><row><cell>NIN [11]</cell><cell>-</cell><cell>8.81</cell></row><row><cell>FitNet(19) [15]</cell><cell>-</cell><cell>8.39</cell></row><row><cell>DSN [10]</cell><cell>-</cell><cell>7.97</cell></row><row><cell>Highway[18]</cell><cell>-</cell><cell>7.60</cell></row><row><cell>ResNet v1(110) [4]</cell><cell>1.7M</cell><cell>6.41</cell></row><row><cell cols="2">Stochastic Depth v1(1202) [6] 19.4M</cell><cell>4.91</cell></row><row><cell>SwapOut v1(20) W × 2 ResNet v2 (1001) [5]</cell><cell>1.09M 10.2M</cell><cell>6.58 4.92</cell></row><row><cell>SwapOut v2(32) W × 4</cell><cell>7.43M</cell><cell>4.76</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>compares on CIFAR-100. On both datasets our shallower but wider model compares well with 1001 layer ResNet model. Swapout uses parameters efficiently: Persistently over tables 1, 3, and 4, Swapout models with fewer parameters outperform other comparable models. For example, Swapout v2(32) W × 4 gets 4.76% error with 7.43M parameters in comparison to the ResNet version at 4.91% with 10.2M parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Swapout is strongly competitive with the best methods on CIFAR-100, and uses parameters efficiently in comparison. A 20 layer model (Swapout v2 (20)) trained with Swapout improves upon the corresponding 20 layer ResNet model (ResNet v2 Ours (20)). Further, a 32 layer wider but much shallower model performs competitively in comparison to a 1001 layer ResNet model (last row).</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell>Error(%)</cell><cell></cell></row><row><cell></cell><cell>NIN [11]</cell><cell></cell><cell></cell><cell>-</cell><cell>35.68</cell></row><row><cell></cell><cell>DSN [10]</cell><cell></cell><cell></cell><cell>-</cell><cell>34.57</cell></row><row><cell></cell><cell>FitNet [15]</cell><cell></cell><cell></cell><cell>-</cell><cell>35.04</cell></row><row><cell></cell><cell cols="2">Highway [18]</cell><cell></cell><cell>-</cell><cell>32.39</cell></row><row><cell></cell><cell cols="2">ResNet v1 (110) [4]</cell><cell></cell><cell>1.7M</cell><cell>27.22</cell></row><row><cell></cell><cell cols="3">Stochastic Depth v1 (110) [6]</cell><cell>1.7M</cell><cell>24.58</cell></row><row><cell></cell><cell cols="2">ResNet v2 (164) [5]</cell><cell></cell><cell>1.7M</cell><cell>24.33</cell></row><row><cell></cell><cell cols="2">ResNet v2 (1001) [5]</cell><cell></cell><cell>10.2M</cell><cell>22.71</cell></row><row><cell></cell><cell cols="4">1.09M SwapOut v2 (20)(Linear(1,0.5)) W × 2 1.10M ResNet v2 Ours (20) W × 2 SwapOut v2 (56)(Linear(1,0.5)) W × 2 3.43M SwapOut v2 (56)(Linear(1,0.8)) W × 2 3.43M SwapOut v2 (32)(Linear(1,0.8)) W × 4 7.46M</cell><cell>28.08 25.86 24.86 23.46 22.72</cell></row><row><cell></cell><cell>9</cell><cell></cell><cell></cell><cell cols="2">θ1 = θ2 = Linear(1, 0.5)</cell></row><row><cell>Mean error rate →</cell><cell>7 8</cell><cell></cell><cell>0.1 0.15 Standard error →</cell><cell cols="2">θ1 = θ2 = 0.5</cell></row><row><cell></cell><cell>6</cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>10</cell><cell>20</cell><cell>30</cell></row><row><cell></cell><cell cols="2">Number of samples →</cell><cell></cell><cell cols="2">Number of samples →</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work is supported in part by ONR MURI Awards N00014-10-1-0934 and N00014-16-1-2007. We would like to thank NVIDIA for donating some of the GPUs used in this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the expressive power of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Algorithmic Learning Theory</title>
		<meeting>the 22nd International Conference on Algorithmic Learning Theory</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Train faster, generalize better: Stability of stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<idno>abs/1509.01240</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1603.09382</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>AISTATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Recurrent convolutional neural networks for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.2795</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Fitnets: Hints for thin deep nets. ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Going deeper with convolutions. CoRR, abs/1409.4842</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3557</idno>
		<title level="m">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
