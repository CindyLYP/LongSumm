<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantifying Generalization in Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<addrLine>San Francisco</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<addrLine>San Francisco</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<addrLine>San Francisco</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehoon</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<addrLine>San Francisco</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">OpenAI</orgName>
								<address>
									<addrLine>San Francisco</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quantifying Generalization in Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent&apos;s ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generalizing between tasks remains difficult for state of the art deep reinforcement learning (RL) algorithms. Although trained agents can solve complex tasks, they struggle to transfer their experience to new environments. Agents that have mastered ten levels in a video game often fail catastrophically when first encountering the eleventh. Humans can seamlessly generalize across such similar tasks, but this ability is largely absent in RL agents. In short, agents become overly specialized to the environments encountered during training.</p><p>That RL agents are prone to overfitting is widely appreciated, yet the most common RL benchmarks still encourage training and evaluating on the same set of environments. We believe there is a need for more metrics that evaluate generalization by explicitly separating training and test environments. In the same spirit as the Sonic Benchmark <ref type="bibr" target="#b15">(Nichol et al., 2018)</ref>, we seek to better quantify an agent's ability to generalize.</p><p>To begin, we train agents on CoinRun, a procedurally generated environment of our own design, and we report the surprising extent to which overfitting occurs. Using this environment, we investigate how several key algorithmic and architectural decisions impact the generalization performance of trained agents.</p><p>The main contributions of this work are as follows:</p><p>1. We show that the number of training environments required for good generalization is much larger than the number used by prior work on transfer in RL.</p><p>2. We propose a generalization metric using the CoinRun environment, and we show how this metric provides a useful signal upon which to iterate.</p><p>3. We evaluate the impact of different convolutional architectures and forms of regularization, finding that these choices can significantly improve generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is most directly inspired by the Sonic Benchmark <ref type="bibr" target="#b15">(Nichol et al., 2018)</ref>, which proposes to measure generalization performance by training and testing RL agents on distinct sets of levels in the Sonic the Hedgehog TM video game franchise. Agents may train arbitrarily long on the training set, but are permitted only 1 million timesteps at test time to perform fine-tuning. This benchmark was designed to address the problems inherent to "training on the test set." <ref type="bibr" target="#b5">(Farebrother et al., 2018)</ref> also address this problem, accurately recognizing that conflating train and test environments has contributed to the lack of regularization in deep RL. They propose using different game modes of Atari 2600 games to measure generalization. They turn to supervised learning for inspiration, finding that both L2 regularization and dropout can help agents learn more generalizable features. <ref type="bibr" target="#b16">(Packer et al., 2018)</ref> propose a different benchmark to measure generalization using six classic environments, each of arXiv:1812.02341v3 <ref type="bibr">[cs.</ref>LG] 14 Jul 2019 which has been modified to expose several internal parameters. By training and testing on environments with different parameter ranges, their benchmark quantifies agents' ability to interpolate and extrapolate. <ref type="bibr" target="#b19">(Zhang et al., 2018a)</ref> measure overfitting in continuous domains, finding that generalization improves as the number of training seeds increases. They also use randomized rewards to determine the extent of undesirable memorization.</p><p>Other works create distinct train and test environments using procedural generation. <ref type="bibr" target="#b9">(Justesen et al., 2018)</ref> use the General Video Game AI (GVG-AI) framework to generate levels from several unique games. By varying difficulty settings between train and test levels, they find that RL agents regularly overfit to a particular training distribution. They further show that the ability to generalize to human-designed levels strongly depends on the level generators used during training. <ref type="bibr" target="#b20">(Zhang et al., 2018b)</ref> conduct experiments on procedurally generated gridworld mazes, reporting many insightful conclusions on the nature of overfitting in RL agents. They find that agents have a high capacity to memorize specific levels in a given training set, and that techniques intended to mitigate overfitting in RL, including sticky actions  and random starts <ref type="bibr" target="#b6">(Hausknecht and Stone, 2015)</ref>, often fail to do so.</p><p>In Section 5.4, we similarly investigate how injecting stochasticity impacts generalization. Our work mirrors <ref type="bibr" target="#b20">(Zhang et al., 2018b)</ref> in quantifying the relationship between overfitting and the number of training environments, though we additionally show how several methods, including some more prevalent in supervised learning, can reduce overfitting in our benchmark.</p><p>These works, as well as our own, highlight the growing need for experimental protocols that directly address generalization in RL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Quantifying Generalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The CoinRun Environment</head><p>We propose the CoinRun environment to evaluate the generalization performance of trained agents. The goal of each CoinRun level is simple: collect the single coin that lies at the end of the level. The agent controls a character that spawns on the far left, and the coin spawns on the far right. Several obstacles, both stationary and non-stationary, lie between the agent and the coin. A collision with an obstacle results in the agent's immediate death. The only reward in the environment is obtained by collecting the coin, and this reward is a fixed positive constant. The level terminates when the agent dies, the coin is collected, or after 1000 time steps.</p><p>We designed the game CoinRun to be tractable for existing algorithms. That is, given a sufficient number of training levels and sufficient training time, our algorithms learn a near optimal policy for all CoinRun levels. Each level is generated deterministically from a given seed, providing agents access to an arbitrarily large and easily quantifiable supply of training data. CoinRun mimics the style of platformer games like Sonic, but it is much simpler. For the purpose of evaluating generalization, this simplicity can be highly advantageous.</p><p>Levels vary widely in difficulty, so the distribution of levels naturally forms a curriculum for the agent. Two different levels are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. See Appendix A for more details about the environment and Appendix B for additional screenshots. Videos of a trained agent playing can be found here, and environment code can be found here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CoinRun Generalization Curves</head><p>Using the CoinRun environment, we can measure how successfully agents generalize from a given set of training lev-   els to an unseen set of test levels. Train and test levels are drawn from the same distribution, so the gap between train and test performance determines the extent of overfitting. As the number of available training levels grows, we expect the performance on the test set to improve, even when agents are trained for a fixed number of timesteps. At test time, we measure the zero-shot performance of each agent on the test set, applying no fine-tuning to the agent's parameters.</p><p>We train 9 agents to play CoinRun, each on a training set with a different number of levels. During training, each new episode uniformly samples a level from the appropriate set. The first 8 agents are trained on sets ranging from of 100 to 16,000 levels. We train the final agent on an unbounded set of levels, where each level is seeded randomly. With 2 32 level seeds, collisions are unlikely. Although this agent encounters approximately 2M unique levels during training, it still does not encounter any test levels until test time. We repeat this whole experiment 5 times, regenerating the training sets each time.</p><p>We first train agents with policies using the same 3-layer convolutional architecture proposed by <ref type="bibr" target="#b14">(Mnih et al., 2015</ref>), which we henceforth call Nature-CNN. Agents are trained with Proximal Policy Optimization  for a total of 256M timesteps across 8 workers. We train agents for the same number of timesteps independent of the number of levels in the training set. We average gradients across all 8 workers on each mini-batch. We use γ = .999, as an optimal agent takes between 50 and 500 timesteps to solve a level, depending on level difficulty. See Appendix D for a full list of hyperparameters.</p><p>Results are shown in <ref type="figure" target="#fig_2">Figure 2a</ref>. We collect each data point by averaging the final agent's performance across 10,000 episodes, where each episode samples a level from the appropriate set. We can see that substantial overfitting occurs when there are less than 4,000 training levels. Even with 16,000 training levels, overfitting is still noticeable. Agents perform best when trained on an unbounded set of levels, when a new level is encountered in every episode. See Appendix E for performance details. Now that we have generalization curves for the baseline architecture, we can evaluate the impact of various algorithmic and architectural decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluating Architectures</head><p>We choose to compare the convolutional architecture used in IMPALA <ref type="bibr" target="#b4">(Espeholt et al., 2018)</ref> against our Nature-CNN baseline. With the IMPALA-CNN, we perform the same experiments described in Section 3.2, with results shown in <ref type="figure" target="#fig_2">Figure 2b</ref>. We can see that across all training sets, the IMPALA-CNN agents perform better at test time than 0.0 0.5 1.0 1.5 2.0 2.5  Nature-CNN agents.</p><p>To evaluate generalization performance, one could train agents on the unbounded level set and directly compare learning curves. In this setting, it is impossible for an agent to overfit to any subset of levels. Since every level is new, the agent is evaluated on its ability to continually generalize. For this reason, performance with an unbounded training set can serve as a reasonable proxy for the more explicit train-to-test generalization performance. <ref type="figure" target="#fig_4">Figure 3a</ref> shows a comparison between training curves for IMPALA-CNN and Nature-CNN, with an unbounded set of training levels. As we can see, the IMPALA-CNN architecture is substantially more sample efficient.</p><p>However, it is important to note that learning faster with an unbounded training set will not always correlate positively with better generalization performance. In particular, well chosen hyperparameters might lead to improved training speed, but they are less likely to lead to improved generalization. We believe that directly evaluating generalization, by training on a fixed set of levels, produces the most useful metric. <ref type="figure" target="#fig_4">Figure 3b</ref> shows the performance of different architectures when training on a fixed set of 500 levels. The same training set is used across seeds.</p><p>In both settings, it is clear that the IMPALA-CNN architecture is better at generalizing across levels of CoinRun. Given the success of the IMPALA-CNN, we experimented with several larger architectures, finding a deeper and wider variant of the IMPALA architecture (IMPALA-Large) that performs even better. This architecture uses 5 residual blocks instead of 3, with twice as many channels at each layer. Results with this architecture are shown in <ref type="figure" target="#fig_4">Figure 3</ref>.</p><p>It is likely that further architectural tuning could yield even greater generalization performance. As is common in supervised learning, we expect much larger networks to have a higher capacity for generalization. In our experiments, however, we noticed diminishing returns increasing the network size beyond IMPALA-Large, particularly as wall clock training time can dramatically increase. In any case, we leave further architectural investigation to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluating Regularization</head><p>Regularization has long played an significant role in supervised learning, where generalization is a more immediate concern. Datasets always include separate training and test sets, and there are several well established regularization techniques for reducing the generalization gap. These regularization techniques are less often employed in deep RL, presumably because they offer no perceivable benefits in the absence of a generalization gap -that is, when the training and test sets are one and the same. Now that we are directly measuring generalization in RL, we have reason to believe that regularization will once again prove effective. Taking inspiration from supervised learning, we choose to investigate the impact of L2 regularization, dropout, data augmentation, and batch normalization in the CoinRun environment.</p><p>Throughout this section we train agents on a fixed set of 500 CoinRun levels, following the same experimental procedure shown in <ref type="figure" target="#fig_4">Figure 3b</ref>. We have already seen that sub-0.0 0.5 1.0 1.5 2.0 2.5 L2 Weight (w)  stantial overfitting occurs, so we expect this setting to provide a useful signal for evaluating generalization. In all subsequent experiments, figures show the mean and standard deviation across 3-5 runs. In these experiments, we use the original IMPALA-CNN architecture with 3 residual blocks, but we notice qualitatively similar results with other architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dropout and L2 Regularization</head><p>We first train agents with either dropout probability p ∈ [0, 0.25] or with L2 penalty w ∈ [0, 2.5 × 10 −4 ]. We train agents with L2 regularization for 256M timesteps, and we train agents with dropout for 512M timesteps. We do this since agents trained with dropout take longer to converge. We report both the final train and test performance. The results of these experiments are shown in <ref type="figure" target="#fig_6">Figure 4</ref>. Both L2 regularization and dropout noticeably reduce the generalization gap, though dropout has a smaller impact. Empirically, the most effective dropout probability is p = 0.1 and the most effective L2 weight is w = 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Data Augmentation</head><p>Data augmentation is often effective at reducing overfitting on supervised learning benchmarks. There have been a wide variety of augmentation transformations proposed for images, including translations, rotations, and adjustments to brightness, contrast, or sharpness. <ref type="bibr" target="#b1">(Cubuk et al., 2018)</ref> search over a diverse space of augmentations and train a policy to output effective data augmentations for a target dataset, finding that different datasets often benefit from different sets of augmentations.</p><p>We take a simple approach in our experiments, using a slightly modified form of Cutout <ref type="bibr" target="#b2">(Devries and Taylor, 2017)</ref>. For each observation, multiple rectangular regions of varying sizes are masked, and these masked regions are assigned a random color. See Appendix C for screenshots. This method closely resembles domain randomization (Tobin et al., 2017), used in robotics to transfer from simulations to the real world. <ref type="figure" target="#fig_6">Figure 4c</ref> shows the boost this data augmentation scheme provides in CoinRun. We expect that other methods of data augmentation would prove similarly effective and that the effectiveness of any given augmentation will vary across environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Batch Normalization</head><p>Batch normalization <ref type="bibr" target="#b8">(Ioffe and Szegedy, 2015)</ref> is known to have a substantial regularizing effect in supervised learning <ref type="bibr" target="#b11">(Luo et al., 2018)</ref>. We investigate the impact of batch normalization on generalization, by augmenting the IMPALA-CNN architecture with batch normalization after every convolutional layer. Training workers normalize with the statistics of the current batch, and test workers normalize with a moving average of these statistics. We show the comparison to baseline generalization in <ref type="figure" target="#fig_6">Figure 4c</ref>. As we can see, batch normalization offers a significant performance boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Stochasticity</head><p>We now evaluate the impact of stochasticity on generalization in CoinRun. We consider two methods, one varying the environment's stochasticity and one varying the policy's stochasticity. First, we inject environmental stochasticity by following -greedy action selection: with probability at each timestep, we override the agent's preferred action with a random action. In previous work, -greedy ac-  tion selection has been used both as a means to encourage exploration and as a theoretical safeguard against overfitting <ref type="bibr" target="#b0">(Bellemare et al., 2012;</ref><ref type="bibr" target="#b13">Mnih et al., 2013)</ref>. Second, we control policy stochasticity by changing the entropy bonus in PPO. Note that our baseline agent already uses an entropy bonus of k H = .01.</p><p>We increase training time to 512M timesteps as training now proceeds more slowly. Results are shown in <ref type="figure" target="#fig_8">Figure 5</ref>. It is clear that an increase in either the environment's or the policy's stochasticity can improve generalization. Furthermore, each method in isolation offers a similar generalization boost. It is notable that training with increased stochasticity improves generalization to a greater extent than any of the previously mentioned regularization methods. In general, we expect the impact of these stochastic methods to vary substantially between environments; we would expect less of a boost in environments whose dynamics are already highly stochastic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Combining Regularization Methods</head><p>We briefly investigate the effects of combining several of the aforementioned techniques. Results are shown in <ref type="figure" target="#fig_6">Figure 4c</ref>. We find that combining data augmentation, batch normalization, and L2 regularization yields slightly better test time performance than using any one of them individually. However, the small magnitude of the effect suggests that these regularization methods are perhaps addressing similar underlying causes of poor generalization. Furthermore, for unknown reasons, we had little success combining -greedy action selection and high entropy bonuses with other forms of regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Additional Environments</head><p>The preceding sections have revealed the high degree overfitting present in one particular environment. We corroborate these results by quantifying overfitting on two additional environments: a CoinRun variant called CoinRun-Platforms and a simple maze navigation environment called RandomMazes.</p><p>We apply the same experimental procedure described in Section 3.2 to both CoinRun-Platforms and RandomMazes, to determine the extent of overfitting. We use the original IMPALA-CNN architecture followed by an LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997)</ref>, as memory is necessary for the agent to explore optimally. These experiments further reveal how susceptible our algorithms are to overfitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">CoinRun-Platforms</head><p>In CoinRun-Platforms, there are several coins that the agent attempts to collect within the 1000 step time-limit. Coins # Training <ref type="table">Levels   0  2  4  6  8  10  12  14  16</ref> Level Score</p><p>Train Test  are randomly scattered across platforms in the level. Levels are a larger than in CoinRun, so the agent must actively explore, sometimes retracing its steps. Collecting any coin gives a reward of 1, and collecting all coins in a level gives an additional reward of 9. Each level contains several moving monsters that the agent must avoid. The episode ends only when all coins are collected, when time runs out, or when the agent dies. See Appendix B for environment screenshots.</p><p>As CoinRun-Platforms is a much harder game, we train each agent for a total of 2B timesteps. <ref type="figure" target="#fig_10">Figure 7</ref> shows that overfitting occurs up to around 4000 training levels. Beyond the extent of overfitting, it is also surprising that agents' training performance increases as a function of the number of training levels, past a certain threshold. This is notably different from supervised learning, where training performance generally decreases as the training set becomes larger. We attribute this trend to the implicit curriculum in the distribution of generated levels. With additional training data, agents are more likely to learn skills that generalize even across training levels, thereby boosting the overall training performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">RandomMazes</head><p>In RandomMazes, each level consists of a randomly generated square maze with dimension uniformly sampled from 3 to 25. Mazes are generated using Kruskal's algorithm <ref type="bibr" target="#b10">(Kruskal, 1956</ref>). The environment is partially observed, with the agent observing the 9 × 9 patch of cells directly surrounding its current location. At every cell is either a wall, an empty space, the goal, or the agent. The episode ends when the agent reaches the goal or when time expires after 500 timesteps. The agent's only actions are to move to an empty adjacent square. If the agent reaches the goal, a constant reward is received. <ref type="figure" target="#fig_11">Figure 8</ref> reveals particularly strong overfitting, with a sizeable generalization gap even when training on 20,000 levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Discussion</head><p>In both CoinRun-Platforms and RandomMazes, agents must learn to leverage recurrence and memory to optimally navigate the environment. The need to memorize and recall past experience presents challenges to generalization unlike those seen in CoinRun. It is unclear how well suited LSTMs are to this task. We empirically observe that given sufficient data and training time, agents using LSTMs eventually converge to a near optimal policy. However, the relatively poor generalization performance raises the question of whether different recurrent architectures might be better suited for generalization in these environments. This investigation is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Our results provide insight into the challenges underlying generalization in RL. We have observed the surprising extent to which agents can overfit to a fixed training set. Using the procedurally generated CoinRun environment, we can precisely quantify such overfitting. With this metric, we can better evaluate key architectural and algorithmic decisions. We believe that the lessons learned from this environment will apply in more complex settings, and we hope to use this benchmark, and others like it, to iterate towards more generalizable agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Level Generation and Environment Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. CoinRun</head><p>Each CoinRun level has a difficulty setting from 1 to 3. To generate a new level, we first uniformly sample over difficulties. Several choices throughout the level generation process are conditioned on this difficulty setting, including the number of sections in the level, the length and height of each section, and the frequency of obstacles. We find that conditioning on difficulty in this way creates a distribution of levels that forms a useful curriculum for the agent. For more efficient training, one could adjust the difficulty of sampled levels based on the agent's current skill, as done in <ref type="bibr" target="#b9">(Justesen et al., 2018)</ref>. However, we chose not to do so in our experiments, for the sake of simplicity.</p><p>At each timestep, the agent receives a 64 × 64 × 3 RGB observation, centered on the agent. Given the game mechanics, an agent must know it's current velocity in order to act optimally. This requirement can be satisfied by using frame stacking or by using a recurrent model. Alternatively, we can include velocity information in each observation by painting two small squares in the upper left corner, corresponding to x and y velocity. In practice, agents can adequately learn under any of these conditions. Directly painting velocity information leads to the fastest learning, and we report results on CoinRun using this method. We noticed similar qualitative results using frame stacking or recurrent models, though with unsurprisingly diminished generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. CoinRun-Platforms</head><p>As with CoinRun, agents receive a 64 × 64 × 3 RGB observation at each timestep in CoinRun-Platforms. We don't paint any velocity information into observations in experiments with CoinRun-Platforms; this information can be encoded by the LSTM used in these experiments. Unlike CoinRun, levels in CoinRun-Platforms have no explicit difficulty setting, so all levels are drawn from the same distribution. In practice, of course, some levels will still be drastically easier than others.</p><p>It is worth emphasizing that CoinRun platforms is a much more difficult game than CoinRun. We trained for 2B timesteps in <ref type="figure" target="#fig_10">Figure 7</ref>, but even this was not enough time for training to fully converge. We found that with unrestricted training levels, training converged after approximately 6B timesteps at a mean score of 20 per level. Although this agent occasionally makes mistakes, it appears to be reasonably near optimal performance. In particular, it demonstrates a robust ability to explore the level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. RandomMazes</head><p>As with the previous environments, agents receive a 64 × 64×3 RGB observation at each timestep in RandomMazes. Given the visual simplicity of the environment, using such a larger observation space is clearly not necessary. However, we chose to do so for the sake of consistency. We did conduct experiments with smaller observation spaces, but we noticed similar levels of overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Augmentation Screenshots</head><p>Example observations augmented with our modified version of Cutout <ref type="bibr" target="#b2">(Devries and Taylor, 2017)</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyperparameters and Settings</head><p>We used the following hyperparameters and settings in our baseline experiments with the 3 environments. Notably, we forgo the LSTM in CoinRun, and we use more environments per worker in CoinRun-Platforms. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Two levels in CoinRun. The level on the left is much easier than the level on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) Final train and test performance of IMPALA-CNN agents after 256M timesteps, as a function of number of training levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Dotted lines denote final mean test performance of the agents trained with an unbounded set of levels. The solid line and shaded regions represent the mean and standard deviation respectively across 5 seeds. Training sets are generated separately for each seed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Performance of Nature-CNN and IMPALA-CNN agents during training, on an unbounded set of training levels.(b) Performance of Nature-CNN and IMPALA-CNN agents during training, on a set of 500 training levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>The lines and shaded regions represent the mean and standard deviation respectively across 3 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Final train and test performance after 256M timesteps as a function of the L2 weight penalty. Mean and standard deviation is shown across 5 runs. Final train and test performance after 512M timesteps as a function of the dropout probability. Mean and standard deviation is shown across 5 runs. effect of using data augmentation, batch normalization and L2 regularization when training on 500 levels. Mean and standard deviation is shown across 3 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>The impact of different forms of regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Final train and test performance forgreedy agents trained with different values of .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>The impact of introducing stochasticity into the environment, via epsilon-greedy action selection and an entropy bonus. Training occurs over 512M timesteps. Mean and standard deviation is shown across 3 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Levels from CoinRun-Platforms (left) and Random-Mazes (right). In RandomMazes, the agent's observation space is shaded in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Final train and test performance in CoinRun-Platforms after 2B timesteps, as a function of the number of training levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Final train and test performance in RandomMazes after 256M timesteps, as a function of the number of training levels.</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno>abs/1207.4708</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>abs/1805.09501</idno>
		<ptr target="http://arxiv.org/abs/1805.09501" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno>abs/1708.04552</idno>
		<ptr target="http://arxiv.org/abs/1708.04552" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhokhov</surname></persName>
		</author>
		<ptr target="https://github.com/openai/baselines" />
		<title level="m">Openai baselines</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1802.01561</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generalization and regularization in DQN. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Farebrother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno>abs/1810.00123</idno>
		<ptr target="http://arxiv.org/abs/1810.00123" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The impact of determinism on learning atari 2600 games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/9564" />
	</analytic>
	<monogr>
		<title level="m">Learning for General Competency in Video Games, Papers from the 2015 AAAI Workshop</title>
		<meeting><address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-01-26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Illuminating generalization in deep reinforcement learning through procedural level generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Justesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Torrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bontrager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Togelius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Risi</surname></persName>
		</author>
		<idno>abs/1806.10729</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the shortest spanning subtree of a graph and the traveling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1956" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="48" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Towards understanding regularization in batch normalization. CoRR, abs/1809.00846</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1809.00846" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Talvitie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.5699</idno>
		<ptr target="https://doi.org/10.1613/jair.5699" />
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="523" to="562" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>abs/1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gotta learn fast: A new benchmark for generalization in RL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno>abs/1804.03720</idno>
		<ptr target="http://arxiv.org/abs/1804.03720" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Assessing generalization in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno>abs/1810.12282</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Proximal policy optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno>abs/1707.06347</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno>abs/1703.06907</idno>
		<ptr target="http://arxiv.org/abs/1703.06907" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A dissection of overfitting and generalization in continuous reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno>abs/1806.07937</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A study on overfitting in deep reinforcement learning. CoRR, abs/1804.06893</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1804.06893" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
