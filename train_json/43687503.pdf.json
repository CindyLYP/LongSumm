{
  "name" : "43687503.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Training recurrent networks online without backtracking",
    "authors" : [ "Yann Ollivier", "Corentin Tallec", "Guillaume Charpiat" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The algorithm essentially maintains, at each time, a single search direction in parameter space. The evolution of this search direction is partly stochastic and is constructed in such a way to provide, at every time, an unbiased random estimate of the gradient of the loss function with respect to the parameters. Because the gradient estimate is unbiased, on average over time the parameter is updated as it should.\nThe resulting gradient estimate can then be fed to a lightweight Kalman-like filter to yield an improved algorithm. For recurrent neural networks, the resulting algorithms scale linearly with the number of parameters.\nSmall-scale experiments confirm the suitability of the approach, showing that the stochastic approximation of the gradient introduced in the algorithm is not detrimental to learning. In particular, the Kalman-like version of NoBackTrack is superior to backpropagation through time (BPTT) when the time span of dependencies in the data is longer than the truncation span for BPTT.\nConsider the problem of training the parameters \uD835\uDF03 of a dynamical system over a variable ℎ ∈ R\uD835\uDC5B subjected to the evolution equation\nℎ(\uD835\uDC61 + 1) = \uD835\uDC53(ℎ(\uD835\uDC61), \uD835\uDC65(\uD835\uDC61), \uD835\uDF03) (1)\nwhere \uD835\uDC53 is a fixed function of ℎ and of an input signal \uD835\uDC65(\uD835\uDC61), depending on parameters \uD835\uDF03. The goal is online minimization of a loss function ∑︀ \uD835\uDC61 ℓ\uD835\uDC61(\uD835\uDC66(\uD835\uDC61), \uD835\uDC66(\uD835\uDC61)) between a desired output \uD835\uDC66(\uD835\uDC61) at time \uD835\uDC61 and a prediction1\n\uD835\uDC66(\uD835\uDC61) = \uD835\uDC4C (ℎ(\uD835\uDC61), \uD835\uDF19) (2) 1The prediction \uD835\uDC66 may not live in the same set as \uD835\uDC66. Often, \uD835\uDC66 encodes a probability distribution over the possible values of \uD835\uDC66, and the loss is the logarithmic loss ℓ = − log \uD835\uDC5D\uD835\uDC66(\uD835\uDC66).\nar X\niv :1\n50 7.\n07 68\n0v 2\n[ cs\n.N E\n] 2\n0 N\nov 2\n01 5\ncomputed from ℎ(\uD835\uDC61) and additional parameters \uD835\uDF19. A typical example we have in mind is a recurrent neural network, with activities \uD835\uDC4E\uD835\uDC56(\uD835\uDC61) := sigm(ℎ\uD835\uDC56(\uD835\uDC61)) and evolution equation ℎ\uD835\uDC56(\uD835\uDC61 + 1) = \uD835\uDC4F\uD835\uDC56 + ∑︀ \uD835\uDC58 \uD835\uDC5F\uD835\uDC58\uD835\uDC56\uD835\uDC65\uD835\uDC58(\uD835\uDC61) + ∑︀ \uD835\uDC57 \uD835\uDC4A\uD835\uDC57\uD835\uDC56\uD835\uDC4E\uD835\uDC57(\uD835\uDC61), with parameter \uD835\uDF03 = (\uD835\uDC4F\uD835\uDC56, \uD835\uDC5F\uD835\uDC58\uD835\uDC56, \uD835\uDC4A\uD835\uDC57\uD835\uDC56)\uD835\uDC56,\uD835\uDC57,\uD835\uDC58.\nIf the full target sequence \uD835\uDC66(\uD835\uDC61)\uD835\uDC61∈[0;\uD835\uDC47 ] is known in advance, one strategy is to use the backpropagation through time algorithm (BPTT, see e.g. [Jae02]) to compute the gradient of the total loss \uD835\uDC3F\uD835\uDC47 := ∑︀\uD835\uDC47 \uD835\uDC61=0 ℓ\uD835\uDC61 with respect to the parameters \uD835\uDF03 and \uD835\uDF19, and use gradient descent on \uD835\uDF03 and \uD835\uDF19. However, if the data \uD835\uDC66(\uD835\uDC61 + 1) arrive one at a time in a streaming fashion, backpropagation through time would require making a full backward computation from time \uD835\uDC61 + 1 to time 0 after each new data point becomes available. This results in an Ω(\uD835\uDC612) complexity and in the necessity to store past states, inputs, and outputs. A possible strategy is to only backtrack by a finite number of time steps [Jae02] rather than going back all the way to \uD835\uDC61 = 0. But this provides biased gradient estimates and may impair detection of time dependencies with a longer range than the backtracking time range.\nBy contrast, methods which are fully online are typically not scalable. One strategy, known as real-time recurrent learning (RTRL) in the recurrent network community,2 maintains the full gradient of the current state with respect to the parameters:\n\uD835\uDC3A(\uD835\uDC61) := \uD835\uDF15ℎ(\uD835\uDC61) \uD835\uDF15\uD835\uDF03\n(3)\nwhich satisfies the evolution equation\n\uD835\uDC3A(\uD835\uDC61 + 1) = \uD835\uDF15\uD835\uDC53(ℎ(\uD835\uDC61), \uD835\uDC65(\uD835\uDC61), \uD835\uDF03) \uD835\uDF15ℎ \uD835\uDC3A(\uD835\uDC61) + \uD835\uDF15\uD835\uDC53(ℎ(\uD835\uDC61), \uD835\uDC65(\uD835\uDC61), \uD835\uDF03) \uD835\uDF15\uD835\uDF03\n(4)\n(by differentiating (1)). Knowing \uD835\uDC3A(\uD835\uDC61) allows to minimize the loss via a stochastic gradient descent on the parameters \uD835\uDF03, namely,3\n\uD835\uDF03 ← \uD835\uDF03 − \uD835\uDF02\uD835\uDC61 \uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF03\n⊤ (5)\nwith learning rate \uD835\uDF02\uD835\uDC61. Indeed, the latter quantity can be computed from \uD835\uDC3A\uD835\uDC61 and from the way the predictions depend on ℎ(\uD835\uDC61), via the chain rule\n\uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF03 = \uD835\uDF15ℓ\uD835\uDC61(\uD835\uDC4C (ℎ(\uD835\uDC61), \uD835\uDF19), \uD835\uDC66(\uD835\uDC61)) \uD835\uDF15ℎ \uD835\uDC3A(\uD835\uDC61) (6)\nHowever, the full gradient \uD835\uDC3A(\uD835\uDC61) is an object of dimension dim ℎ × dim \uD835\uDF03. This prevents computing or even storing \uD835\uDC3A(\uD835\uDC61) for moderately largedimensional dynamical systems, such as recurrent neural networks.\n2This amounts to applying forward automatic differentiation. 3We use the standard convention for Jacobian matrices, namely, \uD835\uDF15\uD835\uDC65/\uD835\uDF15\uD835\uDC66 is the matrix\nwith entries \uD835\uDF15\uD835\uDC65\uD835\uDC56/\uD835\uDF15\uD835\uDC66\uD835\uDC57 . Then the chain rule writes \uD835\uDF15\uD835\uDC65\uD835\uDF15\uD835\uDC66 \uD835\uDF15\uD835\uDC66 \uD835\uDF15\uD835\uDC67 = \uD835\uDF15\uD835\uDC65 \uD835\uDF15\uD835\uDC67 . This makes the derivatives \uD835\uDF15ℓ\uD835\uDC61/\uD835\uDF15\uD835\uDF03 into row vectors so that gradient descent is \uD835\uDF03 ← \uD835\uDF03 − (\uD835\uDF15ℓ\uD835\uDC61/\uD835\uDF15\uD835\uDF03)⊤.\nAlgorithms using a Kalman filter on \uD835\uDF03 also4 rely on this derivative \uD835\uDF15ℓ\uD835\uDC61\uD835\uDF15\uD835\uDF03 (see [Hay04, Jae02] for the case of recurrent networks). So any efficient way of estimating this derivative can be fed, in turn, to a Kalman-type algorithm.\nAlgorithms suggested to train hidden Markov models online (e.g., [Cap11], based on expectation-maximization instead of gradient descent) share the same algebraic structure and suffer from the same problem."
    }, {
      "heading" : "1 The NoBackTrack algorithm",
      "text" : ""
    }, {
      "heading" : "1.1 The rank-one trick: an expectation-preserving reduction",
      "text" : "We propose to build an approximation \uD835̃\uDC3A(\uD835\uDC61) of \uD835\uDC3A(\uD835\uDC61) with a more sustainable algorithmic cost; \uD835̃\uDC3A(\uD835\uDC61) will be random with the property E\uD835̃\uDC3A(\uD835\uDC61) = \uD835\uDC3A(\uD835\uDC61) for all \uD835\uDC61. Then the stochastic gradient (5) based on \uD835̃\uDC3A(\uD835\uDC61) will introduce noise, but no bias, on the learning of \uD835\uDF03: the average change in \uD835\uDF03 after a large number of time steps will reflect the true gradient direction. (This is true only if the noises on \uD835̃\uDC3A(\uD835\uDC61) at different times \uD835\uDC61 are sufficiently decorrelated. This is the case if the dynamical system (1) is sufficiently ergodic.) Such unbiasedness does not hold, for instance, if the gradient estimate is simply projected onto the nearest small-rank or diagonal plus small-rank approximation.5\nThe construction of an unbiased \uD835̃\uDC3A is based on the following “rank-one trick”.\nProposition 1 (Rank-one trick). Given a decomposition of a matrix \uD835\uDC34 as a sum of rank-one outer products, \uD835\uDC34 = ∑︀ \uD835\uDC56 \uD835\uDC63\uD835\uDC56\uD835\uDC64 ⊤ \uD835\uDC56 , and independent uniform random signs \uD835\uDF00\uD835\uDC56 ∈ {−1, 1}, then\n\uD835\uDC34 := ( ∑︀ \uD835\uDC56\uD835\uDF00\uD835\uDC56\uD835\uDC63\uD835\uDC56) (︁∑︀ \uD835\uDC57\uD835\uDF00\uD835\uDC57\uD835\uDC64\uD835\uDC57 )︁⊤\n(7)\nsatisfies E\uD835\uDC34 = ∑︁ \uD835\uDC56 \uD835\uDC63\uD835\uDC56\uD835\uDC64 ⊤ \uD835\uDC56 = \uD835\uDC34 (8)\nthat is, \uD835\uDC34 is an expectation-preserving rank-one approximation of \uD835\uDC34. Moreover, one can minimize the variance of \uD835\uDC34 by taking advantage of additional degrees of freedom in this decomposition, namely, one may first replace \uD835\uDC63\uD835\uDC56 and \uD835\uDC64\uD835\uDC56 with \uD835\uDF0C\uD835\uDC56\uD835\uDC63\uD835\uDC56 and \uD835\uDC64\uD835\uDC56/\uD835\uDF0C\uD835\uDC56 for any \uD835\uDF0C\uD835\uDC56 ∈ R*. The choice of \uD835\uDF0C\uD835\uDC56 which yields minimal variance of \uD835\uDC34 is when the norms of \uD835\uDC63\uD835\uDC56 and \uD835\uDC64\uD835\uDC56 become equal, namely, \uD835\uDF0C\uD835\uDC56 = √︀ ‖\uD835\uDC64\uD835\uDC56‖ / ‖\uD835\uDC63\uD835\uDC56‖.\n4One may use Kalman filtering either on \uD835\uDF03 alone or on the pair (\uD835\uDF03, ℎ). In the first case, \uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF03\nis explicitly needed. In the second case, all the information about how \uD835\uDF03 influences the current state ℎ(\uD835\uDC61) is contained in the covariance between \uD835\uDF03 and ℎ, which the algorithm must maintain, and which is as costly as maintaining \uD835\uDC3A(\uD835\uDC61) above.\n5We tried such methods first, with less satisfying results. In practice, consecutive projections tend to interact badly and reduce too much the older contributions to the gradient.\nThe proof of the first statement is immediate. The statement about minimizing variance is proven in Appendix A. Minimizing variance thanks to \uD835\uDF0C\uD835\uDC56 is quite important in practice, see Section 1.3. The rank-one trick also extends to tensors of arbitrary order; this may be useful in more complex situations.6 The rank-one reduction \uD835\uDC34 depends, not only on the value of \uD835\uDC34, but also on the way \uD835\uDC34 is decomposed as a sum of rank-one terms. In the applications to recurrent networks below, there is a natural such choice.7\nWe use this reduction operation at each step of the dynamical system, to build an approximation \uD835̃\uDC3A of \uD835\uDC3A. A key property is that the evolution equation (4) satisfied by \uD835\uDC3A is affine, so that if \uD835̃\uDC3A(\uD835\uDC61) is an unbiased estimate of \uD835\uDC3A(\uD835\uDC61), then \uD835\uDF15\uD835\uDC53(ℎ(\uD835\uDC61),\uD835\uDC65(\uD835\uDC61),\uD835\uDF03)\uD835\uDF15ℎ \uD835̃\uDC3A(\uD835\uDC61) + \uD835\uDF15\uD835\uDC53(ℎ(\uD835\uDC61),\uD835\uDC65(\uD835\uDC61),\uD835\uDF03) \uD835\uDF15\uD835\uDF03 is an unbiased estimate of \uD835\uDC3A(\uD835\uDC61 + 1).\nThis leads to the NoBackTrack algorithm (Euclidean version) described in Algorithm 1. At each step, this algorithm maintains an approximation of \uD835\uDC3A as\n\uD835̃\uDC3A = \uD835\uDC63\uD835̄\uDC64⊤+ ∑︁\n\uD835\uDC56\n\uD835\uDC52\uD835\uDC56\uD835\uDC64 ⊤ \uD835\uDC56 (9)\nwhere \uD835\uDC52\uD835\uDC56 is the \uD835\uDC56-th basis vector in space ℎ, and \uD835\uDC64\uD835\uDC56 := \uD835\uDF15\uD835\uDC53\uD835\uDC56\uD835\uDF15\uD835\uDF03 ⊤ are sparse vectors.\nTo understand this structure, say that \uD835̃\uDC3A(\uD835\uDC61 − 1) = \uD835\uDC63\uD835̄\uDC64⊤ is a rank-one unbiased approximation of \uD835\uDC3A(\uD835\uDC61− 1). Then the evolution equation (4) for \uD835\uDC3A yields (︁ \uD835\uDF15\uD835\uDC53 \uD835\uDF15ℎ )︁ (︁ \uD835\uDC63\uD835̄\uDC64⊤ )︁ + \uD835\uDF15\uD835\uDC53\uD835\uDF15\uD835\uDF03 = (︁ \uD835\uDF15\uD835\uDC53 \uD835\uDF15ℎ\uD835\uDC63 )︁ \uD835̄\uDC64⊤+ ∑︀ \uD835\uDC56 \uD835\uDC52\uD835\uDC56 \uD835\uDF15\uD835\uDC53\uD835\uDC56 \uD835\uDF15\uD835\uDF03 as an approximation of \uD835̃\uDC3A(\uD835\uDC61). This new approximation is not rank-one any more, but it can be used to perform a gradient step on \uD835\uDF03, and then reduced to a rank-one approximation before the next time step.\nNote that handling \uD835\uDF15\uD835\uDC53\uD835\uDC56\uD835\uDF15\uD835\uDF03 is usually cheap: in many situations, only a small subset of the parameter \uD835\uDF03 directly influences each component ℎ\uD835\uDC56(\uD835\uDC61 + 1) given ℎ(\uD835\uDC61), so that for each component \uD835\uDC56 of the state space, \uD835\uDF15\uD835\uDC53\uD835\uDC56\uD835\uDF15\uD835\uDF03 has few non-zero components. For instance, for a recurrent neural network with activities\n6The most symmetric way to do this is to use complex roots of unity, for instance, ∑︀\n\uD835\uDC56 \uD835\uDC62\uD835\uDC56⊗ \uD835\uDC63\uD835\uDC56 ⊗ \uD835\uDC64\uD835\uDC56 = ERe (︁ ( ∑︀\n\uD835\uDC56 \uD835\uDF01\uD835\uDC56\uD835\uDC62\uD835\uDC56)( ∑︀ \uD835\uDC57 \uD835\uDF01\uD835\uDC57\uD835\uDC63\uD835\uDC57)( ∑︀ \uD835\uDC58 \uD835\uDF01\uD835\uDC58\uD835\uDC64\uD835\uDC58) )︁ where each \uD835\uDF01\uD835\uDC56 is taken independently at\nrandom among {1, e±2\uD835\uDC56\uD835\uDF0B/3}. This involves complex numbers but there is no need to complexify the original dynamical system (1). Another, complex-free possibility is to apply the rank-one trick recursively to tensors of smaller order, for instance, ∑︀ \uD835\uDC56 \uD835\uDC62\uD835\uDC56⊗\uD835\uDC63\uD835\uDC56⊗\uD835\uDC64\uD835\uDC56⊗\uD835\uDC65\uD835\uDC56 =∑︀\n\uD835\uDC56 (\uD835\uDC62\uD835\uDC56 ⊗ \uD835\uDC63\uD835\uDC56)⊗ (\uD835\uDC64\uD835\uDC56 ⊗ \uD835\uDC65\uD835\uDC56) = E\n[︁ ( ∑︀\n\uD835\uDC56 \uD835\uDF00\uD835\uDC56\uD835\uDC62\uD835\uDC56 ⊗ \uD835\uDC63\uD835\uDC56)( ∑︀ \uD835\uDC57 \uD835\uDF00\uD835\uDC57\uD835\uDC64\uD835\uDC57 ⊗ \uD835\uDC65\uD835\uDC57) ]︁\nand then apply independent rank-one decompositions in turn to ∑︀ \uD835\uDC56 \uD835\uDF00\uD835\uDC56\uD835\uDC62\uD835\uDC56 ⊗ \uD835\uDC63\uD835\uDC56 and to ∑︀ \uD835\uDC57\n\uD835\uDF00\uD835\uDC57\uD835\uDC64\uD835\uDC57 ⊗ \uD835\uDC65\uD835\uDC57 . 7The rank-one trick may also be performed using random Gaussian vectors, namely \uD835\uDC34 = E[\uD835\uDF09(\uD835\uDF09⊤Σ−1\uD835\uDC34)] with \uD835\uDF09 = \uD835\uDCA9 (0, Σ). This version does not depend on a chosen decomposition of \uD835\uDC34, but depends on a choice of Σ. Variance can be much larger in this case: for instance, if \uD835\uDC34 = \uD835\uDC63\uD835\uDC64⊤ is actually rank-one, then (\uD835\uDF00\uD835\uDC63)(\uD835\uDF00\uD835\uDC64⊤) = \uD835\uDC63\uD835\uDC64⊤ so that the rank-one trick with random signs is exact, whereas the Gaussian version yields (\uD835\uDF09\uD835\uDF09⊤Σ−1)\uD835\uDC63\uD835\uDC64⊤ which is correct only in expectation. This case is particularly relevant because we are going to apply a reduction at each time step, thus working on objects that stay close to rank-one. The generalization to tensors is also more cumbersome in the Gaussian case.\n\uD835\uDC4E\uD835\uDC56(\uD835\uDC61) := sigm(ℎ\uD835\uDC56(\uD835\uDC61)) and evolution equation ℎ\uD835\uDC56(\uD835\uDC61 + 1) = \uD835\uDC4F\uD835\uDC56 + ∑︀\n\uD835\uDC58 \uD835\uDC5F\uD835\uDC56\uD835\uDC58\uD835\uDC65\uD835\uDC58(\uD835\uDC61) +∑︀ \uD835\uDC57 \uD835\uDC4A\uD835\uDC57\uD835\uDC56\uD835\uDC4E\uD835\uDC57(\uD835\uDC61), the derivative of ℎ\uD835\uDC56(\uD835\uDC61 + 1) with respect to the parameter \uD835\uDF03 = (\uD835\uDC4F, \uD835\uDC5F, \uD835\uDC4A ) only involves the parameters \uD835\uDC4F\uD835\uDC56, \uD835\uDC5F\uD835\uDC56\uD835\uDC58, \uD835\uDC4A\uD835\uDC57\uD835\uDC56 of unit \uD835\uDC56. In such situations, the total cost of computing and storing all the \uD835\uDC64\uD835\uDC56’s is of the same order as the cost of computing ℎ(\uD835\uDC61 + 1) itself. See Section 1.3 for details on this example.\nAfter the reduction step of Algorithm 1, \uD835̄\uDC64 may be interpreted as a “search direction” in parameter space \uD835\uDF03, while \uD835\uDC63 is an estimate of the effect on the current state ℎ(\uD835\uDC61) of changing \uD835\uDF03 in the direction \uD835̄\uDC64. The search direction \uD835̄\uDC64 evolves stochastically, but not fully at random, over time, so that on average \uD835\uDC63\uD835̄\uDC64⊤ is a fair estimate of the actual influence of the parameter \uD835\uDF03.\nNote that in Algorithm 1, the non-recurrent output parameters \uD835\uDF19 are trained according to their exact gradient. The rank-one trick is used only for the recurrent part of the system.\nBy construction, at each step of Algorithm 1, the quantity \uD835̃\uDC3A\uD835\uDC61 := \uD835\uDC63\uD835̄\uDC64⊤+∑︀ \uD835\uDC56 \uD835\uDC52\uD835\uDC56\uD835\uDC64 ⊤ \uD835\uDC56 satisfies E\uD835̃\uDC3A\uD835\uDC61 = \uD835\uDF15ℎ(\uD835\uDC61) \uD835\uDF15\uD835\uDF03 . However, since the value of \uD835\uDF03 changes along the algorithm, we must be careful about the meaning of this statement. Intuitively, this derivative with respect to \uD835\uDF03 is taken along the actual trajectory of parameters \uD835\uDF03\uD835\uDC61 realized by the algorithm.\nMore formally, let \uD835\uDF03 = (\uD835\uDF030, . . . , \uD835\uDF03\uD835\uDC61, . . .) be any sequence of parameters. Let \uD835\uDC53 be any function depending on this sequence \uD835\uDF03, such as the state of the system at time \uD835\uDC61 (all functions considered below will depend only on a finite initial segment of \uD835\uDF03). Define \uD835\uDF03 + \uD835\uDF00 := (\uD835\uDF030 + \uD835\uDF00, . . . , \uD835\uDF03\uD835\uDC61 + \uD835\uDF00, . . .) and say that \uD835\uDC53 has derivative \uD835\uDF15\uD835\uDC53\uD835\uDF15\uD835\uDF03 with respect to \uD835\uDF03 if \uD835\uDC53(\uD835\uDF03 + \uD835\uDF00) = \uD835\uDC53(\uD835\uDF03) + \uD835\uDF00 \uD835\uDF15\uD835\uDC53 \uD835\uDF15\uD835\uDF03 + \uD835\uDC42(\uD835\uDF00\n2) for small \uD835\uDF00.\nThanks to this convention, the evolution equation (4) for the evolution of \uD835\uDC3A(\uD835\uDC61) holds for any sequence of parameters \uD835\uDF03, with \uD835\uDC3A(\uD835\uDC61) defined as \uD835\uDF15ℎ(\uD835\uDC61)\uD835\uDF15\uD835\uDF03 . The following statement is then easily proved by induction.\nProposition 2 (Unbiased rank-one gradient estimate for dynamical systems). At each time step \uD835\uDC61, the quantity \uD835̃\uDC3A\uD835\uDC61 := \uD835\uDC63\uD835̄\uDC64⊤ +∑︀\n\uD835\uDC56 \uD835\uDC52\uD835\uDC56\uD835\uDC64 ⊤ \uD835\uDC56 from Algorithm 1 is an unbiased estimate of the gradient of the state\nof the system with respect to the parameter:\nE\uD835̃\uDC3A\uD835\uDC61 = \uD835\uDF15ℎ(\uD835\uDC61)\n\uD835\uDF15\uD835\uDF03 (19)\nwhere \uD835\uDF03 is the sequence of parameters produced by the algorithm.\nIn particular, for learning rates \uD835\uDF02 tending to 0, the parameter evolves slowly so that the derivative \uD835\uDF15ℎ(\uD835\uDC61)\uD835\uDF15\uD835\uDF03 is close to a derivative with respect to the current value \uD835\uDF03\uD835\uDC61 of the parameter. Thus, in this regime, \uD835\uDF15ℎ(\uD835\uDC61)\uD835\uDF15\uD835\uDF03 tends to \uD835\uDF15ℎ(\uD835\uDC61) \uD835\uDF15\uD835\uDF03\uD835\uDC61\n, and since \uD835̃\uDC3A\uD835\uDC61 is an unbiased estimate of \uD835\uDC3A\uD835\uDC61, the situation gets closer and closer to an ordinary stochastic gradient descent if \uD835\uDF02 is small. Presumably\nParameters: ℎ(0) (initial state), \uD835\uDF030, \uD835\uDF190 (initial value of the internal and output parameters), \uD835\uDF02\uD835\uDC61 (learning rate scheme); Data: \uD835\uDC65(\uD835\uDC61) (input signal), \uD835\uDC66(\uD835\uDC61) (output signal); Maintains: ℎ(\uD835\uDC61) (current state), \uD835\uDF03, \uD835\uDF19 (internal and output parameters), \uD835\uDC63 (column vector of size dim ℎ), \uD835̄\uDC64 (column vector of size dim \uD835\uDF03), \uD835\uDC64\uD835\uDC56 (sparse column vectors of size dim \uD835\uDF03) for \uD835\uDC56 = 1, . . . , dim ℎ. Initialization: \uD835\uDF03 ← \uD835\uDF030, \uD835\uDF19← \uD835\uDF190, \uD835\uDC63 ← 0, \uD835̄\uDC64 ← 0, \uD835\uDC64\uD835\uDC56 ← 0; for \uD835\uDC61 = 0 to end-of-time do\nObservation step: Compute prediction \uD835\uDC66(\uD835\uDC61) = \uD835\uDC4C (ℎ(\uD835\uDC61), \uD835\uDF19) from current state ℎ(\uD835\uDC61). Observe \uD835\uDC66(\uD835\uDC61) and incur loss ℓ\uD835\uDC61(\uD835\uDC66(\uD835\uDC61), \uD835\uDC66(\uD835\uDC61)). Update step: Compute derivative of loss with respect to output parameters, \uD835\uDF15ℓ\uD835\uDC61\uD835\uDF15\uD835\uDF19 = \uD835\uDF15ℓ\uD835\uDC61(\uD835\uDC4C (ℎ(\uD835\uDC61),\uD835\uDF19),\uD835\uDC66(\uD835\uDC61)) \uD835\uDF15\uD835\uDF19 , and update output parameters:\n\uD835\uDF19← \uD835\uDF19− \uD835\uDF02\uD835\uDC61 \uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF19\n⊤ (10)\nCompute derivative of loss with respect to current state,\n\uD835\uDC3B ← \uD835\uDF15ℓ\uD835\uDC61 (\uD835\uDC4C (ℎ(\uD835\uDC61), \uD835\uDF19), \uD835\uDC66(\uD835\uDC61)) \uD835\uDF15ℎ\n(11)\nUpdate internal parameters \uD835\uDF03:\n\uD835\uDF03 ← \uD835\uDF03 − \uD835\uDF02\uD835\uDC61 (\uD835\uDC3B\uD835\uDC63)\uD835̄\uDC64 − \uD835\uDF02\uD835\uDC61 ∑︀ \uD835\uDC56\uD835\uDC3B\uD835\uDC56\uD835\uDC64\uD835\uDC56 (12)\n(this is a gradient step \uD835\uDF03 ← \uD835\uDF03 − \uD835\uDF02\uD835\uDC61(\uD835\uDC3B\uD835̃\uDC3A)⊤ using the current gradient estimate \uD835̃\uDC3A from (9)). Reduction step: Draw independent uniform random signs \uD835\uDF00\uD835\uDC56 = ±1. Let \uD835\uDC52\uD835\uDC56 be the \uD835\uDC56-th basis vector in state space. Compute \uD835\uDF0C := √︀ ‖\uD835̄\uDC64‖ / ‖\uD835\uDC63‖ and \uD835\uDF0C\uD835\uDC56 := √︀ ‖\uD835\uDC64\uD835\uDC56‖ / ‖\uD835\uDC52\uD835\uDC56‖ for each \uD835\uDC56. Update\n\uD835\uDC63 ← \uD835\uDF0C\uD835\uDC63 + ∑︀\n\uD835\uDC56\uD835\uDF00\uD835\uDC56\uD835\uDF0C\uD835\uDC56\uD835\uDC52\uD835\uDC56 (13) \uD835̄\uDC64 ← \uD835̄\uDC64/\uD835\uDF0C + ∑︀ \uD835\uDC56\uD835\uDF00\uD835\uDC56\uD835\uDC64\uD835\uDC56/\uD835\uDF0C\uD835\uDC56 (14) \uD835\uDC64\uD835\uDC56 ← 0 (15)\nTransition step: Observe new value of input signal \uD835\uDC65(\uD835\uDC61) and compute next state ℎ(\uD835\uDC61 + 1) = \uD835\uDC53(ℎ(\uD835\uDC61), \uD835\uDC65(\uD835\uDC61), \uD835\uDF03). Update estimate \uD835̃\uDC3A:\n\uD835\uDC63 ← \uD835\uDF15\uD835\uDC53(ℎ(\uD835\uDC61), \uD835\uDC65(\uD835\uDC61), \uD835\uDF03) \uD835\uDF15ℎ \uD835\uDC63 (16)\n\uD835\uDC64\uD835\uDC56 ← \uD835\uDF15\uD835\uDC53\uD835\uDC56(ℎ(\uD835\uDC61), \uD835\uDC65(\uD835\uDC61), \uD835\uDF03)\n\uD835\uDF15\uD835\uDF03\n⊤ (17)\n\uD835\uDC61← \uD835\uDC61 + 1 (18)\nend Algorithm 1: NoBackTrack algorithm, Euclidean version.\nthis happens whenever the learning rate is small enough for \uD835\uDF03 not to change too much within a time range corresponding to a “forgetting time” of the dynamical system, although more work is needed here.\n1.2 Feeding the gradient estimate to an extended Kalman filter\nThe Euclidean version of the NoBackTrack algorithm presented in Algorithm 1 is not enough to obtain good performance fast. Online estimation often yields best results when using filters from the Kalman family. We refer to [Hay04, Jae02] for a discussion of Kalman filtering applied to recurrent neural networks.\nKalman-based approaches rely on a covariance matrix estimate \uD835\uDC43 (\uD835\uDC61) on \uD835\uDF03. After observing \uD835\uDC66(\uD835\uDC61), the parameter \uD835\uDF03 gets adjusted via8\n\uD835\uDF03 ← \uD835\uDF03 − \uD835\uDC43 (\uD835\uDC61)\uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF03\n⊤ (20)\nwhere the derivative of the loss with respect to \uD835\uDF03 is computed, as above, via the product of the derivative of the loss with respect to the current state ℎ(\uD835\uDC61), and the derivative \uD835\uDC3A(\uD835\uDC61) = \uD835\uDF15ℎ(\uD835\uDC61)\uD835\uDF15\uD835\uDF03 .\nMaintaining a full covariance matrix on \uD835\uDF03 is usually too costly. However, having a good approximation of \uD835\uDC43 (\uD835\uDC61) is not as critical as having a good approximation of \uD835\uDF15ℓ\uD835\uDC61\uD835\uDF15\uD835\uDF03 . Indeed, given an unbiased approximation of \uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF03 , any symmetric positive definite matrix \uD835\uDC43 (\uD835\uDC61) which changes slowly enough in time will yield an unbiased trajectory for \uD835\uDF03.\nThus, we will use more aggressive matrix reduction techniques on \uD835\uDC43 (\uD835\uDC61), such as block-diagonal (as in [Hay04]) or quasi-diagonal [Oll15a] approximations. In our setting, the main point of using the covariance matrix is to get both a sensible scaling of the learning rate for each component of \uD835\uDF03, and reparametrization-invariance properties [Oll15a].\nIn Kalman filtering, in the case when the “true” underlying parameter \uD835\uDF03 in the extended Kalman filter is constant, it is better to work with the inverse covariance matrix \uD835\uDC3D(\uD835\uDC61) := \uD835\uDC43 (\uD835\uDC61)−1, and the extended Kalman filter on \uD835\uDF03 can be rewritten as\n\uD835\uDC3D(\uD835\uDC61)← \uD835\uDC3D(\uD835\uDC61− 1) + \uD835\uDF15\uD835\uDC66\uD835\uDC61 \uD835\uDF15\uD835\uDF03\n⊤ \uD835\uDC3C\uD835\uDC61\n\uD835\uDF15\uD835\uDC66\uD835\uDC61 \uD835\uDF15\uD835\uDF03\n(21)\n\uD835\uDF03 ← \uD835\uDF03 − \uD835\uDC3D(\uD835\uDC61)−1 \uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF03\n⊤ (22)\nwhere \uD835\uDC66\uD835\uDC61 is the prediction at time \uD835\uDC61, where both \uD835\uDF15\uD835\uDC66\uD835\uDC61\uD835\uDF15\uD835\uDF03 and \uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF03 can be computed from ℎ(\uD835\uDC61) via the chain rule if \uD835\uDC3A(\uD835\uDC61) = \uD835\uDF15ℎ(\uD835\uDC61)\uD835\uDF15\uD835\uDF03 is known, and where \uD835\uDC3C\uD835\uDC61 is the 8Indeed, in standard Kalman filter notation, one has \uD835\uDC3E\uD835\uDC61\uD835\uDC45 = \uD835\uDC43\uD835\uDC61\uD835\uDC3B⊤\uD835\uDC61 , so that for the quadratic loss ℓ = 12 (\uD835\uDC66 − \uD835\uDC66) ⊤\uD835\uDC45−1(\uD835\uDC66 − \uD835\uDC66) (log-loss of a Gaussian model with coraviance matrix \uD835\uDC45), the Kalman update for \uD835\uDF03 is equivalent to \uD835\uDF03 ← \uD835\uDF03 − \uD835\uDC43 (\uD835\uDC61) \uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF03 ⊤.\nFisher information matrix of \uD835\uDC66\uD835\uDC61 as a probability distribution on \uD835\uDC66\uD835\uDC61. (For exponential families this is just the Hessian −\uD835\uDF152ℓ\uD835\uDC61\n\uD835\uDF15\uD835\uDC662\uD835\uDC61 of the loss with respect\nto the prediction). This is the so-called information filter, because \uD835\uDC3D(\uD835\uDC61) approximates the Fisher information matrix on \uD835\uDF03 given the observations up to time \uD835\uDC61. This is basically a natural gradient descent on \uD835\uDF03.\nThis approach is summarized in Algorithm 2, which we describe more loosely since matrix approximation schemes may depend on the application.\nAlgorithm 2 uses a decay factor (1−\uD835\uDEFE\uD835\uDC61) on the inverse covariance matrices to limit the influence of old computations made with outdated values of \uD835\uDF03. The factor \uD835\uDEFE\uD835\uDC61 also controls the effective learning rate of the algorithm, since, in line with Kalman filtering, we have not included a learning rate for the update of \uD835\uDF03 (namely, \uD835\uDF02\uD835\uDC61 = 1): the step size is adapted via the magnitude of \uD835\uDC3D . For \uD835\uDEFE\uD835\uDC61 = 0, \uD835\uDC3D grows linearly so that step size is \uD835\uDC42(1/\uD835\uDC61).\nMoreover, we have included a regularization term Λ for matrix inversion; in the Bayesian interpretation of Kalman filtering this corresponds to having a Gaussian prior on the parameters with inverse covariance matrix Λ. This is important to avoid fast divergence in the very first steps.\nIn practice we have used \uD835\uDEFE\uD835\uDC61 = \uD835\uDC42(1/ √\n\uD835\uDC61) and Λ = (dim ℎ). Id. The simplest and fastest way to approximate the Fisher matrix in Algorithm 2 is the outer product approximation (see discussion in [Oll15a]), which we have used in the experiments below. Namely, we simply use \uD835\uDC3C\uD835\uDC61 ← \uD835\uDF15ℓ\uD835\uDC61\uD835\uDF15\uD835\uDC66\uD835\uDC61 ⊤\uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDC66\uD835\uDC61 so that the updates to \uD835\uDC3D\uD835\uDF19 and \uD835\uDC3D\uD835\uDF03 simplify and become rank-one outer product updates using the gradient of the loss, namely, \uD835\uDC3D\uD835\uDF03 ← (1− \uD835\uDEFE\uD835\uDC61)\uD835\uDC3D\uD835\uDF03 + \uD835\uDF15ℓ\uD835\uDC61\uD835\uDF15\uD835\uDF03 ⊤\uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF03 and likewise for \uD835\uDF19. Here the derivative \uD835\uDF15ℓ\uD835\uDC61\uD835\uDF15\uD835\uDF03 is estimated from the current gradient estimate \uD835̃\uDC3A.\nFor the matrix reductions, we have used a block-wise quasi-diagonal reduction as in [Oll15a]. This makes the cost of handling the various matrices linear in the number of parameters."
    }, {
      "heading" : "1.3 Examples",
      "text" : "Let us show how Algorithm 1 works out on explicit examples.\nThe importance of norm rescaling. Let us first consider a simple dynamical system which illustrates the importance of rescaling the norms by \uD835\uDF0C and \uD835\uDF0C\uD835\uDC56. Let 0 < \uD835\uDEFC < 1 and consider the system\nℎ(\uD835\uDC61 + 1) = (1− \uD835\uDEFC)ℎ(\uD835\uDC61) + \uD835\uDF03 (28)\nwith both ℎ and \uD835\uDF03 in R\uD835\uDC5B. This quickly converges towards \uD835\uDF03/\uD835\uDEFC. We have \uD835\uDF15\uD835\uDC53/\uD835\uDF15ℎ = (1 − \uD835\uDEFC) Id and \uD835\uDF15\uD835\uDC53/\uD835\uDF15\uD835\uDF03 = Id and so \uD835\uDF15\uD835\uDC53\uD835\uDC56/\uD835\uDF15\uD835\uDF03⊤ = \uD835\uDC52\uD835\uDC56, the \uD835\uDC56-th basis vector. Then the reduction and transition steps in Algorithm 1, if the scalings\nParameters: ℎ(0) (initial state), \uD835\uDF030, \uD835\uDF190 (initial value of the parameters), 0 6 \uD835\uDEFE\uD835\uDC61 < 1 (covariance decay parameter scheme), Λ\uD835\uDF19 and Λ\uD835\uDF03 (inverse covariance matrix of the prior on the parameters); Maintains: Same as Algorithm 1, plus a representation of matrices \uD835\uDC3D\uD835\uDF03 and \uD835\uDC3D\uD835\uDF19 allowing for efficient inversion; Subroutines: A matrix reduction method MatrixReduce(\uD835\uDC40) which only evaluates a small number of entries of its argument \uD835\uDC40 and returns an approximation of \uD835\uDC40 that can be inverted efficiently; A routine FisherApprox(\uD835\uDC66\uD835\uDC61, \uD835\uDC66\uD835\uDC61) which returns either a positive definite approximation of the Fisher information matrix of \uD835\uDC66\uD835\uDC61 as a probability distribution on \uD835\uDC66\uD835\uDC61, or a positive definite approximation of the Hessian −\uD835\uDF152ℓ\uD835\uDC61\n\uD835\uDF15\uD835\uDC662\uD835\uDC61 of the loss with respect to the prediction.\nInitialization: as in Algorithm 1, and \uD835\uDC3D\uD835\uDF03 ← 0, \uD835\uDC3D\uD835\uDF19 ← 0; for \uD835\uDC61 = 0 to end-of-time do\nObservation step: as in Algorithm 1. Update step: Compute approximate Fisher information matrix w.r.t. \uD835\uDC66\uD835\uDC61:\n\uD835\uDC3C\uD835\uDC61 ← FisherApprox(\uD835\uDC66\uD835\uDC61, \uD835\uDC66\uD835\uDC61) (23)\nCompute derivative of prediction and of loss with respect to output parameters, \uD835\uDF15\uD835\uDC66\uD835\uDC61\uD835\uDF15\uD835\uDF19 and \uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF19 . Update inverse covariance matrix of output parameters \uD835\uDF19:\n\uD835\uDC3D\uD835\uDF19 ← (1− \uD835\uDEFE\uD835\uDC61)\uD835\uDC3D\uD835\uDF19 + MatrixReduce (︃\n\uD835\uDF15\uD835\uDC66\uD835\uDC61 \uD835\uDF15\uD835\uDF19\n⊤ \uD835\uDC3C\uD835\uDC61\n\uD835\uDF15\uD835\uDC66\uD835\uDC61 \uD835\uDF15\uD835\uDF19\n)︃ (24)\nand update output parameters:\n\uD835\uDF19← \uD835\uDF19− (\uD835\uDC3D\uD835\uDF19 + Λ\uD835\uDF19)−1 \uD835\uDF15ℓ\uD835\uDC61 \uD835\uDF15\uD835\uDF19\n⊤ (25)\nCompute derivative \uD835\uDF15\uD835\uDC66\uD835\uDC61\uD835\uDF15ℎ of prediction with respect to current state ℎ(\uD835\uDC61). Update inverse covariance matrix of internal parameters \uD835\uDF03:\n\uD835\uDC3D\uD835\uDF03 ← (1− \uD835\uDEFE\uD835\uDC61)\uD835\uDC3D\uD835\uDF03 + MatrixReduce (︃\n\uD835̃\uDC3A⊤ \uD835\uDF15\uD835\uDC66\uD835\uDC61 \uD835\uDF15ℎ\n⊤ \uD835\uDC3C\uD835\uDC61\n\uD835\uDF15\uD835\uDC66\uD835\uDC61 \uD835\uDF15ℎ \uD835̃\uDC3A\n)︃ (26)\nand update internal parameters \uD835\uDF03:\n\uD835\uDF03 ← \uD835\uDF03 − (\uD835\uDC3D\uD835\uDF03 + Λ\uD835\uDF03)−1\uD835\uDEFF\uD835\uDF03 (27)\nwhere \uD835\uDEFF\uD835\uDF03 := (\uD835\uDC3B\uD835\uDC63)\uD835̄\uDC64 − ∑︀\n\uD835\uDC56\uD835\uDC3B\uD835\uDC56\uD835\uDC64\uD835\uDC56 is the update of \uD835\uDF03 from Algorithm 1. Reduction step: Same as in Algorithm 1, but the norms used to compute \uD835\uDF0C and \uD835\uDF0C\uD835\uDC56 are derived from \uD835\uDC3D−1\uD835\uDF03 (cf. Appendix B). Transition step: Same as in Algorithm 1.\nend Algorithm 2: NoBackTrack algorithm, Kalman version.\n\uD835\uDF0C are not used, amount to\n\uD835\uDC63\uD835\uDC61+1 = (1− \uD835\uDEFC) (\uD835\uDC63\uD835\uDC61 + ∑︀\n\uD835\uDC56\uD835\uDF00\uD835\uDC56(\uD835\uDC61)\uD835\uDC52\uD835\uDC56) (29) \uD835̄\uDC64\uD835\uDC61+1 = \uD835̄\uDC64\uD835\uDC61 + ∑︀ \uD835\uDC56\uD835\uDF00\uD835\uDC56(\uD835\uDC61)\uD835\uDC52\uD835\uDC56 (30)\nwith the \uD835\uDF00\uD835\uDC56(\uD835\uDC61) independent at each step \uD835\uDC61. The resulting estimate of \uD835\uDF15ℎ(\uD835\uDC61)/\uD835\uDF15\uD835\uDF03 is unbiased, but its variance grows linearly with time. Indeed, the dynamics of \uD835\uDC63\uD835\uDC61 is stationary thanks to the factor (1 − \uD835\uDEFC), but the dynamics of \uD835̄\uDC64\uD835\uDC61 is purely additive so that \uD835\uDC64\uD835\uDC61 is just a \uD835\uDC51-dimensional random walk. On the other hand, if rescaling by \uD835\uDF0C is used, then both \uD835\uDC63 and \uD835̄\uDC64 get rescaled by √ 1− \uD835\uDEFC at each step,9 so that their dynamics becomes stationary and variance does not grow.\nRecurrent neural networks. The next example is a standard recurrent neural network (RNN). The state of the system is the set of pre-activation values ℎ\uD835\uDC56(\uD835\uDC61), and the activities are \uD835\uDC4E\uD835\uDC56(\uD835\uDC61) := \uD835\uDF0E(ℎ\uD835\uDC56(\uD835\uDC61)) where \uD835\uDF0E is some activation function such as tanh or sigmoid. The recurrent dynamics of ℎ is\nℎ\uD835\uDC56(\uD835\uDC61 + 1) = ∑︁ \uD835\uDC57→\uD835\uDC56 \uD835\uDC4A\uD835\uDC57\uD835\uDC56 \uD835\uDF0E(ℎ\uD835\uDC57(\uD835\uDC61)) + ∑︁ \uD835\uDC59 \uD835\uDC5F\uD835\uDC59\uD835\uDC56\uD835\uDC65\uD835\uDC59(\uD835\uDC61) (31)\nin which ℎ(\uD835\uDC61), ℎ(\uD835\uDC61+1) ∈ R\uD835\uDC5B, (\uD835\uDC4A\uD835\uDC57\uD835\uDC56)\uD835\uDC57→\uD835\uDC56 are a set of weights defining a graph on \uD835\uDC5B nodes, and (\uD835\uDC5F\uD835\uDC59\uD835\uDC56)(\uD835\uDC56,\uD835\uDC59) are the input weights.10 The parameter is \uD835\uDF03 = (\uD835\uDC4A, \uD835\uDC5F). We hereby omit the output part of the network,11 as it is of no use to analyze the estimation of \uD835\uDF15ℎ(\uD835\uDC61)/\uD835\uDF15\uD835\uDF03.\n(We have chosen the pre-activation values ℎ, rather than the activities \uD835\uDC4E = \uD835\uDF0E(ℎ), as the state of the system. This results in simpler expressions, especially for the input weights \uD835\uDC5F.)\nThus, the function \uD835\uDC53 defining the dynamical system for the variable ℎ is (31). The derivatives of \uD835\uDC53 are immediately computed as \uD835\uDF15\uD835\uDC53\uD835\uDC56/\uD835\uDF15\uD835\uDC4A\uD835\uDC57\uD835\uDC56 = \uD835\uDF0E(ℎ\uD835\uDC57), \uD835\uDF15\uD835\uDC53\uD835\uDC56/\uD835\uDF15\uD835\uDC5F\uD835\uDC59\uD835\uDC56 = \uD835\uDC65\uD835\uDC59, \uD835\uDF15\uD835\uDC53\uD835\uDC56/\uD835\uDF15ℎ\uD835\uDC57 = \uD835\uDC4A\uD835\uDC57\uD835\uDC56 \uD835\uDF0E′(ℎ\uD835\uDC57), and all other derivatives are 0.\nAlgorithm 1 maintains, after the reduction step, an approximation \uD835\uDF15ℎ(\uD835\uDC61)\uD835\uDF15\uD835\uDF03 ≈ \uD835\uDC63(\uD835\uDC61)\uD835̄\uDC64(\uD835\uDC61)⊤. We can decompose \uD835̄\uDC64(\uD835\uDC61) = (\uD835̄\uDC4A (\uD835\uDC61), \uD835\uDC5F(\uD835\uDC61)) into the components corresponding to the internal and input weights of the parameter \uD835\uDF03 = (\uD835\uDC4A, \uD835\uDC5F), so that\n\uD835\uDF15ℎ\uD835\uDC56(\uD835\uDC61) \uD835\uDF15\uD835\uDC4A\uD835\uDC58\uD835\uDC57 ≈ \uD835\uDC63\uD835\uDC56(\uD835\uDC61)\uD835̄\uDC4A\uD835\uDC58\uD835\uDC57(\uD835\uDC61) (32) \uD835\uDF15ℎ\uD835\uDC56(\uD835\uDC61) \uD835\uDF15\uD835\uDC5F\uD835\uDC59\uD835\uDC57 ≈ \uD835\uDC63\uD835\uDC56(\uD835\uDC61)\uD835\uDC5F\uD835\uDC59\uD835\uDC57(\uD835\uDC61). (33)\n9Proof: By induction one has \uD835\uDC63 = \uD835̄\uDC64 after the reduction step and \uD835\uDC63 = (1− \uD835\uDEFC)\uD835̄\uDC64 after the transition step, and \uD835\uDF0C = 1/ √ 1− \uD835\uDEFC.\n10Biases are omitted; they can be treated by the inclusion of an always-activated united \uD835\uDC560 with \uD835\uDC4E\uD835\uDC560 (\uD835\uDC61) ≡ 1.\n11The experiments below use a softmax output with output parameters \uD835\uDF19, see Section 2.\nBy plugging the values of the partial derivatives of \uD835\uDC53 into Algorithm 1, we find the following update equations for the value of \uD835\uDC63, \uD835̄\uDC4A and \uD835\uDC5F right after the reduction step:\n\uD835\uDC63\uD835\uDC56(\uD835\uDC61 + 1) = \uD835\uDF0C ∑︁ \uD835\uDC57→\uD835\uDC56 \uD835\uDC4A\uD835\uDC57\uD835\uDC56 \uD835\uDF0E ′(ℎ\uD835\uDC57(\uD835\uDC61)) \uD835\uDC63\uD835\uDC57(\uD835\uDC61) + \uD835\uDF00\uD835\uDC56\uD835\uDF0C\uD835\uDC56 (34)\n\uD835̄\uDC4A\uD835\uDC58\uD835\uDC57(\uD835\uDC61 + 0) = \uD835̄\uDC4A\uD835\uDC58\uD835\uDC57(\uD835\uDC61)\n\uD835\uDF0C + \uD835\uDF00\uD835\uDC57 \uD835\uDF0E(ℎ\uD835\uDC58(\uD835\uDC61)) \uD835\uDF0C\uD835\uDC57\n(35)\n\uD835\uDC5F\uD835\uDC59\uD835\uDC57(\uD835\uDC61 + 1) = \uD835\uDC5F\uD835\uDC59\uD835\uDC57(\uD835\uDC61)\n\uD835\uDF0C + \uD835\uDF00\uD835\uDC57 \uD835\uDC65\uD835\uDC59(\uD835\uDC61) \uD835\uDF0C\uD835\uDC57\n(36)\nwhere the \uD835\uDF00\uD835\uDC57 are independent symmetric binary random variables, taking values ±1 with probability 12 . Any non-zero choice of \uD835\uDF0C\uD835\uDC57 leads to an unbiased estimation, though the values are to be optimized as mentioned above.\nApplying this update has the same algorithmic cost as implementing one step (31) of the recurrent network itself.\nLeaky recurrent neural networks. To capture long-term dependencies, in the experiments below we also use a leaky RNN, obtained via the addition of a direct feedback term:\nℎ\uD835\uDC56(\uD835\uDC61 + 1) = \uD835\uDEFC\uD835\uDC56ℎ\uD835\uDC56(\uD835\uDC61) + ∑︁\n\uD835\uDC59\n\uD835\uDC5F\uD835\uDC59\uD835\uDC56\uD835\uDC65\uD835\uDC59(\uD835\uDC61) + ∑︁\n\uD835\uDC57\n\uD835\uDC4A\uD835\uDC57\uD835\uDC56\uD835\uDC4E\uD835\uDC57(\uD835\uDC61) \uD835\uDC4E\uD835\uDC57(\uD835\uDC61) := \uD835\uDF0E(ℎ\uD835\uDC57(\uD835\uDC61)) (37)\nwith \uD835\uDEFC\uD835\uDC56 ∈ [0; 1] for all \uD835\uDC56. (See [Jae02] for similar models.) This feedback term reduces the impact of the vanishing gradient issue and keeps a longer memory of past inputs.\nThis only changes the derivative of \uD835\uDC53\uD835\uDC56 with respect to ℎ\uD835\uDC57 , which becomes \uD835\uDF15\uD835\uDC53\uD835\uDC56/\uD835\uDF15ℎ\uD835\uDC57 = \uD835\uDC4A\uD835\uDC57\uD835\uDC56\uD835\uDF0E′(ℎ\uD835\uDC57) + \uD835\uDEFC\uD835\uDC56\uD835\uDEFF\uD835\uDC56\uD835\uDC57 . Consequently the update rules (35)–(36) for \uD835̄\uDC4A and \uD835\uDC5F are unchanged, while the update of \uD835\uDC63 becomes\n\uD835\uDC63\uD835\uDC56(\uD835\uDC61 + 1) = \uD835\uDF0C\uD835\uDEFC\uD835\uDC56\uD835\uDC63\uD835\uDC56(\uD835\uDC61) + \uD835\uDF0C ∑︁ \uD835\uDC57→\uD835\uDC56 \uD835\uDC4A\uD835\uDC57\uD835\uDC56 \uD835\uDF0E ′(ℎ\uD835\uDC57(\uD835\uDC61)) \uD835\uDC63\uD835\uDC57(\uD835\uDC61) + \uD835\uDF00\uD835\uDC56\uD835\uDF0C\uD835\uDC56 (38)\nMultilayer recurrent neural networks. Let us now treat the case of a multilayer recurrent neural network with dynamics\nℎ(1)(\uD835\uDC61 + 1) = \uD835\uDC53 (1)(\uD835\uDC65(\uD835\uDC61), ℎ(1)(\uD835\uDC61), \uD835\uDF031) (39) ℎ(2)(\uD835\uDC61 + 1) = \uD835\uDC53 (2)(\uD835\uDC65(\uD835\uDC61), ℎ(1)(\uD835\uDC61 + 1), ℎ(2)(\uD835\uDC61), \uD835\uDF032) (40) ... (41) ℎ(\uD835\uDC5B)(\uD835\uDC61 + 1) = \uD835\uDC53 (\uD835\uDC5B)(\uD835\uDC65(\uD835\uDC61), ℎ(\uD835\uDC5B−1)(\uD835\uDC61 + 1), ℎ(\uD835\uDC5B)(\uD835\uDC61), \uD835\uDF03\uD835\uDC5B) (42)\nwhere each layer ℎ(\uD835\uDC56) and \uD835\uDC53 (\uD835\uDC56) define an RNN as in (31) above. Directly applying the rank-one approximation to the function \uD835\uDC53 = (\uD835\uDC53 (1), \uD835\uDC53 (2), . . . , \uD835\uDC53 (\uD835\uDC5B))\nwould be cumbersome: since the activity of a neuron of the \uD835\uDC56-th layer at time \uD835\uDC61 + 1 depends on all parameters from the previous \uD835\uDC56− 1 layers, the derivative \uD835\uDF15\uD835\uDC53/\uD835\uDF15\uD835\uDF03 is not sparse.\nTo cope with this, a natural approach is to treat the dynamics in a “rolling” fashion and apply the rank-one approximation at each layer in turn. Formally, this amounts to defining the following model\nℎ̃(1)(\uD835\uDC5B\uD835\uDC61 + 1) = \uD835\uDC53 (1)(\uD835̃\uDC65(\uD835\uDC5B\uD835\uDC61), ℎ̃(1)(\uD835\uDC5B\uD835\uDC61), \uD835\uDF031) (43) ℎ̃(2)(\uD835\uDC5B\uD835\uDC61 + 2) = \uD835\uDC53 (2)(\uD835̃\uDC65(\uD835\uDC5B\uD835\uDC61 + 1), ℎ̃(2)(\uD835\uDC5B\uD835\uDC61 + 1), \uD835\uDF032) (44) ... (45) ℎ̃(\uD835\uDC5B)(\uD835\uDC5B\uD835\uDC61 + \uD835\uDC5B) = \uD835\uDC53 (\uD835\uDC5B)(\uD835̃\uDC65(\uD835\uDC5B\uD835\uDC61 + \uD835\uDC5B− 1), ℎ̃(\uD835\uDC5B)(\uD835\uDC5B\uD835\uDC61 + \uD835\uDC5B− 1), \uD835\uDF03\uD835\uDC5B) (46)\nwith \uD835̃\uDC65(\uD835\uDC61) := \uD835\uDC65(⌊\uD835\uDC61/\uD835\uDC5B⌋), and where states not explicitly appearing in these equations stay unchanged (ℎ(\uD835\uDC56)(\uD835\uDC5B\uD835\uDC61 + \uD835\uDC57) = ℎ(\uD835\uDC56)(\uD835\uDC5B\uD835\uDC61 + \uD835\uDC57 − 1) for \uD835\uDC56 ̸= \uD835\uDC57). Thus, the transition function explicitly depends on time (more precisely, on time modulo the number of layers), and is sparse at each step. Indeed, at each step, applying the transition function amounts to applying one of the \uD835\uDC53 (\uD835\uDC56) to the corresponding layer, and leaving the other layers unchanged. Thus the derivative of \uD835\uDC53 (\uD835\uDC56) with respect to any \uD835\uDF03\uD835\uDC57 , \uD835\uDC57 ̸= \uD835\uDC56, is zero; this leaves only the gradient of \uD835\uDC53 (\uD835\uDC56) wrt \uD835\uDF03\uD835\uDC56 to be dealt with, and Algorithm 1 or 2 can be applied at little cost."
    }, {
      "heading" : "1.4 Extensions",
      "text" : "Rank-\uD835\uDC3E reductions. A first obvious extension is to use higher-rank reductions. The simplest way to achieve this is to take several independent random rank-one \uD835\uDC63\uD835\uDC58\uD835̄\uDC64⊤\uD835\uDC58 reductions in (7) and average them. Note that \uD835\uDC64\uD835\uDC56 (Algorithm 1) has to be evaluated only once in this case. It might be slightly more efficient to first split the parameter components into \uD835\uDC3E blocks (e.g., at random) so that the \uD835\uDC58-th term \uD835̄\uDC64⊤\uD835\uDC58 only involves parameters from the \uD835\uDC58-th block: indeed, applying the evolution equation for \uD835\uDC3A preserves this structure so this requires less memory for storage of the \uD835̄\uDC64\uD835\uDC58.\nAlgorithms similar to RTRL. Other algorithms have been proposed that have the same structure and shortcomings as real-time recurrent learning, for instance, the online EM algorithm for hidden Markov models from [Cap11]. In principle, the approach presented here can be extended to such settings.\nContinuous-time systems. Another extension concerns continuous-time dynamical systems\ndℎ(\uD835\uDC61) d\uD835\uDC61 = \uD835\uDC39 (ℎ(\uD835\uDC61), \uD835\uDC65(\uD835\uDC61), \uD835\uDF03) (47)\nwhich can be discretized as ℎ(\uD835\uDC61 + \uD835\uDEFF\uD835\uDC61) = ℎ(\uD835\uDC61) + \uD835\uDEFF\uD835\uDC61\uD835\uDC39 (ℎ(\uD835\uDC61), \uD835\uDC65(\uD835\uDC61), \uD835\uDF03). Thus this is analogous to the discrete-time case via \uD835\uDC53 = Id +\uD835\uDEFF\uD835\uDC61\uD835\uDC39 , and Algorithm 1 may be applied to this discretization.\nWhen performing the rank-one reduction (7), the scaling by \uD835\uDF0C\uD835\uDC56 = √︀ ‖\uD835\uDC64\uD835\uDC56‖ / ‖\uD835\uDC63\uD835\uDC56‖\nis important in this case: it ensures that both \uD835\uDC63 and \uD835̄\uDC64 change by \uD835\uDC42( √\n\uD835\uDEFF\uD835\uDC61) times a random quantity at each step. This is the expected correct scaling for a continuous-time stochastic evolution equation, corresponding to the increment of a Wiener process during a time interval \uD835\uDEFF\uD835\uDC61. (Without scaling by \uD835\uDF0C\uD835\uDC56, there will be no well-defined limit as \uD835\uDEFF\uD835\uDC61 → 0, because \uD835\uDC63 would change by \uD835\uDC42(1) at each step \uD835\uDC61← \uD835\uDC61 + \uD835\uDEFF\uD835\uDC61, while \uD835̄\uDC64 would evolve by \uD835\uDEFF\uD835\uDC61 times a centered random quantity so that it would be constant in the limit.) Further work is needed to study this continuous-time limit."
    }, {
      "heading" : "2 Experiments",
      "text" : "We report here a series of small-scale experiments on text prediction tasks. The experiments focus on two questions: First, does learning using the rank-one approximation \uD835̃\uDC3A accurately reflect learning based on the actual gradient \uD835\uDC3A computed exactly via RTRL, or is the noise introduced in this method detrimental to learning? Second, how does this approach compare to truncated backpropagation through time?\nWe used the RNN or leaky RNN models described above to predict a sequence of characters \uD835\uDC66(\uD835\uDC61) in a finite alphabet \uD835\uDC9C, given the past observations \uD835\uDC65(\uD835\uDC60) = \uD835\uDC66(\uD835\uDC60) for 1 6 \uD835\uDC60 6 \uD835\uDC61 − 1. At each time, the network outputs a probability distribution on the next character \uD835\uDC67; explicitly, the output at time \uD835\uDC61 is \uD835\uDC66(\uD835\uDC61) ∈ R\uD835\uDC9C defined by\n\uD835\uDC66(\uD835\uDC61)\uD835\uDC67 := \uD835\uDF19\uD835\uDC67 + ∑︁\n\uD835\uDC56\n\uD835\uDF19\uD835\uDC56\uD835\uDC67\uD835\uDC4E\uD835\uDC56(\uD835\uDC61) (48)\nfor each \uD835\uDC67 ∈ \uD835\uDC9C, with parameters \uD835\uDF19 = (\uD835\uDF19\uD835\uDC67, \uD835\uDF19\uD835\uDC56\uD835\uDC67)\uD835\uDC56,\uD835\uDC67. The output \uD835\uDC66 = (\uD835\uDC66\uD835\uDC66)\uD835\uDC66∈\uD835\uDC9C defines a probability distribution on \uD835\uDC9C via a softmax \uD835\uDC5D\uD835\uDC66(\uD835\uDC66) := \uD835\uDC52 \uD835\uDC66\uD835\uDC66∑︀ \uD835\uDC67∈\uD835\uDC34\uD835\uDC59 \uD835\uDC52 \uD835\uDC66\uD835\uDC67 , and the loss function is the log-loss on prediction of the next character, ℓ\uD835\uDC61 := − log2 \uD835\uDC5D\uD835\uDC66(\uD835\uDC61)(\uD835\uDC66(\uD835\uDC61)). The internal and output parameters \uD835\uDF03 and \uD835\uDF19 are trained according to Algorithms 1 and 2.\nWe used three datasets. The first is a “text” representing synthetic music notation with several syntactic, rhythmic and harmonic constraints (Example 3 from [Oll15b]). The data was a file of length ≈ 105 characters, after which the signal cycled over the same file. The second dataset is the classical \uD835\uDC4E\uD835\uDC5B\uD835\uDC4F\uD835\uDC5B example, synthesized by repeatedly picking an integer \uD835\uDC5B at random in some interval, then outputting a series of \uD835\uDC5B \uD835\uDC4E’s followed by a line break, then \uD835\uDC5B \uD835\uDC4F’s and another line break. This model tests the ability of a learning algorithm to learn precise timing and time dependencies. The\nthird example is the full set of Shakespeare’s works, obtained from Project Gutenberg.12 The file is roughly 5.106 characters long.\nThe benchmarks included are gzip, a standard non-online compression algorithm, and context tree weighting (CTW) [BEYY04], a more advanced online text compression algorithm, as well as the actual entropy rate of the generative model for synthetic music and \uD835\uDC4E\uD835\uDC5B\uD835\uDC4F\uD835\uDC5B.\nThe code used in the experiments is available at http://www.yann-ollivier. org/rech/code/nobacktrack/code_nobacktrack_exp.tar.gz\nEuclidean NoBackTrack. We first study whether the low rank approximation in the Euclidean version of NoBackTrack impacts the gradient descent. For this first set of experiments, we use a fully connected RNN with 20 units, as described above, on the synthetic music example. We compared RTRL, Euclidean rank-one NoBackTrack, and Euclidean NoBackTrack using ranktwo and rank-ten reductions (obtained by averaging two or ten independent rank-one reductions, as discussion in Section 1.4).\nThe results are summed up in Figure 1 and Figure 2. All the models were trained using the same learning rate \uD835\uDF02\uD835\uDC61 = 1/ √ \uD835\uDC61 for Figure 1 and \uD835\uDF02\uD835\uDC61 = 0.03/ √ \uD835\uDC61 for Figure 2. The various algorithms were run for the same amount of time. This is reflected in the different curve lengths for the different algorithms; in particular, the curve for RTRL is much shorter, reflecting its higher computational cost. (Note the log scale on the \uD835\uDC61 axis: RTRL is roughly 20 times slower with 20 units.)\nThe impact of stochasticity of the low-rank approximation when using large learning rates is highlighted on Figure 1: Euclidean NoBackTrack with a large learning rate displays instabilities, even when increasing the rank of the approximation.\nSmaller learning rates allow the algorithm to cope with this, as the noise in the gradients is averaged out over longer time spans. This is illustrated in Figure 2, in which the trajectories of Euclidean NoBackTrack track those of RTRL closely even with a rank-two approximation.\nKalman NoBackTrack. Next, we report the results of the Kalman version of NoBackTrack on the same experimental setup. A quasi-diagonal outer product (QDOP) approximation [Oll15a] of the full Kalman inverse covariance matrix is used, to keep complexity low.\nWe compare the low-rank approximations to RTRL. To make the comparison clear, for RTRL we also use a quasi-diagonal (QDOP) approximation of the Kalman filtering algorithm on top of the exact gradient computed by RTRL.\n12www.gutenberg.org\nLearning rates were set to 1 and all algorithms were run for the same amount of time.\nThe use of the QDOP-approximated Kalman inverse covariance appears to fully fix the unstable behaviour. Overall, low-rank approximations appear to be roughly on par with QDOP RTRL. There is no obvious gain, on this particular example, in using higher-rank approximations.\nStill, on this particular task and with this particular network size, none of the RNN algorithms (including BPTT reported below) match the performance of Context Tree Weighting. RNNs beat CTW on this task if trained using a non-online, Riemannian gradient descent [Oll15b] (analogous to using the Kalman inverse covariance). So this is arguably an effect of imperfect online RNN training.\nKalman NoBackTrack and truncated BPTT. Our next set of experiments aims at comparing Kalman NoBackTrack to truncated BPTT, with truncation13 parameter \uD835\uDC47 = 15. As BPTT truncates the full gradient by\n13In the version of BPTT used here, the algorithm does not backtrack by \uD835\uDC47 steps at every time step \uD835\uDC61; rather, it waits for \uD835\uDC47 steps between \uD835\uDC61 and \uD835\uDC61 + \uD835\uDC47 , then backtracks by \uD835\uDC47 steps and collects all gradients in this interval. Otherwise, truncated BPTT would be \uD835\uDC47 times slower, which was unacceptable for our experiments.\nremoving dependencies at distances longer than the truncation parameter, we expect Kalman NoBackTrack to learn better models on datasets presenting long term correlations.\nThe two algorithms are first compared on the synthetic music dataset, with the same experimental setup as above, for the same amount of time, with a learning rate \uD835\uDF02\uD835\uDC61 = 1/ √ \uD835\uDC61 for truncated BPTT and \uD835\uDEFE\uD835\uDC61 = 1/ √ \uD835\uDC61 for Kalman NoBackTrack.14 The results are shown in Figure 4. On this example, truncated BPTT perfoms better than Kalman NoBackTrack, even though the two algorithms display broadly comparable performance. Noticeably, RTRL and truncated BPTT are roughly on par here, with truncated BPTT slightly outperforming RTRL in the end: apparently, maintaining long term dependencies in gradient calculations does not improve learning in this synthetic music example.\nNext, to compare NoBackTrack and truncated BPTT on their specific ability to learn precise middle and long term dependencies, we present experiments on the \uD835\uDC4E\uD835\uDC5B\uD835\uDC4F\uD835\uDC5B example. This will clearly illustrate the biased nature of the gradients computed by truncated BPTT.\n14These learning rates have different meanings for Kalman NoBackTrack and truncated BPTT, and are not directly comparable.\nThe \uD835\uDC4E\uD835\uDC5B\uD835\uDC4F\uD835\uDC5B[\uD835\uDC58;\uD835\uDC59] dataset is synthesized by sequentially picking a number \uD835\uDC5B between \uD835\uDC58 and \uD835\uDC59 uniformly at random, then outputting a series of \uD835\uDC5B \uD835\uDC4E’s followed by a line break, then \uD835\uDC5B \uD835\uDC4F’s and another line break. The true entropy rate is log2(\uD835\uDC59−\uD835\uDC58+1)\uD835\uDC59+\uD835\uDC58+2 in this example.\n15 A roughly 106 character long input sequence was synthesized, using [\uD835\uDC58; \uD835\uDC59] = [1; 32].\nAs standard RNN models do not seem to be able to deal with this example, whatever the training algorithm, we used a leaky RNN16 as presented in Section 1.3, again with 20 fully connected units. All the algorithms used a learning rate of 1/ √ \uD835\uDC61. The results are reported on Figure 5, which also includes the entropy rate of the exact \uD835\uDC4E\uD835\uDC5B\uD835\uDC4F\uD835\uDC5B model and the (twice larger) entropy rate of an \uD835\uDC4E\uD835\uDC5B\uD835\uDC4F\uD835\uDC5D model with independent \uD835\uDC5B and \uD835\uDC5D.\nKalman NoBackTrack clearly outperforms truncated BPTT on this dataset. This was to be expected, as the typical time range of the temporal dependencies exceeds the truncation range for BPTT, so that the\n15Indeed, log2(\uD835\uDC59 − \uD835\uDC58 + 1) bits are needed to encode the value of \uD835\uDC5B in each \uD835\uDC4E\uD835\uDC5B\uD835\uDC4F\uD835\uDC5B block (this is the entropy of a uniform distribution on {\uD835\uDC58, . . . , \uD835\uDC59}), and the average value of \uD835\uDC5B is (\uD835\uDC58 + \uD835\uDC59)/2 so that the average length of an \uD835\uDC4E\uD835\uDC5B\uD835\uDC4F\uD835\uDC5B block, including the two newline symbols, is 2× (\uD835\uDC59 + \uD835\uDC58)/2 + 2.\n16The parameter \uD835\uDEFC of the LRNN can be learned, but this sometimes produces numerical instabilities unless cumbersome changes of variables are introduced. We just initialized \uD835\uDEFC to a random value separately for each unit and kept it fixed.\napproximated gradients computed by truncated BPTT are significantly biased.\nKeeping track of the long term dependencies is key here, and RTRL outperforms all the algorithms epochwise, though it is still penalized by its high complexity. Truncated BPTT is unable to learn the full dependencies between \uD835\uDC4E’s and \uD835\uDC4F’s, and ends up closer to the entropy of an \uD835\uDC4E\uD835\uDC5B\uD835\uDC4F\uD835\uDC5D model with independent values of \uD835\uDC5B and \uD835\uDC5D (presumably, it still manages to learn the \uD835\uDC4E\uD835\uDC5B\uD835\uDC4F\uD835\uDC5B blocks where \uD835\uDC5B is short). At some point the learning curve of truncated BPTT appears not to decrease anymore and even goes slightly up, which is consistent with a biased gradient estimate.\nOn the other hand, Kalman NoBackTrack seems to be mostly successful in learning the dependencies. This is confirmed by visual inspection of the output of the learned model. The small remaining gap between the true model and the learned model could be related to incomplete training, or to an imperfect modelling of the exact uniform law for \uD835\uDC5B ∈ [\uD835\uDC58; \uD835\uDC59].\nFinally, we report performance of truncated BPTT and Kalman NoBackTrack on Shakespeare’s works. The same 20-unit RNN model is used, again with all algorithms run for the same amount of time using the same learning rate 1/ √ \uD835\uDC61. The curves obtained are displayed in Figure 6.\nOn this example, RTRL, truncated BPTT, and Kalman NoBackTrack\nwith various ranks all have a similar performance; it is not clear whether the differences on Figure 6 are statistically significant. This proves, once more, that the stochasticity and rank reduction inherent to NoBackTrack are not detrimental to learning, and allow it to keep up with exact gradient algorithms.\nAll RNN algorithms have a significantly worse performance than CTW on this example, thus proving that a 20-unit RNN does not accurately model Shakespeare’s works.\nConclusion. We have introduced an algorithm that computes a stochastic, provably unbiased estimate of the derivative of the current state of a dynamical system with respect to its parameters, in a fully online fashion. For recurrent neural networks, the computational cost of this algorithm is comparable to that of running the network itself. Previously known algorithms were either not fully online or had a significantly higher computational cost.\nIn our experiments, this algorithm appears as a practical alternative to truncated backpropagation through time, especially in its Kalman version, while the Euclidean version requires smaller learning rates. The (unbiased) noise and rank reduction introduced in the gradient approximation do not appear to prevent learning. The interest of NoBackTrack with respect to truncated BPTT depends on the situation at hand, especially on the scale of time dependencies in the data (which results in biased gradient estimates for BPTT), and on whether the storage of past states and past data required by truncated BPTT is acceptable or not.\nAcknowledgments. The authors would like to thank Hugo Larochelle for his helpful questions that resulted in several clarifications of the text.\nA Variance of the rank-one trick Keep the notation of Proposition 1 and let ‖·‖ be a Euclidean norm on the vector space in which the \uD835\uDC63\uD835\uDC56 and \uD835\uDC64\uD835\uDC56 live.\nTo measure the variance of \uD835\uDC34 we use the Hilbert–Schmidt norm ⃦⃦ \uD835\uDC34 ⃦⃦2\nHS := Tr(\uD835\uDC34⊤\uD835\uDC34). This norm satisfies ⃦⃦ \uD835\uDC63\uD835\uDC64⊤⃦⃦ HS = ‖\uD835\uDC63‖ ‖\uD835\uDC64‖, and ⟨︀ \uD835\uDC631\uD835\uDC64 ⊤ 1 | \uD835\uDC632\uD835\uDC64⊤2 ⟩︀ HS = ⟨ \uD835\uDC631 | \uD835\uDC632 ⟩ ⟨\uD835\uDC641 | \uD835\uDC642 ⟩ for the associated scalar product. Let us evaluate the variance of \uD835\uDC34 in this norm. Since Var \uD835\uDC34 = E ⃦⃦ \uD835\uDC34 ⃦⃦2 HS− ⃦⃦ E\uD835\uDC34 ⃦⃦2\nHS and E\uD835\uDC34 = \uD835\uDC34 is fixed, it is enough to evaluate the second moment E ⃦⃦ \uD835\uDC34 ⃦⃦2\nHS. We claim that\nE ⃦⃦ \uD835\uDC34 ⃦⃦2 HS = ( ∑︁\n\uD835\uDC56\n‖\uD835\uDC63\uD835\uDC56‖2)( ∑︁\n\uD835\uDC57\n‖\uD835\uDC64\uD835\uDC57‖2) + 2 ∑︁\n\uD835\uDC56 ∑︁ \uD835\uDC57 ̸=\uD835\uDC56 ⟨ \uD835\uDC63\uD835\uDC56 | \uD835\uDC63\uD835\uDC57 ⟩ ⟨\uD835\uDC64\uD835\uDC56 | \uD835\uDC64\uD835\uDC57 ⟩ (49)\nIndeed, \uD835\uDC34 = ∑︀\n\uD835\uDC56\uD835\uDC57 \uD835\uDF00\uD835\uDC56\uD835\uDF00\uD835\uDC57\uD835\uDC63\uD835\uDC56\uD835\uDC64 ⊤ \uD835\uDC57 so, by bilinearity of the Hilbert–Schmidt scalar product,\nE ⃦⃦ \uD835\uDC34 ⃦⃦2 HS = E ⟨︀ \uD835\uDC34 | \uD835\uDC34 ⟩︀ HS = E ∑︁ \uD835\uDC56\uD835\uDC57\uD835\uDC58\uD835\uDC59 \uD835\uDF00\uD835\uDC56\uD835\uDF00\uD835\uDC57\uD835\uDF00\uD835\uDC58\uD835\uDF00\uD835\uDC59 ⟨ \uD835\uDC63\uD835\uDC56 | \uD835\uDC63\uD835\uDC57 ⟩ ⟨\uD835\uDC64\uD835\uDC58 | \uD835\uDC64\uD835\uDC59 ⟩ (50)\nSince E\uD835\uDF00\uD835\uDC56 = 0 and E(\uD835\uDF00\uD835\uDC56\uD835\uDF00\uD835\uDC57) = 0 for \uD835\uDC56 ̸= \uD835\uDC57, the only cases to consider are: 1. \uD835\uDC56 = \uD835\uDC57 and \uD835\uDC58 = \uD835\uDC59 and \uD835\uDC56 ̸= \uD835\uDC58: contribution ∑︀\n\uD835\uDC56 ∑︀ \uD835\uDC58 ̸=\uD835\uDC56 ‖\uD835\uDC63\uD835\uDC56‖ 2 ‖\uD835\uDC64\uD835\uDC58‖2\n2. \uD835\uDC56 = \uD835\uDC58 and \uD835\uDC57 = \uD835\uDC59 and \uD835\uDC56 ̸= \uD835\uDC57: contribution ∑︀\n\uD835\uDC56 ∑︀ \uD835\uDC57 ̸=\uD835\uDC56 ⟨ \uD835\uDC63\uD835\uDC56 | \uD835\uDC63\uD835\uDC57 ⟩ ⟨\uD835\uDC64\uD835\uDC56 | \uD835\uDC64\uD835\uDC57 ⟩\n3. \uD835\uDC56 = \uD835\uDC59 and \uD835\uDC57 = \uD835\uDC58 and \uD835\uDC56 ̸= \uD835\uDC57: same contribution as the previous one 4. \uD835\uDC56 = \uD835\uDC57 = \uD835\uDC58 = \uD835\uDC59: contribution ∑︀\n\uD835\uDC56 ‖\uD835\uDC63\uD835\uDC56‖ 2 ‖\uD835\uDC64\uD835\uDC56‖2\n5. all other cases contribute 0. The first and fourth contributions add up to ( ∑︀ \uD835\uDC56 ‖\uD835\uDC63\uD835\uDC56‖ 2)( ∑︀ \uD835\uDC58 ‖\uD835\uDC64\uD835\uDC58‖ 2). This proves (49). Let us minimize variance over the degrees of freedom given by \uD835\uDC63\uD835\uDC56\uD835\uDC64⊤\uD835\uDC56 = (\uD835\uDF0C\uD835\uDC56\uD835\uDC63\uD835\uDC56)(\uD835\uDC64\uD835\uDC56/\uD835\uDF0C\uD835\uDC56)⊤.\n\uD835\uDF0C\uD835\uDC56 does not change the last contribution to E ⃦⃦ \uD835\uDC34 ⃦⃦2\nHS in (49), neither does it change the expectation E\uD835\uDC34 = \uD835\uDC34, so to minimize the variance we only have to minimize the first term ( ∑︀ \uD835\uDC56 ‖\uD835\uDC63\uD835\uDC56‖ 2)( ∑︀ \uD835\uDC58 ‖\uD835\uDC64\uD835\uDC58‖ 2). Applying the scaling, this term becomes\n( ∑︁\n\uD835\uDC56\n‖\uD835\uDC63\uD835\uDC56‖2 \uD835\uDF0C2\uD835\uDC56 )( ∑︁\n\uD835\uDC58\n‖\uD835\uDC64\uD835\uDC58‖2 /\uD835\uDF0C2\uD835\uDC58) (51)\nand, by differentiation with respect to a single \uD835\uDF0C\uD835\uDC56, one checks that this is minimal for \uD835\uDF0C\uD835\uDC56 ∝ √︀ ‖\uD835\uDC64\uD835\uDC56‖ / ‖\uD835\uDC63\uD835\uDC56‖ (52)\n(mutliplying all \uD835\uDF0C\uD835\uDC56’s by a common factor does not change the result). So, after optimal scaling,\n\uD835\uDC34 = (︃∑︁\n\uD835\uDC56\n\uD835\uDF00\uD835\uDC56\uD835\uDC63\uD835\uDC56 √︀ ‖\uD835\uDC64\uD835\uDC56‖ / ‖\uD835\uDC63\uD835\uDC56‖ )︃ ⊗ (︃∑︁ \uD835\uDC56 \uD835\uDF00\uD835\uDC56\uD835\uDC64\uD835\uDC56 √︀ ‖\uD835\uDC63\uD835\uDC56‖ / ‖\uD835\uDC64\uD835\uDC56‖ )︃ (53)\nConsequently, after scaling, the first term in the variance of \uD835\uDC34 in (49) becomes ( ∑︀\n\uD835\uDC56 ‖\uD835\uDC63\uD835\uDC56‖ ‖\uD835\uDC64\uD835\uDC56‖)2. The second term in (49) does not change. Thus, after optimal scaling we find\nE ⃦⃦ \uD835\uDC34 ⃦⃦2 HS = (︃∑︁\n\uD835\uDC56\n‖\uD835\uDC63\uD835\uDC56‖ ‖\uD835\uDC64\uD835\uDC56‖ )︃2 + 2\n∑︁ \uD835\uDC56 ∑︁ \uD835\uDC57 ̸=\uD835\uDC56 ⟨ \uD835\uDC63\uD835\uDC56 | \uD835\uDC63\uD835\uDC57 ⟩ ⟨\uD835\uDC64\uD835\uDC56 | \uD835\uDC64\uD835\uDC57 ⟩ (54)\nTo obtain the variance of \uD835\uDC34, we just subtract the square norm of E\uD835\uDC34 = \uD835\uDC34, which is\n‖\uD835\uDC34‖2HS = ⃦⃦⃦⃦ ⃦∑︁\n\uD835\uDC56\n\uD835\uDC63\uD835\uDC56\uD835\uDC64 ⊤ \uD835\uDC56 ⃦⃦⃦⃦ ⃦ 2\nHS\n= ∑︁\n\uD835\uDC56\n⃦⃦ \uD835\uDC63\uD835\uDC56\uD835\uDC64 ⊤ \uD835\uDC56 ⃦⃦2 HS + ∑︁ \uD835\uDC56 ∑︁ \uD835\uDC57 ̸=\uD835\uDC56 ⟨︀ \uD835\uDC63\uD835\uDC56\uD835\uDC64 ⊤ \uD835\uDC56 | \uD835\uDC63\uD835\uDC57\uD835\uDC64⊤\uD835\uDC57 ⟩︀ HS (55)\n(by bilinearity of the Hilbert–Schmidt scalar product)\n= ∑︁\n\uD835\uDC56\n‖\uD835\uDC63\uD835\uDC56‖2 ‖\uD835\uDC64\uD835\uDC56‖2 + ∑︁\n\uD835\uDC56 ∑︁ \uD835\uDC57 ̸=\uD835\uDC56 ⟨ \uD835\uDC63\uD835\uDC56 | \uD835\uDC63\uD835\uDC57 ⟩ ⟨\uD835\uDC64\uD835\uDC56 | \uD835\uDC64\uD835\uDC57 ⟩ (56)\nThis yields, after optimal scaling,\nVar \uD835\uDC34 = (︃∑︁\n\uD835\uDC56\n‖\uD835\uDC63\uD835\uDC56‖ ‖\uD835\uDC64\uD835\uDC56‖ )︃2 − ∑︁\n\uD835\uDC56\n‖\uD835\uDC63\uD835\uDC56‖2 ‖\uD835\uDC64\uD835\uDC56‖2 + ∑︁\n\uD835\uDC56 ∑︁ \uD835\uDC57 ̸=\uD835\uDC56 ⟨ \uD835\uDC63\uD835\uDC56 | \uD835\uDC63\uD835\uDC57 ⟩ ⟨\uD835\uDC64\uD835\uDC56 | \uD835\uDC64\uD835\uDC57 ⟩ (57)\n= ∑︁\n\uD835\uDC56 ∑︁ \uD835\uDC57 ̸=\uD835\uDC56 ‖\uD835\uDC63\uD835\uDC56‖ ‖\uD835\uDC63\uD835\uDC57‖ ‖\uD835\uDC64\uD835\uDC56‖ ‖\uD835\uDC64\uD835\uDC57‖+ ⟨ \uD835\uDC63\uD835\uDC56 | \uD835\uDC63\uD835\uDC57 ⟩ ⟨\uD835\uDC64\uD835\uDC56 | \uD835\uDC64\uD835\uDC57 ⟩ (58)\nB Invariant norms derived from the Kalman covariance\nAlgorithm 2 is built to offer invariance properties (a Kalman filter over a variable \uD835\uDF03 is invariant by affine reparameterization of \uD835\uDF03, for instance). However, this only holds if the norms ‖\uD835\uDC63‖, ‖\uD835̄\uDC64‖, ‖\uD835\uDC63\uD835\uDC56‖, ‖\uD835\uDC64\uD835\uDC56‖, used to compute the scaling factors \uD835\uDF0C = √︀ ‖\uD835̄\uDC64‖ / ‖\uD835\uDC63‖\nand \uD835\uDF0C\uD835\uDC56 = √︀ ‖\uD835\uDC64\uD835\uDC56‖ / ‖\uD835\uDC52\uD835\uDC56‖, are themselves reparameterization-invariant.\nThis can be achieved if we decide to choose the scalings \uD835\uDF0C as to minimize the variance of \uD835̃\uDC3A computed in the (Mahalanobis) norm defined by the covariance matrix of \uD835\uDF03 and of ℎ appearing in the Kalman filter.\nLet \uD835\uDC36\uD835\uDF03 be the covariance matrix of \uD835\uDF03 obtained in the Kalman filter; in Algorithm 2, \uD835\uDC36\uD835\uDF03 is approximated by \uD835\uDC36\uD835\uDF03 ≈ \uD835\uDC3D−1\uD835\uDF03 .\nAny linear form on \uD835\uDF03, such as \uD835̄\uDC64 and \uD835\uDC64\uD835\uDC56, can be given a norm by\n‖\uD835̄\uDC64‖2 := \uD835̄\uDC64⊤\uD835\uDC36\uD835\uDF03\uD835̄\uDC64 ≈ \uD835̄\uDC64⊤\uD835\uDC3D−1\uD835\uDF03 \uD835̄\uDC64 (59)\nand likewise for \uD835\uDC64\uD835\uDC56. This norm is invariant under \uD835\uDF03-reparameterization. Given the covariance \uD835\uDC36\uD835\uDF03 of \uD835\uDF03 and the dependency \uD835\uDC3A = \uD835\uDF15ℎ\uD835\uDF15\uD835\uDF03 of ℎ with respect to \uD835\uDF03, the covariance of ℎ is \uD835\uDC36ℎ := \uD835\uDC3A\uD835\uDC36\uD835\uDF03\uD835\uDC3A⊤ (60) and its inverse \uD835\uDC3Dℎ := \uD835\uDC36−1ℎ can be used to define a norm for a tangent vector \uD835\uDC63 at state ℎ via ‖\uD835\uDC63‖2 := \uD835\uDC63⊤\uD835\uDC3Dℎ\uD835\uDC63 (61) which is also reparametrization-invariant. (We use \uD835\uDC3D−1\uD835\uDF03 for the norm of \uD835\uDC64 and \uD835\uDC3Dℎ for the norm of \uD835\uDC63 because \uD835\uDC63 is a tangent vector (covariant) at point ℎ, while \uD835\uDC64 is a linear form (contravariant) at point \uD835\uDF03.)\nHowever, handling of full covariance matrices would be too costly. In Algorithm 2, the inverse covariance \uD835\uDC3D\uD835\uDF03 of \uD835\uDF03 is already an approximation (diagonal, quasi-diagonal...) via MatrixReduce. Moreover, here we only have access to an approximation \uD835̃\uDC3A of \uD835\uDC3A. Thus, we simply replace \uD835\uDC3A with \uD835̃\uDC3A in the definition of \uD835\uDC36ℎ, and use a diagonal reduction. This leads to \uD835\uDC36ℎ ≈ Diag(\uD835̃\uDC3A\uD835\uDC3D−1\uD835\uDF03 \uD835̃\uDC3A⊤) and\n\uD835\uDC3Dℎ ≈ (︀ Diag(\uD835̃\uDC3A\uD835\uDC3D−1\uD835\uDF03 \uD835̃\uDC3A ⊤) )︀−1 (62)\nwhere as usual \uD835̃\uDC3A is the gradient approximation given by (9). The diagonal reduction is necessary if \uD835̃\uDC3A is low-rank, since \uD835̃\uDC3A\uD835\uDC3D−1\uD835\uDF03 \uD835̃\uDC3A⊤ will be low-rank as well, and thus non-invertible. Then the scaling factors \uD835\uDF0C and \uD835\uDF0C\uD835\uDC56 can finally be computed as\n\uD835\uDF0C = √︃ ‖\uD835̄\uDC64‖ ‖\uD835\uDC63‖ = (\uD835̄\uDC64⊤\uD835\uDC3D−1\uD835\uDF03 \uD835̄\uDC64)1/4(︀∑︀\n\uD835\uDC56(\uD835̃\uDC3A\uD835\uDC3D −1 \uD835\uDF03 \uD835̃\uDC3A ⊤)−1\uD835\uDC56\uD835\uDC56 \uD835\uDC632\uD835\uDC56 )︀1/4 (63)\nand\n\uD835\uDF0C\uD835\uDC56 = √︃ ‖\uD835\uDC64\uD835\uDC56‖ ‖\uD835\uDC52\uD835\uDC56‖ = (\uD835\uDC64⊤\uD835\uDC56\uD835\uDC3D−1\uD835\uDF03 \uD835\uDC64\uD835\uDC56)1/4(︀\n(\uD835̃\uDC3A\uD835\uDC3D−1\uD835\uDF03 \uD835̃\uDC3A⊤) −1 \uD835\uDC56\uD835\uDC56 )︀1/4 (64) The particular structure of \uD835\uDC3D\uD835\uDF03 (if approximated by, e.g., a block-diagonal matrix) and of \uD835̃\uDC3A = \uD835\uDC63\uD835̄\uDC64⊤+ ∑︀ \uD835\uDC56 \uD835\uDC52\uD835\uDC56\uD835\uDC64 ⊤ \uD835\uDC56 make these computations efficient.\nNote that even with the approximations above, \uD835̃\uDC3A is still an unbiased estimate of \uD835\uDC3A. Indeed, any choice of \uD835\uDF0C has this property; we are simply approximating the optimal \uD835\uDF0C which minimizes the variance of \uD835̃\uDC3A.\nIn practice, small regularization terms are included in the denominator of every division and inversion to avoid numerical overflow."
    } ],
    "references" : [ {
      "title" : "On prediction using variable order markov models",
      "author" : [ "Ron Begleiter", "Ran El-Yaniv", "Golan Yona" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Begleiter et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Begleiter et al\\.",
      "year" : 2004
    }, {
      "title" : "Online EM algorithm for hidden Markov models",
      "author" : [ "Olivier Cappé" ],
      "venue" : "J. Comput. Graph. Statist.,",
      "citeRegEx" : "Cappé.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cappé.",
      "year" : 2011
    }, {
      "title" : "Kalman filtering and neural networks",
      "author" : [ "Simon Haykin" ],
      "venue" : null,
      "citeRegEx" : "Haykin.,? \\Q2004\\E",
      "shortCiteRegEx" : "Haykin.",
      "year" : 2004
    }, {
      "title" : "Tutorial on training recurrent neural networks, covering BPTT, RTRL, EKF and the ‘‘echo state network’",
      "author" : [ "Herbert Jaeger" ],
      "venue" : "Technical Report 159,",
      "citeRegEx" : "Jaeger.,? \\Q2002\\E",
      "shortCiteRegEx" : "Jaeger.",
      "year" : 2002
    }, {
      "title" : "Riemannian metrics for neural networks I: feedforward networks",
      "author" : [ "Yann Ollivier" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "Ollivier.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ollivier.",
      "year" : 2015
    }, {
      "title" : "Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences",
      "author" : [ "Yann Ollivier" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "Ollivier.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ollivier.",
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "We introduce the “NoBackTrack” algorithm to train the parameters of dynamical systems such as recurrent neural networks. This algorithm works in an online, memoryless setting, thus requiring no backpropagation through time, and is scalable, avoiding the large computational and memory cost of maintaining the full gradient of the current state with respect to the parameters. The algorithm essentially maintains, at each time, a single search direction in parameter space. The evolution of this search direction is partly stochastic and is constructed in such a way to provide, at every time, an unbiased random estimate of the gradient of the loss function with respect to the parameters. Because the gradient estimate is unbiased, on average over time the parameter is updated as it should. The resulting gradient estimate can then be fed to a lightweight Kalman-like filter to yield an improved algorithm. For recurrent neural networks, the resulting algorithms scale linearly with the number of parameters. Small-scale experiments confirm the suitability of the approach, showing that the stochastic approximation of the gradient introduced in the algorithm is not detrimental to learning. In particular, the Kalman-like version of NoBackTrack is superior to backpropagation through time (BPTT) when the time span of dependencies in the data is longer than the truncation span for BPTT. Consider the problem of training the parameters θ of a dynamical system over a variable h ∈ Rn subjected to the evolution equation h(t + 1) = f(h(t), x(t), θ) (1) where f is a fixed function of h and of an input signal x(t), depending on parameters θ. The goal is online minimization of a loss function ∑︀ t lt(y(t), y(t)) between a desired output y(t) at time t and a prediction1 y(t) = Y (h(t), φ) (2) 1The prediction y may not live in the same set as y. Often, y encodes a probability distribution over the possible values of y, and the loss is the logarithmic loss l = − log py(y). 1 ar X iv :1 50 7. 07 68 0v 2 [ cs .N E ] 2 0 N ov 2 01 5 computed from h(t) and additional parameters φ. A typical example we have in mind is a recurrent neural network, with activities ai(t) := sigm(hi(t)) and evolution equation hi(t + 1) = bi + ∑︀ k rkixk(t) + ∑︀ j Wjiaj(t), with parameter θ = (bi, rki, Wji)i,j,k. If the full target sequence y(t)t∈[0;T ] is known in advance, one strategy is to use the backpropagation through time algorithm (BPTT, see e.g. [Jae02]) to compute the gradient of the total loss LT := ∑︀T t=0 lt with respect to the parameters θ and φ, and use gradient descent on θ and φ. However, if the data y(t + 1) arrive one at a time in a streaming fashion, backpropagation through time would require making a full backward computation from time t + 1 to time 0 after each new data point becomes available. This results in an Ω(t2) complexity and in the necessity to store past states, inputs, and outputs. A possible strategy is to only backtrack by a finite number of time steps [Jae02] rather than going back all the way to t = 0. But this provides biased gradient estimates and may impair detection of time dependencies with a longer range than the backtracking time range. By contrast, methods which are fully online are typically not scalable. One strategy, known as real-time recurrent learning (RTRL) in the recurrent network community,2 maintains the full gradient of the current state with respect to the parameters: G(t) := ∂h(t) ∂θ (3) which satisfies the evolution equation G(t + 1) = ∂f(h(t), x(t), θ) ∂h G(t) + ∂f(h(t), x(t), θ) ∂θ (4) (by differentiating (1)). Knowing G(t) allows to minimize the loss via a stochastic gradient descent on the parameters θ, namely,3 θ ← θ − ηt ∂lt ∂θ ⊤ (5) with learning rate ηt. Indeed, the latter quantity can be computed from Gt and from the way the predictions depend on h(t), via the chain rule ∂lt ∂θ = ∂lt(Y (h(t), φ), y(t)) ∂h G(t) (6) However, the full gradient G(t) is an object of dimension dim h × dim θ. This prevents computing or even storing G(t) for moderately largedimensional dynamical systems, such as recurrent neural networks. 2This amounts to applying forward automatic differentiation. 3We use the standard convention for Jacobian matrices, namely, ∂x/∂y is the matrix with entries ∂xi/∂yj . Then the chain rule writes ∂x ∂y ∂y ∂z = ∂x ∂z . This makes the derivatives ∂lt/∂θ into row vectors so that gradient descent is θ ← θ − (∂lt/∂θ).",
    "creator" : "LaTeX with hyperref package"
  }
}