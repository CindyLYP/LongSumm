<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/cindy/tmp/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty Propagation in Data Processing Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Manousakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Uncertainty Propagation in Data Processing Systems. In Proceedings of ACM Symposium on Cloud Computing</orgName>
								<address>
									<settlement>Carlsbad</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Íñigo</forename><surname>Goiri</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Uncertainty Propagation in Data Processing Systems. In Proceedings of ACM Symposium on Cloud Computing</orgName>
								<address>
									<settlement>Carlsbad</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
							<email>ricardob@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Uncertainty Propagation in Data Processing Systems. In Proceedings of ACM Symposium on Cloud Computing</orgName>
								<address>
									<settlement>Carlsbad</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Rigo</surname></persName>
							<email>sandro@ic.unicamp.br</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Campinas</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Uncertainty Propagation in Data Processing Systems. In Proceedings of ACM Symposium on Cloud Computing</orgName>
								<address>
									<settlement>Carlsbad</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><forename type="middle">D 2018</forename><surname>Nguyen</surname></persName>
							<email>tdnguyen@cs.rutgers.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Uncertainty Propagation in Data Processing Systems. In Proceedings of ACM Symposium on Cloud Computing</orgName>
								<address>
									<settlement>Carlsbad</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty Propagation in Data Processing Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3267809.3267833</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2021-01-31T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Uncertainty Propagation, DAG Data Processing</keywords>
			</textClass>
			<abstract>
				<p>We are seeing an explosion of uncertain data-i.e., data that is more properly represented by probability distributions or estimated values with error bounds rather than exact values-from sensors in IoT, sampling-based approximate computations and machine learning algorithms. In many cases, performing computations on uncertain data as if it were exact leads to incorrect results. Unfortunately, developing applications for processing uncertain data is a major challenge from both the mathematical and performance perspectives. This paper proposes and evaluates an approach for tackling this challenge in DAG-based data processing systems. We present a framework for uncertainty propagation (UP) that allows developers to modify precise implementations of DAG nodes to process uncertain inputs with modest effort. We implement this framework in a system called UP-MapReduce, and use it to modify ten applications, including AI/ML, image processing and trend analysis applications to process uncertain data. Our evaluation shows that UP-MapReduce propagates uncertainties with high accuracy and, in many cases, low performance overheads. For example, a social network trend analysis application that combines data sampling with UP can reduce execution time by 2.3x when the user can tolerate a maximum relative error of 5% in the final answer. These results demonstrate that our UP framework presents a compelling approach for handling uncertain data in DAG processing. CCS CONCEPTS • Computer systems organization → Architectures;</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Data is being produced and collected at a tremendous pace. The need to process this vast amount of data has led to the design and deployment of data processing systems such as MapReduce, Spark and Scope <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. These frameworks typically allow data processing applications to be expressed as directed acyclic graphs (DAGs) of side-effect free computation nodes, with data flowing through the edges for processing. The frameworks then run applications on clusters of servers, transparently handling issues such as task scheduling, data movement, and fault tolerance.</p><p>At the same time, there is an urgent need for processing an exploding body of data with uncertainties <ref type="bibr" target="#b3">[4]</ref>. For example, data collected using sensors are always estimates that have uncertaintiesthe differences between the estimated and true values-due to sensor inaccuracies. Data uncertainties also arise in many other contexts, including probabilistic modeling <ref type="bibr" target="#b9">[10]</ref>, machine learning <ref type="bibr" target="#b25">[26]</ref>, approximate storage <ref type="bibr" target="#b28">[29]</ref>, and the use of sampling-based approximation that produce estimated outputs with error bounds <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>. For many applications, uncertain data should be represented as probability distributions or estimated values with error bounds rather than exact values. Failure to properly account for this uncertainty may lead to incorrect results. For example, Bornholt et al. have shown that computing speeds from recorded GPS positions can lead to absurd values (e.g., walking speeds above 30mph) when ignoring uncertainties in the recordings <ref type="bibr" target="#b3">[4]</ref>. Unfortunately, developing applications for processing uncertain data is a major challenge from both the mathematical and performance perspectives. Thus, in this paper, we propose and evaluate a general framework that significantly eases this challenging task. Embedding such a framework in systems such as MapReduce and Spark will make it easily available to many developers working in many application domains.</p><p>Our framework is based on techniques that allow programmers to modify precise implementations of DAG computation nodes to handle uncertain inputs with modest effort. Uncertainties can then be propagated locally across each node of the DAG from the point where they are first introduced to the final outputs of the computation. More specifically, we use Differential Analysis (DA) <ref type="bibr" target="#b2">[3]</ref> to propagate uncertainties through DAG nodes that are continuous and differentiable functions. For semi-continuous functions, we propagate uncertainties through a combination of DA and Monte Carlo simulation, where our framework automatically selects the appropriate method based on the input distributions and the locations of function discontinuities. For all other function types, we use Monte Carlo simulation.  As an example of how a developer uses our framework, suppose a company needs to run a revenue prediction model implemented by a two-node logical DAG <ref type="bibr" target="#b0">1</ref> shown in <ref type="figure">Figure 1</ref>(a). The first node approximates the number of customers belonging to different age groups in a database using BlinkDB <ref type="bibr" target="#b1">[2]</ref>. The second node then computes the revenue as a weighted average, with the (uncertain) weights representing the predicted revenue per customer in a given age group. While the approximation can significantly reduce the execution time of the first node, it produces estimates with uncertainties (error bounds), rather than precise values. A developer can use our proposed framework to handle these uncertainties in the second node by providing the derivatives for the weighted average, which are essentially just the weights, with very few code changes to the precise version. This small amount of additional work will allow the answer to be computed as a distribution rather than an exact value that gives a misleading impression of precision. In particular, a precise answer, e.g., red line in <ref type="figure">Figure 1</ref>(c), may predict high revenue leading to profit while ignoring the left side of the distribution in <ref type="figure">Figure 1</ref>(c), which indicates a significant possibility of low revenue leading to an overall loss. Ignoring this possibility can be dangerous if the company is risk-averse.</p><p>We implement the proposed framework in UP-MapReduce, an extension of the Hadoop MapReduce, to handle uncertainty propagation (UP). UP-MapReduce allows programmers to develop applications with UP in much the same way as their precise counterparts. Added efforts come in the form of selecting the appropriate uncertain Mapper and Reducer classes provided by UP-MapReduce and respecting some required constraints on code structures (Section 5). Developers can optionally provide closed-form derivatives for DAG nodes that implement UP with DA to enhance performance.</p><p>We then leverage UP-MapReduce to build a toolbox of operations (e.g., sum, multiply, logarithm) on uncertain data and modify ten applications, including AI/ML, image processing, trend analysis, As explained in Section 4, small logical DAGs will often map to extremely large execution DAGs with thousands of execution nodes running on large server clusters when processing large data sets. and model construction applications, to process uncertain data. Our experience shows that UP-MapReduce is easy to use. Running two of these applications on real data sets demonstrates the tremendous potential for combining sampling-based approximation (early in the DAG) with UP to reduce execution time while properly propagating the introduced uncertainties to the final outputs. This propagation allows users to intelligently trade off accuracy for execution time. For example, in one application, execution time can be reduced by 2.3x if the user can tolerate errors of up to 5%. Further, in one of the two applications, the original data set is a sample of network probes and so any computation on this sample necessarily has to deal with uncertainties. UP-MapReduce allows developers to easily tackle these uncertainties.</p><p>We also perform extensive sensitivity analyses on small to large execution DAGs (ranging up to tens of thousands of nodes), using eight of the applications with synthetic data, which allows us to adjust various input characteristics. Specifically, we explore the impact of UP on the magnitudes of uncertainties (e.g., whether uncertainties become worse after propagation), the accuracy of our UP techniques, overheads of UP, and scalability. Our results show that our UP techniques are highly accurate in most cases. Furthermore, the performance overheads of UP using DA are very low -average of 6% performance degradation -when closed-form derivatives are provided. Performance overheads are more significant when using DA with numerical differentiation or Monte Carlo simulation as input size increases, but this performance impact can be reduced by adding computation resources. Recall that these overheads arise from the need to process uncertain data instead of exact values. Finally, our results demonstrate that UP-MapReduce scales well to a cluster with 512 servers.</p><p>In summary, our contributions include: (1) identifying existing theories appropriate for UP and showing how to apply them to DAG-based data processing frameworks, (2) designing and implementing our proposed UP approach in a MapReduce framework called UP-MapReduce, (3) implementing a suite of data processing applications to explore the accuracy, performance, and scalability of UP-MapReduce, and <ref type="bibr" target="#b3">(4)</ref> showing that our approach is highly effective in many scenarios, allowing applications to efficiently account for data uncertainties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>In this section we first motivate the necessity for UP by presenting a (non-exhaustive) list of common uncertainty sources where UP is required if the data uncertainties are not to be ignored. We then proceed to discuss related work and in particular recent approximate methods that generate uncertainty as byproducts of the approximation. Finally, we review previous work in uncertainty estimation and belief propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sources of Uncertainty.</head><p>Collecting data from imprecise instruments such as temperature, position or other analog sensors often introduces measurement uncertainty. In these applications, acquiring precise data is typically not an option, but it is usually possible to tune precision at the expense of resources such as more expensive sensors, higher response time or energy consumption. An example of such a trade-off is the potential for a sensor network to enter a low-power state to conserve energy at the expense of providing lower quality measurements.</p><p>Similarly, model uncertainty is introduced when computational models used in applications do not precisely describe physical phenomena. For example, in structure strength analysis, one may simulate the macroscopic impact of wind on a high-level model of a bridge structure, rather than modeling the forces on the individual molecules (which might be intractable).</p><p>Approximate computing is an emerging source of approximation uncertainty. In this setting, it may be possible for the user to tradeoff precision (how much uncertainty) against execution time and/or energy consumption. Examples include iterative refinement techniques or aggregate approximation schemes (via sampling) such as BlinkDB <ref type="bibr" target="#b1">[2]</ref> and ApproxHadoop <ref type="bibr" target="#b10">[11]</ref>. This is a particularly interesting scenario since execution time savings achieved via approximation may be offset by the necessity for UP in subsequent nodes of a computation DAG. Other types of approximation-induced uncertainty include statistical estimators (i.e., from maximum likelihood or a posteriori estimation) and approximate storage <ref type="bibr" target="#b28">[29]</ref>.</p><p>Approximate computing with bounded errors. Extensive past work has been done in approximate computing with quality estimates by the systems, hardware and database communities. The purpose of approximate computing is to reduce the required resources (e.g., execution time and/or energy consumption) by relaxing the precision of the output but also providing estimates on the (uncertain) output quality -for example, the mean and variance of the output values.</p><p>Most prior works have focused on reducing the input set by sampling and/or dropping computation. For example, the database community has long considered the problem through approximate query processing. There, database systems sample the input data set and/or drop sub-queries to accelerate top-level queries with the ultimate goal of reducing response time, increasing throughput <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b37">38]</ref>, and/or even providing response time guarantees <ref type="bibr" target="#b1">[2]</ref>.</p><p>Some works identify computational blocks (at compile or runtime) that can be dropped for tunable approximation with or without accuracy estimates <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>. ApproxHadoop accelerates the computation of large-scale aggregations (i.e., sum, count, average) by combining sampling and computation drop <ref type="bibr" target="#b10">[11]</ref> while providing error estimates. Others provide energy bounds by online tuning of the approximation levels <ref type="bibr" target="#b13">[14]</ref>.</p><p>Finally, the hardware community approaches the problem by trading hardware accuracy for energy efficiency, performance and transistors. For example, Esmaeilzadeh et al. <ref type="bibr" target="#b8">[9]</ref> designed an ISA extension that provides approximate operations and proposed a micro-architecture that implements approximate functional units such as adders, multipliers, and approximate load-store units (a problem that was also tackled later by Miguel et al. <ref type="bibr" target="#b22">[23]</ref>).</p><p>Uncertainty estimation and belief propagation. Prior work has been proposed to handle the uncertainty introduced by approximate systems and to perform belief propagation where uncertainty and prior beliefs are combined to perform inference. Approximate programming, for example, seeks to design systems and programming languages that implement and bound the errors of various arithmetic and logical operators (addition, multiplication, and comparison) when handling uncertain (probabilistic) types.</p><p>For example, Uncertain&lt;T&gt; <ref type="bibr" target="#b3">[4]</ref> is a language construct that can be used to estimate the output distribution of a graph of basic operations that compose a program. Uncertain&lt;T&gt; can be used for inference as well, by using Bayesian statistics to derive the posterior distribution. Others have worked on probabilistic programming to implement type systems <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref> and compiler transformations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref> to handle uncertainty, error bounding, and inference for uncertain programs. Sampson et al. <ref type="bibr" target="#b29">[30]</ref> worked on decision making under uncertainty which is necessary to implement branches and assertions in programs. In contrast with arithmetic operations, comparison operators are more challenging, as they involve estimating the tail of the (unknown) distribution -much like our approach for UP through semi-continuous functions.</p><p>Differentiation from prior work. We differentiate from past work in uncertainty estimation as being the first to bring uncertainty propagation techniques to large-scale computational DAGs. In contrast to prior work (e.g., Uncertain &lt;T&gt; and ApproxHadoop) where uncertainty estimation is performed only for basic arithmetic and logical operations, we can handle arbitrary functions. At this high level of abstraction, new challenges arise. For example, accounting for covariances between uncertain data items may become a limiting factor in the performance and scalability of computations on uncertain data.</p><p>Our method also offers, to the best of our knowledge, the only known computationally tractable (and as our evaluation will show, potentially with low overheads) large-scale uncertainty propagation. Several other UP methods, such as polynomial chaos expansion and fast integration can also be used (in fact to estimate the actual distribution of Y instead of just computing the first two moments) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>. However, these methods are very computationally expensive, especially with increasing number of variables as noted by Lee and Chen <ref type="bibr" target="#b18">[19]</ref>. We also do not perform any inference or multi-dimensional convolutions (as in Uncertain &lt;T&gt;) which suffer from high computational complexity and limit their applicability to only a few hundred input variables per node in the execution DAG. On the contrary, we show that our approach can handle millions of input variables with relatively low overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNCERTAINTY PROPAGATION</head><p>In this section we introduce our proposed methods for handling uncertain inputs at a DAG node. Specifically, we discuss how to (approximately) compute Y = f (X), where f is an arbitrary function without side effects, representing the computation of a DAG node, X is a set of random variables representing inputs with uncertainties, and Y is a set of random variables representing outputs with uncertainties. Depending on the nature of f (continuous, semi-continuous or discrete), we leverage a set of three statistical methods to approximate the mean µ Y i and the variance σ 2 Y i for each Y i in Y. These methods are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">UP Through Continuous Functions</head><p>We use first-order Differential Analysis (DA) to approximate the first two moments of Y, i.e., mean and variance, for functions f that are continuous and differentiable <ref type="bibr" target="#b2">[3]</ref>. The general strategy is to compute Y by approximating f using its first-order Taylor series at the expected value of X. This approximation is accurate if f is X Y roughly linear around the support (in other words, neighborhood) of X; errors are being introduced otherwise. As shall be seen in Section 7.3, using the first-order Taylor series gives good accuracy for the majority of the applications we study. For simplicity, we present DA equations for a single output value Y; we refer the reader to <ref type="bibr" target="#b2">[3]</ref> for the full derivation of the multiple input, multiple output case. <ref type="bibr">Let</ref> </p><formula xml:id="formula_0">μ X 3σ X μ Y 3σ Y f ∂f ∂X (μX) f Y m Y 1 ... X n X 1 ... σ X1 σ Xn . . . σ Y1 σ Ym . . . σ Xij σ Ykl (a) (b)</formula><formula xml:id="formula_1">Y = f (X), with X = {X 1 , X 2 , ..., X n }.</formula><p>We can compute an approximationŶ of Y using the first-order Taylor series around a given point</p><formula xml:id="formula_2">X 0 = {X 0 1 , X 0 2 , ..., X 0 n } as: Y = α 0 + n i=1 α i (X i − X 0 i ) (1) α 0 = f (X 0 ) and α i = ∂ f ∂X i (X 0 )</formula><p>We then compute an approximate meanμ Y by setting</p><formula xml:id="formula_3">X 0 = µ X = {µ X 1 , µ X 2 , .</formula><p>.., µ X n } and computing the expected value ofŶ .</p><formula xml:id="formula_4">µ Y = E[Ŷ ] = E α 0 + n i=1 α i (X i − µ X i ) (2) = α 0 + n i=1 (α i E[X i ] − α i µ X i ) = α 0 + n i=1 (α i µ X i − α i µ X i ) = f (µ X )</formula><p>Analogously, we can derive an estimate of the varianceσ Y using the first-order Taylor series:</p><formula xml:id="formula_5">σ 2 Y = n i=1 α 2 i σ 2 X i + n i=1 n j=1,j i α 2 i α 2 j σ 2 X i X j (3) where σ 2 X i X j</formula><p>is the covariance of X i and X j . If we assume that the inputs are independent, so that σ X i X j = 0, i j, then Equation 3 reduces to the left summand.</p><p>We illustrate the computation of the mean and variance for the single-input, single-output case (Y = f (X )) in <ref type="figure" target="#fig_1">Figure 2</ref>(a). For the general case with multiple inputs and multiple outputs, one must also be concerned with the covariances between the outputs as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b). In general, these covariances may be nonzero. Thus, if the multiple outputs are being used as inputs to a later stage of computation as in Y = f (X), Z = д(Y), then we cannot assume that Y i and Y j , i j, in Y are independent. Rather, it would </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">UP Through Semi-continuous Functions</head><p>We can leverage the above approach for semi-continuous functions when the support of each X i in X falls mostly or entirely within a continuous and differentiable part of the function. We adopt two approaches for checking with high confidence which intervals of X lie in continuous parts. The first assumes each X i is approximately normal allowing the estimation of the support using any desired confidence interval through the corresponding covariance matrix of the input. The second approach makes no assumption about the distribution of X. It instead uses a multivariate generalization of the Chebyshev's inequality <ref type="bibr" target="#b16">[17]</ref> to bound the probability that X lies within any interval. For example, suppose we define a filter function as f (X ) = 1 when X &gt; α and 0 otherwise. This is a simple semi-continuous function defined on two intervals. Our framework automatically performs the required run-time checks for each X i . In this case, it will check if X lies entirely (or mostly) in (α, +∞) or (−∞, α]. If the condition is satisfied, it will leverage DA to propagate through the filtering function which in this case leads to an exact result. If X 's support spans the discontinuity, our framework is forced to resort to Monte Carlo simulation which we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">UP Through Black-box Functions</head><p>We use Monte Carlo simulation to approximate Y for functions f that do not meet (or the developers to not know whether they meet) the requirements for DA. Specifically, we evaluate f on n randomly drawn samples of X (input) and use the outputs as an approximation of Y. As n → ∞, the empirical distribution obtained for each Y i converges to the true distribution. To choose n, we use the following expression which bounds the difference between the empirical and the true distribution <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_6">P sup y ∈R (F i,n (y) − F i (y)) &gt; ϵ ≤ 2e −2nϵ 2<label>(4)</label></formula><p>whereF i,n (y) is the empirically derived CDF for Y i and F i (Y ) is the actual CDF for Y i . For example, to approximate the CDF of F i (y) with a 99% probability of achieving an accuracy of ϵ = 0.05, one would need n = 53 samples.</p><p>To generate accurate samples, one must know the joint density of X and pay the heavy computational cost of any rejection-sampling algorithm. Unfortunately, that cost grows exponentially with an increasing size of X and thus we resort to two approximations. The first generates samples from the input marginals X i , when provided or previously estimated, and ignores covariances.</p><formula xml:id="formula_7">X n X n -1 X 2 X 1 ... g μ 3σ g μ 3σ g μ 3σ Y 1 Y 2 Y 3 Y m-1 Y m Z 1 Z 2 f f f f Y m-2 ... μ 3σ Z 3</formula><p>In the absence of full distributional information, the second approximation assumes that each input is normally distributed with the same mean and covariance matrix as the unknown distribution. Surprisingly, although the estimated distributionŶ is only a coarse approximation of the actual but unknown Y , their corresponding mean and variances are similar. To see why, recall that in Eq. 3 we showed that the mean and variance estimation of Y depends solely on the mean and variance of X. Thus, simulating (drawing samples) from any X that matches the required mean and variances, will accurately approximate the corresponding values for Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UP IN DAG DATA PROCESSING</head><p>We now discuss how to apply the UP techniques introduced in the last section to data processing DAGs. <ref type="figure">Figure 3</ref> shows a small example DAG, where uncertainty is introduced in the node labeled s (e.g., via a sampling-based approximation technique). Uncertainties then must be propagated through the two u nodes following s. <ref type="figure" target="#fig_3">Figure 4</ref> shows an example detailed view of the two u nodes designed to highlight the challenges of implementing UP in DAG data processing. This example can correspond to transformations in a Spark program or Map and Reduce phases in a MapReduce program. This figure shows that, in general, we must handle UP through multi-input, multi-output functions for implementation in DAG data processing frameworks. Further, inputs may have nonzero covariances; e.g., Y m−2 and Y m−1 are generated from the same input, and thus are likely to have a non-zero covariance. Finally, the number of inputs and outputs may not be known statically at development time; e.g., a reduce() function in MapReduce has to accept an arbitrary number of values (&gt; 0) for each key.</p><p>It is relatively straightforward to implement UP through blackbox functions using Monte Carlo simulation (henceforth called UP-MC) despite the above complexities. This technique treats any node of the DAG as a black box, dynamically generates samples from the input set (each sample contains a single random value drawn from the distribution of each input data item), and dynamically computes the mean and variance for each output using the empirically derived distributions. Recall that we assume normal input distributions in the absence of this information and we ignore covariances between the inputs when constructing samples (Section 3.3), both of which may lead to inaccuracies.</p><p>The implementation of Differential Analysis (henceforth called UP-DA) is more challenging. Specifically, when a DAG node produces multiple outputs, we view it as being implemented by multiple sub-functions, each producing one of the output. For example, if a function H (X 0 , X 1 , X 2 ) produces two outputs Y 1 and Y 2 , then it is expressible as</p><formula xml:id="formula_8">Y 1 = h 1 (X 0 , X 1 , X 2 ) and Y 2 = h 2 (X 0 , X 1 , X 2 ).</formula><p>In fact, each sub-function may depend only on a subset of the inputs; e.g.,</p><formula xml:id="formula_9">Y 1 = h 1 (X 0 , X 1 ) and Y 2 = h 2 (X 0 , X 2 )</formula><p>. In this case, the UP implementation must be able to identify the inputs used by each sub-function to correctly compute the (co) variances (Equation 3).</p><p>Thus, if a function such as f or д in <ref type="figure" target="#fig_3">Figure 4</ref> produces multiple output values, each output must be produced by an invocation of a sub-function. The output values can be produced by multiple invocations of the same sub-function, or invocation of several different sub-functions. Each invocation must go through an UP interface so that we can track the input-to-output dependencies.</p><p>Input covariances can require additional data flow to be added to the DAG for computing output variances and covariances. For example, consider the scenario where X 1 and X n have a non-zero covariance; even though Y 2 and Y m are generated by different invocations of f , the covariance between X 1 and X n will affect the variance estimates for Y 2 and Y m . The (previously independent) computation of Y 2 and Y m now requires the read-only covariance matrix to be present in all nodes. In general, to propagate covariances properly, each node of the DAG must have the complete covariance matrix of the sibling inputs. This requirement is challenging to implement in practice since it introduces additional data propagation and dependencies among execution DAG nodes, both of which may degrade performance and limit scalability. Our current implementation of UP in MapReduce (Section 5) does not handle all possible covariances, leaving the exploration of the full issue for future work. Meanwhile, our results in Section 7 show that this limitation does not affect accuracy significantly in most applications that we study.</p><p>As shall be seen, having closed-form partial derivatives can significantly reduce the performance overheads of UP-DA compared to numerical differentiation. Thus, an UP-DA implementation should provide an interface for the programmer to provide closed-form partial derivative functions when available. Since the number of inputs may not be known at compile time, the interface must be sufficiently flexible to allow for a parameterized implementation of the partial derivatives. For example, a function that is symmetrical on all inputs (e.g., X 2 i ) has the same partial derivative for all inputs (e.g., 2X i ). In this case, the partial derivative can be implemented using a single function parameterized by X and the index i.</p><p>Finally, <ref type="figure" target="#fig_3">Figure 4</ref> has some interesting performance implications. In the absence of covariances, UP is computed independently at each DAG node, allowing DAGs with UP to be sped up with added computation resources similar to without UP. However, speedup will ultimately be limited by the longest executing node as this "straggler" will determine the minimum execution time of the DAG. For example, д may be an aggregator function that takes inputs from many different invocations of f . If д has to aggregate a large number of inputs, then UP will require the evaluation of many partial derivatives (and possibly many numerical differentiations) for Differential Analysis or multiple evaluations of a function with many inputs for Monte Carlo. Thus, an invocation of д with a comparatively large number of inputs can become a performance bottleneck. Fortunately, we can limit the impact of these stragglers by giving more resources to them. In particular, numerical differentiation and derivative evaluations for different inputs are independent and so can be executed in parallel. Monte Carlo runs are also independent. Parallelizing execution in both cases is quite easy, especially for many-core servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">HADOOP UP-MAPREDUCE</head><p>As a proof of concept, we extend Hadoop MapReduce to include the above UP techniques in multi-stage DAG applications. We first show how our approach can be applied to the MapReduce paradigm. We then describe our implementation called UP-MapReduce.  It is important to note that while the MapReduce model defines that map() only takes one (key, value) pair as input, the value may be a set; e.g., a line of words. Thus, we implement both map() and reduce() as multi-input, multi-output functions. Assuming that only values are uncertain (keys are exact), the discussion in Section 4 applies directly to the implementation of UP-MapReduce. Each map() or reduce() invocation expands to one or multiple UP calls through UP-MapReduce, which automatically estimates the uncertain outputs. UP-MapReduce then streams uncertain outputs from map() → reduce() while in the case of multiple chained programs, temporarily writes these values to HDFS where the next program in the DAG consumes them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">UP-MapReduce Overview</head><p>To support functions with multiple outputs, we introduce the notions of sub-maps and sub-reduces, with each map() (reduce()) containing one or more distinct sub-maps (sub-reduces). Each output must then be produced by the invocation of a sub-map (subreduce) on the correct subset of inputs. We adopt similar approach for implementing semi-continuous map() (reduce()) functions (Section 3.2); user-specified continuous intervals pair with exactly one sub-map (or sub-reducer respectively).</p><p>Due to MapReduce limitations, where map() or reduce() invocations are independent, we currently do not handle the case where input covariances require additional data flow for computing output covariances; e.g., the previously mentioned case of X 1 and X n in <ref type="figure" target="#fig_3">Figure 4</ref> having a non-zero covariance. We do support covariances for the inputs and outputs of a single invocation.  <ref type="figure">Figure 5</ref>: A simple UP-MapReduce program. For readability purposes, we have changed some arrays to single objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation</head><p>We implement UP-MapReduce as an extension of Apache Hadoop 2.7. The extension comprises three Mapper and three Reducer classes that implement UP-MC, UP-DA for continuous functions, and UP-DA for semi-continuous functions, for Map and Reduce, respectively. Developers must choose the correct classes when implementing programs for UP-MapReduce. Our extension also introduces the uncertain type PV (probabilistic value) which implements random variables. A PV variable contains one or more random variables, each described by a mean, a variance-covariance matrix, and possibly an entire empirical distribution. Below, we briefly describe the necessary Reducer classes. UP is implemented similarly for the Mapper classes.</p><p>UPMCReducer. This class implements UP-MC for reduce. It contains a PV object used to store the outputs, two abstract methods eval() and reduce(), and a reduceWithUP() method. The programmer needs to implement the reduce function (e.g., sum) in eval(), which accepts a variable number of double inputs and returns a variable number of double outputs. reduce() accepts a string key and a variable number of inputs in serialized form. reduceWithUP() implements UP-MC, and accepts a variable number of PV inputs. It computes the PV outputs using multiple invocations of eval() using samples derived from the PV inputs.</p><p>A developer would then write her Reducer class by inheriting from this class, implementing eval() and reduce(). reduce() should first parse the input, then call reduceWith UP(), and finally emit the PV object. It is critical that reduce() does not perform any computation on the inputs that affect the output outside of eval(). The developer can specify that eval() should be invoked multiple times, with each invocation processing a particular subset of the inputs. This feature implements the multi-input, multi-output design via sub-maps and sub-reduces.  (X). The developer implements this method to provide a closed-form derivative for UP-DA. The class also implements a reduceWith UP() method that overrides its parent's method with an implementation of UP-DA. This method uses derivative() if it has been implemented, and numerical differentiation otherwise. It also uses input covariances, but expect the input covariance matrix to be loaded into the Hadoop read-only cache externally and prior to the execution of the Reduce phase. Then, it calls eval() as needed for evaluating the reduce function. The developer must implement eval() and reduce() as described above.</p><p>UPDASemiContinuousReducer. This class implements UP-DA for semi-continuous functions and inherits from UPDAContinuousReducer. It allows the developer to specify a list of discontinuities in the reduce function and the range of the support of each input that must be within a continuous portion of the function. It implements a reduceWith UP() method that checks the support of each input against the discontinuities (with the desired accuracy), and chooses to use UP-MC or UP-DA as appropriate. No further implementation is required from the developer.</p><p>Example UP-MapReduce program. <ref type="figure">Figure 5</ref> shows the code for an UP-MapReduce program that computes a weighted average (second DAG node in <ref type="figure">Figure 1</ref>) in the presence of input uncertainties. Changes compared to a precise version are quite minimal.</p><p>Parallelization of MC and numerical differentiation. We have extended the UP Reducer implementations to use multiple threads to speed up the execution of Monte Carlo simulation and numerical differentiation on servers with multi-core and/or hyperthreaded processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPLICATIONS</head><p>We have built a toolbox of common operations (e.g., sum) and modified ten common data processing applications using UP-MapReduce to process uncertain data. We list the applications in <ref type="table" target="#tab_3">Table 1</ref>, along with the kernels comprising each application and shorthand names which we use later in the evaluation section. Below, we briefly discuss each one.</p><p>1) Uncertain toolbox. We apply UP-DA to a variety of continuous operations such as summation, multiplication, logarithms, exponentiation and trigonometric functions with known simple closed-form derivatives. We also include comparison and min/max operations (via UP-DA and UP-MC, respectively). We combine all the above operations to create a toolbox of uncertain elementary operations which can be used as building blocks to construct richer applications. In UP-MapReduce, these uncertain blocks may represent either a logical UP-map or a logical UP-reducer but at runtime, they will expand according to the required dataflow to one or multiple nodes in the execution DAG.</p><p>2) Matrix multiplication (mm). The multiplication of two matrices A (n×m) and B (m×p) can be performed by computing the elements of the output matrix AB (n×p) as AB i j = f (A r ow i , B column j ) = m k =1 A ik B k j (the inner product of A r ow i and B column j ). A MapReduce implementation can use the Map phase to read A and B and emit pairs (k i j , A ik ) and (k i j , B k j ) for 0 &lt; i ≤ n, 0 &lt; j ≤ p, and 0 &lt; k ≤ m. The reduce() function can then sort the A ik 's and B k j 's into a <ref type="figure" target="#fig_1">sequence A i,1 , A i,2 , ..., A i,m , B 1,j , B 2,j , .</ref>.., B m,j , and then compute the inner product.</p><p>Applying UP-DA is then done as follows. The only change needed for map() is the handling of PV rather than precise values. UP is not needed because no computation is being done. The reduce() is rewritten to call eval() after properly arranging the inputs, followed by a call to continuousUP(). eval() computes the inner product. The partial derivatives for inputs from A is ∂f ∂A ik = B k j , and vice versa for inputs from B.</p><p>3) Regression (linreg). Fitting hyperplanes to observations is a frequent task in analytics. In particular, linear regression often relies on the least-squares method, where the sum of the squared differences between the hyperplane and the observed points is minimized. We base our application on linear regression, i.e, we are looking for Y = αX + β. In the presence of noisy observations with known means and variances, we estimate the mean and variance of α and β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Clustering (kmeans).</head><p>Assigning observed data to clusters with k-means is frequent in data exploration. Given a fixed number of clusters and a sequence of observed data points, k-means performs an iterative algorithm, which (may) converge to a solution that minimizes the normed distance between all the points and their corresponding clusters. In the presence of uncertain data points, we extend the precise k-means algorithm with UP to estimate the mean and variance of the estimated cluster coordinates. The algorithm will then operate as a logical DAG with depth equal to the number of iterations required for k-means to converge. The logical DAG will then expand in runtime, to a large execution DAG where UP-MapReduce will propagate the uncertainty at every node. As an example, for d data points, c centroids and n iterations the uncertain execution DAG will comprise of (d 2 × c + 1) × n nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) k-nearest neighbors (kNN).</head><p>A common classification method is performed by estimating the k nearest neighbors around a data point. This computation primarily involves calculating p-norms, which measure distance in multi-dimensional spaces. We extend the traditional notion of norm with UP to estimate the mean and variance, when the input coordinates are uncertain.</p><p>6) Solving systems of linear equations (linsolve). In order to solve large (n×n) systems of linear equations, in the form of Ax = b, one can use the Jacobi method to find the unknown x. Jacobi is an iterative procedure that progressively refines the solution x. We extend Jacobi to support uncertain A and/or b inputs. Then, we compute the mean and variance for each element of x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7)</head><p>Finding eigenpairs (eig). Computing eigenpairs (and especially the dominant eigenvalue and eigenvector) is the central task in solving differential equations and computing eigenfaces. The power iteration iteratively calculates the dominant eigenpair of an input matrix. We create our own version of the power iteration to handle uncertain input data. Specifically, we combine basic uncertain operations (division), mm and Euclidean norms as previously shown to build the necessary iteration. The output is then a random eigenvalue and a random eigenvector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8) Compression (svd)</head><p>. An effective data compression method is the Singular Value Decomposition (SVD). The SVD of an input matrix A is the key kernel in solving problems such as data compression, but also principal component analysis, weather prediction, and signal processing. We can calculate the components of SVD (U , Σ, and V ) by finding the eigenvalues of AA * and A * A. In case A is uncertain, we extend the precise SVD implementation with UP-DA and in particular by using the uncertain toolbox and eig. 9) Data filter (filter). Data filters are common data manipulation tasks in large-scale data processing systems, such as Apache Spark, and built-in procedures in programming languages such as Scala. We implement an uncertain compare-aggregator filter that handles uncertain inputs. During the compare phase, the (uncertain) input data are compared against a user-defined value. The statistics of the intermediate result are forwarded to an aggregator function which estimates the uncertainty of the final result.</p><p>10) Trends in social media (tsocial). A common task in social media analysis is to study potential trends between variables of the social graph. For example, one might want to discover correlations between peoples' age and number of followers in a social media site. Assuming the data is stored in a database, a two-phase workflow (a DAG whose logical nodes execute on different DAG processing systems) will first execute a GROUP BY query with stratified sampling to approximate the average number of friends per age group (with each group representing one day). This stage outputs the mean number of friends and a variance for each age group. The second phase performs uncertain linreg between the uncertain number of friends vs. age using linear regression. It then outputs the mean and variance for the slope and intercept of the fitted line.</p><p>11) Mean US internet latency estimation (latency). Suppose that a content delivery network (CDN) operator wants to improve the average perceived latency of its customers <ref type="bibr" target="#b34">[35]</ref>. He then seeks to maximize the US-wide 10-mile average latency by altering the position of the CDN endpoints. To perform this task, the operator first estimates (via sampling) the mean (10-mile) latency of some candidate locations in the US. Obviously, the operator cannot estimate the desired latency mean on every possible location in US, but instead interpolates the nearby (unobserved) locations. To correctly perform the interpolation though, one should consider that each estimated mean is actually a distribution, as every estimate is being constructed from the appropriate samples.</p><p>We replicate such a scenario and illustrate how UP can be combined in a multi-stage uncertain workflow. The workflow comprises the following stages 1) collect traceroute measurements (within <ref type="figure">Figure 6</ref>: Monte-Carlo simulation is used to construct empirical distributions for the outputs from repeated drawing of random input samples and evaluating the complete application DAG for each sample. the US) from the iPlanes dataset <ref type="bibr" target="#b19">[20]</ref>, 2) estimate the mean for each observed location using the samples, 3) use UP-MapReduce to perform bi-linear interpolations to estimate the mean latencies of unobserved locations and 4) use UP-MapReduce with an uncertain weighted average to simulate the frequency of packet transmission from each location based on known population density to ultimately obtain the mean and variance of the final estimate (population adjusted 10-mile mean latency).</p><formula xml:id="formula_10">... ... v 1 v n ... f g</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EVALUATION</head><p>In this section, we evaluate UP-MapReduce by studying it's accuracy, performance, and scalability. We begin by exploring the two applications, tsocial and latency, that include sampling-based approximations and trade precision for reduced execution times. We show that by developing these applications in UP-MapReduce we can drastically decrease the execution time of both, while propagating the uncertainties introduced by the approximations. We then explore the accuracy of our UP techniques, performance overheads, and scalability via an extensive sensitivity analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Evaluation Methodology</head><p>Input data sets. We leverage real datasets for the two approximate applications under study. Specifically, we evaluate tsocial using the Facebook social structure from SNAP social circles <ref type="bibr" target="#b21">[22]</ref> and latency using traceroute measurements from iPlanes <ref type="bibr" target="#b19">[20]</ref>. For the purpose of the sensitivity analysis (performance, precision and scalability), we generate synthetic input data sets with varying sizes and amounts of uncertainty for each application, similarly to the synthetic data generation in <ref type="bibr" target="#b39">[40]</ref>. For each data set, we first choose a random mean value µ for each input item according to a uniform distribution on a chosen range of values. We then set the variance σ 2 for each input item to achieve a specific relative error defined as 3σ /µ.</p><p>Baseline. We ran a large Monte Carlo experiment that executes a precise version of each application multiple times to accurately compute the empirical distributions for the outputs. Specifically, each experiment consists of n = 10 4 runs of a precise application, where each run is given inputs drawn randomly according to the actual (known) input distributions. Note that this is different than using UP-MC for each node of an application's DAG.</p><p>Here, the entire application is run from beginning to end in each run as shown in <ref type="figure">Figure 6</ref>. For an iterative application, each run executes all iterations for a given input to generate an output sample. This way, all correlations between data items passing through the DAG are correctly preserved. The output samples from the n runs are then used to construct empirical output distributions from which we extract the mean and variance for each output. We consider three different distributions for input uncertainty: normal (Baseline-Normal), skewed with +0.5 skewness (Baseline-Skewed), and uniform (Baseline-Uniform).</p><p>Comparing UP with Baseline. We compare the mean value and relative error for each output computed by UP-MapReduce against the values produced by the corresponding Baseline experiments.</p><p>When an application produces one or a small number of outputs (e.g., linreg), we show the comparison for the output with the largest difference between the two approaches. When an application outputs a vector or matrix (e.g., svd), we show the comparison using the norm of the means ∥µ∥ 2 and the relative error defined as ∥3σ ∥ 2 /∥µ∥ 2 . We expand on a case to show that using the norms do not obfuscate large differences for a subset of estimated outputs. We use the mean produced by Baseline-Normal to compute the relative error for UP in our comparisons (since the mean produced by UP is an estimate). All mean values computed by all methods were very close together, so this choice had little impact. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Approximate Computing and UP</head><p>We now leverage UP-MapReduce to build two multi-stage approximate workflows (tsocial and latency). Both first sample their initial dataset and produce uncertain intermediate values. Then, we leverage UP-MapReduce to process these uncertain values in subsequent stages, ultimately generating the final (uncertain) outputs.</p><p>Our results show that UP is critical for propagating the introduced uncertainties, inform users of the magnitude of the final errors and provide guidelines to control them by adjusting the amount of initial approximation. Specifically, tsocial is a two-stage approximate workflow comprising 1) the execution of an approximate query in BlinkDB [2] on 2 × 10 7 registered individuals, followed by 2) an uncertain linear regression (linreg) in UP-MapReduce. The execution of the approximate query in BlinkDB drastically reduces the execution time of the stage compared to a precise execution, but introduces uncertainties in the form of estimated errors (variance). UP-MapReduce is then used to propagate these uncertainties through the second stage of the computation. The second four-stage workflow (latency) approximates the mean US latency on a grid (2000 locations) by performing latency measurements only on 68 locations. This workflow comprises of 1) latency measurements which generate uncertainty due to sampling 2) generate an uncertain 2-dimensional latency surface on the obtained estimates from these 68 locations 3) perform uncertain bilinear interpolation on the (unobserved) remaining 1932 locations and 4) perform an uncertain weighted average to generate the population-weighted latency average. shows the maximum relative error of the regression coefficients for slope and intercept (left y-axis). We only show UP-DA-numDiff for UP-MapReduce because execution times and errors for all three techniques are similar given the relatively small number of output items from BlinkDB (∼3 × 10 4 ). We observe that significant savings in overall execution time can be achieved despite the overheads of UP. For example, a 5% sampling rate in the first logical DAG node leads to a relative error of just 1.35% and 51% savings in execution time (4s for BlinkDB and 2.9s for UP-MapReduce compared to 14.1s for BlinkDB without sampling plus a negligible amount of time for the precise linear regression computation). Overheads from UP require a sampling rate of 80% before approximation can lead to time savings for the workflow. After that, reduction in execution time increases as the sampling rate decreases since the UP overheads are relatively constant. Reduction in workflow execution time continues to increase until the smallest sampling rate of 0.1% for a maximum of 67.8% savings. However, the relative error increases rapidly to ∼30% after a sampling rate of 1%.</p><p>Similarly, <ref type="figure">Figure 7</ref> (bottom) shows the execution times of postprocessing the obtained traceroute data (excluding the time to perform the traceroutes themselves) and the duration of the subsequent stages (UP-MapReduce bi-linear interpolation and weighed average) for sampling latencies ranging from 1% to 100%. Note that a 100% sampling rate (∼2500 samples per observed location) indicates that we process all the available data; it does not correspond to sampling the entire network (which is not be possible to achieve). The estimated means are still uncertain and they include errors which should be propagated with UP.</p><p>Initially, and for sampling rates of − 100%, we observe a generous reduction in execution time from ∼82 → 19s. For smaller sampling rates, the savings cap at ∼12s. The execution times for UP-MapReduce again stay unaltered as the number of observed (60) and interpolant locations (∼5940) are constant (∼8.1s). The output error of the weighted average, increases quadratically as we decrease sampling rate. For example, sampling just 25% of the data, we can reduce the execution time by 62.5% with an output error of 9.01%. Similarly to tsocial, we only show UP-DA-numDiff, as it was the UP method with the longest running times.</p><p>Interestingly in this case, there is no trade-off between postprocessing and UP-MapReduce execution times (in contrast to tsocial). As we always estimate the means from samples, UP-MapReduce is necessary to propagate the uncertainties. It is then evident that without UP-MapReduce, we would be unaware of the high potential workflow error (which can be as high as 92.8%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Accuracy and Performance</head><p>We now perform a sensitivity analysis to evaluate the accuracy and performance of UP-MapReduce. We include results from all previously described applications except the toolbox, mm and tsocial, as they are included as part of the other applications under study. We start by exploring the accuracy of UP-MapReduce estimation of the means. <ref type="figure" target="#fig_9">Figure 8</ref> plots the relative error (%) of the means (or the corresponding Euclidean norm in case of multivariate outputs) computed by UP-DA using numerical differentiation against the Baseline-Normal. These results are identical for UP-MC. We observe that UP-MapReduce estimates the means with very low bias, especially when the input relative errors are small (&lt; 3%).</p><p>We next study the accuracy of the estimated relative errors. We observe that input uncertainties can be relatively stable, contract, or expand after propagation depending on the application. UP-MapReduce is highly accurate in most cases; i.e., its estimated relative errors are very close to the baseline values for 6 of the applications (linreg, kmeans, latency filter, kNN and linsolve). On the other hand, its estimated relative errors can also deviate noticeably from the baseline values (eig, and svd) when input errors are significant. In these cases, all three UP methods show similar deviations from the baseline although there are small differences between UP-MC and the other two approaches. Deviations for UP-DA-numDiff and UP-DA-closedForm arise from the inaccuracies introduced by Differential Analysis. Deviations for UP-MC arise from the fact that UP-MapReduce performs the Monte Carlo computation independently for each computation node in the DAG, as opposed to executing the entire DAG multiple times as in the  Baseline experiments. As previously mentioned, our current implementation does not account for all covariances and does not consider input covariances when drawing input samples in UP-MC, all of which also contribute to the observed deviations.</p><p>To verify that the computed norms are not obfuscating large differences between the UP-MapReduce estimates and baseline results, we also study the differences for each output in the multioutput applications. For example, <ref type="figure">Figure 10</ref> plots CDFs of relative errors produced by the Baseline-Normal and UP-DA-numDiff when running linsolve for a 50 × 50 linear system. Observe that UP accurately estimates the entire relative error CDF of multivariate outputs for 1% input relative errors <ref type="figure">(Figure 10(a)</ref>), while for larger relative errors of 15% UP precisely estimates a significant portion of errors (79%), with significant deviations for only a very few outputs. We observe similar trends in the remaining multivariate applications (svd, kmeans, linreg, eig and latency).</p><p>Interestingly, UP-DA-closedForm adds very little overhead to the precise version. This is because the derivatives for all functions being evaluated in our applications are simple functions. For example, the partial derivative with respect to x i of the inner product ⟨x, y⟩ = n i=1 x i y i is simply y i , an O(1) computation. Thus, even though the number of evaluations of the derivative functions grows linearly with the number of inputs, each evaluation is extremely cheap and so the computations adds little overhead overall. Out of the eight applications, the maximum overhead (compared to the same application without UP) is 11.4% (kmeans) while the average across them is 6.0%.</p><p>It is important to note that the overhead for UP-DA-closedForm in general depends on the complexity of the derivatives; however, in all applications under consideration it was less expensive to evaluate derivatives of a function than the function itself. Thus, we expect the overheads of UP-DA-closedForm to be routinely lower than the ones for the other two UP techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Scalability</head><p>We finally explore the scalability of UP-MapReduce by running applications 3-11 on a cluster of 512 servers. We also run the original precise applications. We choose the following input sizes: linreg (16•10 6 ), kmeans (10 7 ), kNN (16•10 6 ), linsolve (9•10 6 ), eig (9•10 6 ), svd (9 • 10 6 ), filter (16 • 10 6 ), latency (16 • 10 6 from 150 locations). We illustrate our results (speedups) vs. increasing number of servers from four representative applications in <ref type="figure">Figure 11</ref>. The rest follow similar trends. We draw the following conclusions.</p><p>First, we observe that in all evaluated applications UP-DA-closed Form achieves similar (on average 1.6% difference) scalability as the precise version due to it's low additive overhead. Thus, uncertainty propagation does not deteriorate scalability (which as shown in <ref type="figure">Figure 11</ref> may be poor) of the original application. Second, UP-DA-closedForm and UP-MC-monteCarlo show better scalability in all applications (except kmeans) due to the increased work per task (map and/or reducer) which amortizes the framework overheads. We expect this improvement to hold in applications where UP does not cause heavy execution imbalance (following observation).</p><p>Third, when evaluating kmeans with UP-DA-closedForm or UP-MC-monteCarlo we observe lower scalability (in contrary with the previous point). A study of executions shows that this is a  result of straggler reducers which are caused from data imbalance between different intermediate keys (centroids) and amplified either by the numerical differentiation or the Monte Carlo simulation. The imbalance is not noticeable in the precise and UP-DA-closedForm versions but the other UP methods increase the running times causing reduced scalability. These effects are even more noticeable on UP-DA-numDiff without straggler parallelization which attains a maximum speedup of just 10 when we utilize all our available servers (not shown in <ref type="figure">Figure 11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSIONS</head><p>In this paper, we proposed an approach for propagating data uncertainties through DAG computations. Specifically, we showed how Differential Analysis can be used to propagate uncertainties through DAG nodes implementing continuous (and semi-continuous under certain conditions) and differentiable functions. Our approach falls back to Monte Carlo simulation of nodes otherwise, but uses statistical bounds to minimize overheads while achieving a target error bounds. Our approach also allows the inter-mixing of Differential Analysis and Monte Carlo simulation for different nodes within a DAG, offering flexibility in the operations supported and minimizing performance overheads We have shown how our UP approach can be applied to general DAG frameworks. We have also implemented it in the UP-MapReduce system. Experimentation with ten common data analytic applications revealed that UP-MapReduce is highly accurate in many cases, while its performance overheads are very lowan average of 6% performance degradation -when closed-form derivatives are provided. When numerical differentiation or Monte Carlo simulation must be used, overheads can become much more significant as input size increases. Fortunately, the impact of these overheads on overall execution time can be reduced by allocating additional computing resources. Our scalability results show that UP-MapReduce scales well to a cluster with 512 servers. Finally, using two workflows that couple approximation with UP, we show that significant reductions in execution time can be achieved with approximation, despite the need for UP which propagates estimated uncertainties to the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">ACKNOWLEDGMENTS</head><p>This work was partially supported by NSF grant CCF-1319755.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 Figure 1 :</head><label>31</label><figDesc>(a) A DAG application with two logical nodes; (b) a possible set of outputs (blue line with error bars) from the first node; (c) the output from the second node should be a probabilistic quantity rather than a precise value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Uncertainty propagation through differentiable functions: (a) a single-input, single-output function, and (b) a multiple input-multiple output function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 : 2 Y</head><label>32</label><figDesc>An example DAG where nodes labeled with p are precise computations, s introduce uncertainty (e.g., via a sampling-based approximation technique), and u require uncertain propagation.be necessary to compute the covariances σ i Y j and use them when approximating Z<ref type="bibr" target="#b2">[3]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>An example detailed view of the nodes labeled u inFigure 3, with f being computed in the first logical node and д being computed in the second logical node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>In MapReduce, each program runs in two phases, Map and Reduce. In the Map phase, a user-written side-effect-free map() function is invoked per each input (key, value) pair, and produces a set of intermediate (key, value) pairs, where multiple pairs may have the same key. In the Reduce phase, a user-written side-effect-free reduce() function is called per intermediate key and the set of values associated with that key (produced by all invocations of map() during the Map phase), and produces a set of keys, each with an associated set of values<ref type="bibr" target="#b7">[8]</ref>. MapReduce programs can further be chained together to form complex DAGs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>now maps readily to a MapReduce program (except that the keys are not shown), with the Map phase invoking the map function f and the Reduce phase invoking the reduce function д.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 (Figure 7 :</head><label>77</label><figDesc>top) shows the execution times of tsocial (right yaxis) for sampling rates ranging from 0.1% to 100% (precise). It also Regression error Obtained relative errors and execution times for varying the sampling rate of two approximate workflows (tsocial-top and latency-bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 9(left) plots the relative errors computed by the three variants of UP-MapReduce as a function of the input relative error for 3 representative applications. The figure also plots the values produced by the three Baseline variants. Figure 9(right) plots the execution times of UP-MapReduce as a function of input size (the relative error of the input does not affect execution time). The figure also plots the execution times of precise versions, where there is zero input variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Relative error of means estimated by UP-DA versus Baseline-Normal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Precision (left) and performance (right) of UP compared to Baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Comparison of multivariate error estimation (UP-DA-numDiff vs. Baseline-Normal) when solving a 50 × 50 linear system for input errors of 1% and 15%. Scalability comparison between precise (P) andimplemented UP methods (UP-MapReduce).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>class scale-map extends UPDAContinuousMapper{ HashMap&lt;double, double&gt; weights; double k; //current key double eval(double contacts) return contacts*weights(k); double derivative(double in, int n, double con) return weights(key); //inputs are empty vectors</figDesc><table><row><cell>void map(Text key, PV val) {</cell></row><row><cell>init(key, val); //parse input and init data structures</cell></row><row><cell>this.continuousUP(); //estimate mean and variance</cell></row><row><cell>emit(key, new PV(this.getMean(), this.getStd()));</cell></row><row><cell>}</cell></row><row><cell>}</cell></row><row><cell>class avgReducer extends UPDAContinuousReducer {</cell></row><row><cell>double eval(double[] wContacts)</cell></row><row><cell>return sum(wContacts)/wContacts.length;</cell></row><row><cell>double derivative(double in, int n, double con)</cell></row><row><cell>return 1/n; //inputs are empty vectors</cell></row><row><cell>void reduce(Text key, PV vals) {</cell></row><row><cell>init(key, val); //parse input and init data structures</cell></row><row><cell>this.continuousUP();</cell></row><row><cell>emit(key, new PV(this.getMean(), this.getStd()));</cell></row><row><cell>}</cell></row><row><cell>}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Applications extended to handle uncertain inputs using UP-MapReduce.</figDesc><table><row><cell>index i to compute weights and outputs a double representing ∂f , an array of constants that can be used as ∂ X i ∂f ∂ X i</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">UPDAContinuousReducer. This class implements UP-DA for continuous functions. The class adds an abstract method derivative() that accepts the inputs X in the form of an array of doubles, the</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowing when you&apos;re wrong: building fast and reliable approximate query processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Milner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2014 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
	<note>Barzan Mozafari, and Ion Stoica</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BlinkDB: queries with bounded errors and bounded response times on very large data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barzan</forename><surname>Mozafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Milner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM European Conference on Computer Systems</title>
		<meeting>the 8th ACM European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
	<note>Samuel Madden, and Ion Stoica</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An Introduction To Error Propagation: Derivation, Meaning and Examples of Equation Cy= Fx Cx FxT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arras</surname></persName>
		</author>
		<idno>EPFL-ASL-TR-98-01 R3</idno>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>ETH Zurich</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertain&lt; T&gt;: A first-order type for uncertain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bornholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Mytkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="51" to="66" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probability type inference for flexible approximate programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><surname>Boston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="470" to="487" />
			<date type="published" when="2015" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Proving acceptability properties of relaxed nondeterministic approximate programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deokhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Misailovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin C</forename><surname>Rinard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="169" to="180" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SCOPE: easy and efficient parallel processing of massive data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronnie</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Per-Åke</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Shakib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1265" to="1276" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MapReduce: simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Architecture support for disciplined approximate programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hadi Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="301" to="312" />
			<date type="published" when="2012" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multivariate statistical modelling based on generalized linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Fahrmeir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Tutz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ApproxHadoop: Bringing Approximations to MapReduce Frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Íñigo</forename><surname>Goiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Nagarakatte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="383" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Choice of neighbor order in nearest-neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Byeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="2135" to="2152" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latin hypercube sampling and the propagation of uncertainty in analyses of complex systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freddie Joe</forename><surname>Helton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliability Engineering &amp; System Safety</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="23" to="69" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">JouleGuard: energy guarantees for approximate applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Symposium on Operating Systems Principles. ACM</title>
		<meeting>the 25th Symposium on Operating Systems Principles. ACM</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="198" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data clustering: 50 years beyond K-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="651" to="666" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sampling-based estimators for subset-based queries. The VLDB Journal-The International Journal on Very Large Data Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shantanu</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Jermaine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="181" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tchebycheff systems: With applications in analysis and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Studden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<publisher>Interscience</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eigenvoice modeling with sparse training data. Speech and Audio Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="345" to="354" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A comparative study of uncertainty propagation methods for black-box-type problems. Structural and Multidisciplinary Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Sang Hoon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="239" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">iPlane Nano: Path Prediction for Peer-to-Peer Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Harsha V Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Katz-Bassett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkataramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="137" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Massart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Probability</title>
		<imprint>
			<biblScope unit="page" from="1269" to="1283" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discovering Social Circles in Ego Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556612</idno>
		<ptr target="https://doi.org/10.1145/2556612" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Load value approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>San Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Badr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><forename type="middle">Enright</forename><surname>Jerger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="127" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Chisel: Reliability-and accuracy-aware optimization of approximate computational kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Misailovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Achour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin C</forename><surname>Rinard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="309" to="328" />
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistically accurate program transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Misailovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin C</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rinard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Static Analysis Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="316" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Iterative methods for sparse linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yousef</forename><surname>Saad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Paraprox: Pattern-based approximation for data parallel applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrzad</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davoud</forename><forename type="middle">Anoushe</forename><surname>Jamshidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janghaeng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="35" to="50" />
			<date type="published" when="2014" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Approximate Storage in Solid-State Memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<idno type="DOI">10.1145/2644808</idno>
		<ptr target="https://doi.org/10.1145/2644808" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Expressing and verifying probabilistic assertions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Panchekha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Mytkowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="112" to="122" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Linear regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Seber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">936</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Apache Spark: Lightning-fast cluster computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Spark</surname></persName>
		</author>
		<ptr target="http://spark.apache.org" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.1991.139758</idno>
		<ptr target="https://doi.org/10.1109/CVPR.1991.139758" />
	</analytic>
	<monogr>
		<title level="m">Proceedings. 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Probabilistic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Vajda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cache in the air: exploiting content caching and delivery techniques for 5G systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarik</forename><surname>Taleb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adlen</forename><surname>Ksentini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ECG data compression using truncated singular value decomposition. Information Technology in Biomedicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang-Jan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nai-Kuan</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwo-Jen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="290" to="299" />
			<date type="published" when="2001-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hadoop: The definitive guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Continuous sampling for online aggregation over multiple queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Beng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kian-Lee</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM SIGMOD International Conference on Management of data</title>
		<meeting>the 2010 ACM SIGMOD International Conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="651" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spark: cluster computing with working sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HotCloud</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="10" to="16" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Representative clustering of uncertain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Züfle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Emrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">Arthur</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Mamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Renz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
